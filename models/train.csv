,Unnamed: 0,post_id,title,body,accepted_answer_id,answer_count,comment_count,community_owned_date,creation_date,favorite_count,last_activity_date,last_edit_date,last_editor_display_name,last_editor_user_id,owner_display_name,owner_user_id,parent_id,post_type_id,score,tags,view_count,hot_score,closed_date,pipe_cate,clean_text
819,819,66964492,ImportError: cannot import name 'get_config' from 'tensorflow.python.eager.context',"<p>My notebook was working up till today. At the beginning of my colab notebook I install tf-nightly, but now it is giving me this error</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-1-589c442233c5&gt; in &lt;module&gt;()
      7 import tensorflow as tf
      8 from tensorflow.keras import datasets, layers, models
----&gt; 9 from keras.preprocessing import image
     10 from keras_preprocessing.image import ImageDataGenerator #check underscore or not
     11 from tensorflow.keras.preprocessing import image_dataset_from_directory

2 frames
/usr/local/lib/python3.7/dist-packages/keras/backend.py in &lt;module&gt;()
     35 from tensorflow.python.distribute import distribute_coordinator as dc
     36 from tensorflow.python.distribute import distribute_coordinator_context as dc_context
---&gt; 37 from tensorflow.python.eager.context import get_config
     38 from tensorflow.python.framework import config
     39 from keras import backend_config

ImportError: cannot import name 'get_config' from 'tensorflow.python.eager.context' (/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/context.py)
</code></pre>
<p>This is my code:</p>
<pre><code>!pip install tf-nightly

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from keras.preprocessing import image
from keras_preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
</code></pre>
<p>I have tried installing tensorflow==2.1.0 and this has not been working either. If I comment out that one import, I just get another error on the next line and I have not been able to find much online about this. Any help would be appreciated!</p>",67599606.0,8,2,,2021/4/6 7:34,2.0,2021/8/14 11:47,,,,,5111234.0,,1,12,python|tensorflow|keras,28061,63.9924,,1,importerror import name get config tensorflow python eager context notebook work till today beginning colab notebook install tf nightly give error code try instal tensorflow work either comment one import get another error next line able find much online help would appreciate
181,181,43290373,"What does tensorflow ""op"" do?","<pre><code>self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='op testing')
print(""Extracting the op"",self.center_words.op)
</code></pre>

<p>In above I have created a tf placeholder named ""op testing"". And when I print that <strong>self.center_words.op</strong>it prints out kind of a structure like this </p>

<pre><code>op: ""Placeholder""
attr {
  key: ""dtype""
  value {
    type: DT_INT32
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 128
      }
    }
  }
}
</code></pre>

<p>This works for any tensorflow variable ,function output etc. What is this <strong><em>.op</em></strong>?</p>",47672101.0,3,0,,2017/4/8 5:00,13.0,2021/3/21 23:49,2021/3/21 23:49,,2588210.0,,5915270.0,,1,12,python|tensorflow|neural-network|deep-learning,13328,54.6991,,3,tensorflow op create tf placeholder name op test print self center word opit print kind structure like work tensorflow variable function output etc op
64,64,55546873,How do I flatten a tensor in pytorch?,"<p>Given a tensor of multiple dimensions, how do I flatten it so that it has a single dimension?</p>

<p>Eg:</p>

<pre><code>&gt;&gt;&gt; t = torch.rand([2, 3, 5])
&gt;&gt;&gt; t.shape
torch.Size([2, 3, 5])
</code></pre>

<p>How do I flatten it to have shape:</p>

<pre><code>torch.Size([30])
</code></pre>",55546874.0,4,0,,2019/4/6 7:34,4.0,2020/10/18 21:19,,,,,5353461.0,,1,28,pytorch,63100,107.6,,3,flatten tensor pytorch give tensor multiple dimension flatten single dimension eg flatten shape
248,248,44544766,How do I check if keras is using gpu version of tensorflow?,"<p>When I run a keras script, I get the following output:</p>

<pre><code>Using TensorFlow backend.
2017-06-14 17:40:44.621761: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.1 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621783: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621788: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621791: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:40:44.621795: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use FMA instructions, but these are 
available 
on your machine and could speed up CPU computations.
2017-06-14 17:40:44.721911: I 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful 
NUMA node read from SysFS had negative value (-1), but there must be 
at least one NUMA node, so returning NUMA node zero
2017-06-14 17:40:44.722288: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 
with properties: 
name: GeForce GTX 850M
major: 5 minor: 0 memoryClockRate (GHz) 0.9015
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 3.69GiB
2017-06-14 17:40:44.722302: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-14 17:40:44.722307: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-14 17:40:44.722312: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating 
TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 850M, 
pci bus id: 0000:0a:00.0)
</code></pre>

<p>What does this mean? Am I using GPU or CPU version of tensorflow?</p>

<p>Before installing keras, I was working with the GPU version of tensorflow. </p>

<p>Also <code>sudo pip3 list</code> shows <code>tensorflow-gpu(1.1.0)</code> and nothing like <code>tensorflow-cpu</code>.</p>

<p>Running the command mentioned on [this stackoverflow question], gives the following:</p>

<pre><code>The TensorFlow library wasn't compiled to use SSE4.1 instructions, 
but these are available on your machine and could speed up CPU 
computations.
2017-06-14 17:53:31.424793: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use SSE4.2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424803: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424812: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use AVX2 instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.424820: W 
tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow 
library wasn't compiled to use FMA instructions, but these are 
available on your machine and could speed up CPU computations.
2017-06-14 17:53:31.540959: I 
tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful 
NUMA node read from SysFS had negative value (-1), but there must be 
at least one NUMA node, so returning NUMA node zero
2017-06-14 17:53:31.541359: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 
with properties: 
name: GeForce GTX 850M
major: 5 minor: 0 memoryClockRate (GHz) 0.9015
pciBusID 0000:0a:00.0
Total memory: 3.95GiB
Free memory: 128.12MiB
2017-06-14 17:53:31.541407: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-14 17:53:31.541420: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-14 17:53:31.541441: I 
tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating 
TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 850M, 
pci bus id: 0000:0a:00.0)
2017-06-14 17:53:31.547902: E 
tensorflow/stream_executor/cuda/cuda_driver.cc:893] failed to 
allocate 128.12M (134348800 bytes) from device: 
CUDA_ERROR_OUT_OF_MEMORY
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce 
GTX 850M, pci bus id: 0000:0a:00.0
2017-06-14 17:53:31.549482: I 
tensorflow/core/common_runtime/direct_session.cc:257] Device 
mapping:
/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce 
GTX 850M, pci bus id: 0000:0a:00.0
</code></pre>",44547144.0,3,0,,2017/6/14 12:25,31.0,2020/4/20 5:49,2017/6/15 10:31,,5359882.0,,5847849.0,,1,63,python|tensorflow|neural-network|keras,87692,214.572,,1,check kera use gpu version tensorflow run keras script get following output mean use gpu cpu version tensorflow instal kera work gpu version tensorflow also show nothing like run command mention stackoverflow question give following
379,379,46308374,What is validation data used for in a Keras Sequential model?,"<p>My question is simple, <strong>what is the validation data</strong> passed to model.fit in a Sequential model <strong>used for</strong>?</p>

<p>And, does it affect how the model is trained (normally a validation set is used, for example, to choose hyper-parameters in a model, but I think this does not happen here)?</p>

<p>I am talking about the validation set that can be passed like this:</p>

<pre><code># Create model
model = Sequential()
# Add layers
model.add(...)

# Train model (use 10% of training set as validation set)
history = model.fit(X_train, Y_train, validation_split=0.1)

# Train model (use validation data as validation set)
history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test))
</code></pre>

<p>I investigated a bit, and I saw that <code>keras.models.Sequential.fit</code> calls <code>keras.models.training.fit</code>, which creates variables like <code>val_acc</code>and <code>val_loss</code> (which can be accessed from Callbacks). <code>keras.models.training.fit</code> also calls <code>keras.models.training._fit_loop</code>, which adds the validation data to the <code>callbacks.validation_data</code>, and also calls <code>keras.models.training._test_loop</code>, which will loop the validation data in batches on the <code>self.test_function</code> of the model. The result of this function is used to fill the values of the logs, which are the values accessible from the callbacks.</p>

<p>After seeing all this, I feel that the validation set passed to <code>model.fit</code> is not used to validate anything during training, and its only use is to get feedback on how the trained model will perform in every epoch for a completely independent set. Therefore, it would be okey to use the same validation and test set, right?</p>

<p>Could anyone confirm if the validation set in model.fit has any other goal besides being read from the callbacks?</p>",46308466.0,4,0,,2017/9/19 19:28,21.0,2021/7/30 8:05,,,,,4864422.0,,1,80,python|validation|keras|training-data|keras-2,69053,199.357,,3,validation data use keras sequential model question simple validation data pass model fit sequential model use affect model train normally validation set use example choose hyper parameter model think happen talk validation set pass like investigate bit saw call create variable like access callback also call add validation data also call loop validation data batch model result function use fill value log value accessible callback see feel validation set pass use validate anything training use get feedback trained model perform every epoch completely independent set therefore would okey use validation test set right could anyone confirm validation set model fit goal besides read callback
131,131,42264649,Keras: Binary_crossentropy has negative values,"<p>I'm following <a href=""http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"" rel=""noreferrer"">this tutorial</a> (section 6: Tying it All Together), with my own dataset. I can get the example in the tutorial working, no problem, with the sample dataset provided.</p>

<p>I'm getting a binary cross-entropy error that is negative, and no improvements as epochs progress. I'm pretty sure binary cross-entropy should always be positive, and I should see some improvement in the loss. I've truncated the sample output (and code call) below to 5 epochs. Others seem to run into similar problems sometimes when training CNNs, but I didn't see a clear solution in my case. Does anyone know why this is happening?</p>

<p>Sample output:</p>

<pre><code>Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: GeForce GTX TITAN Black, pci bus id: 0000:84:00.0)
10240/10240 [==============================] - 2s - loss: -5.5378 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 2/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 3/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 4/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
Epoch 5/5
10240/10240 [==============================] - 0s - loss: -7.9712 - acc: 0.5000 - val_loss: -7.9712 - val_acc: 0.5000
</code></pre>

<p>My code:</p>

<pre><code>import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import History

history = History()
seed = 7
np.random.seed(seed)

dataset = np.loadtxt('train_rows.csv', delimiter="","")

#print dataset.shape (10240, 64)

# split into input (X) and output (Y) variables
X = dataset[:, 0:(dataset.shape[1]-2)] #0:62 (63 of 64 columns)
Y = dataset[:, dataset.shape[1]-1]  #column 64 counting from 0

#print X.shape (10240, 62)
#print Y.shape (10240,)

testset = np.loadtxt('test_rows.csv', delimiter="","")

#print testset.shape (2560, 64)

X_test = testset[:,0:(testset.shape[1]-2)]
Y_test = testset[:,testset.shape[1]-1]

#print X_test.shape (2560, 62)
#print Y_test.shape (2560,)

num_units_per_layer = [100, 50]

### create model
model = Sequential()
model.add(Dense(100, input_dim=(dataset.shape[1]-2), init='uniform', activation='relu'))
model.add(Dense(50, init='uniform', activation='relu'))
model.add(Dense(1, init='uniform', activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
## Fit the model
model.fit(X, Y, validation_data=(X_test, Y_test), nb_epoch=5, batch_size=128)
</code></pre>",42275663.0,1,0,,2017/2/16 3:58,5.0,2019/6/6 11:48,,,,,5171213.0,,1,27,python|keras,23950,55.9172,,4,kera binary crossentropy negative value follow tutorial section tie together dataset get example tutorial work problem sample dataset provide get binary cross entropy error negative improvement epochs progress pretty sure binary cross entropy always positive see improvement loss truncate sample output code call epochs others seem run similar problem sometimes train cnns see clear solution case anyone know happen sample output code
812,812,53562417,How to convert a pytorch tensor of ints to a tensor of booleans?,"<p>I would like to cast a tensor of ints to a tensor of booleans.</p>

<p>Specifically I would like to be able to have a function which transforms <code>tensor([0,10,0,16])</code> to <code>tensor([0,1,0,1])</code></p>

<p>This is trivial in Tensorflow by just using <code>tf.cast(x,tf.bool)</code>.</p>

<p>I want the cast to change all ints greater than 0 to a 1 and all ints equal to 0 to a 0. This is the equivalent of <code>!!</code> in most languages.</p>

<p>Since pytorch does not seem to have a dedicated boolean type to cast to, what is the best approach here?</p>

<p>Edit: I am looking for a vectorized solution opposed to looping through each element.</p>",53567692.0,5,4,,2018/11/30 17:45,1.0,2021/3/15 16:38,2020/3/28 23:35,,2956066.0,,7998652.0,,1,11,python|casting|boolean|pytorch|tensor,23758,50.5032,,3,convert pytorch tensor ints tensor booleans would like cast tensor ints tensor booleans specifically would like able function transform trivial tensorflow use want cast change ints great ints equal equivalent language since pytorch seem dedicate boolean type cast best approach edit look vectorized solution oppose loop element
212,212,43895750,Keras input_shape for conv2d and manually loaded images,"<p>I am manually creating my dataset from a number of 384x286 b/w images.</p>

<p>I load an image like this:</p>

<pre><code>x = []
for f in files:
        img = Image.open(f)
        img.load()
        data = np.asarray(img, dtype=""int32"")
        x.append(data)
x = np.array(x)
</code></pre>

<p>this results in x being an array (num_samples, 286, 384)</p>

<pre><code>print(x.shape) =&gt; (100, 286, 384)
</code></pre>

<p>reading the keras documentation, and checking my backend, i should provide to the convolution step an input_shape composed by ( rows, cols, channels )</p>

<p>since i don't arbitrarily know the sample size, i would have expected to pass as an input size, something similar to</p>

<pre><code>( None, 286, 384, 1 )
</code></pre>

<p>the model is built as follows:</p>

<pre><code>model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
# other steps...
</code></pre>

<p>passing as input_shape (286, 384, 1) causes:</p>

<blockquote>
  <p>Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (85, 286, 384)</p>
</blockquote>

<p>passing as_input_shape (None, 286, 384, 1 ) causes:</p>

<blockquote>
  <p>Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5</p>
</blockquote>

<p>what am i doing wrong ? how do i have to reshape the input array?</p>",43897173.0,3,0,,2017/5/10 14:41,8.0,2019/11/27 12:46,2018/3/13 7:26,,5359882.0,,1553662.0,,1,14,python|tensorflow|neural-network|keras|convolution,46196,51.0584,,3,kera input shape conv manually load image manually create dataset number x b w image load image like result x array num sample read kera documentation check backend provide convolution step input shape compose row col channel since arbitrarily know sample size would expect pass input size something similar model build follow pass input shape cause error check input expect conv input dimension get array shape passing input shape none cause input incompatible layer conv expect ndim find ndim wrong reshape input array
544,544,36764791,"In Tensorflow, how to use tf.gather() for the last dimension?","<p>I am trying to gather slices of a tensor in terms of the last dimension for partial connection between layers. Because the output tensor's shape is <code>[batch_size, h, w, depth]</code>, I want to select slices based on the last dimension, such as</p>

<pre><code># L is intermediate tensor
partL = L[:, :, :, [0,2,3,8]]
</code></pre>

<p>However, <code>tf.gather(L, [0, 2,3,8])</code> seems to only work for the first dimension (right?) Can anyone tell me how to do it? </p>",36777781.0,8,0,,2016/4/21 9:04,3.0,2017/11/20 16:58,2017/11/20 16:58,,3924118.0,,3251207.0,,1,17,python|tensorflow|deep-learning,41337,105.665,,3,tensorflow use tf gather last dimension try gather slice tensor term last dimension partial connection layer output tensor shape want select slice base last dimension however seem work first dimension right anyone tell
30,30,54237327,Why is a target network required?,"<p>I have a concern in understanding why a target network is necessary in DQN? I闂佺偨鍎查悰?reading paper on 闂佺偨鍎茬粭涔絤an-level control through deep reinforcement learning闂?/p>

<p>I understand Q-learning. Q-learning is value-based reinforcement learning algorithm that learns 闂佺偨鍎茬粭婕皌imal闂?probability distribution between state-action that will maximize it闂佺偨鍎查悰?long term discounted reward over a sequence of timesteps.</p>

<p>The Q-learning is updated using the bellman equation, and a single step of the q-learning update is given by</p>

<pre><code>Q(S, A) = Q(S, A) + $\alpha$[R_(t+1) + $\gamma$ (Q(s闂?a;闂? - Q(s,a)]
</code></pre>

<p>Where alpha and gamma are learning and discount factors.
I can understand that the reinforcement learning algorithm will become unstable and diverge.</p>

<ul>
<li><p>The experience replay buffer is used so that we do not forget past experiences and to de-correlate datasets provided to learn the probability distribution.</p></li>
<li><p>This is where I fail.</p></li>
<li>Let me break the paragraph from the paper down here for discussion

<ul>
<li>The fact that small updates to $Q$ may significantly change the policy and therefore change the data distribution 闂?understood this part. Changes to Q-network periodically may lead to unstability and changes in distribution. For example, if we always take a left turn or something like this.</li>
<li>and the correlations between the action-values (Q) and the target values <code>r + $gamma$ (argmax(Q(s闂?a闂?)</code> 闂?This says that the reward + gamma * my prediction of the return given that I take what I think is the best action in the current state and follow my policy from then on.</li>
<li>We used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</li>
</ul></li>
</ul>

<p>So, in summary  a target network required because the network keeps changing at each timestep and the 闂佺偨鍎插☉鐚榬get values闂?are being updated at each timestep? </p>

<p>But I do not understand how it is going to solve it?</p>",54238556.0,1,1,,2019/1/17 13:46,9.0,2019/12/13 15:37,,,,,1059860.0,,1,19,deep-learning|artificial-intelligence,10420,61.8715,,0,target network require concern understand target network necessary dqn read paper human level control deep reinforcement learn understand q learning q learning value base reinforcement learn algorithm learn optimal probability distribution state action maximize long term discount reward sequence timesteps q learning update use bellman equation single step q learn update give alpha gamma learn discount factor understand reinforcement learn algorithm become unstable diverge experience replay buffer use forget past experience de correlate datasets provide learn probability distribution fail let break paragraph paper discussion fact small update q may significantly change policy therefore change data distribution understood part change q network periodically may lead unstability change distribution example always take left turn something like correlation action value q target value say reward gamma prediction return give take think best action current state follow policy use iterative update adjust action value q towards target value periodically update thereby reduce correlation target summary target network require network keep change timestep target value update timestep understand go solve
224,224,44172165,How to train the network only on one output when there are multiple outputs?,"<p>I am using a multiple output model in Keras</p>

<pre><code>model1 = Model(input=x, output=[y2, y3])

model1.compile((optimizer='sgd', loss=cutom_loss_function)
</code></pre>

<p>my <code>custom_loss</code> function is</p>

<pre><code>def custom_loss(y_true, y_pred):
   y2_pred = y_pred[0]
   y2_true = y_true[0]

   loss = K.mean(K.square(y2_true - y2_pred), axis=-1)
   return loss
</code></pre>

<p>I only want to train the network on output <code>y2</code>.</p>

<p>What is the shape/structure of the <code>y_pred</code> and <code>y_true</code> argument in loss function when multiple outputs are used?
Can I access them as above? Is it <code>y_pred[0]</code> or <code>y_pred[:,0]</code>?</p>",44451189.0,3,0,,2017/5/25 4:15,11.0,2021/2/10 21:55,2019/12/29 21:06,,3924118.0,,4101250.0,,1,26,model|keras|prediction|loss|multipleoutputs,22262,55.9903,,5,train network one output multiple output use multiple output model kera function want train network output shape structure argument loss function multiple output use access
180,180,43237124,"What is the role of ""Flatten"" in Keras?","<p>I am trying to understand the role of the <code>Flatten</code> function in Keras. Below is my code, which is a simple two-layer network. It takes in 2-dimensional data of shape (3, 2), and outputs 1-dimensional data of shape (1, 4):</p>

<pre><code>model = Sequential()
model.add(Dense(16, input_shape=(3, 2)))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(4))
model.compile(loss='mean_squared_error', optimizer='SGD')

x = np.array([[[1, 2], [3, 4], [5, 6]]])

y = model.predict(x)

print y.shape
</code></pre>

<p>This prints out that <code>y</code> has shape (1, 4). However, if I remove the <code>Flatten</code> line, then it prints out that <code>y</code> has shape (1, 3, 4).</p>

<p>I don't understand this. From my understanding of neural networks, the <code>model.add(Dense(16, input_shape=(3, 2)))</code> function is creating a hidden fully-connected layer, with 16 nodes. Each of these nodes is connected to each of the 3x2 input elements. Therefore, the 16 nodes at the output of this first layer are already ""flat"". So, the output shape of the first layer should be (1, 16). Then, the second layer takes this as an input, and outputs data of shape (1, 4).</p>

<p>So if the output of the first layer is already ""flat"" and of shape (1, 16), why do I need to further flatten it?</p>",43237727.0,8,1,,2017/4/5 16:48,52.0,2021/8/29 13:58,2019/10/15 16:54,,3924118.0,,3320135.0,,1,151,machine-learning|tensorflow|neural-network|deep-learning|keras,127247,538.019,,3,role flatten kera try understand role function kera code simple two layer network take dimensional data shape output dimensional data shape print shape however remove line print shape understand understanding neural network function create hidden fully connect layer node node connect x input element therefore node output first layer already flat output shape first layer second layer take input output data shape output first layer already flat shape need far flatten
96,96,41442276,Keras verbose training progress bar writing a new line on each batch issue,"<p>running a Dense feed-forward neural net in Keras.
there are class_weights for two outputs, and sample_weights for a third output. fore some reason it prints the progress verbose display for each batch calculated, and not updating the print on the same line as its supposed to...
Did this ever happens to you?
How is it fixed?
From the shell:</p>

<pre><code>42336/747322 [====&gt;.........................] - ETA: 79s - loss: 20.7154 - x1_loss: 9.5913 - x2_loss: 10.0536 - x3_loss: 1.0705 - x1_acc: 0.6930 - x2_acc: 0.4433 - x3_acc: 0.6821
143360/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7387 - x1_loss: 9.6131 - x2_loss: 10.0555 - x3_loss: 1.0702 - x1_acc: 0.6930 - x2_acc: 0.4432 - x3_acc: 0.6820
144384/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7362 - x1_loss: 9.6067 - x2_loss: 10.0608 - x3_loss: 1.0687 - x1_acc: 0.6930 - x2_acc: 0.4429 - x3_acc: 0.6817
145408/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7257 - x1_loss: 9.5985 - x2_loss: 10.0571 - x3_loss: 1.0702 - x1_acc: 0.6929 - x2_acc: 0.4428 - x3_acc: 0.6815
146432/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7145 - x1_loss: 9.5849 - x2_loss: 10.0605 - x3_loss: 1.0691 - x1_acc: 0.6932 - x2_acc: 0.4429 - x3_acc: 0.6816
147456/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7208 - x1_loss: 9.5859 - x2_loss: 10.0662 - x3_loss: 1.0688 - x1_acc: 0.6931 - x2_acc: 0.4429 - x3_acc: 0.6815
148480/747322 [====&gt;.........................] - ETA: 78s - loss: 20.7078 - x1_loss: 9.5762 - x2_loss: 10.0636 - x3_loss: 1.0680 - x1_acc: 0.6932 - x2_acc: 0.4430 - x3_acc: 0.6815
149504/747322 [=====&gt;........................] - ETA: 77s - loss: 20.6987 - x1_loss: 9.5749 - x2_loss: 10.0555 - x3_loss: 1.0683 - x1_acc: 0.6931 - x2_acc: 0.4430 - x3_acc: 0.6817
150528/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9883 - x1_loss: 9.5688 - x2_loss: 10.3509 - x3_loss: 1.0686 - x1_acc: 0.6928 - x2_acc: 0.4428 - x3_acc: 0.6819
151552/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9721 - x1_loss: 9.5606 - x2_loss: 10.3435 - x3_loss: 1.0679 - x1_acc: 0.6927 - x2_acc: 0.4426 - x3_acc: 0.6821
152576/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9585 - x1_loss: 9.5558 - x2_loss: 10.3355 - x3_loss: 1.0672 - x1_acc: 0.6926 - x2_acc: 0.4425 - x3_acc: 0.6822
153600/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9409 - x1_loss: 9.5447 - x2_loss: 10.3300 - x3_loss: 1.0662 - x1_acc: 0.6925 - x2_acc: 0.4426 - x3_acc: 0.6822
154624/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9254 - x1_loss: 9.5341 - x2_loss: 10.3250 - x3_loss: 1.0663 - x1_acc: 0.6924 - x2_acc: 0.4425 - x3_acc: 0.6825
155648/747322 [=====&gt;........................] - ETA: 77s - loss: 20.9189 - x1_loss: 9.5270 - x2_loss: 10.3249 - x3_loss: 1.0670 - x1_acc: 0.6925 - x2_acc: 0.4425 - x3_acc: 0.6825
156672/747322 [=====&gt;........................] - ETA: 76s - loss: 20.9069 - x1_loss: 9.5155 - x2_loss: 10.3256 - x3_loss: 1.0658 - x1_acc: 0.6927 - x2_acc: 0.4423 - x3_acc: 0.6827
157696/747322 [=====&gt;........................] - ETA: 76s - loss: 20.9275 - x1_loss: 9.5461 - x2_loss: 10.3163 - x3_loss: 1.0651 - x1_acc: 0.6927 - x2_acc: 0.4422 - x3_acc: 0.6828
158720/747322 [=====&gt;........................] - ETA: 76s - loss: 21.4809 - x1_loss: 10.1018 - x2_loss: 10.3133 - x3_loss: 1.0659 - x1_acc: 0.6928 - x2_acc: 0.4422 - x3_acc: 0.6829
159744/747322 [=====&gt;........................] - ETA: 76s - loss: 21.4617 - x1_loss: 10.0871 - x2_loss: 10.3093 - x3_loss: 1.0653 - x1_acc: 0.6928 - x2_acc: 0.4421 - x3_acc: 0.6830
160768/747322 [=====&gt;........................] - ETA: 76s - loss: 21.5462 - x1_loss: 10.1705 - x2_loss: 10.3105 - x3_loss: 1.0652 - x1_acc: 0.6928 - x2_acc: 0.4420 - x3_acc: 0.6832
161792/747322 [=====&gt;........................] - ETA: 76s - loss: 21.5642 - x1_loss: 10.1849 - x2_loss: 10.3138 - x3_loss: 1.0655 - x1_acc: 0.6928 - x2_acc: 0.4418 - x3_acc: 0.6832
162816/747322 [=====&gt;........................] - ETA: 76s - loss: 21.5508 - x1_loss: 10.1739 - x2_loss: 10.3118 - x3_loss: 1.0651 - x1_acc: 0.6928 - x2_acc: 0.4418 - x3_acc: 0.6833
163840/747322 [=====&gt;........................] - ETA: 76s - loss: 21.5323 - x1_loss: 10.1606 - x2_loss: 10.3057 - x3_loss: 1.0659 - x1_acc: 0.6927 - x2_acc: 0.4419 - x3_acc: 0.6833
164864/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5282 - x1_loss: 10.1607 - x2_loss: 10.3016 - x3_loss: 1.0659 - x1_acc: 0.6926 - x2_acc: 0.4418 - x3_acc: 0.6834
165888/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5321 - x1_loss: 10.1696 - x2_loss: 10.2963 - x3_loss: 1.0662 - x1_acc: 0.6927 - x2_acc: 0.4417 - x3_acc: 0.6834
166912/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5131 - x1_loss: 10.1554 - x2_loss: 10.2912 - x3_loss: 1.0664 - x1_acc: 0.6927 - x2_acc: 0.4416 - x3_acc: 0.6833
167936/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5211 - x1_loss: 10.1649 - x2_loss: 10.2886 - x3_loss: 1.0676 - x1_acc: 0.6929 - x2_acc: 0.4415 - x3_acc: 0.6835
168960/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5049 - x1_loss: 10.1504 - x2_loss: 10.2870 - x3_loss: 1.0676 - x1_acc: 0.6930 - x2_acc: 0.4414 - x3_acc: 0.6835
169984/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5171 - x1_loss: 10.1684 - x2_loss: 10.2818 - x3_loss: 1.0670 - x1_acc: 0.6931 - x2_acc: 0.4414 - x3_acc: 0.6832
171008/747322 [=====&gt;........................] - ETA: 75s - loss: 21.5036 - x1_loss: 10.1541 - x2_loss: 10.2816 - x3_loss: 1.0678 - x1_acc: 0.6931 - x2_acc: 0.4413 - x3_acc: 0.6828
172032/747322 [=====&gt;........................] - ETA: 75s - loss: 21.4870 - x1_loss: 10.1377 - x2_loss: 10.2816 - x3_loss: 1.0677 - x1_acc: 0.6931 - x2_acc: 0.4413 - x3_acc: 0.6827
173056/747322 [=====&gt;........................] - ETA: 75s - loss: 21.4729 - x1_loss: 10.1210 - x2_loss: 10.2836 - x3_loss: 1.0683 - x1_acc: 0.6931 - x2_acc: 0.4413 - x3_acc: 0.6824
174080/747322 [=====&gt;........................] - ETA: 74s - loss: 21.4512 - x1_loss: 10.1085 - x2_loss: 10.2742 - x3_loss: 1.0685 - x1_acc: 0.6931 - x2_acc: 0.4414 - x3_acc: 0.6821
175104/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4315 - x1_loss: 10.0977 - x2_loss: 10.2647 - x3_loss: 1.0690 - x1_acc: 0.6931 - x2_acc: 0.4414 - x3_acc: 0.6817
176128/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4231 - x1_loss: 10.0880 - x2_loss: 10.2656 - x3_loss: 1.0695 - x1_acc: 0.6932 - x2_acc: 0.4412 - x3_acc: 0.6813
177152/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4059 - x1_loss: 10.0732 - x2_loss: 10.2639 - x3_loss: 1.0688 - x1_acc: 0.6931 - x2_acc: 0.4412 - x3_acc: 0.6809
178176/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4289 - x1_loss: 10.0967 - x2_loss: 10.2634 - x3_loss: 1.0688 - x1_acc: 0.6930 - x2_acc: 0.4413 - x3_acc: 0.6807
179200/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4329 - x1_loss: 10.1092 - x2_loss: 10.2557 - x3_loss: 1.0681 - x1_acc: 0.6930 - x2_acc: 0.4414 - x3_acc: 0.6807
180224/747322 [======&gt;.......................] - ETA: 74s - loss: 21.4277 - x1_loss: 10.1099 - x2_loss: 10.2503 - x3_loss: 1.0675 - x1_acc: 0.6930 - x2_acc: 0.4415 - x3_acc: 0.6807
181248/747322 [======&gt;.......................] - ETA: 73s - loss: 21.4088 - x1_loss: 10.0975 - x2_loss: 10.2441 - x3_loss: 1.0671 - x1_acc: 0.6929 - x2_acc: 0.4416 - x3_acc: 0.6808
182272/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3909 - x1_loss: 10.0841 - x2_loss: 10.2405 - x3_loss: 1.0663 - x1_acc: 0.6929 - x2_acc: 0.4415 - x3_acc: 0.6811
183296/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3775 - x1_loss: 10.0699 - x2_loss: 10.2416 - x3_loss: 1.0660 - x1_acc: 0.6927 - x2_acc: 0.4415 - x3_acc: 0.6813
184320/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3682 - x1_loss: 10.0664 - x2_loss: 10.2355 - x3_loss: 1.0662 - x1_acc: 0.6928 - x2_acc: 0.4417 - x3_acc: 0.6818
185344/747322 [======&gt;.......................] - ETA: 73s - loss: 21.4162 - x1_loss: 10.1213 - x2_loss: 10.2291 - x3_loss: 1.0658 - x1_acc: 0.6927 - x2_acc: 0.4417 - x3_acc: 0.6821
186368/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3981 - x1_loss: 10.1050 - x2_loss: 10.2259 - x3_loss: 1.0672 - x1_acc: 0.6928 - x2_acc: 0.4418 - x3_acc: 0.6825
187392/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3793 - x1_loss: 10.0909 - x2_loss: 10.2212 - x3_loss: 1.0673 - x1_acc: 0.6928 - x2_acc: 0.4417 - x3_acc: 0.6827
188416/747322 [======&gt;.......................] - ETA: 73s - loss: 21.3614 - x1_loss: 10.0784 - x2_loss: 10.2163 - x3_loss: 1.0668 - x1_acc: 0.6930 - x2_acc: 0.4418 - x3_acc: 0.6830
189440/747322 [======&gt;.......................] - ETA: 72s - loss: 21.3736 - x1_loss: 10.0909 - x2_loss: 10.2169 - x3_loss: 1.0659 - x1_acc: 0.6930 - x2_acc: 0.4417 - x3_acc: 0.6833
190464/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4615 - x1_loss: 10.0802 - x2_loss: 10.3165 - x3_loss: 1.0648 - x1_acc: 0.6930 - x2_acc: 0.4418 - x3_acc: 0.6836
191488/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4493 - x1_loss: 10.0653 - x2_loss: 10.3194 - x3_loss: 1.0646 - x1_acc: 0.6930 - x2_acc: 0.4417 - x3_acc: 0.6837
192512/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4863 - x1_loss: 10.0997 - x2_loss: 10.3207 - x3_loss: 1.0659 - x1_acc: 0.6927 - x2_acc: 0.4416 - x3_acc: 0.6837
193536/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4750 - x1_loss: 10.0895 - x2_loss: 10.3198 - x3_loss: 1.0657 - x1_acc: 0.6929 - x2_acc: 0.4416 - x3_acc: 0.6839
194560/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4577 - x1_loss: 10.0755 - x2_loss: 10.3168 - x3_loss: 1.0654 - x1_acc: 0.6929 - x2_acc: 0.4416 - x3_acc: 0.6839
195584/747322 [======&gt;.......................] - ETA: 72s - loss: 21.4429 - x1_loss: 10.0627 - x2_loss: 10.3148 - x3_loss: 1.0655 - x1_acc: 0.6929 - x2_acc: 0.4417 - x3_acc: 0.6838
196608/747322 [======&gt;.......................] - ETA: 71s - loss: 21.4307 - x1_loss: 10.0558 - x2_loss: 10.3089 - x3_loss: 1.0660 - x1_acc: 0.6929 - x2_acc: 0.4418 - x3_acc: 0.6834
197632/747322 [======&gt;.......................] - ETA: 71s - loss: 21.4446 - x1_loss: 10.0669 - x2_loss: 10.3107 - x3_loss: 1.0670 - x1_acc: 0.6929 - x2_acc: 0.4418 - x3_acc: 0.6830
198656/747322 [======&gt;.......................] - ETA: 71s - loss: 21.4287 - x1_loss: 10.0552 - x2_loss: 10.3071 - x3_loss: 1.0665 - x1_acc: 0.6930 - x2_acc: 0.4418 - x3_acc: 0.6827
199680/747322 [=======&gt;......................] - ETA: 71s - loss: 21.4168 - x1_loss: 10.0474 - x2_loss: 10.3034 - x3_loss: 1.0660 - x1_acc: 0.6931 - x2_acc: 0.4417 - x3_acc: 0.6823
200704/747322 [=======&gt;......................] - ETA: 71s - loss: 21.4064 - x1_loss: 10.0385 - x2_loss: 10.3015 - x3_loss: 1.0664 - x1_acc: 0.6931 - x2_acc: 0.4417 - x3_acc: 0.6819
201728/747322 [=======&gt;......................] - ETA: 71s - loss: 21.3954 - x1_loss: 10.0320 - x2_loss: 10.2974 - x3_loss: 1.0659 - x1_acc: 0.6931 - x2_acc: 0.4416 - x3_acc: 0.6817
202752/747322 [=======&gt;......................] - ETA: 71s - loss: 21.3870 - x1_loss: 10.0243 - x2_loss: 10.2965 - x3_loss: 1.0662 - x1_acc: 0.6931 - x2_acc: 0.4415 - x3_acc: 0.6816
203776/747322 [=======&gt;......................] - ETA: 70s - loss: 21.3782 - x1_loss: 10.0155 - x2_loss: 10.2954 - x3_loss: 1.0673 - x1_acc: 0.6929 - 

etc...
</code></pre>",,8,5,,2017/1/3 11:17,5.0,2021/5/22 19:55,2017/1/4 9:52,,7295598.0,,7295598.0,,1,27,tensorflow|progress-bar|theano|keras|keras-layer,11646,89.4647,,5,kera verbose train progress bar write new line batch issue run dense feed forward neural net kera class weight two output sample weight third output fore reason print progress verbose display batch calculate update print line suppose ever happen fixed shell
51,51,55233377,Keras Sequential model with multiple inputs,"<p>I am making a MLP model which takes two inputs and produces a single output.</p>

<p>I have two input arrays (one for each input) and 1 output array. The neural network has 1 hidden layer with 2 neurons. Each array has 336 elements.</p>

<pre><code>model0 = keras.Sequential([
keras.layers.Dense(2, input_dim=2, activation=keras.activations.sigmoid, use_bias=True),
keras.layers.Dense(1, activation=keras.activations.relu, use_bias=True),
])

# Compile the neural network #
model0.compile(
    optimizer = keras.optimizers.RMSprop(lr=0.02,rho=0.9,epsilon=None,decay=0),
    loss = 'mean_squared_error',
    metrics=['accuracy']
)
</code></pre>

<p>I tried two ways, both of them are giving errors.</p>

<pre><code>model0.fit(numpy.array([array_1, array_2]),output, batch_size=16, epochs=100)
</code></pre>

<blockquote>
  <p>ValueError: Error when checking input: expected dense_input to have shape (2,) but got array with shape (336,)</p>
</blockquote>

<p>The second way:</p>

<pre><code>model0.fit([array_1, array_2],output, batch_size=16, epochs=100)
</code></pre>

<blockquote>
  <p>ValueError: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 2 arrays:</p>
</blockquote>

<p><a href=""https://stackoverflow.com/questions/53017177/multiple-inputs-to-keras-sequential-model"">Similar question</a>. But not using sequential model.</p>",55234203.0,3,0,,2019/3/19 3:50,5.0,2021/3/23 16:40,,,,,7303913.0,,1,17,python|arrays|tensorflow|keras,32723,61.2594,,3,kera sequential model multiple input make mlp model take two input produce single output two input array one input output array neural network hide layer neuron array element try two way give error valueerror error check input expect dense input shape get array shape second way valueerror error check model input list numpy array pass model size model expect expect see array instead get following list array similar question use sequential model
656,656,37280910,How to design deep convolutional neural networks?,"<p>As I understand it, all CNNs are quite similar. They all have a convolutional layers followed by pooling and relu layers. Some have specialised layers like FlowNet and Segnet. My doubt is how should we decide how many layers to use and how do we set the kernel size for each layer in the network. I have searched for an answer to this question but I couldn't find a concrete answer. Is the network designed using trial and error or are some specific rules that I am not aware of? If you could please clarify this, I would be very grateful to you.</p>",37283058.0,1,0,,2016/5/17 15:47,16.0,2021/4/11 17:50,,,,,5907474.0,,1,22,neural-network|deep-learning|caffe|convolution|conv-neural-network,12454,57.7812,2021/4/11 9:52,0,design deep convolutional neural network understand cnns quite similar convolutional layer follow pool relu layer specialise layer like flownet segnet doubt decide many layer use set kernel size layer network search answer question could find concrete answer network design use trial error specific rule aware could please clarify would grateful
778,778,51956000,What does Keras Tokenizer method exactly do?,"<p>On occasion, circumstances require us to do the following:</p>

<pre><code>from keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=my_max)
</code></pre>

<p>Then, invariably, we chant this mantra:</p>

<pre><code>tokenizer.fit_on_texts(text) 
sequences = tokenizer.texts_to_sequences(text)
</code></pre>

<p>While I (more or less) understand what the total effect is, I can't figure out what each one does separately, regardless of how much research I do (including, obviously, the documentation). I don't think I've ever seen one without the other. </p>

<p>So what does each do? Are there any circumstances where you would use either one without the other? If not, why aren't they simply combined into something like:</p>

<pre><code>sequences = tokenizer.fit_on_texts_to_sequences(text)
</code></pre>

<p>Apologies if I'm missing something obvious, but I'm pretty new at this.</p>",51956230.0,3,1,,2018/8/21 20:08,27.0,2019/6/22 11:09,2018/12/25 11:38,,9448090.0,,9448090.0,,1,88,python|keras|nlp,66740,277.098,,3,keras tokenizer method exactly occasion circumstance require u follow invariably chant mantra less understand total effect figure one separately regardless much research include obviously documentation think ever see one without circumstance would use either one without simply combine something like apology miss something obvious pretty new
55,55,55296013,why set return_sequences=True and stateful=True for tf.keras.layers.LSTM?,"<p>I am learning tensorflow2.0 and follow the <a href=""https://www.tensorflow.org/alpha/tutorials/sequences/text_generation"" rel=""noreferrer"">tutorial</a>. In the <code>rnn</code> example, I found the code:</p>

<pre><code>def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, 
                              batch_input_shape=[batch_size, None]),
    tf.keras.layers.LSTM(rnn_units, 
                        return_sequences=True, 
                        stateful=True, 
                        recurrent_initializer='glorot_uniform'),
    tf.keras.layers.Dense(vocab_size)
  ])
  return model
</code></pre>

<p>My question is: why the code set the argument <code>return_sequences=True</code> and <code>stateful=True</code>? How about using the default argument?</p>",,3,0,,2019/3/22 8:56,7.0,2020/1/3 13:57,,,,,988709.0,,1,8,tensorflow|keras|lstm|recurrent-neural-network,4138,52.2672,,3,set return sequence true stateful true tf kera layer lstm learn tensorflow follow tutorial example find code question code set argument use default argument
243,243,44477489,Keras - Difference between categorical_accuracy and sparse_categorical_accuracy,"<p>What is the difference between <code>categorical_accuracy</code> and <code>sparse_categorical_accuracy</code> in Keras? There is no hint in the <a href=""https://keras.io/metrics/"" rel=""noreferrer"">documentation for these metrics</a>, and by asking Dr. Google, I did not find answers for that either.</p>

<p>The source code can be found <a href=""https://github.com/fchollet/keras/blob/master/keras/metrics.py"" rel=""noreferrer"">here</a>:</p>

<pre><code>def categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.argmax(y_true, axis=-1),
                          K.argmax(y_pred, axis=-1)),
                  K.floatx())


def sparse_categorical_accuracy(y_true, y_pred):
    return K.cast(K.equal(K.max(y_true, axis=-1),
                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),
                  K.floatx())
</code></pre>",44484106.0,4,2,,2017/6/10 19:55,16.0,2021/9/2 14:44,2021/4/24 1:56,,10908375.0,,1204330.0,,1,59,python|tensorflow|machine-learning|keras|deep-learning,45227,207.822,,4,kera difference categorical accuracy sparse categorical accuracy difference kera hint documentation metric ask dr google find answer either source code find
34,34,54366935,Make a deep copy of a keras model in python,"<p>I would like to make a deep copy of a keras model (called <code>model1</code>) of mine in order to be able to use it in a for a loop and then re-initialize for each for-loop iteration and perform <code>fit</code> with one additional sample to the model. I would  like to be able to initialize the model after each iteration since after performing the <code>fit</code> (my model is modified however, I want it keep it as it is when i am loading from the path using the load_weights).  </p>

<p>My code looks like:</p>

<pre><code>model1= create_Model()
model1.compile(optimizer='rmsprop', loss='categorical_crossentropy')
model1.load_weights('my_weights')

model_copy= create_Model()
model_copy.compile(optimizer='rmsprop', loss='categorical_crossentropy')

model_copy= keras.models.clone_model(model1)
for j in range(0, image_size):
      model_copy.fit(sample[j], sample_lbl[j])
      prediction= model_copy.predict(sample[j])
</code></pre>

<p>Also, it is not really efficient for me to load the model each time in the for-loop since that is time-consuming. How can I do properly the deep copy in my case? The code I posted give the following error that concerns the function .fit and my reference model model_copy:</p>

<blockquote>
  <p>RuntimeError: You must compile a model before training/testing. Use <code>model.compile(optimizer, loss)</code>.</p>
</blockquote>",54368176.0,2,0,,2019/1/25 14:09,5.0,2020/7/24 15:03,2019/1/25 14:41,,1194864.0,,1194864.0,,1,30,python|keras,17789,66.0006,,3,make deep copy keras model python would like make deep copy keras model call mine order able use loop initialize loop iteration perform one additional sample model would like able initialize model iteration since perform model modify however want keep load path use load weight code look like also really efficient load model time loop since time consume properly deep copy case code post give following error concern function fit reference model model copy runtimeerror must compile model train testing use
209,209,43855162,RMSE/ RMSLE loss function in Keras,"<p>I try to participate in my first Kaggle competition where <code>RMSLE</code> is given as the required loss function. For I have found nothing how to implement this <code>loss function</code> I tried to settle for <code>RMSE</code>. I know this was part of <code>Keras</code> in the past, is there any way to use it in the latest version, maybe with a customized function via <code>backend</code>?</p>

<p>This is the NN I designed:</p>

<pre><code>from keras.models import Sequential
from keras.layers.core import Dense , Dropout
from keras import regularizers

model = Sequential()
model.add(Dense(units = 128, kernel_initializer = ""uniform"", activation = ""relu"", input_dim = 28,activity_regularizer = regularizers.l2(0.01)))
model.add(Dropout(rate = 0.2))
model.add(Dense(units = 128, kernel_initializer = ""uniform"", activation = ""relu""))
model.add(Dropout(rate = 0.2))
model.add(Dense(units = 1, kernel_initializer = ""uniform"", activation = ""relu""))
model.compile(optimizer = ""rmsprop"", loss = ""root_mean_squared_error"")#, metrics =[""accuracy""])

model.fit(train_set, label_log, batch_size = 32, epochs = 50, validation_split = 0.15)
</code></pre>

<p>I tried a customized <code>root_mean_squared_error</code> function I found on GitHub but for all I know the syntax is not what is required. I think the <code>y_true</code> and the <code>y_pred</code> would have to be defined before passed to the return but I have no idea how exactly, I just started with programming in python and I am really not that good in math...</p>

<pre><code>from keras import backend as K

def root_mean_squared_error(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) 
</code></pre>

<p>I receive the following error with this function: </p>

<pre><code>ValueError: ('Unknown loss function', ':root_mean_squared_error')
</code></pre>

<p>Thanks for your ideas, I appreciate every help!</p>",43863854.0,6,1,,2017/5/8 18:49,12.0,2021/7/2 14:06,,,,,6589039.0,,1,41,python|keras|custom-function|loss-function,52097,183.067,,4,rmse rmsle loss function kera try participate first kaggle competition give required loss function find nothing implement try settle know part past way use late version maybe customized function via nn design try customized function find github know syntax require think would define pass return idea exactly start programming python really good math receive following error function thanks idea appreciate every help
565,565,48915810,PyTorch - What does contiguous() do?,"<p>I was going through this example of a LSTM language model on github <a href=""https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/main.py"" rel=""noreferrer"">(link)</a>.
What it does in general is pretty clear to me. But I'm still struggling to understand what calling <code>contiguous()</code> does, which occurs several times in the code.</p>
<p>For example in line 74/75 of the code input and target sequences of the LSTM are created.
Data (stored in <code>ids</code>) is 2-dimensional where first dimension is the batch size.</p>

<pre class=""lang-python prettyprint-override""><code>for i in range(0, ids.size(1) - seq_length, seq_length):
    # Get batch inputs and targets
    inputs = Variable(ids[:, i:i+seq_length])
    targets = Variable(ids[:, (i+1):(i+1)+seq_length].contiguous())
</code></pre>
<p>So as a simple example, when using batch size 1 and <code>seq_length</code> 10 <code>inputs</code> and <code>targets</code> looks like this:</p>
<pre class=""lang-python prettyprint-override""><code>inputs Variable containing:
0     1     2     3     4     5     6     7     8     9
[torch.LongTensor of size 1x10]

targets Variable containing:
1     2     3     4     5     6     7     8     9    10
[torch.LongTensor of size 1x10]
</code></pre>
<p>So in general my question is, what does <code>contiguous()</code> do and why do I need it?</p>
<p>Further I don't understand why the method is called for the target sequence and but not the input sequence as both variables are comprised of the same data.</p>
<p>How could <code>targets</code> be non-contiguous and <code>inputs</code> still be contiguous?</p>
<hr />
<p><strong>EDIT:</strong></p>
<p><em>I tried to leave out calling <code>contiguous()</code>, but this leads to an  error message when computing the loss.</em></p>
<pre class=""lang-python prettyprint-override""><code>RuntimeError: invalid argument 1: input is not contiguous at .../src/torch/lib/TH/generic/THTensor.c:231
</code></pre>
<p><em>So obviously calling <code>contiguous()</code> in this example is necessary.</em></p>",52229694.0,7,2,,2018/2/21 21:30,49.0,2021/7/22 21:08,2021/4/9 11:58,,9067615.0,,7483494.0,,1,129,python|memory|pytorch|contiguous,82433,522.264,,3,pytorch contiguous go example lstm language model github link general pretty clear still struggle understand call occur several time code example line code input target sequence lstm create data store dimensional first dimension batch size simple example use batch size look like general question need far understand method call target sequence input sequence variable comprise data could non contiguous still contiguous edit try leave call lead error message compute loss obviously call example necessary
680,680,38241410,TensorFlow: Remember LSTM state for next batch (stateful LSTM),"<p>Given a trained LSTM model I want to perform inference for single timesteps, i.e. <code>seq_length = 1</code> in the example below. After each timestep the internal LSTM (memory and hidden) states need to be remembered for the next 'batch'. For the very beginning of the inference the internal LSTM states <code>init_c, init_h</code> are computed given the input. These are then stored in a <code>LSTMStateTuple</code> object which is passed to the LSTM. During training this state is updated every timestep. However for inference I want the <code>state</code> to be saved in between batches, i.e. the initial states only need to be computed at the very beginning and after that the LSTM states should be saved after each 'batch' (n=1). </p>

<p>I found this related StackOverflow question: <a href=""https://stackoverflow.com/questions/37969065/tensorflow-best-way-to-save-state-in-rnns"">Tensorflow, best way to save state in RNNs?</a>. However this only works if <code>state_is_tuple=False</code>, but this behavior is soon to be deprecated by TensorFlow (see <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L265"" rel=""noreferrer"">rnn_cell.py</a>). Keras seems to have a nice wrapper to make <strong>stateful</strong> LSTMs possible but I don't know the best way to achieve this in TensorFlow. This issue on the TensorFlow GitHub is also related to my question: <a href=""https://github.com/tensorflow/tensorflow/issues/2838"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/2838</a></p>

<p>Anyone good suggestions for building a stateful LSTM model? </p>

<pre><code>inputs  = tf.placeholder(tf.float32, shape=[None, seq_length, 84, 84], name=""inputs"")
targets = tf.placeholder(tf.float32, shape=[None, seq_length], name=""targets"")

num_lstm_layers = 2

with tf.variable_scope(""LSTM"") as scope:

    lstm_cell  = tf.nn.rnn_cell.LSTMCell(512, initializer=initializer, state_is_tuple=True)
    self.lstm  = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_lstm_layers, state_is_tuple=True)

    init_c = # compute initial LSTM memory state using contents in placeholder 'inputs'
    init_h = # compute initial LSTM hidden state using contents in placeholder 'inputs'
    self.state = [tf.nn.rnn_cell.LSTMStateTuple(init_c, init_h)] * num_lstm_layers

    outputs = []

    for step in range(seq_length):

        if step != 0:
            scope.reuse_variables()

        # CNN features, as input for LSTM
        x_t = # ... 

        # LSTM step through time
        output, self.state = self.lstm(x_t, self.state)
        outputs.append(output)
</code></pre>",39954167.0,2,1,,2016/7/7 9:02,20.0,2016/12/22 18:42,2017/5/23 12:10,,-1.0,,3419427.0,,1,29,python|tensorflow|lstm|recurrent-neural-network|stateful,16008,57.4173,,3,tensorflow remember lstm state next batch stateful lstm give trained lstm model want perform inference single timesteps e example timestep internal lstm memory hidden state need remember next batch beginning inference internal lstm state compute give input store object pass lstm train state update every timestep however inference want save batch e initial state need compute beginning lstm state save batch n find related stackoverflow question tensorflow best way save state rnns however work behavior soon deprecate tensorflow see rnn cell py kera seem nice wrapper make stateful lstms possible know best way achieve tensorflow issue tensorflow github also relate question anyone good suggestion build stateful lstm model
767,767,51731207,Python: Neural Network - TypeError: 'History' object is not subscriptable,"<p>I have been practicing building and comparing neural networks using Keras and Tensorflow in python, but when I look to plot the models for comparisons I am receiving an error:</p>

<pre><code>TypeError: 'History' object is not subscriptable
</code></pre>

<p>Here is my code for the three models:</p>

<pre><code>############################## Initiate model 1 ###############################
# Model 1 has no hidden layers
from keras.models import Sequential
model1 = Sequential()

# Get layers
from keras.layers import Dense
# Add first layer
n_cols = len(X.columns)
model1.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add output layer
model1.add(Dense(units=2, activation='softmax'))

# Compile the model
model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Define early_stopping_monitor
from keras.callbacks import EarlyStopping
early_stopping_monitor = EarlyStopping(patience=2)

# Fit model
model1.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


############################## Initiate model 2 ###############################
# Model 2 has 1 hidden layer that has the mean number of nodes of input and output layer
model2 = Sequential()

# Add first layer
model2.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add hidden layer
import math
model2.add(Dense(units=math.ceil((n_cols+2)/2), activation='relu'))
# Add output layer
model2.add(Dense(units=2, activation='softmax'))

# Compile the model
model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Fit model
model2.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)

############################## Initiate model 3 ###############################
# Model 3 has 1 hidden layer that is 2/3 the size of the input layer plus the size of the output layer
model3 = Sequential()

# Add first layer
model3.add(Dense(units=n_cols, activation='relu', input_shape=(n_cols,)))
# Add hidden layer
model3.add(Dense(units=math.ceil((n_cols*(2/3))+2), activation='relu'))
# Add output layer
model3.add(Dense(units=2, activation='softmax'))

# Compile the model
model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics= 
['accuracy']) 

# Fit model
model3.fit(X, y, validation_split=0.33, epochs=30, callbacks= 
[early_stopping_monitor], verbose=False)


# Plot the models
plt.plot(model1.history['val_loss'], 'r', model2.history['val_loss'], 'b', 
model3.history['val_loss'], 'g')
plt.xlabel('Epochs')
plt.ylabel('Validation score')
plt.show()
</code></pre>

<p>I have no problems with running any of my models, getting predicted probabilities, plotting ROC curves, or plotting PR curves. However, when I attempt to plot the three curves together I am getting an error from this area of my code:</p>

<pre><code>model1.history['val_loss']

TypeError: 'History' object is not subscriptable
</code></pre>

<p>Does anyone have experience with this type of error and can lead me to what I am doing wrong?</p>

<p>Thank you in advance.</p>",51731446.0,3,2,,2018/8/7 16:10,1.0,2020/5/8 20:22,,,,,6875778.0,,1,23,python|tensorflow|neural-network|keras,20077,60.0108,,4,python neural network typeerror history object subscriptable practice building compare neural network use kera tensorflow python look plot model comparison receive error code three model problem run model get predicted probability plot roc curve plot pr curve however attempt plot three curve together get error area code anyone experience type error lead wrong thank advance
716,716,39921607,How to make a custom activation function with only Python in Tensorflow?,"<p>Suppose you need to make an activation function which is not possible using only pre-defined tensorflow building-blocks, what can you do?</p>

<p>So in Tensorflow it is possible to make your own activation function. But it is quite complicated, you have to write it in C++ and recompile the whole of tensorflow <a href=""https://www.quora.com/Is-it-possible-to-add-new-activation-functions-to-TensorFlow-Theano-Torch-How"" rel=""noreferrer"">[1]</a> <a href=""https://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html"" rel=""noreferrer"">[2]</a>.</p>

<p>Is there a simpler way?</p>",39921608.0,2,2,,2016/10/7 16:08,52.0,2018/6/12 6:53,2018/1/11 9:30,,3990607.0,,3990607.0,,1,57,python|tensorflow|neural-network|deep-learning|activation-function,26411,140.487,,3,make custom activation function python tensorflow suppose need make activation function possible use pre define tensorflow building block tensorflow possible make activation function quite complicate write c recompile whole tensorflow simpler way
143,143,42521005,How the number of parameters associated with BatchNormalization layer is 2048?,"<p>I have the following code.</p>

<pre><code>x = keras.layers.Input(batch_shape = (None, 4096))
hidden = keras.layers.Dense(512, activation = 'relu')(x)
hidden = keras.layers.BatchNormalization()(hidden)
hidden = keras.layers.Dropout(0.5)(hidden)
predictions = keras.layers.Dense(80, activation = 'sigmoid')(hidden)
mlp_model = keras.models.Model(input = [x], output = [predictions])
mlp_model.summary()
</code></pre>

<p>And this is the model summary:</p>

<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_3 (InputLayer)             (None, 4096)          0                                            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 512)           2097664     input_3[0][0]                    
____________________________________________________________________________________________________
batchnormalization_1 (BatchNorma (None, 512)           2048        dense_1[0][0]                    
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 512)           0           batchnormalization_1[0][0]       
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 80)            41040       dropout_1[0][0]                  
====================================================================================================
Total params: 2,140,752
Trainable params: 2,139,728
Non-trainable params: 1,024
____________________________________________________________________________________________________
</code></pre>

<p>The size of the input for the BatchNormalization (BN) layer is 512. According to <a href=""https://keras.io/layers/normalization/"" rel=""noreferrer"">Keras documentation</a>, shape of the output for BN layer is same as input which is 512.</p>

<p>Then how the number of parameters associated with BN layer is 2048?</p>",42524528.0,2,1,,2017/3/1 0:09,12.0,2019/8/3 16:06,,,,,5352399.0,,1,26,keras|batch-normalization,12925,96.8457,,3,number parameter associate batchnormalization layer following code model summary size input batchnormalization bn layer accord keras documentation shape output bn layer input number parameter associate bn layer
355,355,45645276,Negative dimension size caused by subtracting 3 from 1 for 'conv2d_2/convolution',"<p>I got this error message when declaring the input layer in Keras.</p>

<blockquote>
  <p>ValueError: Negative dimension size caused by subtracting 3 from 1 for
  'conv2d_2/convolution' (op: 'Conv2D') with input shapes: [?,1,28,28],
  [3,3,28,32].</p>
</blockquote>

<p>My code is like this</p>

<pre><code>model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))
</code></pre>

<p>Sample application: <a href=""https://github.com/IntellijSys/tensorflow/blob/master/Keras.ipynb"" rel=""noreferrer"">https://github.com/IntellijSys/tensorflow/blob/master/Keras.ipynb</a></p>",45647715.0,6,2,,2017/8/12 0:02,8.0,2021/5/19 12:59,2018/9/3 19:40,,472495.0,,1922589.0,,1,20,python|tensorflow|neural-network|keras|keras-layer,50149,127.801,,4,negative dimension size cause subtract conv convolution get error message declare input layer keras valueerror negative dimension size cause subtract conv convolution op conv input shape code like sample application
587,587,49492255,How to replace (or insert) intermediate layer in Keras model?,"<p>I have a trained Keras model and I would like:</p>

<p>1) to replace Con2D layer with the same but without bias.</p>

<p>2) to add BatchNormalization layer before first Activation</p>

<p>How can I do this?</p>

<pre class=""lang-python prettyprint-override""><code>def keras_simple_model():
    from keras.models import Model
    from keras.layers import Input, Dense,  GlobalAveragePooling2D
    from keras.layers import Conv2D, MaxPooling2D, Activation

    inputs1 = Input((28, 28, 1))
    x = Conv2D(4, (3, 3), activation=None, padding='same', name='conv1')(inputs1)
    x = Activation('relu')(x)
    x = Conv2D(4, (3, 3), activation=None, padding='same', name='conv2')(x)
    x = Activation('relu')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)

    x = Conv2D(8, (3, 3), activation=None, padding='same', name='conv3')(x)
    x = Activation('relu')(x)
    x = Conv2D(8, (3, 3), activation=None, padding='same', name='conv4')(x)
    x = Activation('relu')(x)
    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)

    x = GlobalAveragePooling2D()(x)
    x = Dense(10, activation=None)(x)
    x = Activation('softmax')(x)

    model = Model(inputs=inputs1, outputs=x)
    return model


if __name__ == '__main__':
    model = keras_simple_model()
    print(model.summary())
</code></pre>",54517478.0,4,2,,2018/3/26 13:07,18.0,2020/5/22 11:06,,,,,2115332.0,,1,38,keras,18404,92.4596,,3,replace insert intermediate layer kera model trained kera model would like replace con layer without bias add batchnormalization layer first activation
825,825,37288421,How to plot a chart in the terminal?,"<p>I'm researching ML/Theano, and recently came across this script:  <a href=""https://gist.github.com/notmatthancock/68d52af2e8cde7fbff1c9225b2790a7f"" rel=""noreferrer"">https://gist.github.com/notmatthancock/68d52af2e8cde7fbff1c9225b2790a7f</a> which was cool to play with. And like all ML researchers, I recently upgraded to a server, and while it's more powerful, it also presented me with a problem.</p>

<p>The script is very long, but it ends with this code: </p>

<pre><code>def plot_stuff(inputs, outputs, losses, net_func, n_hidden):
fig,axes = plt.subplots(1,2,figsize=(12,6))

    axes[0].plot(np.arange(losses.shape[0])+1, losses)
    axes[0].set_xlabel('iteration')
    axes[0].set_ylabel('loss')
    axes[0].set_xscale('log')
    axes[0].set_yscale('log')

    x,y = np.mgrid[inputs[:,0].min():inputs[:,0].max():51j, inputs[:,1].min():inputs[:,1].max():51j]
    z = net_func( np.c_[x.flatten(), y.flatten()] ).reshape(x.shape)

    axes[1].contourf(x,y,z, cmap=plt.cm.RdBu, alpha=0.6)
    axes[1].plot(inputs[outputs==0,0], inputs[outputs==0,1], 'or') 
    axes[1].plot(inputs[outputs==1,0], inputs[outputs==1,1], 'sb') 
    axes[1].set_title('Percent missclassified: %0.2f%%' % (((net_func(inputs)&gt;0.5) != outputs.astype(np.bool)).mean()*100))

    fig.suptitle('Shallow net with %d hidden units'%n_hidden)
    plt.show()

if __name__=='__main__':
    n_hidden = 40
    inputs, outputs = gen_data(n_samples_per_class=100)
    losses, net_func = train_neural_network(inputs=inputs, outputs=outputs, n_hidden=n_hidden, n_iters=int(2000), learning_rate=0.1)
    plot_stuff(inputs, outputs, losses, net_func, n_hidden)
</code></pre>

<p>Which generates this chart:</p>

<p><a href=""https://i.stack.imgur.com/QSLdx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QSLdx.png"" alt=""enter image description here""></a>
And when I tried to run it on the server, which being a sever has no screen only a command line, I predictably got this error:</p>

<pre><code>fedora@ip-173-33-18-911:~/scripting/spiral$ python spiral.py
Iteration 2000 / 2000, Loss: 0.172083
Traceback (most recent call last):
  File ""spiral.py"", line 133, in &lt;module&gt;
    plot_stuff(inputs, outputs, losses, net_func, n_hidden)
  File ""spiral.py"", line 110, in plot_stuff
    fig,axes = plt.subplots(1,2,figsize=(12,6))
  File ""/usr/lib/pymodules/python2.7/matplotlib/pyplot.py"", line 1046, in subplots
    fig = figure(**fig_kw)
  File ""/usr/lib/pymodules/python2.7/matplotlib/pyplot.py"", line 423, in figure
    **kwargs)
  File ""/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py"", line 79, in new_figure_manager
    return new_figure_manager_given_figure(num, figure)
  File ""/usr/lib/pymodules/python2.7/matplotlib/backends/backend_tkagg.py"", line 87, in new_figure_manager_given_figure
    window = Tk.Tk()
  File ""/usr/lib/python2.7/lib-tk/Tkinter.py"", line 1767, in __init__
    self.tk = _tkinter.create(screenName, baseName, className, interactive, wantobjects, useTk, sync, use)
_tkinter.TclError: no display name and no $DISPLAY environment variable
</code></pre>

<p>Is there a way/method/function to display charts and graphs in the command line?</p>",37288742.0,8,1,,2016/5/18 0:46,13.0,2021/1/4 17:47,,,,,5719657.0,,1,19,python|python-2.7|matplotlib|command-line|theano,68508,152.743,,5,plot chart terminal research ml theano recently come across script cool play like ml researcher recently upgrade server powerful also present problem script long end code generate chart try run server sever screen command line predictably get error way method function display chart graph command line
134,134,42384602,Implementing skip connections in keras,"<p>I am implementing ApesNet in keras. It has an ApesBlock that has skip connections. How do I add this to a sequential model in keras? The ApesBlock has two parallel layers that merge at the end by element-wise addition.<img src=""https://i.stack.imgur.com/UrFP8.png"" alt=""enter image description here""></p>",42391339.0,2,0,,2017/2/22 6:54,12.0,2020/6/3 17:14,2017/2/22 7:35,,6691357.0,,6691357.0,,1,25,keras|image-segmentation|resnet,25455,71.6231,,3,implement skip connection kera implement apesnet kera apesblock skip connection add sequential model kera apesblock two parallel layer merge end element wise addition
374,374,46196096,ARKit Place a SCNNode facing the camera,"<p>I'm using <code>ARKit</code> to display 3D objects. I managed to place the nodes in the real world in front of the user (aka the camera). But I don't manage to make them to face the camera when I drop them. </p>

<pre><code>let tap_point=CGPoint(x: x, y: y)
let results=arscn_view.hitTest(tap_point, types: .estimatedHorizontalPlane)
guard results.count&gt;0 else{
    return
}
guard let r=results.first else{
    return
}

let hit_tf=SCNMatrix4(r.worldTransform)
let new_pos=SCNVector3Make(hit_tf.m41, hit_tf.m42+Float(0.2), hit_tf.m43)

guard let scene=SCNScene(named: file_name) else{
    return
}
guard let node=scene.rootNode.childNode(withName: ""Mesh"", recursively: true) else{
    return
}
node.position=new_pos
arscn_view.scene.rootNode.addChildNode(node)
</code></pre>

<p>The nodes are well positioned on the plane, in front of the camera. But they are all looking in the same direction. I guess I should <strong>rotate</strong> the <code>SCNNode</code> but I didn't manage to do this. </p>",,6,1,,2017/9/13 11:16,8.0,2019/3/1 11:05,,,,,2588148.0,,1,15,ios|swift|arkit|scnnode|arscnview,9463,52.9041,,3,arkit place scnnode face camera use display object manage place node real world front user aka camera manage make face camera drop node well position plane front camera look direction guess rotate manage
21,21,53903373,Convert PyTorch tensor to python list,"<p>How do I convert a PyTorch <code>Tensor</code> into a python list?</p>

<p>My current use case is to convert a tensor of size <code>[1, 2048, 1, 1]</code> into a list of 2048 elements.</p>

<p>My tensor has floating point values. Is there a solution which also accounts for int and possibly other data types?</p>",53903817.0,3,0,,2018/12/23 11:50,6.0,2021/8/5 6:03,,,,,5353461.0,,1,60,python|pytorch,96011,166.929,,3,convert pytorch tensor python list convert pytorch python list current use case convert tensor size list element tensor float point value solution also account int possibly data type
237,237,44404281,OpenAI Gym: Understanding `action_space` notation (spaces.Box),"<p>I want to setup an RL agent on the OpenAI <code>CarRacing-v0</code> environment, but before that I want to understand the action space. In <a href=""https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py"" rel=""noreferrer"">the code on github</a> line 119 says:</p>

<pre><code>self.action_space = spaces.Box( np.array([-1,0,0]), np.array([+1,+1,+1]))  # steer, gas, brake
</code></pre>

<p>How do I read this line? Although my problem is concrete wrt <code>CarRacing-v0</code> I would like to understand the <code>spaces.Box()</code> notation in general</p>",44404347.0,1,0,,2017/6/7 5:33,4.0,2019/3/28 11:49,2018/5/17 7:24,,3747801.0,,3747801.0,,1,29,reinforcement-learning|openai-gym,19825,64.9889,,3,openai gym understand action space notation space box want setup rl agent openai environment want understand action space code github line say read line although problem concrete wrt would like understand notation general
354,354,45632549,Why is the accuracy for my Keras model always 0 when training?,"<p>I'm pretty new to keras I have built a simple network to try:</p>

<pre><code>import numpy as np;

from keras.models import Sequential;
from keras.layers import Dense,Activation;

data= np.genfromtxt(""./kerastests/mydata.csv"", delimiter=';')
x_target=data[:,29]
x_training=np.delete(data,6,axis=1)
x_training=np.delete(x_training,28,axis=1)

model=Sequential()
model.add(Dense(20,activation='relu', input_dim=x_training.shape[1]))
model.add(Dense(10,activation='relu'))
model.add(Dense(1));

model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])
model.fit(x_training, x_target)
</code></pre>

<p>From my source data, I have removed 2 columns, as you can see. One is a column that came with dates in a string format (in the dataset, besides it, I have a column for the day, another for the month, and another for the year, so I don't need that column) and the other column is the column I use as target for the model).</p>

<p>When I train this model I get this output:</p>

<pre><code>32/816 [&gt;.............................] - ETA: 23s - loss: 13541942.0000 - acc: 0.0000e+00
800/816 [============================&gt;.] - ETA: 0s - loss: 11575466.0400 - acc: 0.0000e+00 
816/816 [==============================] - 1s - loss: 11536905.2353 - acc: 0.0000e+00     
Epoch 2/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 6794785.0000 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5381360.4314 - acc: 0.0000e+00     
Epoch 3/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 6235184.0000 - acc: 0.0000e+00
800/816 [============================&gt;.] - ETA: 0s - loss: 5199512.8700 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5192977.4216 - acc: 0.0000e+00     
Epoch 4/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 4680165.5000 - acc: 0.0000e+00
736/816 [==========================&gt;...] - ETA: 0s - loss: 5050110.3043 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5168771.5490 - acc: 0.0000e+00     
Epoch 5/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 5932391.0000 - acc: 0.0000e+00
768/816 [===========================&gt;..] - ETA: 0s - loss: 5198882.9167 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5159585.9020 - acc: 0.0000e+00     
Epoch 6/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 4488318.0000 - acc: 0.0000e+00
768/816 [===========================&gt;..] - ETA: 0s - loss: 5144843.8333 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5151492.1765 - acc: 0.0000e+00     
Epoch 7/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 6920405.0000 - acc: 0.0000e+00
800/816 [============================&gt;.] - ETA: 0s - loss: 5139358.5000 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5169839.2941 - acc: 0.0000e+00     
Epoch 8/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 3973038.7500 - acc: 0.0000e+00
672/816 [=======================&gt;......] - ETA: 0s - loss: 5183285.3690 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5141417.0000 - acc: 0.0000e+00     
Epoch 9/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 4969548.5000 - acc: 0.0000e+00
768/816 [===========================&gt;..] - ETA: 0s - loss: 5126550.1667 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5136524.5098 - acc: 0.0000e+00     
Epoch 10/10
 32/816 [&gt;.............................] - ETA: 0s - loss: 6334703.5000 - acc: 0.0000e+00
768/816 [===========================&gt;..] - ETA: 0s - loss: 5197778.8229 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5141391.2059 - acc: 0.0000e+00    
</code></pre>

<p>Why is this happening? My data is a time series. I know that for time series people do not usually use <code>Dense</code> neurons, but it is just a test. What really tricks me is that accuracy is always 0. And, with other tests, I did even lose: gets to a ""NAN"" value.</p>

<p>Could anybody help here?</p>",45632689.0,4,1,,2017/8/11 10:08,19.0,2020/4/23 22:28,2020/1/26 21:32,,3924118.0,,574633.0,,1,44,tensorflow|neural-network|keras,42236,137.703,,4,accuracy kera model always training pretty new keras build simple network try source data remove column see one column come date string format dataset besides column day another month another year need column column column use target model train model get output happen data time series know time series people usually use neuron test really trick accuracy always test even lose get nan value could anybody help
606,606,50063613,What is the purpose of the add_loss function in Keras?,"<p>Currently I stumbled across variational autoencoders and tried to make them work on MNIST using keras. I found a tutorial on <a href=""https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py"" rel=""noreferrer"">github</a>.</p>

<p>My question concerns the following lines of code:</p>



<pre class=""lang-python prettyprint-override""><code># Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss
xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(xent_loss + kl_loss)

# Compile
vae.add_loss(vae_loss)
vae.compile(optimizer='rmsprop')
</code></pre>

<p>Why is add_loss used instead of specifying it as compile option? Something like  <code>vae.compile(optimizer='rmsprop', loss=vae_loss)</code> does not seem to work and throws the following error:</p>

<pre class=""lang-python prettyprint-override""><code>ValueError: The model cannot be compiled because it has no loss to optimize.
</code></pre>

<p>What is the difference between this function and a custom loss function, that I can add as an argument for Model.fit()?</p>

<p>Thanks in advance! </p>

<p>P.S.: I know there are several issues concerning this on github, but most of them were open and uncommented. If this has been resolved already, please share the link!</p>

<hr>

<h3>Edit 1</h3>

<p>I removed the line which adds the loss to the model and used the loss argument of the compile function. It looks like this now:</p>



<pre class=""lang-python prettyprint-override""><code># Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss
xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(xent_loss + kl_loss)

# Compile
vae.compile(optimizer='rmsprop', loss=vae_loss)
</code></pre>

<p>This throws an TypeError:</p>

<pre class=""lang-python prettyprint-override""><code>TypeError: Using a 'tf.Tensor' as a Python 'bool' is not allowed. Use 'if t is not None:' instead of 'if t:' to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
</code></pre>

<hr>

<h3>Edit 2</h3>

<p>Thanks to @MarioZ's efforts, I was able to figure out a workaround for this. </p>



<pre class=""lang-python prettyprint-override""><code># Build model
vae = Model(x, x_decoded_mean)

# Calculate custom loss in separate function
def vae_loss(x, x_decoded_mean):
    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)
    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
    vae_loss = K.mean(xent_loss + kl_loss)
    return vae_loss

# Compile
vae.compile(optimizer='rmsprop', loss=vae_loss)

...

vae.fit(x_train, 
    x_train,        # &lt;-- did not need this previously
    shuffle=True,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(x_test, x_test))     # &lt;-- worked with (x_test, None) before
</code></pre>

<p>For some strange reason, I had to explicitly specify y and y_test while fitting the model. Originally, I didn't need to do this. The produced samples seem reasonable to me. </p>

<p>Although I could resolve this, I still don't know what the differences and disadvantages of these two methods are (other than needing a different syntax). Can someone give me more insight?</p>",52683522.0,5,1,,2018/4/27 13:35,12.0,2021/3/5 4:00,2019/10/23 16:33,,3924118.0,,8334261.0,,1,40,neural-network|keras|autoencoder,24880,106.583,,4,purpose add loss function kera currently stumble across variational autoencoders try make work mnist use kera find tutorial github question concern following line code add loss use instead specify compile option something like seem work throw following error difference function custom loss function add argument model fit thanks advance p know several issue concern github open uncommented resolve already please share link edit remove line add loss model use loss argument compile function look like throw typeerror edit thanks marioz effort able figure workaround strange reason explicitly specify test fit model originally need produced sample seem reasonable although could resolve still know difference disadvantage two method need different syntax someone give insight
289,289,56690089,How to graph tf.keras model in Tensorflow-2.0?,"<p>I upgraded to Tensorflow 2.0 and there is no <code>tf.summary.FileWriter(""tf_graphs"", sess.graph)</code>. I was looking through some other StackOverflow questions on this and they said to use <code>tf.compat.v1.summary etc</code>. Surely there must be a way to graph and visualize a tf.keras model in Tensorflow version 2. What is it? I'm looking for a tensorboard output like the one below. Thank you!</p>

<p><a href=""https://i.stack.imgur.com/Zhc3g.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Zhc3g.png"" alt=""enter image description here""></a></p>",56690364.0,4,0,,2019/6/20 16:13,8.0,2020/11/15 16:53,,,,,9820369.0,,1,17,python-3.x|tensorflow|tensorboard|tensorflow2.0|tf.keras,10857,62.7428,,5,graph tf kera model tensorflow upgrade tensorflow look stackoverflow question say use surely must way graph visualize tf kera model tensorflow version look tensorboard output like one thank
631,631,51003027,Computing cosine similarity between two tensors in Keras,"<p>I have been following a tutorial that shows how to make a <code>word2vec</code> model.</p>
<p>This tutorial uses this piece of code:</p>
<p><code>similarity = merge([target, context], mode='cos', dot_axes=0)</code> (no other info was given, but I suppose this comes from <code>keras.layers</code>)</p>
<p>Now, I've researched a bit on the <code>merge</code> method but I couldn't find much about it.
From what I understand, it has been replaced by a lot of functions like <code>layers.Add(), layers.Concat()...</code>.</p>
<p>What should I use? There's <code>.Dot()</code>, which has an <code>axis</code> parameter (which seems to be correct) but no <code>mode</code> parameter.</p>
<p>What can I use in this case?</p>",51003359.0,3,1,,2018/6/23 16:27,6.0,2021/4/29 6:18,2021/4/29 6:18,,9215780.0,,5500312.0,,1,14,python|keras,15845,81.1996,,3,compute cosine similarity two tensor kera follow tutorial show make model tutorial use piece code info give suppose come research bit method could find much understand replace lot function like use parameter seem correct parameter use case
109,109,41668813,How to add and remove new layers in keras after loading weights?,"<p>I am trying to do a transfer learning; for that purpose I want to remove the last two layers of the neural network and add another two layers. This is an example code which also output the same error.</p>

<pre><code>from keras.models import Sequential
from keras.layers import Input,Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dropout, Activation
from keras.layers.pooling import GlobalAveragePooling2D
from keras.models import Model

in_img = Input(shape=(3, 32, 32))
x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)
x = Activation('relu', name='relu_conv1')(x)
x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)
x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)
x = Activation('relu', name='relu_conv2')(x)
x = GlobalAveragePooling2D()(x)
o = Activation('softmax', name='loss')(x)
model = Model(input=in_img, output=[o])
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
#model.load_weights('model_weights.h5', by_name=True)
model.summary()

model.layers.pop()
model.layers.pop()
model.summary()
model.add(MaxPooling2D())
model.add(Activation('sigmoid', name='loss'))
</code></pre>

<p>I removed the layer using <code>pop()</code> but when I tried to add its outputting this error </p>

<blockquote>
  <p>AttributeError: 'Model' object has no attribute 'add'</p>
</blockquote>

<p>I know the most probable reason for the error is improper use of <code>model.add()</code>. what other syntax should I use?</p>

<p><strong>EDIT:</strong></p>

<p>I tried to remove/add layers in keras but its not  allowing it to be added after loading external weights.</p>

<pre><code>from keras.models import Sequential
from keras.layers import Input,Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dropout, Activation
from keras.layers.pooling import GlobalAveragePooling2D
from keras.models import Model
in_img = Input(shape=(3, 32, 32))

def gen_model():
    in_img = Input(shape=(3, 32, 32))
    x = Convolution2D(12, 3, 3, subsample=(2, 2), border_mode='valid', name='conv1')(in_img)
    x = Activation('relu', name='relu_conv1')(x)
    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool1')(x)
    x = Convolution2D(3, 1, 1, border_mode='valid', name='conv2')(x)
    x = Activation('relu', name='relu_conv2')(x)
    x = GlobalAveragePooling2D()(x)
    o = Activation('softmax', name='loss')(x)
    model = Model(input=in_img, output=[o])
    return model

#parent model
model=gen_model()
model.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
model.summary()

#saving model weights
model.save('model_weights.h5')

#loading weights to second model
model2=gen_model()
model2.compile(loss=""categorical_crossentropy"", optimizer=""adam"")
model2.load_weights('model_weights.h5', by_name=True)

model2.layers.pop()
model2.layers.pop()
model2.summary()

#editing layers in the second model and saving as third model
x = MaxPooling2D()(model2.layers[-1].output)
o = Activation('sigmoid', name='loss')(x)
model3 = Model(input=in_img, output=[o])
</code></pre>

<p>its showing this error</p>

<pre><code>RuntimeError: Graph disconnected: cannot obtain value for tensor input_4 at layer ""input_4"". The following previous layers were accessed without issue: []
</code></pre>",41670915.0,4,1,,2017/1/16 2:56,26.0,2021/8/20 8:59,2017/1/20 6:38,,996366.0,,996366.0,,1,66,python|theano|keras|keras-layer,75831,206.319,,3,add remove new layer kera load weight try transfer learning purpose want remove last two layer neural network add another two layer example code also output error remove layer use try add output error attributeerror model object attribute add know probable reason error improper use syntax use edit try remove add layer kera allow add load external weight showing error
61,61,55494488,Caffeine versus Guava cache,"<p>According to these <a href=""https://github.com/ben-manes/caffeine/wiki/Benchmarks"" rel=""noreferrer"">micro benchmarks</a> it turns out that <a href=""https://static.javadoc.io/com.github.ben-manes.caffeine/caffeine/2.1.0/com/github/benmanes/caffeine/cache/Caffeine.html"" rel=""noreferrer"">Caffeine</a> is a way faster than <a href=""https://google.github.io/guava/releases/23.0/api/docs/com/google/common/cache/Cache.html"" rel=""noreferrer"">Guava cache</a> in both read and write operations.</p>

<p>What is the secret of Caffeine implementation? How it differs from the Guava Cache?</p>

<p>Am I right that in case of timed expiration Caffeine use a scheduled executor to perform appropriate maintenance operations in background? </p>",55501343.0,1,0,,2019/4/3 11:51,9.0,2019/9/5 10:32,2019/9/5 10:32,,3796338.0,,3796338.0,,1,27,caching|guava|caffeine,11242,79.6034,,3,caffeine versus guava cache accord micro benchmark turn caffeine way faster guava cache read write operation secret caffeine implementation differ guava cache right case timed expiration caffeine use scheduled executor perform appropriate maintenance operation background
72,72,55933867,"What does ""learning rate warm-up"" mean?","<p>In machine learning, especially deep learning, what does it mean to warm-up?</p>
<p>I've heard sometimes that in some models, warming-up is a phase in training. But honestly, I don't know what it is because I'm very new to ML. Until now I've never used or come across it, but I want to know it because I think it might be useful for me.</p>
<p><strong>What is learning rate warm-up and when do we need it?</strong></p>",55942518.0,3,0,,2019/5/1 9:01,9.0,2020/8/15 11:38,2020/8/15 11:38,,366904.0,,7339624.0,,1,17,machine-learning|neural-network|deep-learning|terminology,21063,71.4941,2020/8/15 11:37,0,learn rate warm mean machine learning especially deep learn mean warm hear sometimes model warm phase training honestly know new ml never use come across want know think might useful learn rate warm need
388,388,46619869,How to specify the correlation coefficient as the loss function in keras,"<p>I am using keras+tensorflow for the first time. I would like to specify the <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pearsonr.html"" rel=""noreferrer"">correlation coefficient</a> as the loss function. It makes sense to square it so that it is a number between 0 and 1 where 0 is bad and 1 is good.</p>

<p>My basic code currently looks like:</p>

<pre><code>def baseline_model():
        model = Sequential()
        model.add(Dense(4000, input_dim=n**2, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        # Compile model
        model.compile(loss='mean_squared_error', optimizer='adam')
        return model

estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=2)))
pipeline = Pipeline(estimators)
kfold = KFold(n_splits=10, random_state=0)
results = cross_val_score(pipeline, X, Y, cv=kfold)
print(""Standardized: %.2f (%.2f) MSE"" % (results.mean(), results.std()))
</code></pre>

<p>How can I change this so that it optimizes to minimize the squared correlation coefficient  instead?</p>

<hr>

<p>I tried the following:</p>

<pre><code>def correlation_coefficient(y_true, y_pred):
    pearson_r, _ = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)
    return 1-pearson_r**2

def baseline_model():
# create model
        model = Sequential()
        model.add(Dense(4000, input_dim=n**2, kernel_initializer='normal', activation='relu'))
#        model.add(Dense(2000, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        # Compile model
        model.compile(loss=correlation_coefficient, optimizer='adam')
        return model
</code></pre>

<p>but this crashes with:</p>

<pre><code>Traceback (most recent call last):
  File ""deeplearning-det.py"", line 67, in &lt;module&gt;
    results = cross_val_score(pipeline, X, Y, cv=kfold)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 321, in cross_val_score
    pre_dispatch=pre_dispatch)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 195, in cross_validate
    for train, test in cv.split(X, y, groups))
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 111, in apply_async
    result = ImmediateResult(func)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py"", line 332, in __init__
    self.results = batch()
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py"", line 131, in &lt;listcomp&gt;
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py"", line 437, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/home/user/.local/lib/python3.5/site-packages/sklearn/pipeline.py"", line 259, in fit
    self._final_estimator.fit(Xt, y, **fit_params)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/wrappers/scikit_learn.py"", line 147, in fit
    history = self.model.fit(x, y, **fit_args)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/models.py"", line 867, in fit
    initial_epoch=initial_epoch)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 1575, in fit
    self._make_train_function()
  File ""/home/user/.local/lib/python3.5/site-packages/keras/engine/training.py"", line 960, in _make_train_function
    loss=self.total_loss)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/home/user/.local/lib/python3.5/site-packages/keras/optimizers.py"", line 432, in get_updates
    m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 856, in binary_op_wrapper
    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=""y"")
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 611, in convert_to_tensor
    as_ref=False)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 676, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 121, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/home/user/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 364, in make_tensor_proto
    raise ValueError(""None values not supported."")
ValueError: None values not supported.
</code></pre>

<hr>

<p><strong>Update 1</strong></p>

<p>Following the answer below the code now runs. Unfortunately, the <code>correlation_coefficient</code> and <code>correlation_coefficient_loss</code> functions give different values from each other and I am not sure either of them is the same as you would get from 1- <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html"" rel=""noreferrer"">scipy.stats.pearsonr</a>()[0]**2.  </p>

<blockquote>
  <p>Why are loss functions giving the wrong outputs and how can they be
  corrected to give the same values as <code>1 -
  scipy.stats.pearsonr()[0]**2</code> would give?</p>
</blockquote>

<p>Here is the completely self contained code that should just run:</p>

<pre><code>import numpy as np
import sys
import math
from scipy.stats import ortho_group
from scipy.stats import pearsonr
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import tensorflow as tf
from keras import backend as K


def permanent(M):
    n = M.shape[0]
    d = np.ones(n)
    j = 0
    s = 1
    f = np.arange(n)
    v = M.sum(axis=0)
    p = np.prod(v)
    while (j &lt; n-1):
        v -= 2*d[j]*M[j]
        d[j] = -d[j]
        s = -s
        prod = np.prod(v)
        p += s*prod
        f[0] = 0
        f[j] = f[j+1]
        f[j+1] = j+1
        j = f[0]
    return p/2**(n-1)


def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(xm * ym)
    r_den = K.sum(K.sum(K.square(xm)) * K.sum(K.square(ym)))
    r = r_num / r_den
    return 1 - r**2


def correlation_coefficient(y_true, y_pred):
    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true)
    # find all variables created for this metric
    metric_vars = [i for i in tf.local_variables() if 'correlation_coefficient' in i.name.split('/')[1]]

    # Add metric variables to GLOBAL_VARIABLES collection.
    # They will be initialized for new session.
    for v in metric_vars:
        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)

    # force to update metric values
    with tf.control_dependencies([update_op]):
        pearson_r = tf.identity(pearson_r)
        return 1-pearson_r**2


def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(4000, input_dim=no_rows**2, kernel_initializer='normal', activation='relu'))
#    model.add(Dense(2000, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    # Compile model
    model.compile(loss=correlation_coefficient_loss, optimizer='adam', metrics=[correlation_coefficient])
    return model


no_rows = 8

print(""Making the input data using seed 7"", file=sys.stderr)
np.random.seed(7)
U = ortho_group.rvs(no_rows**2)
U = U[:, :no_rows]
# U is a random orthogonal matrix
X = []
Y = []
print(U)
for i in range(40000):
        I = np.random.choice(no_rows**2, size = no_rows)
        A = U[I][np.lexsort(np.rot90(U[I]))]
        X.append(A.ravel())
        Y.append(-math.log(permanent(A)**2, 2))

X = np.array(X)
Y = np.array(Y)

estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=32, verbose=2)))
pipeline = Pipeline(estimators)
X_train, X_test, y_train, y_test = train_test_split(X, Y,
                                                    train_size=0.75, test_size=0.25)
pipeline.fit(X_train, y_train)
</code></pre>

<p><strong>Update 2</strong></p>

<p>I have given up on the <code>correlation_coefficient</code> function and am now just using the <code>correlation_coefficient_loss</code> one as given by JulioDanielReyes below.  However, either this is still wrong or keras is dramatically overfitting.  Even when I have:</p>

<pre><code>def baseline_model():
        model = Sequential()
        model.add(Dense(40, input_dim=no_rows**2, kernel_initializer='normal', activation='relu'))
        model.add(Dense(1, kernel_initializer='normal'))
        model.compile(loss=correlation_coefficient_loss, optimizer='adam', metrics=[correlation_coefficient_loss])
        return model
</code></pre>

<p>I get a loss of, for example, 0.6653 after 100 epochs but 0.857 when I test the trained model.</p>

<blockquote>
  <p>How can it be overfitting which such a tiny number of nodes in the
  hidden layer?</p>
</blockquote>",46620771.0,3,13,,2017/10/7 11:59,8.0,2019/11/16 12:36,2017/10/11 18:09,,2179021.0,,2179021.0,,1,16,python|machine-learning|tensorflow|keras,16594,55.4798,,4,specify correlation coefficient loss function kera use kera tensorflow first time would like specify correlation coefficient loss function make sense square number bad good basic code currently look like change optimize minimize squared correlation coefficient instead try follow crash update follow answer code run unfortunately function give different value sure either would get scipy stats pearsonr loss function give wrong output correct give value would give completely self contain code run update give function use one give juliodanielreyes however either still wrong kera dramatically overfitting even get loss example epoch test trained model overfitting tiny number node hidden layer
368,368,46125018,Use LSTM tutorial code to predict next word in a sentence?,"<p>I've been trying to understand the sample code with <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/recurrent</a>
which you can find at <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py</a></p>

<p>(Using tensorflow 1.3.0.)</p>

<p>I've summarized (what I think are) the key parts, for my question, below:</p>

<pre><code> size = 200
 vocab_size = 10000
 layers = 2
 # input_.input_data is a 2D tensor [batch_size, num_steps] of
 #    word ids, from 1 to 10000

 cell = tf.contrib.rnn.MultiRNNCell(
    [tf.contrib.rnn.BasicLSTMCell(size) for _ in range(2)]
    )

 embedding = tf.get_variable(
      ""embedding"", [vocab_size, size], dtype=tf.float32)
 inputs = tf.nn.embedding_lookup(embedding, input_.input_data)

inputs = tf.unstack(inputs, num=num_steps, axis=1)
outputs, state = tf.contrib.rnn.static_rnn(
    cell, inputs, initial_state=self._initial_state)

output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
softmax_w = tf.get_variable(
    ""softmax_w"", [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=data_type())
logits = tf.matmul(output, softmax_w) + softmax_b

# Then calculate loss, do gradient descent, etc.
</code></pre>

<p>My biggest question is <em>how do I use the produced model to actually generate a next word suggestion, given the first few words of a sentence</em>? Concretely, I imagine the flow is like this, but I cannot get my head around what the code for the commented lines would be:</p>

<pre><code>prefix = [""What"", ""is"", ""your""]
state = #Zeroes
# Call static_rnn(cell) once for each word in prefix to initialize state
# Use final output to set a string, next_word
print(next_word)
</code></pre>

<p>My sub-questions are:</p>

<ul>
<li>Why use a random (uninitialized, untrained) word-embedding?</li>
<li>Why use softmax?</li>
<li>Does the hidden layer have to match the dimension of the input (i.e. the dimension of the word2vec embeddings)</li>
<li>How/Can I bring in a pre-trained word2vec model, instead of that uninitialized one?</li>
</ul>

<p>(I'm asking them all as one question, as I suspect they are all connected, and connected to some gap in my understanding.)</p>

<p>What I was expecting to see here was loading an existing word2vec set of word embeddings (e.g. using gensim's <code>KeyedVectors.load_word2vec_format()</code>), convert each word in the input corpus to that representation when loading in each sentence, and then afterwards the LSTM would spit out a vector of the same dimension, and we would try and find the most similar word (e.g. using gensim's <code>similar_by_vector(y, topn=1)</code>).</p>

<p>Is using softmax saving us from the relatively slow <code>similar_by_vector(y, topn=1)</code> call?</p>

<hr>

<p>BTW, for the pre-existing word2vec part of my question <a href=""https://stackoverflow.com/q/42064690/841830"">Using pre-trained word2vec with LSTM for word generation</a> is similar. However the answers there, currently, are not what I'm looking for. What I'm hoping for is a plain English explanation that switches the light on for me, and plugs whatever the gap in my understanding is.闂侀潧妫岄崑鎾绘煏閸″繐浜?a href=""https://stackoverflow.com/questions/44614097/use-pre-trained-word2vec-in-lstm-language-model"">Use pre-trained word2vec in lstm language model?</a> is another similar question.</p>

<p><strong>UPDATE:</strong> <a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> and <a href=""https://stackoverflow.com/q/36286594/841830"">Predicting the next word using the LSTM ptb model tensorflow example</a> are similar questions. However, neither shows the code to actually take the first few words of a sentence, and print out its prediction of the next word. I tried pasting in code from the 2nd question, and from <a href=""https://stackoverflow.com/a/39282697/841830"">https://stackoverflow.com/a/39282697/841830</a> (which comes with a github branch), but cannot get either to run without errors. I think they may be for an earlier version of TensorFlow?</p>

<p><strong>ANOTHER UPDATE:</strong> Yet another question asking basically the same thing: <a href=""https://stackoverflow.com/q/42333101/841830"">Predicting Next Word of LSTM Model from Tensorflow Example</a>
It links to 
<a href=""https://stackoverflow.com/q/33773661/841830"">Predicting next word using the language model tensorflow example</a> (and, again, the answers there are not quite what I am looking for).</p>

<p>In case it still isn't clear, what I am trying to write a high-level function called <code>getNextWord(model, sentencePrefix)</code>, where <code>model</code> is a previously built LSTM that I've loaded from disk, and <code>sentencePrefix</code> is a string, such as ""Open the"", and it might return ""pod"".  I then might call it with ""Open the pod"" and it will return ""bay"", and so on.</p>

<p>An example (with a character RNN, and using mxnet) is the <code>sample()</code> function shown near the end of <a href=""https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb"" rel=""noreferrer"">https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter05_recurrent-neural-networks/simple-rnn.ipynb</a>
You can call <code>sample()</code> during training, but you can also call it after training, and with any sentence you want.</p>",,4,7,,2017/9/8 21:55,13.0,2018/6/8 18:27,2017/9/13 13:35,,841830.0,,841830.0,,1,22,python|tensorflow|lstm|word2vec|word-embedding,8364,53.2897,,3,use lstm tutorial code predict next word sentence try understand sample code find use tensorflow summarize think key part question big question use produced model actually generate next word suggestion give first word sentence concretely imagine flow like get head around code commented line would sub question use random uninitialized untrained word embed use softmax hidden layer match dimension input e dimension word vec embeddings bring pre train word vec model instead uninitialized one ask one question suspect connect connect gap understand expect see load exist word vec set word embeddings e g use gensim convert word input corpus representation loading sentence afterwards lstm would spit vector dimension would try find similar word e g use gensim use softmax save u relatively slow call btw pre exist word vec part question use pre train word vec lstm word generation similar however answer currently look hop plain english explanation switch light plug whatever gap understanding use pre train word vec lstm language model another similar question update predict next word use language model tensorflow example predict next word use lstm ptb model tensorflow example similar question however neither show code actually take first word sentence print prediction next word try paste code nd question come github branch get either run without error think may early version tensorflow another update yet another question ask basically thing predict next word lstm model tensorflow example link predict next word use language model tensorflow example answer quite look case still clear try write high level function call previously build lstm load disk string open might return pod might call open pod return bay example character rnn use mxnet function show near end call training also call train sentence want
389,389,46654424,How to calculate optimal batch size,"<p>Sometimes I run into a problem: </p>

<blockquote>
  <p>OOM when allocating tensor with shape</p>
</blockquote>

<p>e.q.</p>

<blockquote>
  <p>OOM when allocating tensor with shape (1024, 100, 160)</p>
</blockquote>

<p>Where 1024 is my batch size and I don't know what's the rest. If I reduce the batch size or the number of neurons in the model, it runs fine.</p>

<p>Is there a generic way to calculate optimal batch size based on model and GPU memory, so the program doesn't crash?</p>

<p>In short: I want the largest batch size possible in terms of my model, which will fit into my GPU memory and won't crash the program.</p>",46656508.0,5,0,,2017/10/9 20:25,22.0,2021/3/21 10:54,2019/9/21 16:44,,4685471.0,,672018.0,,1,31,machine-learning|neural-network|deep-learning|keras|gradient-descent,38016,113.32,,3,calculate optimal batch size sometimes run problem oom allocate tensor shape e q oom allocate tensor shape batch size know rest reduce batch size number neuron model run fine generic way calculate optimal batch size base model gpu memory program crash short want large batch size possible term model fit gpu memory crash program
515,515,34716454,Where do I call the BatchNormalization function in Keras?,"<p>If I want to use the BatchNormalization function in Keras, then do I need to call it once only at the beginning?</p>

<p>I read this documentation for it: <a href=""http://keras.io/layers/normalization/"">http://keras.io/layers/normalization/</a></p>

<p>I don't see where I'm supposed to call it. Below is my code attempting to use it:</p>

<pre><code>model = Sequential()
keras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(64, init='uniform'))
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)
</code></pre>

<p>I ask because if I run the code with the second line including the batch normalization and if I run the code without the second line I get similar outputs. So either I'm not calling the function in the right place, or I guess it doesn't make that much of a difference.</p>",37979391.0,8,0,,2016/1/11 7:47,87.0,2021/1/15 18:08,2019/6/4 18:55,,8927035.0,,4984897.0,,1,193,python|keras|neural-network|data-science|batch-normalization,192653,785.939,,3,call batchnormalization function kera want use batchnormalization function kera need call beginning read documentation see suppose call code attempt use ask run code second line include batch normalization run code without second line get similar output either call function right place guess make much difference
704,704,39379792,Install Cuda without root,"<p>I know that I can install Cuda with the following:</p>

<pre>
wget http://developer.download.nvidia.com/compute/cuda/7_0/Prod/local_installers/cuda_7.0.28_linux.run
chmod +x cuda_7.0.28_linux.run
./cuda_7.0.28_linux.run -extract=`pwd`/nvidia_installers
cd nvidia_installers
sudo ./NVIDIA-Linux-x86_64-346.46.run 
sudo modprobe nvidia
sudo ./cuda-linux64-rel-7.0.28-19326674.run 
</pre>

<p>Just wondering if I can install Cuda without root?</p>

<p>Thanks,</p>",39379952.0,3,4,,2016/9/7 22:13,15.0,2019/10/22 12:56,,,,,200340.0,,1,21,cuda|tensorflow|gpu|theano,26341,91.2825,,1,install cuda without root know install cuda follow wget chmod x cuda linux run cuda linux run extract pwd nvidia installers cd nvidia installers sudo nvidia linux x run sudo modprobe nvidia sudo cuda linux rel run wonder install cuda without root thanks
171,171,43034960,Many to one and many to many LSTM examples in Keras,"<p>I try to understand LSTMs and how to build them with Keras. I found out, that there are principally the 4 modes to run a RNN (the 4 right ones in the picture)</p>

<p><a href=""https://i.stack.imgur.com/b4sus.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/b4sus.jpg"" alt=""enter image description here""></a>
Image source: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">Andrej Karpathy</a></p>

<p>Now I wonder how a minimalistic code snippet for each of them would look like in Keras.
So something like</p>

<pre><code>model = Sequential()
model.add(LSTM(128, input_shape=(timesteps, data_dim)))
model.add(Dense(1))
</code></pre>

<p>for each of the 4 tasks, maybe with a little bit of explanation.</p>",43047615.0,2,0,,2017/3/26 21:47,88.0,2020/11/9 6:17,2018/2/27 23:01,,951894.0,,3869448.0,,1,140,machine-learning|neural-network|deep-learning|keras|recurrent-neural-network,60615,244.13,,3,many one many many lstm example kera try understand lstms build kera find principally mode run rnn right one picture image source andrej karpathy wonder minimalistic code snippet would look like kera something like task maybe little bit explanation
641,641,36841158,Fine Tuning of GoogLeNet Model,"<p>I trained GoogLeNet model from scratch. But it didn't give me the promising results.<br>
As an alternative, I would like to do fine tuning of GoogLeNet model on my dataset. Does anyone know what are the steps should I follow? </p>",36842553.0,2,0,,2016/4/25 12:52,8.0,2018/12/9 16:15,2016/4/25 13:32,,1714410.0,,5833345.0,,1,15,machine-learning|computer-vision|neural-network|deep-learning|caffe,10587,57.0991,,3,fine tuning googlenet model train googlenet model scratch give promising result alternative would like fine tuning googlenet model dataset anyone know step follow
324,324,58678836,NotImplementedError: Layers with arguments in `__init__` must override `get_config`,"<p>I'm trying to save my TensorFlow model using <code>model.save()</code>, however - I am getting this error.</p>

<p>The model summary is provided here:
<a href=""https://i.stack.imgur.com/nkrR1.png"" rel=""noreferrer"">Model Summary</a></p>

<p>The code for the transformer model:</p>

<pre class=""lang-py prettyprint-override""><code>def transformer(vocab_size, num_layers, units, d_model, num_heads, dropout, name=""transformer""):
    inputs = tf.keras.Input(shape=(None,), name=""inputs"")
    dec_inputs = tf.keras.Input(shape=(None,), name=""dec_inputs"")

    enc_padding_mask = tf.keras.layers.Lambda(
        create_padding_mask, output_shape=(1, 1, None),
        name='enc_padding_mask')(inputs)
    # mask the future tokens for decoder inputs at the 1st attention block
    look_ahead_mask = tf.keras.layers.Lambda(
        create_look_ahead_mask,
        output_shape=(1, None, None),
        name='look_ahead_mask')(dec_inputs)
    # mask the encoder outputs for the 2nd attention block
    dec_padding_mask = tf.keras.layers.Lambda(
        create_padding_mask, output_shape=(1, 1, None),
        name='dec_padding_mask')(inputs)

    enc_outputs = encoder(
        vocab_size=vocab_size,
        num_layers=num_layers,
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
    )(inputs=[inputs, enc_padding_mask])

    dec_outputs = decoder(
        vocab_size=vocab_size,
        num_layers=num_layers,
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

    outputs = tf.keras.layers.Dense(units=vocab_size, name=""outputs"")(dec_outputs)

    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
</code></pre>

<p>I don't understand why it's giving this error since the model trains perfectly fine. 
Any help would be appreciated.</p>

<p>My saving code for reference:</p>

<pre class=""lang-py prettyprint-override""><code>print(""Saving the model."")
saveloc = ""C:/tmp/solar.h5""
model.save(saveloc)
print(""Model saved to: "" + saveloc + "" succesfully."")
</code></pre>",58799021.0,2,1,,2019/11/3 9:23,9.0,2021/7/26 13:45,,,,,12315223.0,,1,36,python|tensorflow|keras,22228,89.7876,,4,notimplementederror layer argument init must override get config try save tensorflow model use however get error model summary provide model summary code transformer model understand give error since model train perfectly fine help would appreciate save code reference
798,798,52776622,"Keras callbacks keep skip saving checkpoints, claiming val_acc is missing","<p>I'll run some larger models and want to try intermediate results.</p>

<p>Therefore, I try to use checkpoints to save the best model after each epoch.</p>

<p>This is my code:</p>

<pre><code>model = Sequential()
model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(700, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(700))
model.add(Dropout(0.2))
model.add(Dense(Y_modified.shape[1], activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Save the checkpoint in the /output folder
filepath = ""output/text-gen-best.hdf5""

# Keep only a single checkpoint, the best over test accuracy.
checkpoint = ModelCheckpoint(filepath,
                            monitor='val_acc',
                            verbose=1,
                            save_best_only=True,
                            mode='max')
model.fit(X_modified, Y_modified, epochs=100, batch_size=50, callbacks=[checkpoint])
</code></pre>

<p>But I am still getting the warning after the first epoch:</p>

<pre><code>/usr/local/lib/python3.6/site-packages/keras/callbacks.py:432: RuntimeWarning: Can save best model only with val_acc available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
</code></pre>

<p>To add <code>metrics=['accuracy']</code> to the model was in other SO questions (e.g. <a href=""https://stackoverflow.com/questions/46438247/unable-to-save-weights-while-using-pre-trained-vgg16-model?rq=1"">Unable to save weights while using pre-trained VGG16 model</a>) the solution, but here the error still remains.</p>",52776963.0,3,0,,2018/10/12 9:36,2.0,2020/7/11 12:38,,,,,4399454.0,,1,10,python-3.x|keras|checkpointing,11003,50.166,,4,kera callback keep skip save checkpoint claim val acc miss run large model want try intermediate result therefore try use checkpoint save best model epoch code still get warning first epoch add model question e g unable save weight use pre train vgg model solution error still remain
13,13,62181162,cannot import name 'open' from 'smart_open',"<p>I was doing this and got this error :</p>

<pre><code>from gensim.models import Word2Vec

ImportError: cannot import name 'open' from 'smart_open' (C:\ProgramData\Anaconda3\lib\site-packages\smart_open\__init__.py)
</code></pre>

<p>Then I did this :</p>

<pre><code>import smart_open
dir(smart_open)

['BZ2File','BytesIO','DEFAULT_ERRORS','IS_PY2','P','PATHLIB_SUPPORT','SSLError','SYSTEM_ENCODING','Uri','__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__path__','__spec__','boto','codecs','collections','gzip','hdfs','http','importlib','io','logger','logging','os','pathlib','pathlib_module','requests','s3','s3_iter_bucket','six','smart_open','smart_open_hdfs','smart_open_http','smart_open_lib','smart_open_s3','smart_open_webhdfs','sys','urlparse','urlsplit','warnings','webhdfs']
</code></pre>

<p>As you can see there is no 'open' in it so how should I solve this. I tried to install different versions
and I upgraded all version too.</p>",,7,0,,2020/6/3 19:29,1.0,2021/7/10 22:23,,,,,13412418.0,,1,11,deep-learning|nlp|importerror|gensim,10781,68.5306,,4,import name open smart open get error see open solve try install different version upgrade version
417,417,47588682,How to cast a 1-d IntTensor to int in Pytorch,"<p>I get a 1-D IntTensor,but i want to convert it to a integer.
I try it by this method:</p>

<pre><code>print(dictionary[IntTensor.int()])
</code></pre>

<p>but got an error:</p>

<pre><code>KeyError: Variable containing:
 423
[torch.IntTensor of size 1]
</code></pre>

<p>Thanks~</p>",47588776.0,2,1,,2017/12/1 7:38,9.0,2021/3/24 16:34,,,,,3562782.0,,1,24,pytorch|tensor,39858,89.0021,,3,cast inttensor int pytorch get inttensor want convert integer try method get error thanks
236,236,44381450,doubts regarding batch size and time steps in RNN,"<p>In Tensorflow's tutorial of RNN: <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/recurrent</a>
. It mentions two parameters: batch size and time steps. I am confused by the concepts. In my opinion, RNN introduces batch is because the fact that the to-train sequence can be very long such that backpropagation cannot compute that long(exploding/vanishing gradients). So we divide the long to-train sequence into shorter sequences, each of which is a mini-batch and whose size is called ""batch size"". Am I right here?</p>

<p>Regarding time steps, RNN consists of only a cell (LSTM or GRU cell, or other cell) and this cell is sequential. We can understand the sequential concept by unrolling it. But unrolling a sequential cell is a concept, not real which means we do not implement it in unroll way. Suppose the to-train sequence is a text corpus. Then we feed one word each time to the RNN cell and then update the weights. So why do we have time steps here? Combining my understanding of the above ""batch size"", I am even more confused. Do we feed the cell one word or multiple words (batch size)?</p>",,3,0,,2017/6/6 4:32,18.0,2018/3/22 3:54,,,,,665023.0,,1,16,recurrent-neural-network,17371,51.5593,,3,doubt regard batch size time step rnn tensorflow tutorial rnn mention two parameter batch size time step confuse concept opinion rnn introduces batch fact train sequence long backpropagation compute long explode vanish gradient divide long train sequence short sequence mini batch whose size call batch size right regard time step rnn consists cell lstm gru cell cell cell sequential understand sequential concept unroll unroll sequential cell concept real mean implement unroll way suppose train sequence text corpus fee one word time rnn cell update weight time step combine understanding batch size even confused fee cell one word multiple word batch size
554,554,48547688,Tensorflow Keras Copy Weights From One Model to Another,"<p>Using Keras from Tensorflow 1.4.1, how does one copy weights from one model to another?</p>

<p>As some background, I'm trying to implement a deep-q network (DQN) for Atari games following the DQN publication by DeepMind.  My understanding is that the implementation uses two networks, Q and Q'.  The weights of Q are trained using gradient descent, and then the weights are copied periodically to Q'.</p>

<p>Here's how I build Q and Q':</p>

<pre><code>ACT_SIZE   = 4
LEARN_RATE = 0.0025
OBS_SIZE   = 128

def buildModel():
  model = tf.keras.models.Sequential()

  model.add(tf.keras.layers.Lambda(lambda x: x / 255.0, input_shape=OBS_SIZE))
  model.add(tf.keras.layers.Dense(128, activation=""relu""))
  model.add(tf.keras.layers.Dense(128, activation=""relu""))
  model.add(tf.keras.layers.Dense(ACT_SIZE, activation=""linear""))
  opt = tf.keras.optimizers.RMSprop(lr=LEARN_RATE)

  model.compile(loss=""mean_squared_error"", optimizer=opt)

  return model
</code></pre>

<p>I call that twice to get Q and Q'.</p>

<p>I have an <code>updateTargetModel</code> method below that is my attempt at copying weights.  The code runs fine, but my overall DQN implementation is failing.  I'm really just trying to verify if this is a valid way of copying weights from one network to another.</p>

<pre><code>def updateTargetModel(model, targetModel):
  modelWeights       = model.trainable_weights
  targetModelWeights = targetModel.trainable_weights

  for i in range(len(targetModelWeights)):
    targetModelWeights[i].assign(modelWeights[i])
</code></pre>

<p>There's another question here that discusses saving and loading weights to and from disk (<a href=""https://stackoverflow.com/questions/38065448/tensorflow-copy-weights-issue"">Tensorflow Copy Weights Issue</a>), but there's no accepted answer.  There is also a question about loading weights from individual layers (<a href=""https://stackoverflow.com/questions/47667541/copying-weights-from-one-conv2d-layer-to-another"">Copying weights from one Conv2D layer to another</a>), but I'm wanting to copy the entire model's weights.</p>",48552179.0,1,2,,2018/1/31 17:08,3.0,2018/5/22 21:02,2018/4/24 11:35,,5974433.0,,3977276.0,,1,27,python-3.x|tensorflow|machine-learning|neural-network|keras,22913,88.8403,,3,tensorflow kera copy weight one model another use kera tensorflow one copy weight one model another background try implement deep q network dqn atari game follow dqn publication deepmind understanding implementation use two network q q weight q train use gradient descent weight copy periodically q build q q call twice get q q method attempt copy weight code run fine overall dqn implementation fail really try verify valid way copy weight one network another another question discuss save load weight disk tensorflow copy weight issue accepted answer also question load weight individual layer copy weight one conv layer another want copy entire model weight
446,446,48225729,ImportError('Could not import PIL.Image. ' working with keras-ternsorflow,"<p>I'm following some lectures from lynda.com about deep learning using Keras-TensorFlow in a PyCharmCE enviroment and they didn't have this problem.
I get this error:</p>
<blockquote>
<p>raise ImportError('Could not import PIL.Image. '
ImportError: Could not import PIL.Image. The use of <code>array_to_img</code> requires PIL.</p>
</blockquote>
<p>I have checked if others get the same error, but for me installing pillow using pip with the command <code>pip install Pillow</code> doesn't solve anything.</p>
<blockquote>
<p>MacBook-Pro-de-Rogelio:~ Rogelio$ pip install Pillow
Requirement already satisfied: Pillow in ./anaconda3/lib/python3.6/site-packages
MacBook-Pro-de-Rogelio:~ Rogelio$</p>
</blockquote>
<p>Any solution?</p>",,13,3,,2018/1/12 11:49,9.0,2021/4/7 6:57,2021/4/7 6:57,,3964927.0,,9208685.0,,1,62,image-processing|machine-learning|keras,82302,340.862,,1,importerror could import pil image work kera ternsorflow follow lecture lynda com deep learning use kera tensorflow pycharmce enviroment problem get error raise importerror could import pil image importerror could import pil image use require pil check others get error instal pillow use pip command solve anything macbook pro de rogelio rogelio pip install pillow requirement already satisfy pillow anaconda lib python site package macbook pro de rogelio rogelio solution
564,564,48893528,keras vs. tensorflow.python.keras - which one to use?,"<p>Which one is the recommended (or more future-proof) way to use Keras?</p>

<p>What are the advantages/disadvantages of each?</p>

<p>I guess there are more differences than simply saving one <code>pip install</code> step and writing <code>tensorflow.python.keras</code> instead of <code>keras</code>.</p>",49073626.0,2,0,,2018/2/20 20:16,8.0,2019/10/29 11:43,,,,,1866775.0,,1,32,python|tensorflow|pip|deep-learning|keras,5931,73.8925,,3,kera vs tensorflow python keras one use one recommended future proof way use keras advantage disadvantage guess difference simply save one step write instead
67,67,55627780,Evaluating pytorch models: `with torch.no_grad` vs `model.eval()`,"<p>When I want to evaluate the performance of my model on the validation set, is it preferred to use <code>with torch.no_grad:</code> or <code>model.eval()</code>?</p>",55627781.0,2,1,,2019/4/11 8:16,20.0,2021/5/9 13:04,2021/5/9 13:04,,9067615.0,,5353461.0,,1,43,python|machine-learning|deep-learning|pytorch|autograd,12231,89.5499,,5,evaluate pytorch model torch grad v model eval want evaluate performance model validation set prefer use
419,419,47605558,ImportError: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work,"<p>
I have seen similar issue but it is not solved either, so I decided to ask.</p>

<p>I am trying to visualize my model in keras using</p>

<pre><code>from keras.utils import plot_model
plot_model(model, to_file='model.png')
</code></pre>

<p>First, it showed error</p>

<pre><code>ImportError: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.
</code></pre>

<p>Accordingly, I installed  pydot and graphviz through Anaconda prompt activating my environment using</p>

<pre><code>conda install -c https://conda.binstar.org/t/TOKEN/j14r pydot
conda install -c https://conda.binstar.org/t/TOKEN/j14r graphviz
</code></pre>

<p>Then, I closed spyder and reopened it. When I run code snippet, it is still showing the same error. 
What am I missing?</p>",,16,7,,2017/12/2 7:15,7.0,2021/6/22 13:13,2017/12/2 7:45,,6002424.0,,6002424.0,,1,48,keras|graphviz|importerror|pydot,51228,255.438,,1,importerror fail import pydot must install pydot graphviz pydotprint work see similar issue solve either decide ask try visualize model kera use first show error accordingly instal pydot graphviz anaconda prompt activate environment use close spyder reopen run code snippet still show error miss
182,182,43306323,Keras Conv2D and input channels,"<p>The Keras layer documentation specifies the input and output sizes for convolutional layers:
<a href=""https://keras.io/layers/convolutional/"" rel=""noreferrer"">https://keras.io/layers/convolutional/</a></p>

<p>Input shape: <code>(samples, channels, rows, cols)</code></p>

<p>Output shape: <code>(samples, filters, new_rows, new_cols)</code></p>

<p>And the kernel size is a spatial parameter, i.e. detemines only width and height.</p>

<p>So an input with <code>c</code> channels will yield an output with <code>filters</code> channels regardless of the value of <code>c</code>. It must therefore apply 2D convolution with a spatial <code>height x width</code> filter and then aggregate the results somehow for each learned filter. </p>

<p>What is this aggregation operator? is it a summation across channels? can I control it? I couldn't find any information on the Keras documentation.</p>

<ul>
<li>Note that in TensorFlow the filters are specified in the depth channel as well:
<a href=""https://www.tensorflow.org/api_guides/python/nn#Convolution"" rel=""noreferrer"">https://www.tensorflow.org/api_guides/python/nn#Convolution</a>,
So the depth operation is clear.</li>
</ul>

<p>Thanks.</p>",45055094.0,3,4,,2017/4/9 11:46,17.0,2018/12/26 9:38,,,,,635622.0,,1,49,python|keras,32291,155.436,,3,kera conv input channel kera layer documentation specify input output size convolutional layer input shape output shape kernel size spatial parameter e detemines width height input channel yield output channel regardless value must therefore apply convolution spatial filter aggregate result somehow learn filter aggregation operator summation across channel control could find information keras documentation note tensorflow filter specify depth channel well convolution depth operation clear thanks
682,682,38286717,"TensorFlow - regularization with L2 loss, how to apply to all weights, not just last one?","<p>I am playing with a ANN which is part of Udacity DeepLearning course.</p>

<p>I have an assignment which involves introducing generalization to the network with one hidden ReLU layer using L2 loss. I wonder how to properly introduce it so that ALL weights are penalized, not only weights of the output layer.</p>

<p>Code for network <em>without</em> generalization is at the bottom of the post (code to actually run the training is out of the scope of the question).</p>

<p>Obvious way of introducing the L2 is to replace the loss calculation with something like this (if beta is 0.01):</p>

<pre><code>loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels) + 0.01*tf.nn.l2_loss(out_weights))
</code></pre>

<p>But in such case it will take into account values of output layer's weights. I am not sure, how do we properly penalize the weights which come INTO the hidden ReLU layer. Is it needed at all or introducing penalization of output layer will somehow keep the hidden weights in check also?</p>

<pre><code>#some importing
from __future__ import print_function
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle
from six.moves import range

#loading data
pickle_file = '/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle'

with open(pickle_file, 'rb') as f:
  save = pickle.load(f)
  train_dataset = save['train_dataset']
  train_labels = save['train_labels']
  valid_dataset = save['valid_dataset']
  valid_labels = save['valid_labels']
  test_dataset = save['test_dataset']
  test_labels = save['test_labels']
  del save  # hint to help gc free up memory
  print('Training set', train_dataset.shape, train_labels.shape)
  print('Validation set', valid_dataset.shape, valid_labels.shape)
  print('Test set', test_dataset.shape, test_labels.shape)


#prepare data to have right format for tensorflow
#i.e. data is flat matrix, labels are onehot

image_size = 28
num_labels = 10

def reformat(dataset, labels):
  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('Training set', train_dataset.shape, train_labels.shape)
print('Validation set', valid_dataset.shape, valid_labels.shape)
print('Test set', test_dataset.shape, test_labels.shape)


#now is the interesting part - we are building a network with
#one hidden ReLU layer and out usual output linear layer

#we are going to use SGD so here is our size of batch
batch_size = 128

#building tensorflow graph
graph = tf.Graph()
with graph.as_default():
      # Input data. For the training data, we use a placeholder that will be fed
  # at run time with a training minibatch.
  tf_train_dataset = tf.placeholder(tf.float32,
                                    shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  #now let's build our new hidden layer
  #that's how many hidden neurons we want
  num_hidden_neurons = 1024
  #its weights
  hidden_weights = tf.Variable(
    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))
  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))

  #now the layer itself. It multiplies data by weights, adds biases
  #and takes ReLU over result
  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)

  #time to go for output linear layer
  #out weights connect hidden neurons to output labels
  #biases are added to output labels  
  out_weights = tf.Variable(
    tf.truncated_normal([num_hidden_neurons, num_labels]))  

  out_biases = tf.Variable(tf.zeros([num_labels]))  

  #compute output  
  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases
  #our real output is a softmax of prior result
  #and we also compute its cross-entropy to get our loss
  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(out_layer, tf_train_labels))

  #now we just minimize this loss to actually train the network
  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

  #nice, now let's calculate the predictions on each dataset for evaluating the
  #performance so far
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(out_layer)
  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)
  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) 

  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)
  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)
</code></pre>",38287616.0,3,1,,2016/7/9 22:00,31.0,2020/3/19 11:10,,,,,3633250.0,,1,67,machine-learning|neural-network|tensorflow|deep-learning|regularized,74156,246.681,,4,tensorflow regularization l loss apply weight last one play ann part udacity deeplearning course assignment involve introduce generalization network one hidden relu layer use l loss wonder properly introduce weight penalize weight output layer code network without generalization bottom post code actually run training scope question obvious way introduce l replace loss calculation something like beta case take account value output layer weight sure properly penalize weight come hidden relu layer need introduce penalization output layer somehow keep hidden weight check also
526,526,35382409,ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32,"<p>I get following error </p>

<pre><code>ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(""Placeholder_1:0"", shape=TensorShape([Dimension(128), Dimension(2)]), dtype=int32)'
</code></pre>

<p>when I try to calculate cross entropy loss</p>

<pre><code>losses = tf.nn.softmax_cross_entropy_with_logits(scores, input_y)
</code></pre>

<p>I use Python 3.4.3. </p>

<p>Any ideas why? </p>",35382604.0,2,0,,2016/2/13 16:35,3.0,2021/4/11 14:42,2020/1/23 18:00,,3924118.0,,5318180.0,,1,20,neural-network|tensorflow|conv-neural-network,30553,53.9402,,4,valueerror tensor conversion request dtype float tensor dtype int get follow error try calculate cross entropy loss use python idea
496,496,32177764,What is `weight_decay` meta parameter in Caffe?,"<p>Looking at an example <a href=""https://github.com/BVLC/caffe/blob/tutorial/examples/cifar10/cifar10_full_solver.prototxt#L15"" rel=""noreferrer""><code>'solver.prototxt'</code></a>, posted on BVLC/caffe git, there is a training meta parameter</p>

<pre><code>weight_decay: 0.04
</code></pre>

<p>What does this meta parameter mean? And what value should I assign to it?</p>",32178158.0,2,0,,2015/8/24 8:33,12.0,2017/9/23 6:00,2017/1/4 14:36,,3250829.0,,1714410.0,,1,24,machine-learning|neural-network|deep-learning|caffe|gradient-descent,35187,87.7855,,4,weight decay meta parameter caffe look example post bvlc caffe git training meta parameter meta parameter mean value assign
112,112,41711190,"Keras, How to get the output of each layer?","<p>I have trained a binary classification model with CNN, and here is my code</p>

<pre><code>model = Sequential()
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                        border_mode='valid',
                        input_shape=input_shape))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=pool_size))
# (16, 16, 32)
model.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters*2, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=pool_size))
# (8, 8, 64) = (2048)
model.add(Flatten())
model.add(Dense(1024))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(2))  # define a binary classification problem
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.fit(x_train, y_train,
          batch_size=batch_size,
          nb_epoch=nb_epoch,
          verbose=1,
          validation_data=(x_test, y_test))
</code></pre>

<p>And here, I wanna get the output of each layer just like TensorFlow, how can I do that?</p>",41712013.0,12,0,,2017/1/18 4:07,116.0,2021/5/14 15:36,,,,,5046896.0,,1,189,python|tensorflow|deep-learning|keras,230198,935.048,,5,keras get output layer train binary classification model cnn code wan na get output layer like tensorflow
504,504,33687103,How to install Theano on Anaconda Python 2.7 x64 on Windows?,"<p>I wonder how to install Theano on Anaconda Python 2.7 x64 on Windows 7 x64. The Theano website provides some <a href=""http://deeplearning.net/software/theano/install_windows.html"">instructions</a> but is not clear as to what is specific to Anaconda.</p>",33706634.0,4,0,,2015/11/13 6:31,10.0,2020/1/5 19:07,,,,,395857.0,,1,27,python-2.7|windows-7|anaconda|theano,47570,91.3093,,1,install theano anaconda python x window wonder install theano anaconda python x window x theano website provide instruction clear specific anaconda
739,739,40785224,Tensorflow: Cannot interpret feed_dict key as Tensor,"<p>I am trying to build a neural network model with one hidden layer (1024 nodes). The hidden layer is nothing but a relu unit. I am also processing the input data in batches of 128. </p>

<p>The inputs are images of size 28 * 28. In the following code I get the error in line</p>

<pre><code>_, c = sess.run([optimizer, loss], feed_dict={x: batch_x, y: batch_y})
Error: TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(""Placeholder_64:0"", shape=(128, 784), dtype=float32) is not an element of this graph.
</code></pre>

<p>Here is the code I have written</p>

<pre class=""lang-py prettyprint-override""><code>#Initialize

batch_size = 128

layer1_input = 28 * 28
hidden_layer1 = 1024
num_labels = 10
num_steps = 3001

#Create neural network model
def create_model(inp, w, b):
    layer1 = tf.add(tf.matmul(inp, w['w1']), b['b1'])
    layer1 = tf.nn.relu(layer1)
    layer2 = tf.matmul(layer1, w['w2']) + b['b2']
    return layer2

#Initialize variables
x = tf.placeholder(tf.float32, shape=(batch_size, layer1_input))
y = tf.placeholder(tf.float32, shape=(batch_size, num_labels))

w = {
'w1': tf.Variable(tf.random_normal([layer1_input, hidden_layer1])),
'w2': tf.Variable(tf.random_normal([hidden_layer1, num_labels]))
}
b = {
'b1': tf.Variable(tf.zeros([hidden_layer1])),
'b2': tf.Variable(tf.zeros([num_labels]))
}

init = tf.initialize_all_variables()
train_prediction = tf.nn.softmax(model)

tf_valid_dataset = tf.constant(valid_dataset)
tf_test_dataset = tf.constant(test_dataset)

model = create_model(x, w, b)

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(model, y))    
optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

#Process
with tf.Session(graph=graph1) as sess:
    tf.initialize_all_variables().run()
    total_batch = int(train_dataset.shape[0] / batch_size)

    for epoch in range(num_steps):    
        loss = 0
        for i in range(total_batch):
            batch_x, batch_y = train_dataset[epoch * batch_size:(epoch+1) * batch_size, :], train_labels[epoch * batch_size:(epoch+1) * batch_size,:]

            _, c = sess.run([optimizer, loss], feed_dict={x: batch_x, y: batch_y})
            loss = loss + c
        loss = loss / total_batch
        if epoch % 500 == 0:
            print (""Epoch :"", epoch, "". cost = {:.9f}"".format(avg_cost))
            print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))
            valid_prediction = tf.run(tf_valid_dataset, {x: tf_valid_dataset})
            print(""Validation accuracy: %.1f%%"" % accuracy(valid_prediction.eval(), valid_labels))
    test_prediction = tf.run(tf_test_dataset,  {x: tf_test_dataset})
    print(""TEST accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))
</code></pre>",40785890.0,8,1,,2016/11/24 11:26,9.0,2021/5/19 18:28,2018/4/7 17:08,,155137.0,,762819.0,,1,29,neural-network|tensorflow|deep-learning,47650,175.112,,4,tensorflow interpret feed dict key tensor try build neural network model one hidden layer node hidden layer nothing relu unit also process input data batch input image size following code get error line code write
195,195,43589842,Test score vs test accuracy when evaluating model using Keras,"<p>Im using a neural network implemented with the Keras library and below is the results during training. At the end it prints a test score and a test accuracy. I can't figure out exactly what the score represents, but the accuracy I assume to be the number of predictions that was correct when running the test.</p>

<blockquote>
  <p>Epoch 1/15 1200/1200 [==============================] - 4s - loss:
  0.6815 - acc: 0.5550 - val_loss: 0.6120 - val_acc: 0.7525</p>
  
  <p>Epoch 2/15 1200/1200 [==============================] - 3s - loss:
  0.5481 - acc: 0.7250 - val_loss: 0.4645 - val_acc: 0.8025</p>
  
  <p>Epoch 3/15 1200/1200 [==============================] - 3s - loss:
  0.5078 - acc: 0.7558 - val_loss: 0.4354 - val_acc: 0.7975</p>
  
  <p>Epoch 4/15 1200/1200 [==============================] - 3s - loss:
  0.4603 - acc: 0.7875 - val_loss: 0.3978 - val_acc: 0.8350</p>
  
  <p>Epoch 5/15 1200/1200 [==============================] - 3s - loss:
  0.4367 - acc: 0.7992 - val_loss: 0.3809 - val_acc: 0.8300</p>
  
  <p>Epoch 6/15 1200/1200 [==============================] - 3s - loss:
  0.4276 - acc: 0.8017 - val_loss: 0.3884 - val_acc: 0.8350</p>
  
  <p>Epoch 7/15 1200/1200 [==============================] - 3s - loss:
  0.3975 - acc: 0.8167 - val_loss: 0.3666 - val_acc: 0.8400</p>
  
  <p>Epoch 8/15 1200/1200 [==============================] - 3s - loss:
  0.3916 - acc: 0.8183 - val_loss: 0.3753 - val_acc: 0.8450</p>
  
  <p>Epoch 9/15 1200/1200 [==============================] - 3s - loss:
  0.3814 - acc: 0.8233 - val_loss: 0.3505 - val_acc: 0.8475</p>
  
  <p>Epoch 10/15 1200/1200 [==============================] - 3s - loss:
  0.3842 - acc: 0.8342 - val_loss: 0.3672 - val_acc: 0.8450</p>
  
  <p>Epoch 11/15 1200/1200 [==============================] - 3s - loss:
  0.3674 - acc: 0.8375 - val_loss: 0.3383 - val_acc: 0.8525</p>
  
  <p>Epoch 12/15 1200/1200 [==============================] - 3s - loss:
  0.3624 - acc: 0.8367 - val_loss: 0.3423 - val_acc: 0.8650</p>
  
  <p>Epoch 13/15 1200/1200 [==============================] - 3s - loss:
  0.3497 - acc: 0.8475 - val_loss: 0.3069 - val_acc: 0.8825</p>
  
  <p>Epoch 14/15 1200/1200 [==============================] - 3s - loss:
  0.3406 - acc: 0.8500 - val_loss: 0.2993 - val_acc: 0.8775</p>
  
  <p>Epoch 15/15 1200/1200 [==============================] - 3s - loss:
  0.3252 - acc: 0.8600 - val_loss: 0.2960 - val_acc: 0.8775</p>
  
  <p>400/400 [==============================] - 0s</p>
  
  <p>Test score: 0.299598811865</p>
  
  <p>Test accuracy: 0.88</p>
</blockquote>

<p>Looking at the <a href=""https://keras.io/models/model/"" rel=""noreferrer"">Keras documentation</a>, I still don't understand what score is. For the evaluate function, it says: </p>

<blockquote>
  <p>Returns the loss value &amp; metrics values for the model in test mode.</p>
</blockquote>

<p>One thing I noticed is that when the test accuracy is lower, the score is higher, and when accuracy is higher, the score is lower.</p>",,2,3,,2017/4/24 13:45,6.0,2018/6/19 4:54,,,,,7714735.0,,1,23,neural-network|keras,39073,57.5675,,4,test score vs test accuracy evaluate model use kera im use neural network implement kera library result training end print test score test accuracy figure exactly score represent accuracy assume number prediction correct run test epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc epoch loss acc val loss val acc test score test accuracy look keras documentation still understand score evaluate function say return loss value metric value model test mode one thing notice test accuracy low score high accuracy high score low
28,28,54011173,What is the default weight initializer in Keras?,"<p>I just read about the Keras weight initializers in <a href=""https://keras.io/initializers/"" rel=""noreferrer"">here</a>. In the documentation, only different initializers has been introduced. Such as:</p>

<pre><code>model.add(Dense(64, kernel_initializer='random_normal'))
</code></pre>

<p>I want to know what is the <strong>default weight</strong> when I don't specify the <code>kernel_initializer</code> argument. 
Is there a way to access it?</p>",54011300.0,1,3,,2019/1/2 18:11,8.0,2020/8/3 22:35,2019/10/14 3:29,,2512500.0,,2512500.0,,1,26,python|machine-learning|keras|neural-network,33896,63.3206,,3,default weight initializer kera read kera weight initializers documentation different initializers introduce want know default weight specify argument way access
519,519,34987509,TensorFlow: Max of a tensor along an axis,"<p>My question is in two connected parts:</p>

<ol>
<li><p>How do I calculate the max along a certain axis of a tensor? For example, if I have</p>

<pre><code>x = tf.constant([[1,220,55],[4,3,-1]])
</code></pre>

<p>I want something like </p>

<pre><code>x_max = tf.max(x, axis=1)
print sess.run(x_max)

output: [220,4]
</code></pre>

<p>I know there is a <code>tf.argmax</code> and a <code>tf.maximum</code>, but neither give the maximum value along an axis of a single tensor. For now I have a workaround:</p>

<pre><code>x_max = tf.slice(x, begin=[0,0], size=[-1,1])
for a in range(1,2):
    x_max = tf.maximum(x_max , tf.slice(x, begin=[0,a], size=[-1,1]))
</code></pre>

<p>But it looks less than optimal. Is there a better way to do this?</p></li>
<li><p>Given the indices of an <code>argmax</code> of a tensor, how do I index into another tensor using those indices? Using the example of <code>x</code> above, how do I do something like the following:</p>

<pre><code>ind_max = tf.argmax(x, dimension=1)    #output is [1,0]
y = tf.constant([[1,2,3], [6,5,4])
y_ = y[:, ind_max]                     #y_ should be [2,6]
</code></pre>

<p>I know slicing, like the last line, does not exist in TensorFlow yet (<a href=""https://github.com/tensorflow/tensorflow/issues/206"" rel=""noreferrer"">#206</a>). </p>

<p>My question is: <em>what is the best workaround for my specific case (maybe using other methods like gather, select, etc.)?</em></p>

<p>Additional information: I know <code>x</code> and <code>y</code> are going to be two dimensional tensors only!</p></li>
</ol>",34988069.0,2,0,,2016/1/25 7:49,6.0,2018/10/12 18:11,2018/10/12 18:11,,2956066.0,,5813490.0,,1,31,python|tensorflow|deep-learning|max|tensor,58455,99.4673,,3,tensorflow max tensor along axis question two connected part calculate max along certain axis tensor example want something like know neither give maximum value along axis single tensor workaround look less optimal good way give index tensor index another tensor use index use example something like follow know slice like last line exist tensorflow yet question best workaround specific case maybe use method like gather select etc additional information know go two dimensional tensor
709,709,39691902,Ordering of batch normalization and dropout?,"<p><em>The original question was in regard to TensorFlow implementations specifically. However, the answers are for implementations in general. This general answer is also the correct answer for TensorFlow.</em></p>

<p>When using batch normalization and dropout in TensorFlow (specifically using the contrib.layers) do I need to be worried about the ordering?</p>

<p>It seems possible that if I use dropout followed immediately by batch normalization there might be trouble. For example, if the shift in the batch normalization trains to the larger scale numbers of the training outputs, but then that same shift is applied to the smaller (due to the compensation for having more outputs) scale numbers without dropout during testing, then that shift may be off. Does the TensorFlow batch normalization layer automatically compensate for this? Or does this not happen for some reason I'm missing?</p>

<p>Also, are there other pitfalls to look out for in when using these two together? For example, assuming I'm using them in the correct order in regards to the above (assuming there <em>is</em> a correct order), could there be trouble with using both batch normalization and dropout on multiple successive layers? I don't immediately see a problem with that, but I might be missing something.</p>

<p>Thank you much!</p>

<p><strong>UPDATE:</strong></p>

<p>An experimental test <em>seems</em> to suggest that ordering <em>does</em> matter. I ran the same network twice with only the batch norm and dropout reverse. When the dropout is before the batch norm, validation loss seems to be going up as training loss is going down. They're both going down in the other case. But in my case the movements are slow, so things may change after more training and it's just a single test. A more definitive and informed answer would still be appreciated.</p>",40295999.0,9,0,,2016/9/25 21:12,93.0,2020/7/25 2:38,2019/2/3 21:14,,1191087.0,,1191087.0,,1,174,python|neural-network|tensorflow|conv-neural-network,105324,640.29,,3,ordering batch normalization dropout original question regard tensorflow implementation specifically however answer implementation general general answer also correct answer tensorflow use batch normalization dropout tensorflow specifically use contrib layer need worry order seem possible use dropout follow immediately batch normalization might trouble example shift batch normalization train large scale number training output shift apply small due compensation output scale number without dropout test shift may tensorflow batch normalization layer automatically compensate happen reason miss also pitfall look use two together example assume use correct order regard assuming correct order could trouble use batch normalization dropout multiple successive layer immediately see problem might miss something thank much update experimental test seem suggest order matter run network twice batch norm dropout reverse dropout batch norm validation loss seem go training loss go go case case movement slow thing may change training single test definitive informed answer would still appreciate
91,91,41399481,How do you decode one-hot labels in Tensorflow?,"<p>Been looking, but can't seem to find any examples of how to decode or convert back to a single integer from a one-hot value in TensorFlow.</p>

<p>I used <code>tf.one_hot</code> and was able to train my model but am a bit confused on how to make sense of the label after my classification. My data is being fed in via a <code>TFRecords</code> file that I created. I thought about storing a text label in the file but wasn't able to get it to work. It appeared as if <code>TFRecords</code> couldn't store text string or maybe I was mistaken.</p>",41399559.0,4,1,,2016/12/30 16:30,5.0,2020/5/4 7:21,2020/5/4 7:21,,5763590.0,,6112394.0,,1,11,python|tensorflow|machine-learning|deep-learning|one-hot-encoding,22076,59.1757,,3,decode one hot label tensorflow look seem find example decode convert back single integer one hot value tensorflow use able train model bit confuse make sense label classification data feed via file create think store text label file able get work appear could store text string maybe mistake
832,832,48284427,Why should we normalize data for deep learning in Keras?,"<p>I was testing some network architectures in Keras for classifying the MNIST dataset. I have implemented one that is similar to the LeNet.</p>

<p>I have seen that in the examples that I have found on the internet, there is a step of data normalization. For example:</p>

<pre><code>X_train /= 255
</code></pre>

<p>I have performed a test without this normalization and I have seen that the performance (accuracy) of the network has decreased (keeping the same number of epochs). Why has this happened? </p>

<p>If I increase the number of epochs, the accuracy can reach the same level reached by the model trained with normalization?</p>

<p>So, the normalization affects the accuracy, or only the training speed?</p>

<p>The complete source code of my training script is below:</p>

<pre><code>from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras.datasets import mnist
from keras.utils import np_utils
from keras.optimizers import SGD, RMSprop, Adam
import numpy as np
import matplotlib.pyplot as plt
from keras import backend as k


def build(input_shape, classes):
    model = Sequential()

    model.add(Conv2D(20, kernel_size=5, padding=""same"",activation='relu',input_shape=input_shape))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Conv2D(50, kernel_size=5, padding=""same"", activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

    model.add(Flatten())
    model.add(Dense(500))
    model.add(Activation(""relu""))

    model.add(Dense(classes))
    model.add(Activation(""softmax""))

    return model


NB_EPOCH = 4 # number of epochs
BATCH_SIZE = 128 # size of the batch
VERBOSE = 1 # set the training phase as verbose
OPTIMIZER = Adam() # optimizer
VALIDATION_SPLIT=0.2 # percentage of the training data used for 
evaluating the loss function
IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions
NB_CLASSES = 10 # number of outputs = number of digits
INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS) # shape of the input

(X_train, y_train), (X_test, y_test) = mnist.load_data()

k.set_image_dim_ordering(""th"")

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

X_train = X_train[:, np.newaxis, :, :]
X_test = X_test[:, np.newaxis, :, :]
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

y_train = np_utils.to_categorical(y_train, NB_CLASSES)
y_test = np_utils.to_categorical(y_test, NB_CLASSES)

model = build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)
model.compile(loss=""categorical_crossentropy"", 
optimizer=OPTIMIZER,metrics=[""accuracy""])

history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)

model.save(""model2"")

score = model.evaluate(X_test, y_test, verbose=VERBOSE)
print('Test accuracy:', score[1])
</code></pre>",,3,3,,2018/1/16 15:15,7.0,2021/5/13 13:39,2020/1/7 16:49,,3924118.0,,1286970.0,,1,22,machine-learning|neural-network|deep-learning|keras|conv-neural-network,22043,71.5731,,3,normalize data deep learning kera test network architecture kera classify mnist dataset implement one similar lenet see example find internet step data normalization example perform test without normalization see performance accuracy network decrease keep number epochs happen increase number epoch accuracy reach level reach model train normalization normalization affect accuracy training speed complete source code training script
66,66,55563376,Pytorch. How does pin_memory work in Dataloader?,"<p>I want to understand how pin_memory in Dataloader works.</p>
<p>According to the documentation:</p>
<pre><code>pin_memory (bool, optional) 闂?If True, the data loader will copy tensors into CUDA pinned memory before returning them.
</code></pre>
<p>Below is a self-contained code example.</p>
<pre><code>import torchvision
import torch

print('torch.cuda.is_available()', torch.cuda.is_available())
train_dataset = torchvision.datasets.CIFAR10(root='cifar10_pytorch', download=True, transform=torchvision.transforms.ToTensor())
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, pin_memory=True)
x, y = next(iter(train_dataloader))
print('x.device', x.device)
print('y.device', y.device)
</code></pre>
<p>Producing the following output:</p>
<pre><code>torch.cuda.is_available() True
x.device cpu
y.device cpu
</code></pre>
<p>But I was expecting something like this, because I specified flag <code>pin_memory=True</code> in <code>Dataloader</code>.</p>
<pre><code>torch.cuda.is_available() True
x.device cuda:0
y.device cuda:0
</code></pre>
<p>Also I run some benchmark:</p>
<pre><code>import torchvision
import torch
import time
import numpy as np

pin_memory=True
train_dataset =torchvision.datasets.CIFAR10(root='cifar10_pytorch', download=True, transform=torchvision.transforms.ToTensor())
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, pin_memory=pin_memory)
print('pin_memory:', pin_memory)
times = []
n_runs = 10
for i in range(n_runs):
    st = time.time()
    for bx, by in train_dataloader:
        bx, by = bx.cuda(), by.cuda()
    times.append(time.time() - st)
print('average time:', np.mean(times))
</code></pre>
<p>I got the following results.</p>
<pre><code>pin_memory: False
average time: 6.5701503753662

pin_memory: True
average time: 7.0254474401474
</code></pre>
<p>So <code>pin_memory=True</code> only makes things slower.
Can someone explain me this behaviour?</p>",55564072.0,1,1,,2019/4/7 20:27,5.0,2021/6/30 14:05,2021/5/5 5:53,,2171857.0,,8509167.0,,1,23,deep-learning|pytorch|torch,17210,61.5431,,3,pytorch pin memory work dataloader want understand pin memory dataloader work accord documentation self contain code example produce following output expect something like specify flag also run benchmark get following result make thing slow someone explain behaviour
396,396,47072859,How to append data to one specific dataset in a hdf5 file with h5py,"<p>I am looking for a possibility to append data to an existing dataset inside a <code>.h5</code> file using Python (<code>h5py</code>).</p>

<p>A short intro to my project: I try to train a CNN using medical image data. Because of the huge amount of data and heavy memory usage during the transformation of the data to NumPy arrays, I needed to split the ""transformation"" into a few data chunks: load and preprocess the first 100 medical images and save the NumPy arrays to hdf5 file, then load the next 100 datasets and append the existing <code>.h5</code> file, and so on.</p>

<p>Now, I tried to store the first 100 transformed NumPy arrays as follows:</p>

<pre><code>import h5py
from LoadIPV import LoadIPV

X_train_data, Y_train_data, X_test_data, Y_test_data = LoadIPV()

with h5py.File('.\PreprocessedData.h5', 'w') as hf:
    hf.create_dataset(""X_train"", data=X_train_data, maxshape=(None, 512, 512, 9))
    hf.create_dataset(""X_test"", data=X_test_data, maxshape=(None, 512, 512, 9))
    hf.create_dataset(""Y_train"", data=Y_train_data, maxshape=(None, 512, 512, 1))
    hf.create_dataset(""Y_test"", data=Y_test_data, maxshape=(None, 512, 512, 1))
</code></pre>

<p>As can be seen, the transformed NumPy arrays are splitted into four different ""groups"" that are stored into the four <code>hdf5</code> datasets<code>[X_train, X_test, Y_train, Y_test]</code>.
The <code>LoadIPV()</code> function performs the preprocessing of the medical image data.</p>

<p>My problem is that I would like to store the next 100 NumPy arrays into the same <code>.h5</code> file into the existing datasets: that means that I would like to append to, for example, the existing <code>X_train</code> dataset of shape <code>[100, 512, 512, 9]</code> with the next 100 NumPy arrays, such that <code>X_train</code> becomes of shape <code>[200, 512, 512, 9]</code>. The same should work for the other three datasets <code>X_test</code>, <code>Y_train</code> and <code>Y_test</code>.</p>",47074545.0,2,0,,2017/11/2 10:23,15.0,2021/6/30 4:34,2019/9/21 19:55,,3924118.0,,8438176.0,,1,56,python|numpy|deep-learning|hdf5|h5py,41081,104.855,,2,append data one specific dataset hdf file h py look possibility append data exist dataset inside file use python short intro project try train cnn use medical image data huge amount data heavy memory usage transformation data numpy array need split transformation data chunk load preprocess first medical image save numpy array hdf file load next datasets append exist file try store first transform numpy array follow see transformed numpy array splitted four different group store four datasets function perform preprocessing medical image data problem would like store next numpy array file exist datasets mean would like append example exist dataset shape next numpy array becomes shape work three datasets
156,156,42733971,Understanding the dimensions of a fully-connected layer that follows a max-pooling layer,"<p>In the diagram (architecture) below, how was the (fully-connected) dense layer of 4096 units derived from last max-pool layer (on the right) of dimensions <code>256x13x13</code>? Instead of 4096, shouldn't it be 256*13*13=43264 ?</p>

<p><a href=""https://i.stack.imgur.com/aBHPw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aBHPw.png"" alt=""Convolutional Neural Network""></a></p>",,6,1,,2017/3/11 10:18,10.0,2019/2/11 21:53,2019/2/11 21:51,,3924118.0,,7108418.0,,1,21,neural-network|deep-learning|conv-neural-network,17732,85.195,2020/4/30 19:26,0,understand dimension fully connect layer follow max pool layer diagram architecture fully connect dense layer unit derive last max pool layer right dimension instead
802,802,53135439,Issue with add method in tensorflow : AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike',"<pre><code>import keras as K
from keras.models import Sequential
from keras.layers import Dense
from tensorflow import set_random_seed

for hidden_neuron in hidden_neurons:
  model = Sequential()
</code></pre>

<p><code>model.add(Dense(hidden_neuron, input_dim=61, activation='relu'))</code></p>

<p>-> i am getting error at this line. I am not really sure what am i missing here.</p>

<p>Traceback (most recent call last):</p>

<blockquote>
  <p>File ""PycharmProjects/HW2/venv/bin/hw3q4.py"", line 46, in 
      model.add(Dense(hidden_neuron, input_dim=61, activation='relu'))   File
  ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/sequential.py"",
  line 165, in add
      layer(x)   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/base_layer.py"",
  line 414, in <strong>call</strong>
      self.assert_input_compatibility(inputs)   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/base_layer.py"",
  line 279, in assert_input_compatibility
      K.is_keras_tensor(x)   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"",
  line 472, in is_keras_tensor
      if not is_tensor(x):   File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py"",
  line 480, in is_tensor
      return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)
  <strong>AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'</strong></p>
</blockquote>",,7,0,,2018/11/3 20:54,4.0,2021/5/10 8:00,2018/11/3 21:23,,10451097.0,,8443897.0,,1,22,python|python-3.x|tensorflow|keras,35849,99.0179,,4,issue add method tensorflow attributeerror module tensorflow python framework ops attribute tensorlike get error line really sure miss traceback recent call last file pycharmprojects hw venv bin hw q py line model add dense hidden neuron input dim activation arelu file library framework python framework version lib python site package keras engine sequential py line add layer x file library framework python framework version lib python site package keras engine base layer py line call self assert input compatibility input file library framework python framework version lib python site package keras engine base layer py line assert input compatibility k keras tensor x file library framework python framework version lib python site package keras backend tensorflow backend py line keras tensor tensor x file library framework python framework version lib python site package keras backend tensorflow backend py line tensor return isinstance x tf ops tensorlike tf ops dense tensor like x attributeerror module tensorflow python framework ops attribute tensorlike
44,44,54843067,"No module named ""Torch""","<p>I installed pytorch via</p>
<p><code>conda install pytorch-cpu torchvision-cpu -c pytorch</code></p>
<p>And I also tried</p>
<pre><code>pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl

pip3 install torchvision
</code></pre>
<p>Both installed successfully!</p>
<p>But, it only works in jupiter notebook. Whenever I try to execute a script from the console, I get the error message:
No module named &quot;torch&quot;</p>
<p>How can I fix this?</p>",56809312.0,18,3,,2019/2/23 15:21,5.0,2021/7/9 8:54,2020/9/4 21:43,,1839439.0,,11086331.0,,1,27,python|pip|pytorch|conda,146597,172.865,,1,module name torch instal pytorch via also try instal successfully work jupiter notebook whenever try execute script console get error message module name torch fix
393,393,46826218,PyTorch: How to get the shape of a Tensor as a list of int,"<p>In numpy, <code>V.shape</code> gives a tuple of ints of dimensions of V.</p>

<p>In tensorflow <code>V.get_shape().as_list()</code> gives a list of integers of the dimensions of V.</p>

<p>In pytorch, <code>V.size()</code> gives a size object, but how do I convert it to ints?</p>",46826785.0,4,0,,2017/10/19 8:59,12.0,2021/3/9 22:09,,,,,3990607.0,,1,60,python|pytorch|tensor,193748,160.149,,3,pytorch get shape tensor list int numpy give tuple ints dimension v tensorflow give list integer dimension v pytorch give size object convert ints
178,178,43216256,Running Tensorflow in Jupyter Notebook,"<p>I am trying to do some deep learning work. For this, I first installed all the packages for deep learning in my Python environment. </p>

<p>Here is what I did. </p>

<p>In Anaconda, I created an environment called <code>tensorflow</code> as follows</p>

<pre><code>conda create -n tensorflow
</code></pre>

<p>Then installed the data science Python packages, like Pandas, NumPy, etc., inside it. I also installed TensorFlow and Keras there. Here is the list of packages in that environment</p>

<pre><code>(tensorflow) SFOM00618927A:dl i854319$ conda list
# packages in environment at /Users/i854319/anaconda/envs/tensorflow:
#
appdirs                   1.4.3                     &lt;pip&gt;
appnope                   0.1.0                    py36_0  
beautifulsoup4            4.5.3                    py36_0  
bleach                    1.5.0                    py36_0  
cycler                    0.10.0                   py36_0  
decorator                 4.0.11                   py36_0  
entrypoints               0.2.2                    py36_1  
freetype                  2.5.5                         2  
html5lib                  0.999                    py36_0  
icu                       54.1                          0  
ipykernel                 4.5.2                    py36_0  
ipython                   5.3.0                    py36_0  
ipython_genutils          0.2.0                    py36_0  
ipywidgets                6.0.0                    py36_0  
jinja2                    2.9.5                    py36_0  
jsonschema                2.5.1                    py36_0  
jupyter                   1.0.0                    py36_3  
jupyter_client            5.0.0                    py36_0  
jupyter_console           5.1.0                    py36_0  
jupyter_core              4.3.0                    py36_0  
Keras                     2.0.2                     &lt;pip&gt;
libpng                    1.6.27                        0  
markupsafe                0.23                     py36_2  
matplotlib                2.0.0               np112py36_0  
mistune                   0.7.4                    py36_0  
mkl                       2017.0.1                      0  
nbconvert                 5.1.1                    py36_0  
nbformat                  4.3.0                    py36_0  
notebook                  4.4.1                    py36_0  
numpy                     1.12.1                    &lt;pip&gt;
numpy                     1.12.1                   py36_0  
openssl                   1.0.2k                        1  
packaging                 16.8                      &lt;pip&gt;
pandas                    0.19.2              np112py36_1  
pandocfilters             1.4.1                    py36_0  
path.py                   10.1                     py36_0  
pexpect                   4.2.1                    py36_0  
pickleshare               0.7.4                    py36_0  
pip                       9.0.1                    py36_1  
prompt_toolkit            1.0.13                   py36_0  
protobuf                  3.2.0                     &lt;pip&gt;
ptyprocess                0.5.1                    py36_0  
pygments                  2.2.0                    py36_0  
pyparsing                 2.1.4                    py36_0  
pyparsing                 2.2.0                     &lt;pip&gt;
pyqt                      5.6.0                    py36_2  
python                    3.6.1                         0  
python-dateutil           2.6.0                    py36_0  
pytz                      2017.2                   py36_0  
PyYAML                    3.12                      &lt;pip&gt;
pyzmq                     16.0.2                   py36_0  
qt                        5.6.2                         0  
qtconsole                 4.3.0                    py36_0  
readline                  6.2                           2  
scikit-learn              0.18.1              np112py36_1  
scipy                     0.19.0              np112py36_0  
setuptools                34.3.3                    &lt;pip&gt;
setuptools                27.2.0                   py36_0  
simplegeneric             0.8.1                    py36_1  
sip                       4.18                     py36_0  
six                       1.10.0                    &lt;pip&gt;
six                       1.10.0                   py36_0  
sqlite                    3.13.0                        0  
tensorflow                1.0.1                     &lt;pip&gt;
terminado                 0.6                      py36_0  
testpath                  0.3                      py36_0  
Theano                    0.9.0                     &lt;pip&gt;
tk                        8.5.18                        0  
tornado                   4.4.2                    py36_0  
traitlets                 4.3.2                    py36_0  
wcwidth                   0.1.7                    py36_0  
wheel                     0.29.0                    &lt;pip&gt;
wheel                     0.29.0                   py36_0  
widgetsnbextension        2.0.0                    py36_0  
xz                        5.2.2                         1  
zlib                      1.2.8                         3  
(tensorflow) SFOM00618927A:dl i854319$
</code></pre>

<p>You can see that <code>jupyter</code> is also installed.</p>

<p>Now, when I open up the Python interpreter in this environment and I run the basic TensorFlow command, it all works fine.  However, I wanted to do the same thing in the Jupyter notebook. So, I created a new directory (outside of this environment). </p>

<pre><code>mkdir dl
</code></pre>

<p>In that, I activated <code>tensorflow</code> environment</p>

<pre><code>SFOM00618927A:dl i854319$ source activate tensorflow
(tensorflow) SFOM00618927A:dl i854319$ conda list
</code></pre>

<p>And I can see the same list of packages in that. </p>

<p>Now, I open up a Jupyter notebook</p>

<pre><code>SFOM00618927A:dl i854319$ source activate tensorflow
(tensorflow) SFOM00618927A:dl i854319$ jupyter notebook
</code></pre>

<p>It opens up a new notebook in the browser. But when I just import basic python libraries in that, like pandas, it says ""no packages available"". I am not sure why is that when the same environment has all those packages and in the same directory, if I use Python interpreter it shows all packages. </p>

<pre><code>import pandas
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-4-d6ac987968b6&gt; in &lt;module&gt;()
----&gt; 1 import pandas

ModuleNotFoundError: No module named 'pandas'
</code></pre>

<p>Why jupyter notebook is not picking up these modules?</p>

<p>So, Jupyter notebook doesn't show env as the interpreter </p>

<p><a href=""https://i.stack.imgur.com/whyaq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/whyaq.png"" alt=""enter image description here""></a></p>",,11,7,,2017/4/4 19:40,12.0,2021/6/18 8:25,2018/7/2 14:28,,3924118.0,,2769240.0,,1,35,python|tensorflow|jupyter-notebook|keras,173497,194.957,,1,run tensorflow jupyter notebook try deep learning work first instal package deep learning python environment anaconda create environment call follow instal data science python package like panda numpy etc inside also instal tensorflow keras list package environment see also instal open python interpreter environment run basic tensorflow command work fine however want thing jupyter notebook create new directory outside environment activate environment see list package open jupyter notebook open new notebook browser import basic python library like panda say package available sure environment package directory use python interpreter show package jupyter notebook pick module jupyter notebook show env interpreter
493,493,31427094,A guide to convert_imageset.cpp,"<p>I am relatively new to machine learning/python/ubuntu.</p>

<p>I have a set of images in .jpg format where half contain a feature I want caffe to learn and half don't. I'm having trouble in finding a way to convert them to the required lmdb format.</p>

<p>I have the necessary text input files. </p>

<p>My question is can anyone provide a step by step guide on how to use <code>convert_imageset.cpp</code> in the ubuntu terminal?</p>

<p>Thanks</p>",31431716.0,1,0,,2015/7/15 9:53,22.0,2019/7/9 5:21,2019/7/9 5:21,,1714410.0,,5118798.0,,1,33,image-processing|machine-learning|deep-learning|computer-vision|caffe,26611,87.3002,,2,guide convert imageset cpp relatively new machine learn python ubuntu set image jpg format half contain feature want caffe learn half trouble find way convert required lmdb format necessary text input file question anyone provide step step guide use ubuntu terminal thanks
430,430,47840527,Using Tensorflow Huber loss in Keras,"<p>I am trying to use huber loss in a keras model (writing DQN), but I am getting bad result, I think I am something doing wrong. My is code is below.</p>

<pre><code>model = Sequential()
model.add(Dense(output_dim=64, activation='relu', input_dim=state_dim))
model.add(Dense(output_dim=number_of_actions, activation='linear'))
loss = tf.losses.huber_loss(delta=1.0)
model.compile(loss=loss, opt='sgd')
return model
</code></pre>",48791563.0,4,0,,2017/12/15 22:10,4.0,2019/9/16 17:33,,,,,7643343.0,,1,14,python|tensorflow|keras|reinforcement-learning,14842,63.886,,4,use tensorflow huber loss kera try use huber loss keras model write dqn get bad result think something wrong code
114,114,41749398,Using Keras ImageDataGenerator in a regression model,"<p>I want to use the </p>

<pre><code>flow_from_directory
</code></pre>

<p>method of the </p>

<pre><code>ImageDataGenerator
</code></pre>

<p>to generate training data for a regression model, where the target value can be any float value between 1 and -1. </p>

<pre><code>flow_from_directory
</code></pre>

<p>has a ""class_mode"" parameter with the descripton</p>

<blockquote>
  <p>class_mode: one of ""categorical"", ""binary"", ""sparse"" or None. Default:
  ""categorical"". Determines the type of label arrays that are returned:
  ""categorical"" will be 2D one-hot encoded labels, ""binary"" will be 1D
  binary labels, ""sparse"" will be 1D integer labels.</p>
</blockquote>

<p>Which of these values should I take? None of them seems to really fit...</p>",41774345.0,4,4,,2017/1/19 18:44,5.0,2018/11/5 19:56,2017/1/21 13:06,,5974433.0,,1934212.0,,1,21,machine-learning|neural-network|regression|keras,10024,59.8042,,3,use kera imagedatagenerator regression model want use method generate training data regression model target value float value class mode parameter descripton class mode one categorical binary sparse none default categorical determine type label array return categorical one hot encode label binary binary label sparse integer label value take none seem really fit
345,345,45393429,Keras: How to save model and continue training?,"<p>I have a model that I've trained for 40 epochs. I kept checkpoints for each epochs, and I have also saved the model with <code>model.save()</code>. The code for training is:</p>

<pre><code>n_units = 1000
model = Sequential()
model.add(LSTM(n_units, input_shape=(None, vec_size), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(n_units, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(n_units))
model.add(Dropout(0.2))
model.add(Dense(vec_size, activation='linear'))
model.compile(loss='mean_squared_error', optimizer='adam')
# define the checkpoint
filepath=""word2vec-{epoch:02d}-{loss:.4f}.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
model.fit(x, y, epochs=40, batch_size=50, callbacks=callbacks_list)
</code></pre>

<p>However, when I load the model and try training it again, it starts all over as if it hasn't been trained before. The loss doesn't start from the last training.</p>

<p>What confuses me is when I load the model and redefine the model structure and use <code>load_weight</code>, <code>model.predict()</code> works well. Thus, I believe the model weights are loaded:</p>

<pre><code>model = Sequential()
model.add(LSTM(n_units, input_shape=(None, vec_size), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(n_units, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(n_units))
model.add(Dropout(0.2))
model.add(Dense(vec_size, activation='linear'))
filename = ""word2vec-39-0.0027.hdf5""
model.load_weights(filename)
model.compile(loss='mean_squared_error', optimizer='adam')
</code></pre>

<p>However, When I continue training with this, the loss is as high as the initial stage:</p>

<pre><code>filepath=""word2vec-{epoch:02d}-{loss:.4f}.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
model.fit(x, y, epochs=40, batch_size=50, callbacks=callbacks_list)
</code></pre>

<p>I searched and found some examples of saving and loading models <a href=""http://machinelearningmastery.com/save-load-keras-deep-learning-models/"" rel=""noreferrer"">here</a> and <a href=""https://github.com/fchollet/keras/issues/1872"" rel=""noreferrer"">here</a>. However, none of them work.</p>

<hr>

<p><strong>Update 1</strong></p>

<p>I looked at <a href=""https://stackoverflow.com/questions/42666046/loading-a-trained-keras-model-and-continue-training"">this question</a>, tried it and it works: </p>

<pre><code>model.save('partly_trained.h5')
del model
load_model('partly_trained.h5')
</code></pre>

<p>But when I close Python and reopen it, then run <code>load_model</code> again, it fails. The loss is as high as the initial state.</p>

<hr>

<p><strong>Update 2</strong></p>

<p>I tried <a href=""https://stackoverflow.com/a/45428197/5305519"">Yu-Yang's example code</a> and it works. However, when I use my code again, it still failed.</p>

<p>This is result form the original training. The second epoch should start with loss = 3.1***:</p>

<pre><code>13700/13846 [============================&gt;.] - ETA: 0s - loss: 3.0519
13750/13846 [============================&gt;.] - ETA: 0s - loss: 3.0511
13800/13846 [============================&gt;.] - ETA: 0s - loss: 3.0512Epoch 00000: loss improved from inf to 3.05101, saving model to LPT-00-3.0510.h5

13846/13846 [==============================] - 81s - loss: 3.0510    
Epoch 2/60

   50/13846 [..............................] - ETA: 80s - loss: 3.1754
  100/13846 [..............................] - ETA: 78s - loss: 3.1174
  150/13846 [..............................] - ETA: 78s - loss: 3.0745
</code></pre>

<p>I closed Python, reopened it, loaded the model with <code>model = load_model(""LPT-00-3.0510.h5"")</code> then train with:</p>

<pre><code>filepath=""LPT-{epoch:02d}-{loss:.4f}.h5""
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
model.fit(x, y, epochs=60, batch_size=50, callbacks=callbacks_list)
</code></pre>

<p>The loss starts with 4.54:</p>

<pre><code>Epoch 1/60
   50/13846 [..............................] - ETA: 162s - loss: 4.5451
   100/13846 [..............................] - ETA: 113s - loss: 4.3835
</code></pre>",45428197.0,7,3,,2017/7/29 19:48,34.0,2021/7/5 3:06,2020/5/11 23:52,,5305519.0,,7367243.0,,1,47,python|keras,75105,148.303,,4,keras save model continue training model train epoch keep checkpoint epoch also save model code training however load model try train start train loss start last training confuse load model redefine model structure use work well thus believe model weight load however continue train loss high initial stage search find example save load model however none work update look question try work close python reopen run fail loss high initial state update try yu yang example code work however use code still fail result form original training second epoch start loss close python reopen load model train loss start
119,119,41867191,How does one inspect variables in a checkpoint file in TensorFlow when TensorFlow can't find the tools attribute?,"<p>I was trying to inspect checkpoints using the code at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/inspect_checkpoint.py"" rel=""noreferrer"">inspect_checkpoint.py</a>. However, I wasn't able to have it work because they didn't really provide an example. I tried the simplest thing I thought would work:</p>

<pre><code>tf.python.tools.inspect_checkpoint.print_tensors_in_checkpoint_file(file_name='./tmp/mdl_ckpt',tensor_name='',all_tensors='')
</code></pre>

<p>however I get that <code>python</code> has no attribute <code>tools</code>:</p>

<pre><code>AttributeError: module 'tensorflow.python' has no attribute 'tools'
</code></pre>

<p>it seems like a (embarrassingly) trivial bug/issue. Does someone know what is going on? Why can't it find tools? Also, even if it did find it, how would one run the function provided in that file?</p>

<hr>

<p>Unfortunately, the very related question did not really provide an answer of how to get around this issue. The question is here <a href=""https://stackoverflow.com/questions/38218174/how-can-find-the-variable-names-that-saved-in-tensorflow-checkpoint"">How can find the variable names that saved in tensorflow checkpoint?</a></p>",,5,3,,2017/1/26 5:01,7.0,2020/1/3 3:49,2017/5/23 12:24,,-1.0,,1601580.0,,1,11,python|machine-learning|tensorflow|deep-learning,15146,58.7212,,3,one inspect variables checkpoint file tensorflow tensorflow find tool attribute try inspect checkpoint use code inspect checkpoint py however able work really provide example try simple thing think would work however get attribute seem like embarrassingly trivial bug issue someone know go find tool also even find would one run function provide file unfortunately related question really provide answer get around issue question find variable name save tensorflow checkpoint
721,721,40198364,How can I implement a weighted cross entropy loss in tensorflow using sparse_softmax_cross_entropy_with_logits,"<p>I am starting to use tensorflow (coming from Caffe), and I am using the loss <code>sparse_softmax_cross_entropy_with_logits</code>. The function accepts labels like <code>0,1,...C-1</code> instead of onehot encodings. Now, I want to use a weighting depending on the class label; I know that this could be done maybe with a matrix multiplication if I use <code>softmax_cross_entropy_with_logits</code> (one hot encoding), Is there any way to do the same with <code>sparse_softmax_cross_entropy_with_logits</code>?</p>",,3,0,,2016/10/23 0:20,12.0,2018/2/2 17:32,2018/2/2 17:32,,712995.0,,2847699.0,,1,14,python|tensorflow|deep-learning|caffe|cross-entropy,18015,52.4225,,4,implement weighted cross entropy loss tensorflow use sparse softmax cross entropy logits start use tensorflow come caffe use loss function accept label like instead onehot encoding want use weighting depend class label know could maybe matrix multiplication use one hot encoding way
7,7,61550026,"ValueError: Shapes (None, 1) and (None, 3) are incompatible","<p>I have a 3 dimensional dataset of audio files where <code>X.shape</code> is <code>(329,20,85)</code>. I want to have a simpl bare-bones model running, so please don't nitpick and address only the issue at hand. Here is the code:</p>
<pre><code>model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(32, return_sequences=True, stateful=False, input_shape = (20,85,1)))
model.add(tf.keras.layers.LSTM(20))
model.add(tf.keras.layers.Dense(nb_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[&quot;accuracy&quot;])
model.summary()
print(&quot;Train...&quot;)
model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=50, validation_data=(X_test, y_test))
</code></pre>
<p>But then I had the error mentioned in the title:
<code>ValueError: Shapes (None, 1) and (None, 3) are incompatible</code></p>
<p>Here is the <code>model.summary()</code></p>
<pre><code>Model: &quot;sequential_13&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_21 (LSTM)               (None, 20, 32)            15104     
_________________________________________________________________
lstm_22 (LSTM)               (None, 20)                4240      
_________________________________________________________________
dense_8 (Dense)              (None, 3)                 63        
=================================================================
Total params: 19,407
Trainable params: 19,407
Non-trainable params: 0
_________________________________________________________________
Train...
</code></pre>
<p>For this, I followed <a href=""https://stackoverflow.com/questions/61267737/valueerror-shapes-none-50-and-none-1-are-incompatible-in-tensorflow-and-c"">this</a> post and updated Tensorflow to the latest version, but the issue persists. <a href=""https://stackoverflow.com/questions/43901767/valueerror-shapes-2-1-and-are-incompatible"">This</a> post is completely unrelated and highly unreliable.<a href=""https://stackoverflow.com/questions/60789274/tf-estimator-add-metrics-ends-in-shapes-none-12-and-none-are-incompatible"">This</a> post although a bit relatable is unanswered for a while now.</p>
<p><strong>Update 1.0:</strong></p>
<p>I strongly think the problem has something to do with the final <code>Dense</code> layer where I pass nb_classes as 3, since I am classifying for 3 categories in <code>y</code>.</p>
<p>So I changed the <code>Dense</code> layer's <code>nb_classes</code> to 1, which ran the model and gives me this output, which I am positive is wrong.</p>
<pre><code>Train...
9/9 [==============================] - 2s 177ms/step - loss: 0.0000e+00 - accuracy: 0.1520 - val_loss: 0.0000e+00 - val_accuracy: 0.3418

&lt;tensorflow.python.keras.callbacks.History at 0x7f50f1dcebe0&gt;
</code></pre>
<p><strong>Update 2.0:</strong></p>
<p>I one hot encoded the <code>y</code>s and resolved the shape issue. But now the above output with <code>&lt;tensorflow.python.keras.callbacks.History at 0x7f50f1dcebe0&gt;</code> persists. Any help with this? Or should I post a new question for this? Thanks for all the help.</p>
<p>How should I proceed, or what should I be changing?</p>",61550151.0,2,0,,2020/5/1 20:10,5.0,2021/3/23 6:15,2021/1/18 12:41,,10908375.0,,4060622.0,,1,15,python|tensorflow|keras,38562,57.3446,,4,valueerror shape none none incompatible dimensional dataset audio file want simpl bare bone model run please nitpick address issue hand code error mention title follow post update tensorflow late version issue persist post completely unrelated highly unreliable post although bit relatable unanswered update strongly think problem something final layer pass nb class since classify category change layer run model give output positive wrong update one hot encode resolve shape issue output persists help post new question thanks help proceed change
472,472,59129812,"How to avoid ""CUDA out of memory"" in PyTorch","<p>I think it's a pretty common message for PyTorch users with low GPU memory:</p>

<pre><code>RuntimeError: CUDA out of memory. Tried to allocate  MiB (GPU ;  GiB total capacity;  GiB already allocated;  MiB free;  cached)
</code></pre>

<p>I want to research object detection algorithms for my coursework. And many deep learning architectures require a large capacity of GPU-memory, so my machine can't train those models. I tried to process an image by loading each layer to GPU and then loading it back:</p>

<pre class=""lang-py prettyprint-override""><code>for m in self.children():
   m.cuda()
   X = m(X)
   m.cpu()
   torch.cuda.empty_cache()
</code></pre>

<p>But it doesn't seem to be very effective. I'm wondering is there any tips and tricks to train large deep learning models while using little GPU memory. Thanks in advance!</p>

<p><strong>Edit:</strong> I'm a beginner in deep learning. Apologize if it's a dummy question:) </p>",62556666.0,11,3,,2019/12/1 20:46,7.0,2021/8/18 15:33,2021/6/15 21:05,,10908375.0,,9543830.0,,1,39,python|deep-learning|pytorch|object-detection|low-memory,100023,175.8,,4,avoid cuda memory pytorch think pretty common message pytorch user low gpu memory want research object detection algorithm coursework many deep learning architecture require large capacity gpu memory machine train model try process image load layer gpu load back seem effective wondering tip trick train large deep learning model use little gpu memory thanks advance edit beginner deep learning apologize dummy question
644,644,36946671,Keras model.summary() result - Understanding the # of Parameters,"<p>I have a simple NN model for detecting hand-written digits from a 28x28px image written in python using Keras (Theano backend):</p>

<pre><code>model0 = Sequential()

#number of epochs to train for
nb_epoch = 12
#amount of data each iteration in an epoch sees
batch_size = 128

model0.add(Flatten(input_shape=(1, img_rows, img_cols)))
model0.add(Dense(nb_classes))
model0.add(Activation('softmax'))
model0.compile(loss='categorical_crossentropy', 
         optimizer='sgd',
         metrics=['accuracy'])

model0.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
      verbose=1, validation_data=(X_test, Y_test))

score = model0.evaluate(X_test, Y_test, verbose=0)

print('Test score:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>This runs well and I get ~90% accuracy. I then perform the following command to get a summary of my network's structure by doing <code>print(model0.summary())</code>. This outputs the following:</p>

<pre><code>Layer (type)         Output Shape   Param #     Connected to                     
=====================================================================
flatten_1 (Flatten)   (None, 784)     0           flatten_input_1[0][0]            
dense_1 (Dense)     (None, 10)       7850        flatten_1[0][0]                  
activation_1        (None, 10)          0           dense_1[0][0]                    
======================================================================
Total params: 7850
</code></pre>

<p>I don't understand how they get to 7850 total params and what that actually means?</p>",,6,1,,2016/4/29 20:09,32.0,2019/10/17 16:27,2017/7/10 9:01,,5974433.0,,3501476.0,,1,65,python|machine-learning|neural-network|keras|theano,117466,187.28,,5,kera model summary result understand parameter simple nn model detect hand write digit x px image write python use kera theano backend run well get accuracy perform following command get summary network structure output follow understand get total params actually mean
211,211,43882796,When does keras reset an LSTM state?,"<p>I read all sorts of texts about it, and none seem to answer this very basic question. It's always ambiguous:</p>

<p>In a <code>stateful = False</code> LSTM layer, does keras reset states after:</p>

<ul>
<li>Each sequence; or    </li>
<li>Each batch?</li>
</ul>

<p>Suppose I have X_train shaped as (1000,20,1), meaning 1000 sequences of 20 steps of a single value. If I make:</p>

<pre><code>model.fit(X_train, y_train, batch_size=200, nb_epoch=15)
</code></pre>

<p>Will it reset states for every single sequence (resets states 1000 times)?<br>
Or will it reset states for every batch (resets states 5 times)?</p>",46331227.0,5,1,,2017/5/10 2:46,26.0,2018/11/6 7:37,2017/5/10 3:05,,2097240.0,,2097240.0,,1,49,keras|lstm|keras-layer,21553,154.334,,3,keras reset lstm state read sort text none seem answer basic question always ambiguous lstm layer keras reset state sequence batch suppose x train shape meaning sequence step single value make reset state every single sequence reset state time reset state every batch reset state time
448,448,48226783,What is the difference between performing upsampling together with strided transpose convolution and transpose convolution with stride 1 only?,"<p>I noticed in a number of places that people use something like this, usually in fully convolutional networks, autoencoders, and similar:</p>

<pre><code>model.add(UpSampling2D(size=(2,2)))
model.add(Conv2DTranspose(kernel_size=k, padding='same', strides=(1,1))
</code></pre>

<p>I am wondering what is the difference between that and simply:</p>

<pre><code>model.add(Conv2DTranspose(kernel_size=k, padding='same', strides=(2,2))
</code></pre>

<p>Links towards any papers that explain this difference are welcome.</p>",48228379.0,2,1,,2018/1/12 12:58,8.0,2021/6/7 1:32,2020/2/23 9:46,,1033581.0,,5985209.0,,1,24,deep-learning|keras|conv-neural-network|convolution|deconvolution,9226,52.4601,,0,difference perform upsampling together stride transpose convolution transpose convolution stride notice number place people use something like usually fully convolutional network autoencoders similar wonder difference simply links towards paper explain difference welcome
480,480,59802608,TypeError: '>' not supported between instances of 'NoneType' and 'float',"<p>I have this code and it raise an error in python 3 and such a comparison can work on python 2
how can I change it?</p>
<pre><code>import tensorflow as tf 
def train_set():
    class MyCallBacks(tf.keras.callbacks.Callback):
        def on_epoch_end(self,epoch,logs={}):
            if(logs.get('acc')&gt;0.95):
                print('the training will stop !')
                self.model.stop_training=True
    callbacks=MyCallBacks()
    mnist_dataset=tf.keras.datasets.mnist 
    (x_train,y_train),(x_test,y_test)=mnist_dataset.load_data()
    x_train=x_train/255.0
    x_test=x_test/255.0
    classifier=tf.keras.Sequential([
                                    tf.keras.layers.Flatten(input_shape=(28,28)),
                                    tf.keras.layers.Dense(512,activation=tf.nn.relu),
                                    tf.keras.layers.Dense(10,activation=tf.nn.softmax)
                                    ])
    classifier.compile(
                        optimizer='sgd',
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy']
                       )    
    history=classifier.fit(x_train,y_train,epochs=20,callbacks=[callbacks])
    return history.epoch,history.history['acc'][-1]
train_set()
</code></pre>",,8,1,,2020/1/18 16:36,4.0,2021/6/13 19:42,2020/10/29 9:50,,5604562.0,,11214617.0,,1,15,python|tensorflow|machine-learning|keras|typeerror,18697,82.0871,,4,typeerror support instance nonetype float code raise error python comparison work python change
625,625,50825936,Confusion matrix on images in CNN keras,"<p>I have trained my model(multiclass classification) of CNN using keras and now I want to evaluate the model on my test set of images.</p>

<p>What are the possible options for evaluating my model apart from the accuracy, precision and recall? I know how to get the precision and recall from a custom script. But I cannot find a way to get the confusion matrix for my 12 classes of <strong>images</strong>. 
Scikit-learn shows a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""noreferrer"">way</a>, but not for images.
I am using model.fit_generator ()</p>

<p>Is there a way to create confusion matrix for all my classes or finding classification confidence on my classes? I am using Google Colab, though I can download the model and run it locally.</p>

<p>Any help would be appreciated.</p>

<p>Code:</p>

<pre><code>train_data_path = 'dataset_cfps/train'
validation_data_path = 'dataset_cfps/validation'

#Parametres
img_width, img_height = 224, 224

vggface = VGGFace(model='resnet50', include_top=False, input_shape=(img_width, img_height, 3))

#vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))

last_layer = vggface.get_layer('avg_pool').output
x = Flatten(name='flatten')(last_layer)
xx = Dense(256, activation = 'sigmoid')(x)
x1 = BatchNormalization()(xx)
x2 = Dropout(0.3)(x1)
y = Dense(256, activation = 'sigmoid')(x2)
yy = BatchNormalization()(y)
y1 = Dropout(0.6)(yy)
x3 = Dense(12, activation='sigmoid', name='classifier')(y1)

custom_vgg_model = Model(vggface.input, x3)


# Create the model
model = models.Sequential()

# Add the convolutional base model
model.add(custom_vgg_model)

model.summary()
#model = load_model('facenet_resnet_lr3_SGD_sameas1.h5')

def recall(y_true, y_pred):
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
     recall = true_positives / (possible_positives + K.epsilon())
     return recall

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,
      width_shift_range=0.2,
      height_shift_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')


validation_datagen = ImageDataGenerator(rescale=1./255)

# Change the batchsize according to your system RAM
train_batchsize = 32
val_batchsize = 32

train_generator = train_datagen.flow_from_directory(
        train_data_path,
        target_size=(img_width, img_height),
        batch_size=train_batchsize,
        class_mode='categorical')

validation_generator = validation_datagen.flow_from_directory(
        validation_data_path,
        target_size=(img_width, img_height),
        batch_size=val_batchsize,
        class_mode='categorical',
        shuffle=True)

# Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.SGD(lr=1e-3),
              metrics=['acc', recall, precision])
# Train the model
history = model.fit_generator(
      train_generator,
      steps_per_epoch=train_generator.samples/train_generator.batch_size ,
      epochs=100,
      validation_data=validation_generator,
      validation_steps=validation_generator.samples/validation_generator.batch_size,
      verbose=1)

# Save the model
model.save('facenet_resnet_lr3_SGD_new_FC.h5')
</code></pre>",50843465.0,3,3,,2018/6/12 21:15,7.0,2020/12/14 1:53,2018/6/13 5:20,,9871400.0,,9871400.0,,1,12,python|scikit-learn|keras|conv-neural-network,37618,60.5016,,5,confusion matrix image cnn kera train model multiclass classification cnn use kera want evaluate model test set image possible option evaluate model apart accuracy precision recall know get precision recall custom script find way get confusion matrix class image scikit learn show way image use model fit generator way create confusion matrix class find classification confidence class use google colab though download model run locally help would appreciate code
530,530,35582521,How to calculate receptive field size?,"<p>I'm reading paper about using CNN(Convolutional neural network) for object detection.</p>

<p><a href=""https://www.cs.berkeley.edu/~rbg/papers/r-cnn-cvpr.pdf"" rel=""noreferrer"">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p>

<p>Here is a quote about receptive field:</p>

<pre><code>The pool5 feature map is 6x6x256 = 9216 dimensional. Ignoring boundary effects, each pool5 unit has a receptive field of 195x195 pixels in the original 227x227 pixel input. A central pool5 unit has a nearly global view,
while one near the edge has a smaller, clipped support.
</code></pre>

<p>My questions are:</p>

<ol>
<li>What is definition of receptive field?</li>
<li>How they compute size and location of receptive field?</li>
<li>How we can compute bounding rect of receptive field using caffe/pycaffe? </li>
</ol>",35582860.0,6,1,,2016/2/23 16:11,9.0,2019/12/11 17:25,2019/12/4 14:48,,3104974.0,,1179925.0,,1,21,deep-learning|computer-vision|receptive-field,21172,69.503,2018/11/8 5:48,0,calculate receptive field size read paper use cnn convolutional neural network object detection rich feature hierarchy accurate object detection semantic segmentation quote receptive field question definition receptive field compute size location receptive field compute bound rect receptive field use caffe pycaffe
2,2,60987997,Why `torch.cuda.is_available()` returns False even after installing pytorch with cuda?,"<p>On a Windows 10 PC with an NVidia GeForce 820M 
I installed CUDA 9.2 and cudnn 7.1 successfully,
and then installed PyTorch using the instructions at pytorch.org.</p>

<p>Specifically I used the command</p>

<pre><code>pip install torch==1.4.0+cu92 torchvision==0.5.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>

<p>because i use pip and not Anaconda.</p>

<p>Yet I get the following</p>

<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.cuda.is_available()
False
</code></pre>

<p>Why is this happening?</p>",61034368.0,2,4,,2020/4/2 9:12,16.0,2020/10/15 15:59,2020/4/4 18:31,,2790047.0,,11512643.0,,1,20,python|pytorch,42727,79.5228,,1,torch cuda available return false even instal pytorch cuda windows pc nvidia geforce instal cuda cudnn successfully instal pytorch use instruction pytorch org specifically use command use pip anaconda yet get following happen
497,497,32294261,What is Depth of a convolutional neural network?,"<p>I was taking a look at Convolutional Neural Network from <a href=""http://cs231n.github.io/convolutional-networks/"">CS231n Convolutional Neural Networks for Visual Recognition</a>. In Convolutional Neural Network, the neurons are arranged in 3 dimensions(<code>height</code>, <code>width</code>, <code>depth</code>). I am having trouble with the <code>depth</code> of the CNN. I can't visualize what it is. </p>

<p>In the link they said <code>The CONV layer's parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume</code>. </p>

<p>For example loook at this picture. Sorry if the image is too crappy. <a href=""https://i.stack.imgur.com/qmf0m.jpg""><img src=""https://i.stack.imgur.com/qmf0m.jpg"" alt=""crappy picture""></a></p>

<p>I can grasp the idea that we take a small area off the image, then compare it with the ""Filters"". So the filters will be collection of small images? Also they said <code>We will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron.</code> So is the receptive field has the same dimension as the filters? Also what will be the depth here? And what do we signify using the depth of a CNN?</p>

<p>So, my question mainly is, if i take an image having dimension of <code>[32*32*3]</code> (Lets say i have 50000 of these images, making the dataset <code>[50000*32*32*3]</code>), what shall i choose as its depth and what would it mean by the depth. Also what will be the dimension of the filters?</p>

<p>Also it will be much helpful if anyone can provide some link that gives some intuition on this.</p>

<p>EDIT:
So in one part of the tutorial(Real-world example part), it says <code>The Krizhevsky et al. architecture that won the ImageNet challenge in 2012 accepted images of size [227x227x3]. On the first Convolutional Layer, it used neurons with receptive field size F=11, stride S=4 and no zero padding P=0. Since (227 - 11)/4 + 1 = 55, and since the Conv layer had a depth of K=96, the Conv layer output volume had size [55x55x96].</code> </p>

<p>Here we see the depth is 96. So is depth something that i choose arbitrarily? or something i compute? Also in the example above(Krizhevsky et al) they had 96 depths. So what does it mean by its 96 depths? Also the tutorial stated <code>Every filter is small spatially (along width and height), but extends through the full depth of the input volume</code>.</p>

<p>So that means the depth will be like this? If so then can i assume <code>Depth = Number of Filters</code>?
<a href=""https://i.stack.imgur.com/txz5T.jpg""><img src=""https://i.stack.imgur.com/txz5T.jpg"" alt=""enter image description here""></a></p>",32294316.0,7,0,,2015/8/30 7:26,17.0,2018/5/4 22:13,2015/8/30 14:44,,4341948.0,,4341948.0,,1,31,machine-learning|neural-network|deep-learning|conv-neural-network,29848,96.2997,,0,depth convolutional neural network take look convolutional neural network c n convolutional neural network visual recognition convolutional neural network neuron arrange dimension trouble cnn visualize link say example loook picture sorry image crappy grasp idea take small area image compare filter filter collection small image also say receptive field dimension filter also depth signify use depth cnn question mainly take image dimension let say image make dataset shall choose depth would mean depth also dimension filter also much helpful anyone provide link give intuition edit one part tutorial real world example part say see depth depth something choose arbitrarily something compute also example krizhevsky et al depth mean depth also tutorial state mean depth like assume
218,218,44061208,How to Implement the Conv1DTranspose in keras?,"<p>I Know there is the Conv2DTranspose in keras which can be used in Image. We need to use it in NLP, so the 1D deconvolution is needed. </p>

<p>How do we implement the Conv1DTranspose in keras?</p>",,4,0,,2017/5/19 3:58,6.0,2020/5/20 17:47,,,,,1282982.0,,1,29,keras|keras-layer,12312,66.5613,,3,implement conv dtranspose kera know conv dtranspose kera use image need use nlp deconvolution need implement conv dtranspose kera
531,531,35687678,Using a pre-trained word embedding (word2vec or Glove) in TensorFlow,"<p>I've recently reviewed an interesting implementation for <a href=""http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"">convolutional text classification</a>. However all TensorFlow code I've reviewed uses a random (not pre-trained) embedding vectors like the following:</p>

<pre><code>with tf.device('/cpu:0'), tf.name_scope(""embedding""):
    W = tf.Variable(
        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
        name=""W"")
    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)
    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)
</code></pre>

<p>Does anybody know how to use the results of Word2vec or a GloVe pre-trained word embedding instead of a random one?</p>",35688187.0,6,0,,2016/2/28 20:11,74.0,2020/9/19 16:36,2016/2/28 21:02,,3574081.0,,3147590.0,,1,96,python|numpy|tensorflow|deep-learning,56915,322.221,,3,use pre train word embed word vec glove tensorflow recently review interesting implementation convolutional text classification however tensorflow code review use random pre trained embed vector like following anybody know use result word vec glove pre train word embed instead random one
183,183,43327464,How to make Keras use Tensorflow backend in Anaconda?,"<p>I have install tensorflow-gpu in my Anaconda environment. They both work well. </p>

<p>Now I am trying to install Keras with Tensorflow backend. According to the <a href=""https://keras.io/#installation"" rel=""noreferrer"">instruction</a> I just run:</p>

<pre><code>pip install keras
</code></pre>

<p>But it doesn't install keras, then I tried:</p>

<pre><code>conda install -c conda-forge keras=2.0.2
</code></pre>

<p>Then I am now able import keras in python. But the problem is, it always use the Theano backend. I am trying to change this, but not knowing how to do it.</p>

<p>I also tried edit the file <strong>~/.keras</strong>, but actually default backend was tensorflow already.</p>

<p>Please help.. Thank you so much!</p>",43328647.0,11,0,,2017/4/10 15:38,8.0,2020/3/10 7:41,,,,,4352606.0,,1,23,python|tensorflow|anaconda|backend|keras,32206,126.632,,1,make kera use tensorflow backend anaconda install tensorflow gpu anaconda environment work well try install kera tensorflow backend accord instruction run install kera try able import kera python problem always use theano backend try change know also try edit file kera actually default backend tensorflow already please help thank much
302,302,57727372,How do I get the value of a tensor in PyTorch?,"<p>Printing the tensor gives:</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([3])
&gt;&gt;&gt; print(x)
tensor([3])
</code></pre>
<p>Likewise indexing its <code>.data</code> gives:</p>
<pre><code>&gt;&gt;&gt; x.data[0]
tensor(3)
</code></pre>
<p>How do I get just the value <code>3</code>?</p>",57727559.0,4,1,,2019/8/30 13:08,10.0,2021/7/22 20:48,2021/3/30 22:00,,9067615.0,,10855529.0,,1,48,python|pytorch|tensor,52475,218.28,,3,get value tensor pytorch print tensor give likewise index give get value
157,157,42755820,How to use return_sequences option and TimeDistributed layer in Keras?,"<p>I have a dialog corpus like below. And I want to implement a LSTM model which predicts a system action. The system action is described as a bit vector. And a user input is calculated as a word-embedding which is also a bit vector.</p>

<pre><code>t1: user: ""Do you know an apple?"", system: ""no""(action=2)
t2: user: ""xxxxxx"", system: ""yyyy"" (action=0)
t3: user: ""aaaaaa"", system: ""bbbb"" (action=5)
</code></pre>

<p>So what I want to realize is ""many to many (2)"" model. When my model receives a user input, it must output a system action.
<a href=""https://i.stack.imgur.com/13opm.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/13opm.jpg"" alt=""enter image description here""></a>
But I cannot understand <code>return_sequences</code> option and <code>TimeDistributed</code> layer after LSTM. To realize ""many-to-many (2)"", <code>return_sequences==True</code> and adding a <code>TimeDistributed</code> after LSTMs are required? I appreciate if you would give more description of them.</p>

<blockquote>
  <p><strong>return_sequences</strong>: Boolean. Whether to return the last output in the output sequence, or the full sequence.</p>
  
  <p><strong>TimeDistributed</strong>: This wrapper allows to apply a layer to every temporal slice of an input.</p>
</blockquote>

<h3>Updated 2017/03/13 17:40</h3>

<p>I think I could understand the <code>return_sequence</code> option. But I am not still sure about <code>TimeDistributed</code>. If I add a <code>TimeDistributed</code> after LSTMs, is the model the same as ""my many-to-many(2)"" below? So I think Dense layers are applied for each output.
<a href=""https://i.stack.imgur.com/DiPyQ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DiPyQ.jpg"" alt=""enter image description here""></a></p>",42758532.0,2,3,,2017/3/13 2:35,30.0,2018/4/16 19:18,2018/4/16 19:18,,3548063.0,,3548063.0,,1,52,deep-learning|keras|lstm|recurrent-neural-network,23736,100.302,,3,use return sequence option timedistributed layer kera dialog corpus like want implement lstm model predict system action system action describe bit vector user input calculate word embed also bit vector want realize many many model model receive user input must output system action understand option layer lstm realize many many add lstms require appreciate would give description return sequence boolean whether return last output output sequence full sequence timedistributed wrapper allow apply layer every temporal slice input update think could understand option still sure add lstms model many many think dense layer apply output
748,748,41123879,numpy random choice in Tensorflow,"<p>Is there an equivalent function to numpy random choice in Tensorflow. 
In numpy we can get an item randomly from the given list with its weights. </p>

<pre><code> np.random.choice([1,2,3,5], 1, p=[0.1, 0, 0.3, 0.6, 0])
</code></pre>

<p>This code will select an item from the given list with p weights. </p>",41124526.0,8,1,,2016/12/13 14:45,7.0,2021/5/20 12:06,,,,,797880.0,,1,24,python|numpy|tensorflow|deep-learning,21295,84.7131,,3,numpy random choice tensorflow equivalent function numpy random choice tensorflow numpy get item randomly give list weight code select item give list p weight
272,272,44829085,Tensorflow not running on GPU,"<p><strong><em>I have aldready spent a considerable of time digging around on stack overflow and else looking for the answer, but couldn't find anything</em></strong></p>

<p>Hi all,</p>

<p>I am running Tensorflow with Keras on top.
I am 90% sure I installed Tensorflow GPU, is there any way to check which install I did?</p>

<p>I was trying to do run some CNN models from Jupyter notebook and I noticed that Keras was running the model on the CPU (checked task manager, CPU was at 100%).</p>

<p>I tried running this code from the tensorflow website:</p>

<pre><code># Creates a graph.
a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
c = tf.matmul(a, b)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(c))
</code></pre>

<p>And this is what I got:</p>

<pre><code>MatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0
2017-06-29 17:09:38.783183: I c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\common_runtime\simple_placer.cc:847] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0
b: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-06-29 17:09:38.784779: I c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\common_runtime\simple_placer.cc:847] b: (Const)/job:localhost/replica:0/task:0/cpu:0
a: (Const): /job:localhost/replica:0/task:0/cpu:0
2017-06-29 17:09:38.786128: I c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\common_runtime\simple_placer.cc:847] a: (Const)/job:localhost/replica:0/task:0/cpu:0
[[ 22.  28.]
 [ 49.  64.]]
</code></pre>

<p>Which to me shows I am running on my CPU, for some reason.</p>

<p>I have a GTX1050 (driver version 382.53), I installed CUDA, and Cudnn, and tensorflow installed without any problems. I installed Visual Studio 2015 as well since it was listed as a compatible version.</p>

<p>I remember CUDA mentioning something about an incompatible driver being installed, but if I recall correctly CUDA should have installed its own driver.</p>

<p><strong>Edit:</strong>
I ran theses commands to list the available devices</p>

<pre><code>from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
</code></pre>

<p>and this is what I get</p>

<pre><code>[name: ""/cpu:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 14922788031522107450
]
</code></pre>

<p>and a whole lot of warnings like this</p>

<pre><code>2017-06-29 17:32:45.401429: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
</code></pre>

<p><strong>Edit 2</strong></p>

<p>Tried running</p>

<pre><code>pip3 install --upgrade tensorflow-gpu
</code></pre>

<p>and I get</p>

<pre><code>Requirement already up-to-date: tensorflow-gpu in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages
Requirement already up-to-date: markdown==2.2.0 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: html5lib==0.9999999 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: werkzeug&gt;=0.11.10 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: wheel&gt;=0.26 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: bleach==1.5.0 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: six&gt;=1.10.0 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: protobuf&gt;=3.2.0 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: backports.weakref==1.0rc1 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: numpy&gt;=1.11.0 in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from tensorflow-gpu)
Requirement already up-to-date: setuptools in c:\users\xxx\appdata\local\programs\python\python35\lib\site-packages (from protobuf&gt;=3.2.0-&gt;tensorflow-gpu)
</code></pre>

<p><strong>Solved:</strong>
Check comments for solution.
Thanks to all who helped!</p>

<p>I am new to this, so any help is greatly appreciated!
Thank you.</p>",44829372.0,5,18,,2017/6/29 15:17,12.0,2021/8/10 6:54,2020/4/13 20:23,,5127598.0,,5127598.0,,1,38,tensorflow|keras|nvidia|cudnn,58102,117.057,,1,tensorflow run gpu aldready spend considerable time dig around stack overflow else look answer could find anything hi run tensorflow kera top sure instal tensorflow gpu way check install try run cnn model jupyter notebook notice kera run model cpu check task manager cpu try run code tensorflow website get show run cpu reason gtx driver version instal cuda cudnn tensorflow instal without problem instal visual studio well since list compatible version remember cuda mention something incompatible driver instal recall correctly cuda instal driver edit run thesis command list available device get whole lot warning like edit try run get solve check comment solution thanks help new help greatly appreciated thank
376,376,46204569,How to handle variable sized input in CNN with Keras?,"<p>I am trying to perform the usual classification on the MNIST database but with randomly cropped digits. 
Images are cropped the following way : removed randomly first/last and/or row/column.</p>

<p>I would like to use a Convolutional Neural Network using Keras (and Tensorflow backend) to perform convolution and then the usual classification.</p>

<p>Inputs are of variable size and i can't manage to get it to work.</p>

<p>Here is how I cropped digits</p>

<pre><code>import numpy as np
from keras.utils import to_categorical
from sklearn.datasets import load_digits

digits = load_digits()

X = digits.images
X = np.expand_dims(X, axis=3)

X_crop = list()
for index in range(len(X)):
    X_crop.append(X[index, np.random.randint(0,2):np.random.randint(7,9), np.random.randint(0,2):np.random.randint(7,9), :])
X_crop = np.array(X_crop)

y = to_categorical(digits.target)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_crop, y, train_size=0.8, test_size=0.2)
</code></pre>

<p>And here is the architecture of the model I want to use</p>

<pre><code>from keras.layers import Dense, Dropout
from keras.layers.convolutional import Conv2D
from keras.models import Sequential

model = Sequential()

model.add(Conv2D(filters=10, 
                 kernel_size=(3,3), 
                 input_shape=(None, None, 1), 
                 data_format='channels_last'))

model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(10, activation='softmax'))


model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

model.summary()

model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))
</code></pre>

<ol>
<li><p>Does someone have an idea on how to handle variable sized input in my neural network? </p></li>
<li><p>And how to perform classification?</p></li>
</ol>",46206061.0,2,0,,2017/9/13 18:31,6.0,2017/10/29 19:37,2017/10/29 19:37,,5974433.0,,8036988.0,,1,21,python|machine-learning|neural-network|deep-learning|keras,20137,59.61600000000001,,3,handle variable size input cnn kera try perform usual classification mnist database randomly crop digit image crop following way remove randomly first last row column would like use convolutional neural network use kera tensorflow backend perform convolution usual classification input variable size manage get work crop digit architecture model want use someone idea handle variable size input neural network perform classification
0,0,60440292,RuntimeError: expected scalar type Long but found Float,"<p>I can't get the dtypes to match, either the loss wants long or the model wants float if I change my tensors to long. The shape of the tensors are 42000, 1, 28, 28 and 42000. I'm not sure where I can change what dtypes are required for the model or loss. </p>

<p>I'm not sure if dataloader is required, using Variable didn't work either.</p>

<pre><code>dataloaders_train = torch.utils.data.DataLoader(Xt_train, batch_size=64)

dataloaders_test = torch.utils.data.DataLoader(Yt_train, batch_size=64)

class Network(nn.Module):
    def __init__(self):
        super().__init__()


        self.hidden = nn.Linear(42000, 256)

        self.output = nn.Linear(256, 10)


        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):

        x = self.hidden(x)
        x = self.sigmoid(x)
        x = self.output(x)
        x = self.softmax(x)

        return x

model = Network()

input_size = 784
hidden_sizes = [28, 64]
output_size = 10 
model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),
                      nn.ReLU(),
                      nn.Linear(hidden_sizes[1], output_size),
                      nn.Softmax(dim=1))
print(model)

criterion = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.003)

epochs = 5

for e in range(epochs):
    running_loss = 0
    for images, labels in zip(dataloaders_train, dataloaders_test):

        images = images.view(images.shape[0], -1)
        #images, labels = Variable(images), Variable(labels)
        print(images.dtype)
        print(labels.dtype)

        optimizer.zero_grad()

        output = model(images)
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
    else:
        print(f""Training loss: {running_loss}"")
</code></pre>

<p>Which gives</p>

<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-128-68109c274f8f&gt; in &lt;module&gt;
     11 
     12         output = model(images)
---&gt; 13         loss = criterion(output, labels)
     14         loss.backward()
     15         optimizer.step()

/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    530             result = self._slow_forward(*input, **kwargs)
    531         else:
--&gt; 532             result = self.forward(*input, **kwargs)
    533         for hook in self._forward_hooks.values():
    534             hook_result = hook(self, input, result)

/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
    202 
    203     def forward(self, input, target):
--&gt; 204         return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
    205 
    206 

/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   1836                          .format(input.size(0), target.size(0)))
   1837     if dim == 2:
-&gt; 1838         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   1839     elif dim == 4:
   1840         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

RuntimeError: expected scalar type Long but found Float
</code></pre>",60440460.0,1,0,,2020/2/27 19:17,3.0,2021/1/22 14:50,2021/1/22 14:50,,10908375.0,user12975267,,,1,14,python|machine-learning|deep-learning|neural-network|pytorch,37847,54.1121,,3,runtimeerror expect scalar type long find float get dtypes match either loss want long model want float change tensor long shape tensor sure change dtypes require model loss sure dataloader require use variable work either give
818,818,53698035,"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize,","<p>In Tensorflow/ Keras when running the code from <a href=""https://github.com/pierluigiferrari/ssd_keras"" rel=""noreferrer"">https://github.com/pierluigiferrari/ssd_keras</a>, use the estimator: ssd300_evaluation. I received this error. </p>

<blockquote>
  <p>Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.</p>
</blockquote>

<p>This is very similar to the unsolved question: <a href=""https://stackoverflow.com/questions/53414185/google-colab-error-failed-to-get-convolution-algorithm-this-is-probably-becaus"">Google Colab Error : Failed to get convolution algorithm.This is probably because cuDNN failed to initialize</a></p>

<p>With the issue I'm running:</p>

<p>python: 3.6.4.</p>

<p>Tensorflow Version: 1.12.0.</p>

<p>Keras Version: 2.2.4.</p>

<p>CUDA: V10.0.</p>

<p>cuDNN: V7.4.1.5.</p>

<p>NVIDIA GeForce GTX 1080.</p>

<p>Also I ran:</p>

<pre><code>import tensorflow as tf
with tf.device('/gpu:0'):
      a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
      b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
      c = tf.matmul(a, b)
with tf.Session() as sess:
print (sess.run(c))
</code></pre>

<p>With no errors or issues.</p>

<p>The minimalist example is: </p>

<pre><code> from keras import backend as K
 from keras.models import load_model
 from keras.optimizers import Adam
 from scipy.misc import imread
 import numpy as np
 from matplotlib import pyplot as plt

 from models.keras_ssd300 import ssd_300
 from keras_loss_function.keras_ssd_loss import SSDLoss
 from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes
 from keras_layers.keras_layer_DecodeDetections import DecodeDetections
 from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast
 from keras_layers.keras_layer_L2Normalization import L2Normalization
 from data_generator.object_detection_2d_data_generator import DataGenerator
 from eval_utils.average_precision_evaluator import Evaluator
 import tensorflow as tf
 %matplotlib inline
 import keras
 keras.__version__



 # Set a few configuration parameters.
 img_height = 300
 img_width = 300
 n_classes = 20
 model_mode = 'inference'


 K.clear_session() # Clear previous models from memory.

 model = ssd_300(image_size=(img_height, img_width, 3),
            n_classes=n_classes,
            mode=model_mode,
            l2_regularization=0.0005,
            scales=[0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05], # The scales 
 for MS COCO [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05]
            aspect_ratios_per_layer=[[1.0, 2.0, 0.5],
                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                                     [1.0, 2.0, 0.5, 3.0, 1.0/3.0],
                                     [1.0, 2.0, 0.5],
                                     [1.0, 2.0, 0.5]],
            two_boxes_for_ar1=True,
            steps=[8, 16, 32, 64, 100, 300],
            offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5],
            clip_boxes=False,
            variances=[0.1, 0.1, 0.2, 0.2],
            normalize_coords=True,
            subtract_mean=[123, 117, 104],
            swap_channels=[2, 1, 0],
            confidence_thresh=0.01,
            iou_threshold=0.45,
            top_k=200,
            nms_max_output_size=400)

 # 2: Load the trained weights into the model.

 # TODO: Set the path of the trained weights.
 weights_path = 'C:/Users/USAgData/TF SSD 
 Keras/weights/VGG_VOC0712Plus_SSD_300x300_iter_240000.h5'

 model.load_weights(weights_path, by_name=True)

 # 3: Compile the model so that Keras won't complain the next time you load it.

 adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

 ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)

 model.compile(optimizer=adam, loss=ssd_loss.compute_loss)


dataset = DataGenerator()

# TODO: Set the paths to the dataset here.
dir= ""C:/Users/USAgData/TF SSD Keras/VOC/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/""
Pascal_VOC_dataset_images_dir = dir+ 'JPEGImages'
Pascal_VOC_dataset_annotations_dir = dir + 'Annotations/'
Pascal_VOC_dataset_image_set_filename = dir+'ImageSets/Main/test.txt'

# The XML parser needs to now what object class names to look for and in which order to map them to integers.
classes = ['background',
           'aeroplane', 'bicycle', 'bird', 'boat',
           'bottle', 'bus', 'car', 'cat',
           'chair', 'cow', 'diningtable', 'dog',
           'horse', 'motorbike', 'person', 'pottedplant',
           'sheep', 'sofa', 'train', 'tvmonitor']

dataset.parse_xml(images_dirs=[Pascal_VOC_dataset_images_dir],
                  image_set_filenames=[Pascal_VOC_dataset_image_set_filename],
                  annotations_dirs=[Pascal_VOC_dataset_annotations_dir],
                  classes=classes,
                  include_classes='all',
                  exclude_truncated=False,
                  exclude_difficult=False,
                  ret=False)



evaluator = Evaluator(model=model,
                      n_classes=n_classes,
                      data_generator=dataset,
                      model_mode=model_mode)



results = evaluator(img_height=img_height,
                    img_width=img_width,
                    batch_size=8,
                    data_generator_mode='resize',
                    round_confidences=False,
                    matching_iou_threshold=0.5,
                    border_pixels='include',
                    sorting_algorithm='quicksort',
                    average_precision_mode='sample',
                    num_recall_points=11,
                    ignore_neutral_boxes=True,
                    return_precisions=True,
                    return_recalls=True,
                    return_average_precisions=True,
                    verbose=True)
</code></pre>",53783381.0,29,1,,2018/12/10 0:19,19.0,2021/5/21 9:11,2021/1/2 2:03,,681865.0,,3366418.0,,1,63,python|tensorflow|keras,89594,557.2090000000002,,1,fail get convolution algorithm probably cudnn fail initialize tensorflow kera run code use estimator ssd evaluation receive error fail get convolution algorithm probably cudnn fail initialize try look see warning log message print similar unsolved question google colab error fail get convolution algorithm probably cudnn fail initialize issue run python tensorflow version kera version cuda v cudnn v nvidia geforce gtx also run error issue minimalist example
315,315,58352326,Running the Tensorflow 2.0 code gives 'ValueError: tf.function-decorated function tried to create variables on non-first call'. What am I doing wrong?,"<p><a href=""https://colab.research.google.com/drive/1lipWy-B6BrtZlARM_amdnc7zgKiFaSRR"" rel=""noreferrer"">error_giving_notebook</a></p>

<p><a href=""https://colab.research.google.com/drive/1ksJgAXniJZIm8S4EmPDIE3hA7DuwWs0a"" rel=""noreferrer"">non_problematic_notebook</a></p>

<p>As it can be seen that I have used tf.function decorator in the 'error_giving_notebook' and it throws a ValueError while the same notebook without any changes except for removing the tf.function decorator runs smoothly in 'non_problematic_notebook'. What can be the reason? </p>",58526969.0,2,3,,2019/10/12 8:32,3.0,2020/8/28 2:04,2019/10/12 9:04,,9435613.0,,9435613.0,,1,16,keras|deep-learning|keras-layer|tensorflow2.0|tf.keras,37223,81.6832,,4,run tensorflow code give valueerror tf function decorate function try create variable non first call wrong error give notebook non problematic notebook see use tf function decorator error give notebook throw valueerror notebook without change except remove tf function decorator run smoothly non problematic notebook reason
572,572,49161174,Tensorflow : logits and labels must have the same first dimension,"<p>I am new in tensoflow and I want to adapt the MNIST tutorial <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers</a> with my own data (images of 40x40).
This is my model function : </p>

<pre><code>def cnn_model_fn(features, labels, mode):
        # Input Layer
        input_layer = tf.reshape(features, [-1, 40, 40, 1])

        # Convolutional Layer #1
        conv1 = tf.layers.conv2d(
                inputs=input_layer,
                filters=32,
                kernel_size=[5, 5],
                #  To specify that the output tensor should have the same width and height values as the input tensor
                # value can be ""same"" ou ""valid""
                padding=""same"",
                activation=tf.nn.relu)

        # Pooling Layer #1
        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

        # Convolutional Layer #2 and Pooling Layer #2
        conv2 = tf.layers.conv2d(
                inputs=pool1,
                filters=64,
                kernel_size=[5, 5],
                padding=""same"",
                activation=tf.nn.relu)
        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

        # Dense Layer
        pool2_flat = tf.reshape(pool2, [-1, 10 * 10 * 64])
        dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
        dropout = tf.layers.dropout(
                inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

        # Logits Layer
        logits = tf.layers.dense(inputs=dropout, units=2)

        predictions = {
            # Generate predictions (for PREDICT and EVAL mode)
            ""classes"":       tf.argmax(input=logits, axis=1),
            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
            # `logging_hook`.
            ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
        }

        if mode == tf.estimator.ModeKeys.PREDICT:
            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

        # Calculate Loss (for both TRAIN and EVAL modes)
        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

        # Configure the Training Op (for TRAIN mode)
        if mode == tf.estimator.ModeKeys.TRAIN:
            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
            train_op = optimizer.minimize(
                    loss=loss,
                    global_step=tf.train.get_global_step())
            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

        # Add evaluation metrics (for EVAL mode)
        eval_metric_ops = {
            ""accuracy"": tf.metrics.accuracy(
                    labels=labels, predictions=predictions[""classes""])}
        return tf.estimator.EstimatorSpec(
                mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
</code></pre>

<p>I have a shape size error between labels and logits : </p>

<p><strong>InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1]</strong> </p>

<p>filenames_array is an array of 16 string </p>

<pre><code>[""file1.png"", ""file2.png"", ""file3.png"", ...]
</code></pre>

<p>and labels_array is an array of 16 integer </p>

<pre><code>[0,0,1,1,0,1,0,0,0,...]
</code></pre>

<p>The main function is :</p>

<pre><code># Create the Estimator
mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=""/tmp/test_convnet_model"")

# Train the model
cust_train_input_fn = lambda: train_input_fn_custom(
        filenames_array=filenames, labels_array=labels, batch_size=1)

mnist_classifier.train(
        input_fn=cust_train_input_fn,
        steps=20000,
        hooks=[logging_hook])
</code></pre>

<p>I tried to reshape logits without success :</p>

<p>logits = tf.reshape(logits, [1, 2])</p>

<p>I need your help, thanks</p>

<hr>

<p><strong>EDIT</strong></p>

<p>After more time to search, in the first line of my model function</p>

<pre><code>input_layer = tf.reshape(features, [-1, 40, 40, 1])
</code></pre>

<p>the ""-1"" that signifies that the batch_size dimension will be dynamically calculated have here the value ""3"". The same ""3"" as in my error : <strong>logits and labels must have the same first dimension, got logits shape [3,2] and labels shape [1]</strong></p>

<p>If I force the value to ""1"" I have this new error :</p>

<p><strong>Input to reshape is a tensor with 4800 values, but the requested shape has 1600</strong></p>

<p>Maybe a problem with my features ?</p>

<hr>

<p><strong>EDIT2 :</strong> </p>

<p>the complete code is here : <a href=""https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e"" rel=""noreferrer"">https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e</a></p>

<hr>

<p><strong>EDIT3</strong></p>

<p>I updated the gist with </p>

<pre><code>logits = tf.layers.dense(inputs=dropout, units=1)
</code></pre>

<p><a href=""https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e"" rel=""noreferrer"">https://gist.github.com/geoffreyp/cc8e97aab1bff4d39e10001118c6322e</a></p>

<p>But I don't completely understand your answer about the batch size, how the batch size can be 3 here whereas I choose a batch size of 1 ? </p>

<p>If I choose a batch_size = 3 I have this error :
<strong>logits and labels must have the same first dimension, got logits shape [9,1] and labels shape [3]</strong></p>

<p>I tried to reshape labels : </p>

<pre><code>labels = tf.reshape(labels, [3, 1])
</code></pre>

<p>and I updated features and labels structure : </p>

<pre><code>    filenames_train = [['blackcorner-data/1.png', 'blackcorner-data/2.png', 'blackcorner-data/3.png',
                   'blackcorner-data/4.png', 'blackcorner-data/n1.png'],
                   ['blackcorner-data/n2.png',
                   'blackcorner-data/n3.png', 'blackcorner-data/n4.png',
                   'blackcorner-data/11.png', 'blackcorner-data/21.png'],
                   ['blackcorner-data/31.png',
                   'blackcorner-data/41.png', 'blackcorner-data/n11.png', 'blackcorner-data/n21.png',
                   'blackcorner-data/n31.png']
                   ]

labels = [[0, 0, 0, 0, 1], [1, 1, 1, 0, 0], [0, 0, 1, 1, 1]]
</code></pre>

<p>but without success...</p>",62286888.0,6,0,,2018/3/7 21:05,9.0,2021/8/1 23:59,2020/9/15 20:51,,10375049.0,,4227291.0,,1,18,python|tensorflow|keras|tensorflow-datasets|tensorflow-estimator,50176,127.402,,4,tensorflow logits label must first dimension new tensoflow want adapt mnist tutorial data image x model function shape size error label logits invalidargumenterror see traceback logits label must first dimension get logits shape label shape filename array array string labels array array integer main function try reshape logits without success logits tf reshape logits need help thanks edit time search first line model function signify batch size dimension dynamically calculate value error logits label must first dimension get logits shape label shape force value new error input reshape tensor value requested shape maybe problem feature edit complete code edit update gist completely understand answer batch size batch size whereas choose batch size choose batch size error logits label must first dimension get logits shape label shape try reshape label update feature label structure without success
6,6,61488902,Cannot import Pytorch [WinError 126] The specified module could not be found,"<p>I'm trying to do a basic install and import of Pytorch/Torchvision on Windows 10. I installed a Anaconda and created a new virtual environment named photo. I opened Anaconda prompt, activated the environment, and I ran:</p>

<pre><code>(photo) C:\Users\&lt;user&gt;\anaconda3\envs&gt;conda install pytorch torchvision cudatoolkit=10.2 -c pytorch**
</code></pre>

<p>This installed pytorch successfully. Running <strong>conda list</strong> I see: </p>

<pre><code>  pytorch            pytorch/win-64::pytorch-1.5.0-py3.7_cuda102_cudnn7_0

  torchvision        pytorch/win-64::torchvision-0.6.0-py37_cu102
</code></pre>

<p>Then I open a python command prompt while in the virtual environment, and type:</p>

<p><code>import torch</code></p>

<p>The following error is printed:</p>

<blockquote>
  <p>Traceback (most recent call last):
    File """", line 1, in 
    File ""C:\Users\njord\anaconda3\envs\photo\lib\site-packages\torch__init__.py"", line 81, in 
      ctypes.CDLL(dll)
    File ""C:\Users\njord\anaconda3\envs\photo\lib\ctypes__init__.py"", line 364, in <strong>init</strong>
      self._handle = _dlopen(self._name, mode)
  OSError: [WinError 126] The specified module could not be found</p>
</blockquote>

<p>I have uninstalled/reinstalled python and anaconda but still run into the same issue. Advice appreciated.</p>",,5,0,,2020/4/28 19:57,3.0,2020/9/18 14:10,2020/4/29 18:37,,13428108.0,,13428108.0,,1,22,python|windows|pytorch|environment,20732,66.2666,,1,import pytorch winerror specified module could find try basic install import pytorch torchvision window instal anaconda create new virtual environment name photo open anaconda prompt activate environment run installed pytorch successfully run conda list see open python command prompt virtual environment type following error print traceback recent call last file line file c user jord anaconda envs photo lib site package torch init py line ctypes cdll dll file c user jord anaconda envs photo lib ctypes init py line init self handle dlopen self name mode oserror winerror specify module could find uninstalled reinstall python anaconda still run issue advice appreciate
505,505,33802336,Visualizing output of convolutional layer in tensorflow,"<p>I'm trying to visualize the output of a convolutional layer in tensorflow using the function <code>tf.image_summary</code>. I'm already using it successfully in other instances (e. g. visualizing the input image), but have some difficulties reshaping the output here correctly. I have the following conv layer:</p>

<pre class=""lang-py prettyprint-override""><code>img_size = 256
x_image = tf.reshape(x, [-1,img_size, img_size,1], ""sketch_image"")

W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
</code></pre>

<p>So the output of <code>h_conv1</code> would have the shape <code>[-1, img_size, img_size, 32]</code>. Just using <code>tf.image_summary(""first_conv"", tf.reshape(h_conv1, [-1, img_size, img_size, 1]))</code> Doesn't account for the 32 different kernels, so I'm basically slicing through different feature maps here.</p>

<p>How can I reshape them correctly? Or is there another helper function I could use for including this output in the summary?</p>",33816991.0,5,0,,2015/11/19 11:14,22.0,2019/9/7 13:40,2017/8/24 15:54,,2464597.0,,867505.0,,1,36,tensorflow|conv-neural-network,22401,93.4011,,5,visualize output convolutional layer tensorflow try visualize output convolutional layer tensorflow use function already use successfully instance e g visualize input image difficulty reshape output correctly following conv layer output would shape use account different kernel basically slice different feature map reshape correctly another helper function could use include output summary
676,676,38080035,How to calculate the number of parameters of an LSTM network?,"<p>Is there a way to calculate the total number of parameters in a LSTM network.</p>

<p>I have found a example but I'm unsure of how correct <a href=""https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"">this</a> is or If I have understood it correctly.</p>

<p>For eg consider the following example:-</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import LSTM
model = Sequential()
model.add(LSTM(256, input_dim=4096, input_length=16))
model.summary()
</code></pre>

<h1>Output</h1>

<pre><code>____________________________________________________________________________________________________
Layer (type)                       Output Shape        Param #     Connected to                     
====================================================================================================
lstm_1 (LSTM)                      (None, 256)         4457472     lstm_input_1[0][0]               
====================================================================================================
Total params: 4457472
____________________________________________________________________________________________________
</code></pre>

<p>As per My understanding <code>n</code> is the input vector lenght.
And <code>m</code> is the number of time steps. and in this example they consider the number of hidden layers to be 1.</p>

<p>Hence according to the formula in <a href=""https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"">the post.</a> <code>4(nm+n^2)</code> in my example <code>m=16</code>;<code>n=4096</code>;<code>num_of_units=256</code></p>

<pre><code>4*((4096*16)+(4096*4096))*256 = 17246978048
</code></pre>

<p>Why is there such a difference? 
Did I misunderstand the example or was the formula wrong ?</p>",38086903.0,6,1,,2016/6/28 15:13,16.0,2020/6/21 9:48,2017/7/4 10:37,,5974433.0,,2527680.0,,1,24,machine-learning|neural-network|deep-learning|keras|lstm,28654,131.629,,3,calculate number parameter lstm network way calculate total number parameter lstm network find example unsure correct understand correctly eg consider following example output per understanding input vector lenght number time step example consider number hidden layer hence accord formula post example difference misunderstand example formula wrong
39,39,54652536,"Keras (Tensorflow backend) Error - Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph","<p>When trying to predict using a simple model I've previously trained I get the following error:</p>

<p><strong>Tensor input_1:0, specified in either feed_devices or fetch_devices was not found in the Graph</strong></p>

<p>at line:</p>

<pre><code>seatbelt_model.predict(image_arr, verbose=1)
</code></pre>

<p>in code:</p>

<pre><code>from tensorflow import keras
import tensorflow as tf
import numpy as np

graph = tf.get_default_graph()

seatbelt_model = keras.models.load_model(filepath='./graphs/seatbelt_A_3_81.h5')

class SeatbeltPredictor:
    INPUT_SHAPE = (-1, 120, 160, 1)

    @staticmethod
    def predict_seatbelt(image_arr):
        with graph.as_default():
            image_arr = np.array(image_arr).reshape(SeatbeltPredictor.INPUT_SHAPE)
            predicted_labels = seatbelt_model.predict(image_arr, verbose=1)
            return predicted_labels
</code></pre>

<p>The model has the following shape:</p>

<pre><code>input_layer = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1))
conv_0 = keras.layers.Conv2D(filters=32, kernel_size=[5, 5], activation=tf.nn.relu, padding=""SAME"")(input_layer)
pool_0 = keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=""VALID"")(conv_0)
conv_1 = keras.layers.Conv2D(filters=32, kernel_size=[5, 5], activation=tf.nn.relu, padding=""SAME"")(pool_0)
pool_1 = keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=""VALID"")(conv_1)
flat_0 = keras.layers.Flatten()(pool_1)
dense_0 = keras.layers.Dense(units=1024, activation=tf.nn.relu)(flat_0)
drop_0 = keras.layers.Dropout(rate=0.4, trainable=True)(dense_0)
dense_1 = keras.layers.Dense(units=2, activation=tf.nn.softmax)(drop_0)
</code></pre>

<p>If I run the following, I get a tensor result:</p>

<pre><code>graph.get_tensor_by_name('input_1:0')
&lt;tf.Tensor 'input_1:0' shape=(?, 120, 160, 1) dtype=float32&gt;
</code></pre>

<p>The name of the first layer is input_1</p>

<p>image_arr is of shape (1, 120, 160, 1)</p>

<p>Tensorflow 1.12</p>

<p>Any ideas?</p>",54672965.0,3,2,,2019/2/12 14:39,7.0,2020/6/4 9:12,2019/2/13 7:50,,4166932.0,,4166932.0,,1,18,python|tensorflow|keras,16321,63.651,,4,kera tensorflow backend error tensor input specify either feed device fetch device find graph try predict use simple model previously train get following error tensor input specify either feed device fetch device find graph line code model following shape run following get tensor result name first layer input image arr shape tensorflow idea
735,735,40690598,Can Keras with Tensorflow backend be forced to use CPU or GPU at will?,"<p>I have Keras installed with the Tensorflow backend and CUDA.  I'd like to sometimes on demand force Keras to use CPU.  Can this be done without say installing a separate CPU-only Tensorflow in a virtual environment?  If so how?  If the backend were Theano, the flags could be set, but I have not heard of Tensorflow flags accessible via Keras.  </p>",42750563.0,7,0,,2016/11/19 8:04,39.0,2019/11/7 8:33,2016/11/19 8:17,,3993741.0,,3993741.0,,1,107,python|machine-learning|tensorflow|keras,137896,488.358,,1,keras tensorflow backend force use cpu gpu keras instal tensorflow backend cuda like sometimes demand force kera use cpu without say instal separate cpu tensorflow virtual environment backend theano flag could set hear tensorflow flag accessible via kera
189,189,43457862,"What's the difference between ""samples_per_epoch"" and ""steps_per_epoch"" in fit_generator","<p>I was confused by this problem for several days...</p>

<p>My question is that why the training time has such massive difference between that I set the batch_size to be ""1"" and ""20"" for my generator.</p>

<p>If I set the <strong>batch_size</strong> to be <strong>1</strong>, the <strong>training time</strong> of <em><strong>1 epoch</strong></em> is approximately <strong>180 ~ 200 sec</strong>.
If I set the <strong>batch_size</strong> to be <strong>20</strong>, the <strong>training time</strong> of <strong>1 epoch</strong> is approximately <strong>3000 ~ 3200 sec</strong>. </p>

<p>However, this horrible difference between these training times seems to be abnormal..., since it should be the reversed result:
batch_size = 1, training time -> 3000 ~ 3200 sec.
batch_size = 20, training time -> 180 ~ 200 sec.</p>

<p>The input to my generator is not the file path, but the numpy arrays which are already loaded into the
memory via calling ""np.load()"".
So I think the I/O trade-off issue doesn't exist.</p>

<p>I'm using Keras-2.0.3 and my backend is tensorflow-gpu 1.0.1</p>

<p>I have seen the update of this merged <a href=""https://github.com/fchollet/keras/pull/5879/files"" rel=""noreferrer"">PR</a>,
but it seems that this change won't affect anything at all. (the usage is just the same with original one)</p>

<p>The <a href=""https://gist.github.com/HappyStorm/cb6c22ffec18a8fbb4912e9c79b6d87c"" rel=""noreferrer"">link</a> here is the gist of my self-defined generator and the part of my fit_generator.</p>",43459357.0,4,0,,2017/4/17 19:05,17.0,2020/12/29 13:16,2019/7/31 12:50,,1140335.0,,3844231.0,,1,35,keras,40505,90.43,,3,difference sample per epoch step per epoch fit generator confuse problem several day question training time massive difference set batch size generator set batch size training time epoch approximately sec set batch size training time epoch approximately sec however horrible difference training time seem abnormal since reversed result batch size training time sec batch size training time sec input generator file path numpy array already load memory via call np load think trade issue exist use kera backend tensorflow gpu see update merge pr seem change affect anything usage original one link gist self define generator part fit generator
444,444,48187283,What's the difference between LSTM() and LSTMCell()?,"<p>I've checked the source code for both functions, and it seems that LSTM() makes the LSTM network in general, while LSTMCell() only returns one cell. </p>

<p>However, in most cases people only use one LSTM Cell in their program. Does this mean when you have only one LSTM Cell (ex. in simple Seq2Seq), calling LSTMCell() and LSTM() would make no difference?</p>",48187516.0,1,0,,2018/1/10 12:14,11.0,2019/9/24 16:10,,,,,6017074.0,,1,29,machine-learning|keras,12689,66.2137,,3,difference lstm lstmcell check source code function seem lstm make lstm network general lstmcell return one cell however case people use one lstm cell program mean one lstm cell ex simple seq seq call lstmcell lstm would make difference
70,70,55894693,Understanding PyTorch einsum,"<p>I'm familiar with how <a href=""https://en.wikipedia.org/wiki/Einstein_notation"" rel=""noreferrer""><strong><code>einsum</code></strong></a> works in NumPy. A similar functionality is also offered by PyTorch: <a href=""https://pytorch.org/docs/stable/torch.html#torch.einsum"" rel=""noreferrer""><strong>torch.einsum()</strong></a>. What are the similarities and differences, either in terms of functionality or performance? The information available at PyTorch documentation is rather scanty and doesn't provide any insights regarding this.</p>",55894780.0,1,0,,2019/4/28 21:23,20.0,2020/9/30 2:38,2020/9/30 2:38,,2956066.0,,2956066.0,,1,20,python|numpy|pytorch|tensor|numpy-einsum,14586,75.6557,,3,understand pytorch einsum familiar work numpy similar functionality also offer pytorch torch einsum similarity difference either term functionality performance information available pytorch documentation rather scanty provide insight regard
363,363,45979848,Merge 2 sequential models in Keras,"<p>I a trying to merge 2 sequential models in keras. Here is the code:</p>

<pre><code>model1 = Sequential(layers=[
    # input layers and convolutional layers
    Conv1D(128, kernel_size=12, strides=4, padding='valid', activation='relu', input_shape=input_shape),
    MaxPooling1D(pool_size=6),
    Conv1D(256, kernel_size=12, strides=4, padding='valid', activation='relu'),
    MaxPooling1D(pool_size=6),
    Dropout(.5),

])

model2 = Sequential(layers=[
    # input layers and convolutional layers
    Conv1D(128, kernel_size=20, strides=5, padding='valid', activation='relu', input_shape=input_shape),
    MaxPooling1D(pool_size=5),
    Conv1D(256, kernel_size=20, strides=5, padding='valid', activation='relu'),
    MaxPooling1D(pool_size=5),
    Dropout(.5),

])

model = merge([model1, model2], mode = 'sum')
Flatten(),
Dense(256, activation='relu'),
Dropout(.5),
Dense(128, activation='relu'),
Dropout(.35),
# output layer
Dense(5, activation='softmax')
return model
</code></pre>

<p>Here is the error log:</p>

<blockquote>
  <p>File
  ""/nics/d/home/dsawant/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"",
  line 392, in is_keras_tensor
      raise ValueError('Unexpectedly found an instance of type <code>' + str(type(x)) + '</code>. ' ValueError: Unexpectedly found an instance of
  type <code>&lt;class 'keras.models.Sequential'&gt;</code>. Expected a symbolic tensor
  instance.</p>
</blockquote>

<p>Some more log:</p>

<blockquote>
  <p>ValueError: Layer merge_1 was called with an input that isn't a
  symbolic tensor. Received type: class 'keras.models.Sequential'.
  Full input: [keras.models.Sequential object at 0x2b32d518a780,
  keras.models.Sequential object at 0x2b32d521ee80]. All inputs to the
  layer should be tensors.</p>
</blockquote>

<p>How can I merge these 2 Sequential models that use different window sizes and apply functions like 'max', 'sum' etc to them?</p>",45981666.0,1,4,,2017/8/31 10:59,9.0,2019/9/29 5:56,,,,,7309225.0,,1,17,python|machine-learning|neural-network|keras|conv-neural-network,29980,55.3073,,3,merge sequential model kera try merge sequential model kera code error log file nics home dsawant anaconda lib python site package keras backend tensorflow backend py line keras tensor raise valueerror unexpectedly find instance type valueerror unexpectedly find instance type expect symbolic tensor instance log valueerror layer merge call input symbolic tensor receive type class kera model sequential full input kera model sequential object x b keras model sequential object x b ee input layer tensor merge sequential model use different window size apply function like max sum etc
265,265,44738273,Torch: How to shuffle a tensor by its rows?,"<p>I am currently working in torch to implement a random shuffle (on the rows, the first dimension in this case) on some input data. I am new to torch, so I have some troubles figuring out how permutation works..</p>

<p>The following is supposed to shuffle the data:</p>

<pre><code>if argshuffle then 
    local perm = torch.randperm(sids:size(1)):long()
    print(""\n\n\nSize of X and y before"")
    print(X:view(-1, 1000, 128):size())
    print(y:size())
    print(sids:size())
    print(""\nPerm size is: "")
    print(perm:size())
    X = X:view(-1, 1000, 128)[{{perm},{},{}}]
    y = y[{{perm},{}}]
    print(sids[{{1}, {}}])
    sids = sids[{{perm},{}}]
    print(sids[{{1}, {}}])
    print(X:size())
    print(y:size())
    print(sids:size())
    os.exit(69)
end
</code></pre>

<p>This prints out</p>

<pre><code>Size of X and y before 
99 
1000
128
[torch.LongStorage of size 3]

99 
1
[torch.LongStorage of size 2]

99 
1
[torch.LongStorage of size 2]

Perm size is: 
99 
[torch.LongStorage of size 1]
5
[torch.LongStorage of size 1x1]
5
[torch.LongStorage of size 1x1]


99 
1000
128
[torch.LongStorage of size 3]

99 
1
[torch.LongStorage of size 2]

99 
1
[torch.LongStorage of size 2]
</code></pre>

<p>Out of the value, I can imply that the function did not shuffle the data. How can I make it shuffle correctly, and what is the common solution in lua/torch? </p>",,4,1,,2017/6/24 16:00,2.0,2021/8/20 10:24,2021/8/20 10:24,,9067615.0,,3262787.0,,1,14,lua|pytorch|permutation|tensor|torch,17283,59.1505,,3,torch shuffle tensor row currently work torch implement random shuffle row first dimension case input data new torch trouble figure permutation work following suppose shuffle data print value imply function shuffle data make shuffle correctly common solution lua torch
443,443,48158017,Pytorch Operation to detect NaNs,"<p>
Is there a Pytorch-internal procedure to detect <code>NaN</code>s in Tensors? Tensorflow has the <code>tf.is_nan</code> and the <code>tf.check_numerics</code> operations ... Does Pytorch have something similar, somewhere? I could not find something like this in the docs... </p>

<p>I am looking specifically for a Pytorch internal routine, since I would like this to happen on the GPU as well as on the CPU. This excludes numpy - based solutions (like <code>np.isnan(sometensor.numpy()).any()</code>) ...</p>",48171528.0,5,1,,2018/1/8 21:03,7.0,2021/2/9 23:57,,,,,4385912.0,,1,42,pytorch,45552,189.634,,3,pytorch operation detect nan pytorch internal procedure detect tensor tensorflow operation pytorch something similar somewhere could find something like doc look specifically pytorch internal routine since would like happen gpu well cpu exclude numpy base solution like
677,677,38155836,Data Augmentation Image Data Generator Keras Semantic Segmentation,"<p>I'm fitting full convolutional network on some image data for semantic segmentation using Keras.  However, I'm having some problems overfitting. I don't have that much data and I want to do data augmentation.  However, as I want to do pixel-wise classification, I need any augmentations like flips, rotations, and shifts to apply to both feature images and the label images. Ideally I'd like to use the Keras ImageDataGenerator for on-the-fly transformations. However, as far as I can tell, you cannot do equivalent transformations on both the feature and label data.</p>

<p>Does anyone know if this is the case and if not, does anyone have any ideas? Otherwise, I'll use other tools to create a larger dataset and just feed it in all at once.</p>

<p>Thanks!</p>",39062323.0,2,4,,2016/7/2 2:04,12.0,2017/5/11 9:36,,,,,4631796.0,,1,26,computer-vision|deep-learning|image-segmentation|keras,17611,59.3831,,2,data augmentation image data generator keras semantic segmentation fit full convolutional network image data semantic segmentation use kera however problem overfitting much data want data augmentation however want pixel wise classification need augmentation like flip rotation shift apply feature image label image ideally like use kera imagedatagenerator fly transformation however far tell equivalent transformation feature label data anyone know case anyone idea otherwise use tool create large dataset fee thanks
765,765,51724309,AttributeError: module 'tensorflow' has no attribute 'name_scope' with Keras,"<p>I am trying to run a script, but I struggle already at the imports.
This import </p>

<pre><code>from keras.preprocessing.image import save_img
</code></pre>

<p>raises the following error:</p>

<pre><code>AttributeError: module 'tensorflow' has no attribute 'name_scope'.
</code></pre>

<p>I am using the following packages.</p>

<pre><code>Keras                     2.2.2,                     
Keras-Applications        1.0.4,                   
Keras-Preprocessing       1.0.2,                   
tensorflow                1.9.0,                     
tensorflow-gpu            1.9.0                
</code></pre>",51724531.0,7,0,,2018/8/7 10:16,4.0,2020/10/8 14:27,2018/8/7 10:16,,3297613.0,,4014969.0,,1,12,python|tensorflow|keras,41623,54.2773,,1,attributeerror module tensorflow attribute name scope kera try run script struggle already import import raise following error use following package
459,459,22471072,Convolutional Neural Network (CNN) for Audio,"<p>I have been following the tutorials on DeepLearning.net to learn how to implement a convolutional neural network that extracts features from images. The tutorial are well explained, easy to understand and follow.</p>

<p>I want to extend the same CNN to extract multi-modal features from videos (images + audio) at the same time.</p>

<p>I understand that video input is nothing but a sequence of images (pixel intensities) displayed in a period of time (ex. 30 FPS) associated with audio. However, I don't really understand what audio is, how it works, or how it is broken down to be feed into the network.</p>

<p>I have read a couple of papers on the subject (multi-modal feature extraction/representation), but none have explained how audio is inputed to the network.</p>

<p>Moreover, I understand from my studies that multi-modality representation is the way our brains really work as we don't deliberately filter out our senses to achieve understanding. It all happens simultaneously without us knowing about it through (joint representation). A simple example would be, if we hear a lion roar we instantly compose a mental image of a lion, feel danger and vice-versa. Multiple neural patterns are fired in our brains to achieve a comprehensive understanding of what a lion looks like, sounds like, feels like, smells like, etc.</p>

<p>The above mentioned is my ultimate goal, but for the time being I'm breaking down my problem for the sake of simplicity. </p>

<p>I would really appreciate if anyone can shed light on how audio is dissected and then later on represented in a convolutional neural network. I would also appreciate your thoughts with regards to multi-modal synchronisation, joint representations, and what is the proper way to train a CNN with multi-modal data.</p>

<p><strong>EDIT:</strong>
I have found out the audio can be represented as spectrograms. It as a common format for audio and is represented as a graph with two geometric dimensions where the horizontal line represents time and the vertical represents frequency. </p>

<p><img src=""https://i.stack.imgur.com/CktN1.jpg"" alt=""enter image description here""></p>

<p>Is it possible to use the same technique with images on these spectrograms? In other words can I simply use these spectrograms as input images for my convolutional neural network?</p>",33064750.0,2,0,,2014/3/18 5:28,25.0,2018/3/6 2:55,2014/3/27 4:51,,667127.0,,667127.0,,1,33,neural-network|convolution|feature-extraction|supervised-learning|deep-learning,20255,59.4261,2019/1/6 2:07,0,convolutional neural network cnn audio follow tutorial deeplearning net learn implement convolutional neural network extract feature image tutorial well explain easy understand follow want extend cnn extract multi modal feature videos image audio time understand video input nothing sequence image pixel intensity display period time ex fps associate audio however really understand audio work break fee network read couple paper subject multi modal feature extraction representation none explain audio input network moreover understand study multi modality representation way brain really work deliberately filter sens achieve understand happen simultaneously without u know joint representation simple example would hear lion roar instantly compose mental image lion feel danger vice versa multiple neural pattern fire brain achieve comprehensive understanding lion look like sound like feel like smell like etc mention ultimate goal time break problem sake simplicity would really appreciate anyone shed light audio dissect later represent convolutional neural network would also appreciate thought regard multi modal synchronisation joint representation proper way train cnn multi modal data edit find audio represent spectrogram common format audio represent graph two geometric dimension horizontal line represent time vertical represent frequency possible use technique image spectrogram word simply use spectrogram input image convolutional neural network
351,351,45576576,"keras ""unknown loss function"" error after defining custom loss function","<p>I defined a new loss function in keras in losses.py file. I close and relaunch anaconda prompt, but I got <code>ValueError: ('Unknown loss function', ':binary_crossentropy_2')</code>. I'm running keras using python2.7 and anaconda on windows 10.</p>

<p>I temporarily solve it by adding the loss function in the python file I compile my model.</p>",,5,2,,2017/8/8 19:27,2.0,2020/10/28 20:05,2017/8/9 14:04,,5051564.0,,5051564.0,,1,14,keras,17021,69.9239,,4,kera unknown loss function error define custom loss function define new loss function kera loss py file close relaunch anaconda prompt get run kera use python anaconda window temporarily solve add loss function python file compile model
600,600,49941426,AttributeError: 'collections.OrderedDict' object has no attribute 'eval',"<p>I have a model file which looks like this</p>

<pre><code>OrderedDict([('inp.conv1.conv.weight', 
          (0 ,0 ,0 ,.,.) = 
           -1.5073e-01  6.4760e-02  1.9156e-01
            1.2175e-01  3.5886e-02  1.3992e-01
           -1.5903e-01  8.2055e-02  1.7820e-01

          (0 ,0 ,1 ,.,.) = 
            1.0604e-01 -1.3653e-01  1.4803e-01
            6.0276e-02 -1.4674e-02  2.3059e-06
           -6.2192e-02 -5.1061e-03 -7.4145e-03

          (0 ,0 ,2 ,.,.) = 
           -5.5632e-02  3.5326e-02  6.5108e-02
            1.1411e-01 -4.4160e-02  8.2610e-02
            8.9979e-02 -3.5454e-02  4.2549e-02

          (1 ,0 ,0 ,.,.) = 
            4.8523e-02 -4.3961e-02  5.3614e-02
           -1.2644e-01  1.2777e-01  8.9547e-02
            3.8392e-02  2.7016e-02 -1.4552e-01

          (1 ,0 ,1 ,.,.) = 
            9.5537e-02  2.8748e-02  3.9772e-02
           -6.2410e-02  1.1264e-01  7.8663e-02
           -2.6374e-02  1.4401e-01 -1.7109e-01

          (1 ,0 ,2 ,.,.) = 
            5.1791e-02 -1.6388e-01 -1.7605e-01
            3.5028e-02  7.7164e-02 -1.4499e-01
           -2.9189e-02  2.7064e-03 -2.3228e-02

          (2 ,0 ,0 ,.,.) = 
           -7.4446e-03 -9.7202e-02 -1.4704e-01
           -1.0019e-02  8.1780e-02 -5.3530e-02
           -1.8412e-01  1.5988e-01 -1.3450e-01

          (2 ,0 ,1 ,.,.) = 
           -1.1075e-01 -5.2478e-02  6.0658e-02
            1.6739e-01 -2.9360e-02  1.2621e-01
            2.0686e-02  1.1468e-01  1.2282e-01
</code></pre>

<p>I want to do inference on this model, but when i do model.eval() i get,</p>

<p><code>AttributeError: 'collections.OrderedDict' object has no attribute 'eval</code>
Not really sure how to go about this, any suggestions on how i can get this fixed will be really helpful,Thanks in advance</p>",49942523.0,2,0,,2018/4/20 12:14,2.0,2020/3/5 10:00,,,,,8176285.0,,1,12,deep-learning|pytorch,44926,55.41,,4,attributeerror collection ordereddict object attribute eval model file look like want inference model model eval get really sure go suggestion get fix really helpful thanks advance
296,296,57237352,"What does ""unsqueeze"" do in Pytorch?","<p>I cannot understand how the example in the <a href=""https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"" rel=""noreferrer"">PyTorch documentation</a> corresponds to the explanation:</p>
<blockquote>
<p>Returns a new tensor with a dimension of size one inserted at the specified position. [...]</p>
<pre><code>&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])
&gt;&gt;&gt; torch.unsqueeze(x, 0)
tensor([[ 1,  2,  3,  4]])
&gt;&gt;&gt; torch.unsqueeze(x, 1)
tensor([[ 1],
        [ 2],
        [ 3],
        [ 4]])
</code></pre>
</blockquote>",57237378.0,5,0,,2019/7/28 1:43,20.0,2021/4/30 14:18,2021/3/19 10:07,,9067615.0,,6549804.0,,1,71,python|pytorch|torch,55508,226.977,,3,unsqueeze pytorch understand example pytorch documentation correspond explanation return new tensor dimension size one insert specified position
727,727,40430186,"TensorFlow ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)'","<p>I am new to TensorFlow and machine learning. I am trying to classify two objects a cup and a pendrive (jpeg images). I have trained and exported a model.ckpt successfully. Now I am trying to restore the saved model.ckpt for prediction. Here is the script: </p>

<pre><code>import tensorflow as tf
import math
import numpy as np
from PIL import Image
from numpy import array


# image parameters
IMAGE_SIZE = 64
IMAGE_CHANNELS = 3
NUM_CLASSES = 2

def main():
    image = np.zeros((64, 64, 3))
    img = Image.open('./IMG_0849.JPG')

    img = img.resize((64, 64))
    image = array(img).reshape(64,64,3)

    k = int(math.ceil(IMAGE_SIZE / 2.0 / 2.0 / 2.0 / 2.0)) 
    # Store weights for our convolution and fully-connected layers
    with tf.name_scope('weights'):
        weights = {
            # 5x5 conv, 3 input channel, 32 outputs each
            'wc1': tf.Variable(tf.random_normal([5, 5, 1 * IMAGE_CHANNELS, 32])),
            # 5x5 conv, 32 inputs, 64 outputs
            'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
            # 5x5 conv, 64 inputs, 128 outputs
            'wc3': tf.Variable(tf.random_normal([5, 5, 64, 128])),
            # 5x5 conv, 128 inputs, 256 outputs
            'wc4': tf.Variable(tf.random_normal([5, 5, 128, 256])),
            # fully connected, k * k * 256 inputs, 1024 outputs
            'wd1': tf.Variable(tf.random_normal([k * k * 256, 1024])),
            # 1024 inputs, 2 class labels (prediction)
            'out': tf.Variable(tf.random_normal([1024, NUM_CLASSES]))
        }

    # Store biases for our convolution and fully-connected layers
    with tf.name_scope('biases'):
        biases = {
            'bc1': tf.Variable(tf.random_normal([32])),
            'bc2': tf.Variable(tf.random_normal([64])),
            'bc3': tf.Variable(tf.random_normal([128])),
            'bc4': tf.Variable(tf.random_normal([256])),
            'bd1': tf.Variable(tf.random_normal([1024])),
            'out': tf.Variable(tf.random_normal([NUM_CLASSES]))
        }

   saver = tf.train.Saver()
   with tf.Session() as sess:
       saver.restore(sess, ""./model.ckpt"")
       print ""...Model Loaded...""   
       x_ = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE , IMAGE_SIZE , IMAGE_CHANNELS])
       y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES])
       keep_prob = tf.placeholder(tf.float32)

       init = tf.initialize_all_variables()

       sess.run(init)
       my_classification = sess.run(tf.argmax(y_, 1), feed_dict={x_:image})
       print 'Neural Network predicted', my_classification[0], ""for your image""


if __name__ == '__main__':
     main()
</code></pre>

<p>When I run the above script for prediction I get the following error:</p>

<pre><code>ValueError: Cannot feed value of shape (64, 64, 3) for Tensor u'Placeholder:0', which has shape '(?, 64, 64, 3)' 
</code></pre>

<p>What am I doing wrong? And how do I fix the shape of numpy array?</p>",40430291.0,2,0,,2016/11/4 19:10,16.0,2018/4/20 5:14,2017/8/21 12:00,,3714940.0,,6438307.0,,1,43,python|numpy|tensorflow|deep-learning,110706,88.3767,,4,tensorflow valueerror fee value shape tensor u placeholder shape new tensorflow machine learning try classify two object cup pendrive jpeg image train export model ckpt successfully try restore saved model ckpt prediction script run script prediction get following error wrong fix shape numpy array
637,637,51127344,Tensor is not an element of this graph; deploying Keras model,"<p>Im deploying a keras model and sending the test data to the model via a flask api. I have two files:</p>

<p>First: My Flask App:</p>

<pre><code># Let's startup the Flask application
app = Flask(__name__)

# Model reload from jSON:
print('Load model...')
json_file = open('models/model_temp.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
keras_model_loaded = model_from_json(loaded_model_json)
print('Model loaded...')

# Weights reloaded from .h5 inside the model
print('Load weights...')
keras_model_loaded.load_weights(""models/Model_temp.h5"")
print('Weights loaded...')

# URL that we'll use to make predictions using get and post
@app.route('/predict',methods=['GET','POST'])
def predict():
    data = request.get_json(force=True)
    predict_request = [data[""month""],data[""day""],data[""hour""]] 
    predict_request = np.array(predict_request)
    predict_request = predict_request.reshape(1,-1)
    y_hat = keras_model_loaded.predict(predict_request, batch_size=1, verbose=1)
    return jsonify({'prediction': str(y_hat)}) 

if __name__ == ""__main__"":
    # Choose the port
    port = int(os.environ.get('PORT', 9000))
    # Run locally
    app.run(host='127.0.0.1', port=port)
</code></pre>

<p>Second: The file Im using to send the json data sending to the api endpoint:</p>

<pre><code>response = rq.get('api url has been removed')
data=response.json()
currentDT = datetime.datetime.now()
Month = currentDT.month
Day = currentDT.day
Hour = currentDT.hour

url= ""http://127.0.0.1:9000/predict""
post_data = json.dumps({'month': month, 'day': day, 'hour': hour,})
r = rq.post(url,post_data)
</code></pre>

<p>Im getting this response from Flask regarding Tensorflow:</p>

<p>ValueError: Tensor Tensor(""dense_6/BiasAdd:0"", shape=(?, 1), dtype=float32) is not an element of this graph.</p>

<p>My keras model is a simple 6 dense layer model and trains with no errors.</p>

<p>Any ideas?</p>",,7,2,,2018/7/1 22:07,8.0,2021/2/2 18:30,,,,,890803.0,,1,27,python|tensorflow|flask|keras,14928,115.496,,3,tensor element graph deploy kera model im deploy keras model send test data model via flask api two file first flask app second file im use send json data send api endpoint im get response flask regard tensorflow valueerror tensor tensor dense biasadd shape dtype float element graph kera model simple dense layer model train error idea
649,649,37007495,Caffe didn't see hdf5.h when compiling,"<p>I am having trouble when installing Caffe Deep Learning Framework on Python:</p>

<p>When I run <code>make</code> command at caffe directory, it says </p>

<blockquote>
  <p>hdf5.h:no such directory</p>
</blockquote>

<p>The steps I have done:</p>

<ul>
<li><p>Update and upgrade my Ubuntu Server</p></li>
<li><p>Install Python 2.7</p></li>
<li><p>Having all of the dependencies base on <a href=""http://caffe.berkeleyvision.org/install_apt.html"" rel=""noreferrer"">http://caffe.berkeleyvision.org/install_apt.html</a></p></li>
<li><p>Run cp cp Makefile.config.example Makefile.config</p></li>
<li><p>Uncomment cpu_only = 1 in Makefile.config</p></li>
</ul>

<p>I will be grateful if someone can help me.</p>

<p>Error message: </p>

<pre><code>CXX src/caffe/util/hdf5.cpp
in file include from src/caffe/util/hdf5.cpp:1:0:
./include/caffe/util/hdf5.hpp:6:18: fatal error: hdf5.h: No such file or directory
compilation terminated 

Makefile:572 recipe for target '.build_release/src/caffe/util/hdf5.o'       
failed Make:*** [.build_release/src/caffe/util/hdf5.o] Error 1
</code></pre>",,5,5,,2016/5/3 14:53,8.0,2020/8/9 10:09,2016/5/24 6:40,,584518.0,,6286446.0,,1,26,python|deep-learning|caffe,30469,136.935,,1,caffe see hdf h compile trouble instal caffe deep learning framework python run command caffe directory say hdf h directory step update upgrade ubuntu server install python dependency base run cp cp makefile config example makefile config uncomment cpu makefile config grateful someone help error message
503,503,33519182,How can I run theano on GPU,"<p>If I run the following code with python 3.5</p>

<pre><code>import numpy as np
import time
import theano
A = np.random.rand(1000,10000).astype(theano.config.floatX)
B = np.random.rand(10000,1000).astype(theano.config.floatX)
np_start = time.time()
AB = A.dot(B)
np_end = time.time()
X,Y = theano.tensor.matrices('XY')
mf = theano.function([X,Y],X.dot(Y))
t_start = time.time()
tAB = mf(A,B)
t_end = time.time()
print (""NP time: %f[s], theano time: %f[s] **(times should be close when run
on CPU!)**"" %(np_end-np_start, t_end-t_start))
print (""Result difference: %f"" % (np.abs(AB-tAB).max(), ))
</code></pre>

<p>I get the output</p>

<pre><code>NP time: 0.161123[s], theano time: 0.167119[s] (times should be close when
run on CPU!)
Result difference: 0.000000
</code></pre>

<p>it says if the times are close, it means that I am running on my CPU.</p>

<p>How can I run this code on my GPU?</p>

<p><strong>NOTE:</strong></p>

<ul>
<li>I have a workstation with Nvidia Quadro k4200.</li>
<li>I have installed Cuda toolkit </li>
<li>I have successfully worked an cuda vectorAdd sample project on VS2012.</li>
</ul>",33519932.0,3,0,,2015/11/4 10:17,2.0,2018/10/18 14:14,2018/10/18 14:14,,2786884.0,,1935415.0,,1,8,python|cuda|gpu|theano,24905,50.3851,,4,run theano gpu run following code python get output say time close mean run cpu run code gpu note workstation nvidia quadro k instal cuda toolkit successfully work cuda vectoradd sample project v
380,380,46397258,"How to ""Merge"" Sequential models in Keras 2.0?","<p>I am trying to merge two Sequential models In Keras 2.0, using the following line:</p>

<pre><code>merged_model.add(Merge([model1, model2], mode='concat'))
</code></pre>

<p>This still works fine, but gives a warning: </p>

<pre><code>""The `Merge` layer is deprecated and will be removed after 08/2017. Use
instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc."" 
</code></pre>

<p>However, studying the Keras documentation and trying add, Add(), has not resulted in something that works. I have read several posts from people with the same problem, but found no solution that works in my case below. Any suggestions?</p>

<pre><code>model = Sequential()
model1 = Sequential()
model1.add(Dense(300, input_dim=40, activation='relu', name='layer_1'))
model2 = Sequential()
model2.add(Dense(300, input_dim=40, activation='relu', name='layer_2'))
merged_model = Sequential()

merged_model.add(Merge([model1, model2], mode='concat'))

merged_model.add(Dense(1, activation='softmax', name='output_layer'))
merged_model.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy'])

checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc',
save_best_only=True, verbose=2)
early_stopping = EarlyStopping(monitor=""val_loss"", patience=5)

merged_model.fit([x1, x2], y=y, batch_size=384, epochs=200,
             verbose=1, validation_split=0.1, shuffle=True, 
callbacks=[early_stopping, checkpoint])
</code></pre>

<p>EDIT: When I tried (as suggested below by Kent Sommer):</p>

<pre><code>from keras.layers.merge import concatenate
merged_model.add(concatenate([model1, model2]))
</code></pre>

<p>This was the error message:</p>

<pre><code>Traceback (most recent call last):
  File ""/anaconda/lib/python3.6/site- packages/keras/engine/topology.py"", line 425, 
in assert_input_compatibility
    K.is_keras_tensor(x)
  File ""/anaconda/lib/python3.6/site-
packages/keras/backend/tensorflow_backend.py"", line 403, in     is_keras_tensor
    raise ValueError('Unexpectedly found an instance of type `' +
 str(type(x)) + '`. '
ValueError: Unexpectedly found an instance of type 
`&lt;class'keras.models.Sequential'&gt;`. Expected a symbolic tensor instance.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""quoradeeptest_simple1.py"", line 78, in &lt;module&gt;
    merged_model.add(concatenate([model1, model2]))
  File ""/anaconda/lib/python3.6/site-packages/keras/layers/merge.py"",
 line 600, in concatenate return Concatenate(axis=axis, **kwargs)(inputs)
  File ""/anaconda/lib/python3.6/site-   packages/keras/engine/topology.py"", 
line 558, in __call__self.assert_input_compatibility(inputs)
  File ""/anaconda/lib/python3.6/site-packages/keras/engine/topology.py"", line 431, 
 in assert_input_compatibility str(inputs) + '.All inputs to the layer '
ValueError: Layer concatenate_1 was called with an input that isn't a
symbolic tensor. Received type: &lt;class 'keras.models.Sequential'&gt;. 
Full input: [&lt;keras.models.Sequential object at 0x140fa7ba8&gt;,
&lt;keras.models.Sequential object at 0x140fabdd8&gt;]. All inputs to the
layer should be tensors.
</code></pre>",46397522.0,3,1,,2017/9/25 3:39,2.0,2019/10/21 9:31,2017/9/25 5:18,,8612523.0,,8612523.0,,1,10,python|keras,29144,59.8582,,3,merge sequential model kera try merge two sequential model kera use following line still work fine give warning however study keras documentation try add add result something work read several post people problem find solution work case suggestion edit try suggest kent sommer error message
151,151,42689066,Convolutional Neural Net-Keras-val_acc Keyerror 'acc',"<p>I am trying to implement CNN by Theano. I used Keras library. My data set is 55 alphabet images, 28x28. </p>

<p>In the last part I get this error:
<a href=""https://i.stack.imgur.com/sQ0Cb.png]"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sQ0Cb.png]"" alt=""enter image description here""></a></p>

<pre><code>train_acc=hist.history['acc']
KeyError: 'acc'
</code></pre>

<p>Any help would be much appreciated. Thanks.</p>

<p>This is part of my code:</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>from keras.models import Sequential
from keras.models import Model
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD, RMSprop, adam
from keras.utils import np_utils

import matplotlib
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from urllib.request import urlretrieve
import pickle
import os
import gzip
import numpy as np
import theano
import lasagne
from lasagne import layers
from lasagne.updates import nesterov_momentum
from nolearn.lasagne import NeuralNet
from nolearn.lasagne import visualize
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from PIL import Image
import PIL.Image
#from Image import *
import webbrowser
from numpy import *
from sklearn.utils import shuffle
from sklearn.cross_validation import train_test_split
from tkinter import *
from tkinter.ttk import *
import tkinter

from keras import backend as K
K.set_image_dim_ordering('th')
%%%%%%%%%%

batch_size = 10

# number of output classes
nb_classes = 6

# number of epochs to train
nb_epoch = 5

# input iag dimensions
img_rows, img_clos = 28,28

# number of channels
img_channels = 3

# number of convolutional filters to use
nb_filters = 32

# number of convolutional filters to use
nb_pool = 2

# convolution kernel size
nb_conv = 3

%%%%%%%%

model = Sequential()

model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
                        border_mode='valid',
                        input_shape=(1, img_rows, img_clos)))
convout1 = Activation('relu')
model.add(convout1)
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
convout2 = Activation('relu')
model.add(convout2)
model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adadelta')

%%%%%%%%%%%%

hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
              show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))
            
            
hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
              show_accuracy=True, verbose=1, validation_split=0.2)
%%%%%%%%%%%%%%

train_loss=hist.history['loss']
val_loss=hist.history['val_loss']
train_acc=hist.history['acc']
val_acc=hist.history['val_acc']
xc=range(nb_epoch)
#xc=range(on_epoch_end)

plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss)
plt.plot(xc,val_loss)
plt.xlabel('num of Epochs')
plt.ylabel('loss')
plt.title('train_loss vs val_loss')
plt.grid(True)
plt.legend(['train','val'])
print (plt.style.available) # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])

plt.figure(2,figsize=(7,5))
plt.plot(xc,train_acc)
plt.plot(xc,val_acc)
plt.xlabel('num of Epochs')
plt.ylabel('accuracy')
plt.title('train_acc vs val_acc')
plt.grid(True)
plt.legend(['train','val'],loc=4)
#print plt.style.available # use bmh, classic,ggplot for big pictures
plt.style.use(['classic'])</code></pre>
</div>
</div>
</p>",55304690.0,8,0,,2017/3/9 7:20,3.0,2021/4/3 11:41,2017/3/9 9:07,,1886270.0,,7682122.0,,1,15,python|keras,26752,84.7094,,4,convolutional neural net kera val acc keyerror acc try implement cnn theano use kera library data set alphabet image x last part get error help would much appreciate thanks part code
569,569,49036993,Pytorch softmax: What dimension to use?,"<p>The function <code>torch.nn.functional.softmax</code> takes two parameters: <code>input</code> and <code>dim</code>.  According to its documentation, the softmax operation is applied to all slices of <code>input</code> along the specified <code>dim</code>, and will rescale them so that the elements lie in the range <code>(0, 1)</code> and sum to 1. </p>

<p>Let input be:</p>

<pre><code>input = torch.randn((3, 4, 5, 6))
</code></pre>

<p>Suppose I want the following, so that every entry in that array is 1:</p>

<pre><code>sum = torch.sum(input, dim = 3) # sum's size is (3, 4, 5, 1)
</code></pre>

<p>How should I apply softmax?</p>

<pre><code>softmax(input, dim = 0) # Way Number 0
softmax(input, dim = 1) # Way Number 1
softmax(input, dim = 2) # Way Number 2
softmax(input, dim = 3) # Way Number 3
</code></pre>

<p>My intuition tells me that is the last one, but I am not sure. English is not my first language and the use of the word <code>along</code> seemed confusing to me because of that.</p>

<p>I am not very clear on what ""along"" means, so I will use an example that could clarify things.  Suppose we have a tensor of size (s1, s2, s3, s4), and I want this to happen</p>",49103921.0,5,3,,2018/2/28 19:17,15.0,2021/6/30 11:24,,,,,2701950.0,,1,39,python|pytorch,56779,119.017,,3,pytorch softmax dimension use function take two parameter accord documentation softmax operation apply slice along specify rescale element lie range sum let input suppose want follow every entry array apply softmax intuition tell last one sure english first language use word seem confuse clear along mean use example could clarify thing suppose tensor size want happen
650,650,37020754,How to increase validation accuracy with deep neural net?,"<p>I am trying to build a 11 class image classifier with 13000 training images and 3000 validation images. I am using deep neural network which is being trained using mxnet. Training accuracy is increasing and reached above 80% but validation accuracy is coming in range of 54-57% and its not increasing. 
What can be the issue here? Should I increase the no of images?</p>",37023844.0,2,3,,2016/5/4 7:07,18.0,2016/5/4 9:34,,,,,3114663.0,,1,13,deep-learning|caffe|mxnet,31300,65.1822,,4,increase validation accuracy deep neural net try build class image classifier training image validation image use deep neural network train use mxnet training accuracy increase reach validation accuracy come range increase issue increase image
822,822,48085182,Cross Validation in Keras,"<p>I'm implementing a Multilayer Perceptron in Keras and using scikit-learn to perform cross-validation. For this, I was inspired by the code found in the issue <a href=""https://github.com/keras-team/keras/issues/1711#issuecomment-185801662"" rel=""nofollow noreferrer"">Cross Validation in Keras</a></p>

<pre class=""lang-python prettyprint-override""><code>from sklearn.cross_validation import StratifiedKFold

def load_data():
    # load your data using this function

def create model():
    # create your model using this function

def train_and_evaluate__model(model, data[train], labels[train], data[test], labels[test)):
    # fit and evaluate here.

if __name__ == &quot;__main__&quot;:
    X, Y = load_model()
    kFold = StratifiedKFold(n_splits=10)
    for train, test in kFold.split(X, Y):
        model = None
        model = create_model()
        train_evaluate(model, X[train], Y[train], X[test], Y[test])
</code></pre>
<p>In my studies on neural networks, I learned that the knowledge representation of the neural network is in the synaptic weights and during the network tracing process, the weights that are updated to thereby reduce the network error rate and improve its performance. (In my case, I'm using Supervised Learning)</p>
<p>For better training and assessment of neural network performance, a common method of being used is cross-validation that returns partitions of the data set for training and evaluation of the model.</p>
<p>My doubt is...</p>
<p>In this code snippet:</p>
<pre class=""lang-python prettyprint-override""><code>for train, test in kFold.split(X, Y):
    model = None
    model = create_model()
    train_evaluate(model, X[train], Y[train], X[test], Y[test])
</code></pre>
<p>We define, train and evaluate a new neural net for each of the generated partitions?</p>
<p>If my goal is to fine-tune the network for the entire dataset, why is it not correct to define a single neural network and train it with the generated partitions?</p>
<p>That is, why is this piece of code like this?</p>
<pre class=""lang-python prettyprint-override""><code>for train, test in kFold.split(X, Y):
    model = None
    model = create_model()
    train_evaluate(model, X[train], Y[train], X[test], Y[test])
</code></pre>
<p>and not so?</p>
<pre class=""lang-python prettyprint-override""><code>model = None
model = create_model()
for train, test in kFold.split(X, Y):
    train_evaluate(model, X[train], Y[train], X[test], Y[test])
</code></pre>
<p>Is my understanding of how the code works wrong? Or my theory?</p>",48087663.0,5,0,,2018/1/3 21:11,7.0,2021/7/12 13:39,2021/7/12 13:39,,4685471.0,,9169838.0,,1,16,machine-learning|keras|scikit-learn|neural-network|cross-validation,27565,64.7614,,3,cross validation kera implement multilayer perceptron kera use scikit learn perform cross validation inspire code find issue cross validation kera study neural network learn knowledge representation neural network synaptic weight network trace process weight update thereby reduce network error rate improve performance case use supervise learn good training assessment neural network performance common method use cross validation return partition data set training evaluation model doubt code snippet define train evaluate new neural net generated partition goal fine tune network entire dataset correct define single neural network train generated partition piece code like understanding code work wrong theory
603,603,50002543,What are transforms in PyTorch used for?,"<p>I am new with Pytorch and not very expert in CNN.
I have done a successful classifier with the tutorial that they provide <a href=""http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"" rel=""noreferrer"">Tutorial Pytorch</a>, but I don't really understand what I am doing when loading the data.</p>

<p>They do some data augmentation and normalisation for training, but when I try to modify the parameters, the code does not work.</p>

<pre><code># Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
</code></pre>

<p>Am I extending my training dataset? I don't see the data augmentation.</p>

<p>Why if I modify the value of transforms.RandomResizedCrop(224) the data loading stop working? </p>

<p>Do I need to transform as well the test dataset?</p>

<p>I am a bit confused with this data transformation that they do.</p>",50008841.0,2,1,,2018/4/24 13:08,3.0,2019/1/31 13:21,2019/1/5 13:15,,3924118.0,,2656616.0,,1,21,image|input|transformation|pytorch,31943,67.4175,,3,transforms pytorch use new pytorch expert cnn successful classifier tutorial provide tutorial pytorch really understand load data data augmentation normalisation training try modify parameter code work extend training dataset see data augmentation modify value transforms randomresizedcrop data load stop work need transform well test dataset bit confused data transformation
406,406,47299624,How to understand loss acc val_loss val_acc in Keras model fitting,"<p>I'm new on Keras and have some questions on how to understanding my model results. Here is my result:(for your convenience, I only paste the loss acc val_loss val_acc after each epoch here)</p>

<p>Train on 4160 samples, validate on 1040 samples as below:</p>

<pre><code>Epoch 1/20
4160/4160 - loss: 3.3455 - acc: 0.1560 - val_loss: 1.6047 - val_acc: 0.4721

Epoch 2/20
4160/4160 - loss: 1.7639 - acc: 0.4274 - val_loss: 0.7060 - val_acc: 0.8019

Epoch 3/20
4160/4160 - loss: 1.0887 - acc: 0.5978 - val_loss: 0.3707 - val_acc: 0.9087

Epoch 4/20
4160/4160 - loss: 0.7736 - acc: 0.7067 - val_loss: 0.2619 - val_acc: 0.9442

Epoch 5/20
4160/4160 - loss: 0.5784 - acc: 0.7690 - val_loss: 0.2058 - val_acc: 0.9433

Epoch 6/20
4160/4160 - loss: 0.5000 - acc: 0.8065 - val_loss: 0.1557 - val_acc: 0.9750

Epoch 7/20
4160/4160 - loss: 0.4179 - acc: 0.8296 - val_loss: 0.1523 - val_acc: 0.9606

Epoch 8/20
4160/4160 - loss: 0.3758 - acc: 0.8495 - val_loss: 0.1063 - val_acc: 0.9712

Epoch 9/20
4160/4160 - loss: 0.3202 - acc: 0.8740 - val_loss: 0.1019 - val_acc: 0.9798

Epoch 10/20
4160/4160 - loss: 0.3028 - acc: 0.8788 - val_loss: 0.1074 - val_acc: 0.9644

Epoch 11/20
4160/4160 - loss: 0.2696 - acc: 0.8923 - val_loss: 0.0581 - val_acc: 0.9856

Epoch 12/20
4160/4160 - loss: 0.2738 - acc: 0.8894 - val_loss: 0.0713 - val_acc: 0.9837

Epoch 13/20
4160/4160 - loss: 0.2609 - acc: 0.8913 - val_loss: 0.0679 - val_acc: 0.9740

Epoch 14/20
4160/4160 - loss: 0.2556 - acc: 0.9022 - val_loss: 0.0599 - val_acc: 0.9769

Epoch 15/20
4160/4160 - loss: 0.2384 - acc: 0.9053 - val_loss: 0.0560 - val_acc: 0.9846

Epoch 16/20
4160/4160 - loss: 0.2305 - acc: 0.9079 - val_loss: 0.0502 - val_acc: 0.9865

Epoch 17/20
4160/4160 - loss: 0.2145 - acc: 0.9185 - val_loss: 0.0461 - val_acc: 0.9913

Epoch 18/20
4160/4160 - loss: 0.2046 - acc: 0.9183 - val_loss: 0.0524 - val_acc: 0.9750

Epoch 19/20
4160/4160 - loss: 0.2055 - acc: 0.9120 - val_loss: 0.0440 - val_acc: 0.9885

Epoch 20/20
4160/4160 - loss: 0.1890 - acc: 0.9236 - val_loss: 0.0501 - val_acc: 0.9827
</code></pre>

<p>Here are my understandings:</p>

<ol>
<li><p>The two losses (both loss and val_loss) are decreasing and the tow acc (acc and val_acc) are increasing. So this indicates the modeling is trained in a good way.</p></li>
<li><p>The val_acc is the measure of how good the predictions of your model are. So for my case, it looks like the model was trained pretty well after 6 epochs, and the rest training is not necessary.</p></li>
</ol>

<p>My Questions are:</p>

<ol>
<li><p>The acc (the acc on training set) is always smaller, actually much smaller, than val_acc. Is this normal? Why this happens?In my mind, acc should usually similar to better than val_acc.</p></li>
<li><p>After 20 epochs, the acc is still increasing. So should I use more epochs and stop when acc stops increasing? Or I should stop where val_acc stops increasing, regardless of the trends of acc?</p></li>
<li><p>Is there any other thoughts on my results? </p></li>
</ol>

<p>Thanks!</p>",,2,0,,2017/11/15 4:56,11.0,2021/3/19 18:56,2020/12/16 16:56,,10908375.0,,4096462.0,,1,33,python|tensorflow|machine-learning|keras|deep-learning,40266,69.6198,,4,understand loss acc val loss val acc kera model fitting new kera question understand model result result convenience paste loss acc val loss val acc epoch train sample validate sample understanding two loss loss val loss decrease tow acc acc val acc increase indicate modeling train good way val acc measure good prediction model case look like model train pretty well epoch rest training necessary question acc acc training set always small actually much small val acc normal happen mind acc usually similar good val acc epochs acc still increase use epoch stop acc stop increase stop val acc stop increase regardless trend acc thought result thanks
540,540,36610290,Tensorflow and Multiprocessing: Passing Sessions,"<p>I have recently been working on a project that uses a neural network for virtual robot control. I used tensorflow to code it up and it runs smoothly. So far, I used sequential simulations to evaluate how good the neural network is, however, I want to run several simulations <strong>in parallel</strong> to reduce the amount of time it takes to get data.</p>

<p>To do this I am importing python's <code>multiprocessing</code> package. Initially I was passing the sess variable (<code>sess=tf.Session()</code>) to a function that would run the simulation. However, once I get to any statement that uses this <code>sess</code> variable, the process quits without a warning. After searching around for a bit I found these two posts:
<a href=""https://stackoverflow.com/questions/34900246/tensorflow-passing-a-session-to-a-python-multiprocess"">Tensorflow: Passing a session to a python multiprocess</a>
and <a href=""https://stackoverflow.com/questions/33758669/running-multiple-tensorflow-sessions-concurrently"">Running multiple tensorflow sessions concurrently</a></p>

<p>While they are highly related I haven't been able to figure out how to make it work. I tried creating a session for each individual process and assigning the weights of the neural net to its trainable parameters without success. I've also tried saving the session into a file and then loading it within a process, but no luck there either.</p>

<p>Has someone been able to pass a session (or clones of sessions) to several processes?</p>

<p>Thanks.</p>",54562773.0,2,7,,2016/4/13 21:54,17.0,2019/2/6 21:26,2017/5/23 10:29,,-1.0,,3691859.0,,1,26,python|parallel-processing|multiprocessing|tensorflow|reinforcement-learning,26020,54.0612,,3,tensorflow multiprocessing passing session recently work project use neural network virtual robot control use tensorflow code run smoothly far use sequential simulation evaluate good neural network however want run several simulation parallel reduce amount time take get data import python package initially pass sess variable function would run simulation however get statement use variable process quit without warning search around bit find two post tensorflow pass session python multiprocess run multiple tensorflow session concurrently highly relate able figure make work try create session individual process assign weight neural net trainable parameter without success also try save session file load within process luck either someone able pass session clone session several process thanks
335,335,45068243,How to check if keras tensorflow backend is GPU or CPU version?,"<p>I understand that when installing tensorflow, you either install the GPU or CPU version. How can I check which one is installed (I use linux). </p>

<p>If the GPU version is installed, would it be automatically running on CPU if GPU is unavailable or would it throw an error? And if GPU is available, is there a specific field or value you need to set to make sure it's running on GPU?</p>",,2,3,,2017/7/12 22:04,19.0,2017/11/22 9:38,2017/7/12 22:39,,681865.0,,5051564.0,,1,59,tensorflow|keras,142335,147.213,2018/7/1 12:37,1,check kera tensorflow backend gpu cpu version understand instal tensorflow either install gpu cpu version check one instal use linux gpu version instal would automatically run cpu gpu unavailable would throw error gpu available specific field value need set make sure run gpu
486,486,60137572,"Issues installing PyTorch 1.4 - ""No matching distribution found for torch===1.4.0""","<p>Used the install guide on <code>pytorch.org</code> on how to install it and the command I'm using is</p>
<pre><code>pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>But it's coming up with this error;</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement torch===1.4.0 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)</p>
<p>ERROR: No matching distribution found for torch===1.4.0</p>
</blockquote>
<p>Is this even a me-related issue? Can other people use this command?</p>
<p>Pip is installed and works for other modules, Python 3.8, CUDA version 10.1, Windows 10 Home 2004</p>",60141990.0,9,1,,2020/2/9 14:21,3.0,2020/10/6 18:39,2020/6/20 9:12,,-1.0,,7104503.0,,1,25,python|python-3.x|pip|installation|pytorch,44483,98.5928,,1,issue instal pytorch match distribution find torch use install guide install command use come error error could find version satisfy requirement torch version post post error match distribution find torch even related issue people use command pip instal work module python cuda version windows home
15,15,62465620,"Error ""Keras requires TensorFlow 2.2 or higher""","<p>I just installed Visual Studio 2019 and Tensorflow, but I cannot import Keras because I get the following error message:</p>
<blockquote>
<p>Keras requires TensorFlow 2.2 or higher. Install TensorFlow via <code>pip install tensorflow</code></p>
</blockquote>
<p>The problem is that I had no choice but to install Tensorflow 1.15, because I have the following setup:</p>
<ul>
<li>Visual Studio 2019</li>
<li>Python 3.7</li>
<li>CPU i7 920 (no avs, only SSE)</li>
<li>OS Windows 7 64</li>
<li>Nvidia GPU</li>
<li>CUDA 10.1</li>
</ul>
<p>I had to download and install a wheel for that Python version, my CPU, and that CUDA version named &quot;tensorflow-1.15.0-cp37-cp37m-win_amd64&quot;.</p>
<p>Tensorflow seems to work (it detects my GPU and prints a &quot;hello world&quot; message) but the problem is that Visual Studio installs the newest version of Keras.</p>
<p>How can I specify an older, compatible version, and what is the newer version compatible?</p>",62482183.0,6,2,,2020/6/19 7:46,8.0,2021/2/17 15:50,2020/9/14 20:11,,1945525.0,,9404261.0,,1,34,visual-studio|keras,86665,146.55100000000004,,1,error keras require tensorflow high instal visual studio tensorflow import kera get following error message keras require tensorflow high install tensorflow via problem choice install tensorflow following setup visual studio python cpu av sse os window nvidia gpu cuda download install wheel python version cpu cuda version name tensorflow cp cp win amd tensorflow seem work detect gpu print hello world message problem visual studio install new version kera specify old compatible version new version compatible
161,161,42815131,Keras for implement convolution neural network,"<p>I have just install tensorflow and keras. And I have the simple demo as follow:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
import numpy
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load pima indians dataset
dataset = numpy.loadtxt(""pima-indians-diabetes.csv"", delimiter="","")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = Sequential()
model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))
model.add(Dense(8, init='uniform', activation='relu'))
model.add(Dense(1, init='uniform', activation='sigmoid'))
# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Fit the model
model.fit(X, Y, nb_epoch=10, batch_size=10)
# evaluate the model
scores = model.evaluate(X, Y)
print(""%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
</code></pre>

<p>And I have this warning:</p>

<pre><code>/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=""relu"", kernel_initializer=""uniform"", input_dim=8)` '` call to the Keras 2 API: ' + signature)
/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, activation=""relu"", kernel_initializer=""uniform"")` '` call to the Keras 2 API: ' + signature)
/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=""sigmoid"", kernel_initializer=""uniform"")` '` call to the Keras 2 API: ' + signature)
/usr/local/lib/python2.7/dist-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`. warnings.warn('The `nb_epoch` argument in `fit` '
</code></pre>

<p>So, How can I handle this? </p>",42820423.0,3,1,,2017/3/15 16:11,2.0,2020/5/22 11:48,2017/12/4 15:31,,216356.0,,7701969.0,,1,19,python|deep-learning|keras,14683,62.0673,,3,kera implement convolution neural network install tensorflow kera simple demo follow warning handle
674,674,37984304,how to save a scikit-learn pipline with keras regressor inside to disk?,"<p>I have a scikit-learn pipline with kerasRegressor in it:</p>

<pre><code>estimators = [
    ('standardize', StandardScaler()),
    ('mlp', KerasRegressor(build_fn=baseline_model, nb_epoch=5, batch_size=1000, verbose=1))
    ]
pipeline = Pipeline(estimators)
</code></pre>

<p>After, training the pipline, I am trying to save to disk using joblib...</p>

<pre><code>joblib.dump(pipeline, filename , compress=9)
</code></pre>

<p>But I am getting an error:</p>

<blockquote>
  <p>RuntimeError: maximum recursion depth exceeded</p>
</blockquote>

<p>How would you save the pipeline to disk?</p>",43415459.0,2,2,,2016/6/23 6:57,9.0,2020/9/2 22:05,,,,,314548.0,,1,19,python|machine-learning|scikit-learn|keras|joblib,8398,53.2967,,4,save scikit learn pipline kera regressor inside disk scikit learn pipline kerasregressor train pipline try save disk use joblib get error runtimeerror maximum recursion depth exceed would save pipeline disk
210,210,43876770,Pandas DataFrame and Keras,"<p>I'm trying to perform a sentiment analysis in Python using Keras. To do so, I need to do a word embedding of my texts. The problem appears when I try to fit the data to my model:</p>

<pre><code>model_1 = Sequential()
model_1.add(Embedding(1000,32, input_length = X_train.shape[0]))
model_1.add(Flatten())
model_1.add(Dense(250, activation='relu'))
model_1.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>The shape of my train data is</p>

<pre><code>(4834,)
</code></pre>

<p>And is a Pandas series object. When I try to fit my model and validate it with some other data I get this error:</p>

<pre><code>model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=64, verbose=2)
</code></pre>

<blockquote>
  <p>ValueError: Error when checking model input: expected
  embedding_1_input to have shape (None, 4834) but got array with shape
  (4834, 1)</p>
</blockquote>

<p>How can I reshape my data to make it suited for Keras? I've been trying with np.reshape but I cannot place None elements with that function.</p>

<p>Thanks in advance</p>",43877033.0,4,0,,2017/5/9 17:55,7.0,2021/2/26 20:15,,,,,6588261.0,,1,18,python|pandas|keras,34592,66.5559,,3,panda dataframe keras try perform sentiment analysis python use kera need word embedding text problem appear try fit data model shape train data pandas series object try fit model validate data get error valueerror error check model input expect embed input shape none get array shape reshape data make suit kera try np reshape place none element function thanks advance
177,177,43196636,How to concatenate two layers in keras?,"<p>I have an example of a neural network with two layers. The first layer takes two arguments and has one output. The second should take one argument as result of the first layer and one additional argument. It should looks like this:</p>

<pre><code>x1  x2  x3
 \  /   /
  y1   /
   \  /
    y2
</code></pre>

<p>So, I'd created a model with two layers and tried to merge them but it returns an error: <code>The first layer in a Sequential model must get an ""input_shape"" or ""batch_input_shape"" argument.</code> on the line <code>result.add(merged)</code>.</p>

<p>Model:</p>

<pre class=""lang-python prettyprint-override""><code>first = Sequential()
first.add(Dense(1, input_shape=(2,), activation='sigmoid'))

second = Sequential()
second.add(Dense(1, input_shape=(1,), activation='sigmoid'))

result = Sequential()
merged = Concatenate([first, second])
ada_grad = Adagrad(lr=0.1, epsilon=1e-08, decay=0.0)
result.add(merged)
result.compile(optimizer=ada_grad, loss=_loss_tensor, metrics=['accuracy'])
</code></pre>",43196972.0,3,1,,2017/4/4 0:56,48.0,2021/7/21 11:37,2021/7/21 11:37,,4565615.0,,944000.0,,1,106,python|machine-learning|keras|neural-network|hierarchical,137895,230.158,,3,concatenate two layer kera example neural network two layer first layer take two argument one output second take one argument result first layer one additional argument look like create model two layer try merge return error line model
684,684,38303974,tensorflow running error with cublas,"<p>when I successfully install tensorflow on cluster, I immediately running mnist demo to check if it's going well, but here I came up with a problem. I don't know what is this all about, but it looks like the error is coming from CUDA</p>

<pre><code>python3 -m tensorflow.models.image.mnist.convolutional
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
Extracting data/train-images-idx3-ubyte.gz
Extracting data/train-labels-idx1-ubyte.gz
Extracting data/t10k-images-idx3-ubyte.gz
Extracting data/t10k-labels-idx1-ubyte.gz
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: Tesla K20m
major: 3 minor: 5 memoryClockRate (GHz) 0.7055
pciBusID 0000:03:00.0
Total memory: 5.00GiB
Free memory: 4.92GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K20m, pci bus id: 0000:03:00.0)
Initialized!
E tensorflow/stream_executor/cuda/cuda_blas.cc:461] failed to run cuBLAS routine cublasSgemm_v2: CUBLAS_STATUS_EXECUTION_FAILED
Traceback (most recent call last):
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 715, in _do_call
return fn(*args)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 697, in _run_fn
status, run_metadata)
  File ""/home/gpuusr/local/lib/python3.5/contextlib.py"", line 66, in __exit__
next(self.gen)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"", line 450, in raise_exception_on_not_ok_status
pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(64, 3136), b.shape=(3136, 512), m=64, n=512, k=3136
 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, Variable_4/read)]]
 [[Node: add_5/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_299_add_5"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/gpuusr/local/lib/python3.5/runpy.py"", line 170, in _run_module_as_main
""__main__"", mod_spec)
  File ""/home/gpuusr/local/lib/python3.5/runpy.py"", line 85, in _run_code
exec(code, run_globals)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 316, in &lt;module&gt;
tf.app.run()
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 30, in run
sys.exit(main(sys.argv))
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 294, in main
feed_dict=feed_dict)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 372, in run
run_metadata_ptr)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 636, in _run
feed_dict_string, options, run_metadata)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 708, in _do_run
target_list, options, run_metadata)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 728, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(64, 3136), b.shape=(3136, 512), m=64, n=512, k=3136
 [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](Reshape, Variable_4/read)]]
 [[Node: add_5/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_299_add_5"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
Caused by op 'MatMul', defined at:
  File ""/home/gpuusr/local/lib/python3.5/runpy.py"", line 170, in _run_module_as_main
""__main__"", mod_spec)
  File ""/home/gpuusr/local/lib/python3.5/runpy.py"", line 85, in _run_code
exec(code, run_globals)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 316, in &lt;module&gt;
tf.app.run()
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 30, in run
sys.exit(main(sys.argv))
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 221, in main
logits = model(train_data_node, True)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/models/image/mnist/convolutional.py"", line 213, in model
hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py"", line 1209, in matmul
name=name)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 1178, in _mat_mul
transpose_b=transpose_b, name=name)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py"", line 704, in apply_op
op_def=op_def)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2260, in create_op
original_op=self._default_original_op, op_def=op_def)
  File ""/home/gpuusr/local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1230, in __init__
self._traceback = _extract_stack()

Segmentation fault (core dumped)
</code></pre>",,6,9,,2016/7/11 9:53,5.0,2020/11/29 18:40,2016/7/11 11:40,,681865.0,,6573915.0,,1,11,gpu|tensorflow|deep-learning|cublas,27311,62.9454,,1,tensorflow run error cublas successfully install tensorflow cluster immediately run mnist demo check go well come problem know look like error come cuda
478,478,59737875,Keras: change learning rate,"<p>I'm trying to <strong>change</strong> the learning rate of my model after it has been trained with a different learning rate.</p>

<p>I read <a href=""https://github.com/keras-team/keras/issues/888"" rel=""noreferrer"">here</a>, <a href=""https://github.com/keras-team/keras/issues/898"" rel=""noreferrer"">here</a>, <a href=""https://stackoverflow.com/questions/57301698/how-to-change-a-learning-rate-for-adam-in-tf2"">here</a> and some other places i can't even find anymore.</p>

<p>I tried:</p>

<pre><code>model.optimizer.learning_rate.set_value(0.1)
model.optimizer.lr = 0.1
model.optimizer.learning_rate = 0.1
K.set_value(model.optimizer.learning_rate, 0.1)
K.set_value(model.optimizer.lr, 0.1)
model.optimizer.lr.assign(0.1)
</code></pre>

<p>... but none of them worked!
I don't understand how there could be such confusion around such a simple thing. Am I missing something?</p>

<p><strong>EDIT: Working example</strong></p>

<p>Here is a working example of what I'd like to do:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
import keras
import numpy as np

model = Sequential()

model.add(Dense(1, input_shape=(10,)))

optimizer = keras.optimizers.Adam(lr=0.01)
model.compile(loss='mse',
              optimizer=optimizer)

model.fit(np.random.randn(50,10), np.random.randn(50), epochs=50)

# Change learning rate to 0.001 and train for 50 more epochs

model.fit(np.random.randn(50,10), np.random.randn(50), initial_epoch=50, epochs=50)
</code></pre>",62113860.0,6,4,,2020/1/14 16:22,14.0,2021/8/17 23:24,2020/1/14 16:36,,4690023.0,,4690023.0,,1,26,python|tensorflow|keras,38402,119.537,,4,kera change learn rate try change learning rate model train different learning rate read place even find anymore try none work understand could confusion around simple thing miss something edit work example working example like
69,69,55890813,How to fix 'Object arrays cannot be loaded when allow_pickle=False' for imdb.load_data() function?,"<p>I'm trying to implement the binary classification example using the IMDb dataset in <strong>Google Colab</strong>. I have implemented this model before. But when I tried to do it again after a few days, it returned a <code>value error: 'Object arrays cannot be loaded when allow_pickle=False'</code> for the load_data() function.</p>
<p>I have already tried solving this, referring to an existing answer for a similar problem: <a href=""https://stackoverflow.com/questions/55824625/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-in-the-sketc?answertab=votes#tab-top"">How to fix &#39;Object arrays cannot be loaded when allow_pickle=False&#39; in the sketch_rnn algorithm</a>.
But it turns out that just adding an allow_pickle argument isn't sufficient.</p>
<p>My code:</p>
<pre><code>from keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
</code></pre>
<p>The error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-1-2ab3902db485&gt; in &lt;module&gt;()
      1 from keras.datasets import imdb
----&gt; 2 (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

2 frames
/usr/local/lib/python3.6/dist-packages/keras/datasets/imdb.py in load_data(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)
     57                     file_hash='599dadb1135973df5b59232a0e9a887c')
     58     with np.load(path) as f:
---&gt; 59         x_train, labels_train = f['x_train'], f['y_train']
     60         x_test, labels_test = f['x_test'], f['y_test']
     61 

/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py in __getitem__(self, key)
    260                 return format.read_array(bytes,
    261                                          allow_pickle=self.allow_pickle,
--&gt; 262                                          pickle_kwargs=self.pickle_kwargs)
    263             else:
    264                 return self.zip.read(key)

/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py in read_array(fp, allow_pickle, pickle_kwargs)
    690         # The array contained Python objects. We need to unpickle the data.
    691         if not allow_pickle:
--&gt; 692             raise ValueError(&quot;Object arrays cannot be loaded when &quot;
    693                              &quot;allow_pickle=False&quot;)
    694         if pickle_kwargs is None:

ValueError: Object arrays cannot be loaded when allow_pickle=False
</code></pre>",56243777.0,25,4,,2019/4/28 13:39,24.0,2021/7/1 19:52,2020/10/15 16:01,,6073.0,,11423025.0,,1,136,python|numpy|keras,185999,1148.08,,4,fix object array load allow pickle false imdb load data function try implement binary classification example use imdb dataset google colab implement model try day return load data function already try solve refer exist answer similar problem fix object array load allow pickle false sketch rnn algorithm turn add allow pickle argument sufficient code error
801,801,53033556,How should the learning rate change as the batch size change?,"<p>When I increase/decrease batch size of the mini-batch used in SGD, should I change learning rate? If so, then how?</p>

<p>For reference, I was discussing with someone, and it was said that, when batch size is increased, the learning rate should be decreased by some extent. </p>

<p>My understanding is when I increase batch size, computed average gradient will be less noisy and so I either keep same learning rate or increase it. </p>

<p>Also, if I use an adaptive learning rate optimizer, like Adam or RMSProp, then I guess I can leave learning rate untouched.</p>

<p>Please correct me if I am mistaken and give any insight on this.</p>",53046624.0,3,0,,2018/10/28 16:17,33.0,2021/4/4 1:10,2020/5/24 5:00,,996260.0,,996260.0,,1,55,machine-learning|deep-learning,21820,158.355,2021/4/6 1:31,0,learn rate change batch size change increase decrease batch size mini batch use sgd change learn rate reference discuss someone say batch size increase learn rate decrease extent understanding increase batch size compute average gradient less noisy either keep learn rate increase also use adaptive learning rate optimizer like adam rmsprop guess leave learn rate untouched please correct mistake give insight
804,804,53256877,How to convert keras(h5) file to a tflite file?,"<p>I got an keras(h5) file. I need to convert it to tflite??
I researched, First i need to go via h5 -> pb -> tflite
(because h5 - tflite sometimes results in some issue)</p>",,8,0,,2018/11/12 6:25,10.0,2021/8/10 5:59,,,,user10638958,,,1,30,python|tensorflow|machine-learning|keras,31658,113.002,,3,convert kera h file tflite file get keras h file need convert tflite research first need go via h pb tflite h tflite sometimes result issue
640,640,66516388,AttributeError: module 'torchtext.data' has no attribute 'Field',"<p>I want to run a git <a href=""https://github.com/fastnlp/style-transformer"" rel=""noreferrer"">project</a> used pytorch and torchtext but when I run it, it raise error:</p>
<pre><code>  File &quot;main.py&quot;, line 60, in &lt;module&gt;
    main()
  File &quot;main.py&quot;, line 50, in main
    train_iters, dev_iters, test_iters, vocab = load_dataset(config)
  File &quot;/home/esmailza/style transfer/style-transformer/data.py&quot;, line 23, in load_dataset
    TEXT = data.Field(batch_first=True, eos_token='&lt;eos&gt;')
AttributeError: module 'torchtext.data' has no attribute 'Field'
</code></pre>
<p>torch version  = 1.8.0
torchtext version = 0.9</p>
<pre><code>
def load_dataset(config, train_pos='train.pos', train_neg='train.neg',
                 dev_pos='dev.pos', dev_neg='dev.neg',
                 test_pos='test.pos', test_neg='test.neg'):

    root = config.data_path
    TEXT = data.Field(batch_first=True, eos_token='&lt;eos&gt;')
    
    dataset_fn = lambda name: data.TabularDataset(
        path=root + name,
        format='tsv',
        fields=[('text', TEXT)]
    )
</code></pre>",66517960.0,2,2,,2021/3/7 12:12,1.0,2021/7/6 23:23,2021/3/7 12:27,,4110056.0,,4110056.0,,1,15,python|pytorch,10808,56.135,,4,attributeerror module torchtext data attribute field want run git project use pytorch torchtext run raise error torch version torchtext version
37,37,54527439,"Differences in SciKit Learn, Keras, or Pytorch","<p>Are these libraries fairly interchangeable?</p>

<p>Looking here, <a href=""https://stackshare.io/stackups/keras-vs-pytorch-vs-scikit-learn"" rel=""noreferrer"">https://stackshare.io/stackups/keras-vs-pytorch-vs-scikit-learn</a>, it seems the major difference is the underlying framework (at least for PyTorch).</p>",54532702.0,1,3,,2019/2/5 3:42,16.0,2019/11/14 14:01,2019/11/14 14:01,,9286096.0,,9286096.0,,1,33,python|machine-learning|keras|scikit-learn|pytorch,23294,117.069,2019/2/6 15:17,3,difference scikit learn kera pytorch library fairly interchangeable look seem major difference underlying framework least pytorch
386,386,46503816,Keras conv1d layer parameters: filters and kernel_size,"<p>I am very confused by these two parameters in the conv1d layer from keras:
<a href=""https://keras.io/layers/convolutional/#conv1d"" rel=""noreferrer"">https://keras.io/layers/convolutional/#conv1d</a></p>

<p>the documentation says:</p>

<pre><code>filters: Integer, the dimensionality of the output space (i.e. the number output of filters in the convolution).
kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.
</code></pre>

<p>But that does not seem to relate to the standard terminologies I see on many tutorials such as <a href=""https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner"" rel=""noreferrer"">https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner</a>'s-Guide-To-Understanding-Convolutional-Neural-Networks/ and <a href=""https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/</a></p>

<p>Using the second tutorial link which uses Keras, I'd imagine that in fact 'kernel_size' is relevant to the conventional 'filter' concept which defines the sliding window on the input feature space. But what about the 'filter' parameter in conv1d? What does it do? </p>

<p>For example, in the following code snippet:</p>

<pre><code>model.add(embedding_layer)
model.add(Dropout(0.2))
model.add(Conv1D(filters=100, kernel_size=4, padding='same', activation='relu'))
</code></pre>

<p>suppose the embedding layer outputs a matrix of dimension 50 (rows, each row is a word in a sentence) x 300 (columns, the word vector dimension), how does the conv1d layer transforms that matrix?</p>

<p>Many thanks</p>",46504997.0,2,0,,2017/9/30 14:48,27.0,2021/7/8 12:43,,,,,1783398.0,,1,39,keras|convolution,35230,89.7877,,3,kera conv layer parameter filter kernel size confused two parameter conv layer kera conv documentation say seem relate standard terminology see many tutorial guide understanding convolutional neural network use second tutorial link use kera imagine fact kernel size relevant conventional filter concept define slide window input feature space filter parameter conv example following code snippet suppose embed layer output matrix dimension row row word sentence x columns word vector dimension conv layer transforms matrix many thanks
809,809,53483685,Keras breaks Anaconda Prompt,"<p>I am switching from tensorflow to keras on my Anaconda distribution and am having some problems with the latter. I install it through Anaconda prompt with the command</p>

<pre><code>conda install keras
</code></pre>

<p>and I do not think the installation is properly finished since it runs the command</p>

<pre><code>python -c ""import keras""  1&gt;nul 2&gt;&amp;1
</code></pre>

<p>and closes the prompt. Afterwards, if I am to open the command line it automatically runs the command above and closes it, so I am unable to use the prompt. This has happened for both Anaconda 5.3.1 (Python 3.7) and Anaconda 5.2.0 (Python 3.6).</p>

<p>Thank you very much in advance. Any help will be much appreciated.</p>",,9,5,,2018/11/26 14:51,6.0,2020/10/8 15:19,,,,,6173146.0,,1,10,python|keras|anaconda,8283,50.6728,,1,kera break anaconda prompt switch tensorflow keras anaconda distribution problem latter install anaconda prompt command think installation properly finish since run command close prompt afterwards open command line automatically run command close unable use prompt happen anaconda python anaconda python thank much advance help much appreciate
805,805,53266350,How to tell PyTorch to not use the GPU?,"<p>I want to do some timing comparisons between CPU &amp; GPU as well as some profiling and would like to know if there's a way to tell <a href=""/questions/tagged/pytorch"" class=""post-tag"" title=""show questions tagged &#39;pytorch&#39;"" rel=""tag"">pytorch</a> to not use the GPU and instead use the CPU only? I realize I could install another CPU-only <a href=""/questions/tagged/pytorch"" class=""post-tag"" title=""show questions tagged &#39;pytorch&#39;"" rel=""tag"">pytorch</a>, but hoping there's an easier way.</p>",53266932.0,5,0,,2018/11/12 16:30,11.0,2021/8/1 17:35,2021/5/24 18:47,,9215780.0,,46190.0,,1,37,pytorch,43913,132.57,,1,tell pytorch use gpu want timing comparison cpu gpu well profiling would like know way tell pytorch use gpu instead use cpu realize could install another cpu pytorch hop easy way
175,175,43151775,How to have parallel convolutional layers in keras?,"<p>I am a little new to neural networks and keras. I have some images with size 6*7 and the size of the filter is 15. I want to have several filters and train a convolutional layer separately on each and then combine them. I have looked at one example here:</p>

<pre><code>model = Sequential()
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],
                    border_mode='valid',
                    input_shape=input_shape))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=pool_size))
model.add(Dropout(0.25))
model.add(Flatten(input_shape=input_shape))
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('tanh'))
</code></pre>

<p>This model works with one filter. Can anybody give me some hints on how to modify the model to work with parallel convolutional layers.</p>

<p>Thanks</p>",43173316.0,2,0,,2017/4/1 1:22,15.0,2021/7/12 3:58,,,,,2462485.0,,1,22,neural-network|keras|conv-neural-network|keras-layer,19254,78.9381,,3,parallel convolutional layer kera little new neural network kera image size size filter want several filter train convolutional layer separately combine look one example model work one filter anybody give hint modify model work parallel convolutional layer thanks
834,834,49174342,How to effectively make use of a GPU for reinforcement learning?,"<p>Recently i looked into reinforcement learning and there was one question bugging me, that i could not find an answer for: How is training effectively done using GPUs? To my understanding constant interaction with an environment is required, which for me seems like a huge bottleneck, since this task is often non-mathematical / non-parallelizable. Yet for example Alpha Go uses multiple TPUs/GPUs. So how are they doing it?</p>",49180318.0,2,0,,2018/3/8 13:32,6.0,2018/3/15 21:47,,,,,4812335.0,,1,16,gpu|reinforcement-learning,7072,53.7982,,0,effectively make use gpu reinforcement learning recently look reinforcement learning one question bug could find answer train effectively use gpus understanding constant interaction environment require seem like huge bottleneck since task often non mathematical non parallelizable yet example alpha go use multiple tpus gpus
482,482,59847045,Should I use @tf.function for all functions?,"<p>An <a href=""https://www.tensorflow.org/tutorials/customization/performance"" rel=""noreferrer"">official tutorial</a> on <code>@tf.function</code> says:</p>

<blockquote>
  <p>To get peak performance and to make your model deployable anywhere,
  use tf.function to make graphs out of your programs. Thanks to
  AutoGraph, a surprising amount of Python code just works with
  tf.function, but there are still pitfalls to be wary of.</p>
  
  <p>The main takeaways and recommendations are:</p>
  
  <ul>
  <li>Don't rely on Python side effects like object mutation or list appends.</li>
  <li>tf.function works best with TensorFlow ops, rather than NumPy ops or Python primitives.</li>
  <li>When in doubt, use the for x in y idiom.</li>
  </ul>
</blockquote>

<p>It only mentions <strong>how</strong> to implement <code>@tf.function</code> annotated functions but not <strong>when</strong> to use it.</p>

<p>Is there a heuristic on how to decide whether I should at least try to annotate a function with <code>tf.function</code>? It seems that there are no reasons not to do it, unless I am to lazy to remove side effects or change some things like <code>range()</code>-> <code>tf.range()</code>. But if I am willing to do this...</p>

<p><strong>Is there any reason not to use <code>@tf.function</code> for all functions?</strong></p>",,3,6,,2020/1/21 18:21,11.0,2020/9/26 10:50,2020/4/23 16:35,,502727.0,,502727.0,,1,24,tensorflow|keras|tensorflow2.0|tf.keras|tensorflow2.x,5362,60.3173,,3,use tf function function official tutorial say get peak performance make model deployable anywhere use tf function make graph program thanks autograph surprising amount python code work tf function still pitfall wary main takeaway recommendation rely python side effect like object mutation list appends tf function work best tensorflow ops rather numpy ops python primitive doubt use x idiom mention implement annotated function use heuristic decide whether least try annotate function seem reason unless lazy remove side effect change thing like willing reason use function
246,246,44524901,How to do product of matrices in PyTorch,"<p>In numpy I can do a simple matrix multiplication like this:</p>

<pre><code>a = numpy.arange(2*3).reshape(3,2)
b = numpy.arange(2).reshape(2,1)
print(a)
print(b)
print(a.dot(b))
</code></pre>

<p>However, when I am trying this with PyTorch Tensors, this does not work:</p>

<pre><code>a = torch.Tensor([[1, 2, 3], [1, 2, 3]]).view(-1, 2)
b = torch.Tensor([[2, 1]]).view(2, -1)
print(a)
print(a.size())

print(b)
print(b.size())

print(torch.dot(a, b))
</code></pre>

<p>This code throws the following error: </p>

<blockquote>
  <p>RuntimeError: inconsistent tensor size at
  /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:503</p>
</blockquote>

<p>Any ideas how matrix multiplication can be conducted in PyTorch?</p>",44525687.0,4,0,,2017/6/13 14:50,21.0,2020/7/20 13:43,2018/4/17 23:09,,5141296.0,,4139024.0,,1,79,python|matrix|pytorch,108550,249.343,,3,product matrix pytorch numpy simple matrix multiplication like however try pytorch tensor work code throw following error runtimeerror inconsistent tensor size user soumith code builder wheel pytorch src torch lib th generic thtensormath c idea matrix multiplication conduct pytorch
215,215,43989310,Can I slice tensors with logical indexing or lists of indices?,"<p>I'm trying to slice a PyTorch tensor using a logical index on the columns. I want the columns that correspond to a 1 value in the index vector. Both slicing and logical indexing are possible, but are they possible together? If so, how? My attempt keeps throwing the unhelpful error</p>
<blockquote>
<p>TypeError: indexing a tensor with an object of type ByteTensor. The
only supported types are integers, slices, numpy scalars and
torch.LongTensor or torch.ByteTensor as the only argument.</p>
</blockquote>
<h2>MCVE</h2>
<p>Desired Output</p>
<pre><code>import torch

C = torch.LongTensor([[1, 3], [4, 6]])
# 1 3
# 4 6
</code></pre>
<p><strong>Logical indexing</strong> on the columns only:</p>
<pre><code>A_log = torch.ByteTensor([1, 0, 1]) # the logical index
B = torch.LongTensor([[1, 2, 3], [4, 5, 6]])
C = B[:, A_log] # Throws error
</code></pre>
<p>If the vectors are the same size, logical indexing works:</p>
<pre><code>B_truncated = torch.LongTensor([1, 2, 3])
C = B_truncated[A_log]
</code></pre>
<p>And I can get the desired result by repeating the logical index so that it has the same size as the tensor I am indexing, but then I also have to reshape the output.</p>
<pre><code>C = B[A_log.repeat(2, 1)] # [torch.LongTensor of size 4]
C = C.resize_(2, 2)
</code></pre>
<p>I also tried using a <strong>list of indices</strong>:</p>
<pre><code>A_idx = torch.LongTensor([0, 2]) # the index vector
C = B[:, A_idx] # Throws error
</code></pre>
<p>If I want contiguous ranges of indices, <strong>slicing</strong> works:</p>
<pre><code>C = B[:, 1:2]
</code></pre>",44021217.0,3,0,,2017/5/15 21:48,,2021/3/19 9:10,2021/3/19 9:10,,9067615.0,,3303546.0,,1,17,python|matrix-indexing|pytorch,27167,51.9362,,3,slice tensor logical indexing list index try slice pytorch tensor use logical index column want column correspond value index vector slice logical indexing possible possible together attempt keep throw unhelpful error typeerror index tensor object type bytetensor supported type integer slice numpy scalar torch longtensor torch bytetensor argument mcve desire output logical indexing column vector size logical indexing work get desire result repeat logical index size tensor index also reshape output also try use list index want contiguous range index slice work
413,413,47483733,Print exact value of PyTorch tensor (floating point precision),"<p>I'm trying to print <code>torch.FloatTensor</code> like:</p>
<pre><code>a = torch.FloatTensor(3,3)
print(a)
</code></pre>
<p>This way I can get a value like:</p>
<pre><code>0.0000e+00  0.0000e+00  3.2286e-41
1.2412e-40  1.2313e+00  1.6751e-37
2.6801e-36  3.5873e-41  9.4463e+21
</code></pre>
<p>But I want to get more accurate value, like 10 decimal point:</p>
<pre><code>0.1234567891+01
</code></pre>
<p>With other python numerical objects, I could get it with:</p>
<pre><code>print('{:.10f}'.format(a))
</code></pre>
<p>but in the case of a tensor, I get this error:</p>
<pre><code>TypeError: unsupported format string passed to torch.FloatTensor.__format__
</code></pre>
<p>How can I print more precise values of tensors?</p>",47483819.0,2,0,,2017/11/25 7:47,3.0,2021/3/15 0:04,2021/3/15 0:04,,9067615.0,,7245057.0,,1,18,python|pytorch,16203,66.0384,,3,print exact value pytorch tensor float point precision try print like way get value like want get accurate value like decimal point python numerical object could get case tensor get error print precise value tensor
471,471,59090404,"Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14","<p>I am having an error regarding (Keras that does not support TensorFlow 2.0. We recommend using <code>tf.keras</code>, or alternatively, downgrading to TensorFlow 1.14.) any recommendations. </p>

<p>thanks </p>

<pre><code>import keras
#For building the Neural Network layer by layer
from keras.models import Sequential
#To randomly initialize the weights to small numbers close to 0(But not 0)
from keras.layers import Dense

classifier=tf.keras.Sequential()

classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))




RuntimeError: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14.
</code></pre>",59090507.0,7,0,,2019/11/28 13:51,3.0,2020/5/20 14:41,2019/12/7 7:58,,10685378.0,,11396830.0,,1,13,python|tensorflow|keras|neural-network|tf.keras,23333,58.6719,,1,kera support tensorflow recommend use tf kera alternatively downgrade tensorflow error regard kera support tensorflow recommend use alternatively downgrade tensorflow recommendation thanks
463,463,24906126,How to unpack pkl file?,"<p>I have a pkl file from MNIST dataset, which consists of handwritten digit images.</p>

<p>I'd like to take a look at each of those digit images, so I need to unpack the pkl file, except I can't find out how.</p>

<p>Is there a way to unpack/unzip pkl file?</p>",25079162.0,4,0,,2014/7/23 8:58,32.0,2020/1/27 11:59,2019/2/22 23:01,,355230.0,,639973.0,,1,97,python|pickle|deep-learning|mnist,242053,291.136,,2,unpack pkl file pkl file mnist dataset consist handwritten digit image like take look digit image need unpack pkl file except find way unpack unzip pkl file
306,306,57896357,How to repeat tensor in a specific new dimension in PyTorch,"<p>If I have a tensor <code>A</code> which has shape <code>[M, N]</code>,
I want to repeat the tensor K times so that the result <code>B</code> has shape <code>[M, K, N]</code>
and each slice <code>B[:, k, :]</code> should has the same data as <code>A</code>.
Which is the best practice without a for loop.
<code>K</code> might be in other dimension.</p>

<p><code>torch.repeat_interleave()</code> and <code>tensor.repeat()</code> does not seem to work. Or I am using it in a wrong way.</p>",57896754.0,3,1,,2019/9/11 20:26,2.0,2021/7/16 13:27,,,,,7037749.0,,1,14,pytorch|repeat,12084,51.7288,,3,repeat tensor specific new dimension pytorch tensor shape want repeat tensor k time result shape slice data best practice without loop might dimension seem work use wrong way
402,402,47197885,How to do fully connected batch norm in PyTorch?,"<p><code>torch.nn</code> has classes <code>BatchNorm1d</code>, <code>BatchNorm2d</code>, <code>BatchNorm3d</code>, but it doesn't have a fully connected BatchNorm class? What is the standard way of doing normal Batch Norm in PyTorch? </p>",47202205.0,2,1,,2017/11/9 9:13,2.0,2020/2/2 20:36,2018/3/13 12:34,,3990607.0,,3990607.0,,1,10,python|neural-network|deep-learning|pytorch|batch-normalization,19195,55.1328,,3,fully connect batch norm pytorch class fully connect batchnorm class standard way normal batch norm pytorch
153,153,42703500,Best way to save a trained model in PyTorch?,"<p>I was looking for alternative ways to save a trained model in PyTorch. So far, I have found two alternatives.</p>

<ol>
<li><a href=""https://github.com/torch/torch7/blob/master/doc/serialization.md#torchsavefilename-object--format-referenced"" rel=""noreferrer"">torch.save()</a> to save a model and <a href=""https://github.com/torch/torch7/blob/master/doc/serialization.md#object-torchloadfilename--format-referenced"" rel=""noreferrer"">torch.load()</a> to load a model.</li>
<li><a href=""http://pytorch.org/docs/nn.html#torch.nn.Module.state_dict"" rel=""noreferrer"">model.state_dict()</a> to save a trained model and <a href=""http://pytorch.org/docs/nn.html#torch.nn.Module.load_state_dict"" rel=""noreferrer"">model.load_state_dict()</a> to load the saved model.</li>
</ol>

<p>I have come across to this <a href=""https://discuss.pytorch.org/t/how-to-save-load-torch-models/718"" rel=""noreferrer"">discussion</a> where approach 2 is recommended over approach 1.</p>

<p>My question is, why the second approach is preferred? Is it only because <a href=""http://pytorch.org/docs/nn.html"" rel=""noreferrer"">torch.nn</a> modules have those two function and we are encouraged to use them?</p>",43819235.0,7,6,,2017/3/9 19:06,134.0,2021/7/22 20:46,2018/6/6 12:14,,2956066.0,,5352399.0,,1,271,python|serialization|deep-learning|pytorch|tensor,212512,934.71,,5,best way save trained model pytorch look alternative way save trained model pytorch far find two alternative torch save save model torch load load model model state dict save trained model model load state dict load saved model come across discussion approach recommend approach question second approach prefer torch nn module two function encourage use
663,663,37609892,"Keras, sparse matrix issue","<p>I am trying to feed a huge sparse matrix to Keras model. As the dataset doesn`t fit into RAM, the way around is to train the model on a data generated batch-by-batch by a generator. </p>

<p>To test this approach and make sure my solution works fine, I slightly modified a <a href=""https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py"" rel=""noreferrer"">Kera`s simple MLP on the Reuters newswire topic classification task</a>. So, the idea is to compare original and edited models.  I just convert numpy.ndarray into scipy.sparse.csr.csr_matrix and feed it to the model.</p>

<p>But my model crashes at some point and I need a hand to figure out a reason.</p>

<p>Here is the original model and my additions below</p>

<pre><code>from __future__ import print_function
import numpy as np
np.random.seed(1337)  # for reproducibility

from keras.datasets import reuters
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.utils import np_utils
from keras.preprocessing.text import Tokenizer

max_words = 1000
batch_size = 32
nb_epoch = 5

print('Loading data...')
(X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)
print(len(X_train), 'train sequences')
print(len(X_test), 'test sequences')

nb_classes = np.max(y_train)+1
print(nb_classes, 'classes')

print('Vectorizing sequence data...')
tokenizer = Tokenizer(nb_words=max_words)
X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')
X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')
print('X_train shape:', X_train.shape)
print('X_test shape:', X_test.shape)

print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)
print('Y_train shape:', Y_train.shape)
print('Y_test shape:', Y_test.shape)


print('Building model...')
model = Sequential()
model.add(Dense(512, input_shape=(max_words,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])

history = model.fit(X_train, Y_train,
                nb_epoch=nb_epoch, batch_size=batch_size,
                verbose=1)#, validation_split=0.1)
#score = model.evaluate(X_test, Y_test,
#                       batch_size=batch_size, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>It outputs:  </p>

<pre><code>Loading data...  
8982 train sequences  
2246 test sequences  
46 classes  
Vectorizing sequence data...  
X_train shape: (8982, 1000)  
X_test shape: (2246, 1000)  
Convert class vector to binary class matrix (for use with   categorical_crossentropy)  
Y_train shape: (8982, 46)  
Y_test shape: (2246, 46)  
Building model...  
Epoch 1/5
8982/8982 [==============================] - 5s - loss: 1.3932 - acc: 0.6906     
Epoch 2/5
8982/8982 [==============================] - 4s - loss: 0.7522 - acc: 0.8234     
Epoch 3/5
8982/8982 [==============================] - 5s - loss: 0.5407 - acc: 0.8681     
Epoch 4/5
8982/8982 [==============================] - 5s - loss: 0.4160 - acc: 0.8980     
Epoch 5/5
8982/8982 [==============================] - 5s - loss: 0.3338 - acc: 0.9136     
Test score: 1.01453569163
Test accuracy: 0.797417631398
</code></pre>

<p>Finally, here is my part </p>

<pre><code>X_train_sparse = sparse.csr_matrix(X_train)

def batch_generator(X, y, batch_size):
    n_batches_for_epoch = X.shape[0]//batch_size
    for i in range(n_batches_for_epoch):
        index_batch = range(X.shape[0])[batch_size*i:batch_size*(i+1)]       
        X_batch = X[index_batch,:].todense()
        y_batch = y[index_batch,:]
        yield(np.array(X_batch),y_batch)

model.fit_generator(generator=batch_generator(X_train_sparse, Y_train, batch_size),
                    nb_epoch=nb_epoch, 
                    samples_per_epoch=X_train_sparse.shape[0])
</code></pre>

<p>The crash:  </p>

<pre><code>Exception                                 Traceback (most recent call last)
&lt;ipython-input-120-6722a4f77425&gt; in &lt;module&gt;()  
      1 model.fit_generator(generator=batch_generator(X_trainSparse, Y_train, batch_size),  
      2                     nb_epoch=nb_epoch,
----&gt; 3                     samples_per_epoch=X_trainSparse.shape[0])  

/home/kk/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/models.pyc in fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, **kwargs)  
    648                                         nb_val_samples=nb_val_samples,  
    649                                         class_weight=class_weight,  
--&gt; 650                                         max_q_size=max_q_size)  
    651   
    652     def evaluate_generator(self, generator, val_samples, max_q_size=10, **kwargs):  

/home/kk/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc in fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size)  
   1356                     raise Exception('output of generator should be a tuple '  
   1357                                     '(x, y, sample_weight) '  
-&gt; 1358                                     'or (x, y). Found: ' + str(generator_output))  
   1359                 if len(generator_output) == 2:  
   1360                     x, y = generator_output  

Exception: output of generator should be a tuple (x, y, sample_weight) or (x, y). Found: None  
</code></pre>

<p>I believe the problem is due to wrong setup of samples_per_epoch. I`d trully appreciate if someone could comment on this.  </p>",,2,4,,2016/6/3 8:46,14.0,2018/1/18 11:53,2016/6/3 18:37,,6418050.0,,6418050.0,,1,26,machine-learning|neural-network|keras,18905,50.5063,,4,kera sparse matrix issue try fee huge sparse matrix keras model dataset fit ram way around train model data generate batch batch generator test approach make sure solution work fine slightly modify kera simple mlp reuters newswire topic classification task idea compare original edited model convert numpy ndarray scipy sparse csr csr matrix fee model model crash point need hand figure reason original model addition output finally part crash believe problem due wrong setup sample per epoch trully appreciate someone could comment
347,347,45466020,How to export Keras .h5 to tensorflow .pb?,"<p>I have fine-tuned inception model with a new dataset and saved it as "".h5"" model in Keras. now my goal is to run my model on android Tensorflow which accepts "".pb"" extension only. question is that is there any library in Keras or tensorflow to do this conversion? I have seen this post so far :  <a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""noreferrer"">https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html</a> but can't figure out yet. </p>",45466355.0,13,0,,2017/8/2 16:16,45.0,2021/8/14 9:13,2018/9/26 15:05,,1782792.0,,1098770.0,,1,70,python|tensorflow|keras,95296,376.916,,5,export kera h tensorflow pb fine tune inception model new dataset save h model kera goal run model android tensorflow accept pb extension question library kera tensorflow conversion see post far figure yet
165,165,42918446,How to add an attention mechanism in keras?,"<p>I'm currently using this code that i get from <a href=""https://github.com/fchollet/keras/issues/4962"" rel=""noreferrer"">one discussion on github</a>
Here's the code of the attention mechanism:</p>

<pre><code>_input = Input(shape=[max_length], dtype='int32')

# get the embedding layer
embedded = Embedding(
        input_dim=vocab_size,
        output_dim=embedding_size,
        input_length=max_length,
        trainable=False,
        mask_zero=False
    )(_input)

activations = LSTM(units, return_sequences=True)(embedded)

# compute importance for each step
attention = Dense(1, activation='tanh')(activations)
attention = Flatten()(attention)
attention = Activation('softmax')(attention)
attention = RepeatVector(units)(attention)
attention = Permute([2, 1])(attention)


sent_representation = merge([activations, attention], mode='mul')
sent_representation = Lambda(lambda xin: K.sum(xin, axis=-2), output_shape=(units,))(sent_representation)

probabilities = Dense(3, activation='softmax')(sent_representation)
</code></pre>

<p>Is this the correct way to do it? i was sort of expecting the existence of time distributed layer since attention mechanism is distributed in every time step of the RNN. I need someone to confirm that this implementation(the code) is a correct implementation of attention mechanism. Thank you.</p>",44387553.0,5,1,,2017/3/21 4:26,26.0,2020/12/9 14:26,2018/7/3 15:37,,249341.0,,7676016.0,,1,23,python|keras,38721,63.3518,,3,add attention mechanism kera currently use code get one discussion github code attention mechanism correct way sort expect existence time distribute layer since attention mechanism distribute every time step rnn need someone confirm implementation code correct implementation attention mechanism thank
63,63,55531427,"How to define max_queue_size, workers and use_multiprocessing in keras fit_generator()?","<p>I am applying transfer-learning on a pre-trained network using the GPU version of keras. I don't understand how to define the parameters <strong><code>max_queue_size</code></strong>, <strong><code>workers</code></strong>, and <strong><code>use_multiprocessing</code></strong>. If I change these parameters (primarily to speed-up learning), I am unsure whether all data is still seen per epoch.</p>

<p><strong><code>max_queue_size</code></strong>:</p>

<ul>
<li><p>maximum size of the internal training queue which is used to ""precache"" samples from the generator </p></li>
<li><p><em>Question:</em> Does this refer to how many batches are prepared on CPU? How is it related to <code>workers</code>? How to define it optimally?</p></li>
</ul>

<p><strong><code>workers</code></strong>: </p>

<ul>
<li><p>number of threads generating batches in parallel. Batches are computed in parallel on the CPU and passed on the fly onto the GPU for neural network computations </p></li>
<li><p><em>Question:</em> How do I find out how many batches my CPU can/should generate in parallel?</p></li>
</ul>

<p><strong><code>use_multiprocessing</code></strong>: </p>

<ul>
<li><p>whether to use process-based threading</p></li>
<li><p><em>Question:</em> Do I have to set this parameter to true if I change <code>workers</code>? Does it relate to CPU usage?</p></li>
</ul>

<p><strong>Related questions</strong> can be found here:</p>

<ul>
<li><a href=""https://github.com/keras-team/keras/issues/8540"" rel=""noreferrer"">Detailed explanation of model.fit_generator() parameters: queue size, workers and use_multiprocessing</a></li>
<li><p><a href=""https://stackoverflow.com/questions/51790943/what-does-worker-mean-in-fit-generator-in-keras"">What does worker mean in fit_generator in Keras?</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/36986815/what-is-the-parameter-max-q-size-used-for-in-model-fit-generator/36989864#36989864"">What is the parameter 闂佺偨鍎茬粭姊恱_q_size闂?used for in 闂佺偨鍎茬粭姊燿el.fit_generator闂?</a></p></li>
<li><p><a href=""https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"" rel=""noreferrer"">A detailed example of how to use data generators with Keras</a>.</p></li>
</ul>

<p>I am using <code>fit_generator()</code> as follows:</p>

<pre><code>    history = model.fit_generator(generator=trainGenerator,
                                  steps_per_epoch=trainGenerator.samples//nBatches,     # total number of steps (batches of samples)
                                  epochs=nEpochs,                   # number of epochs to train the model
                                  verbose=2,                        # verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch
                                  callbacks=callback,               # keras.callbacks.Callback instances to apply during training
                                  validation_data=valGenerator,     # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch
                                  validation_steps=
                                  valGenerator.samples//nBatches,   # number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch
                                  class_weight=classWeights,                # optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function
                                  max_queue_size=10,                # maximum size for the generator queue
                                  workers=1,                        # maximum number of processes to spin up when using process-based threading
                                  use_multiprocessing=False,        # whether to use process-based threading
                                  shuffle=True,                     # whether to shuffle the order of the batches at the beginning of each epoch
                                  initial_epoch=0)   
</code></pre>

<p>The specs of my machine are:</p>

<pre><code>CPU : 2xXeon E5-2260 2.6 GHz
Cores: 10
Graphic card: Titan X, Maxwell, GM200
RAM: 128 GB
HDD: 4TB
SSD: 512 GB
</code></pre>",55709360.0,1,0,,2019/4/5 8:39,13.0,2019/4/16 13:46,2019/4/16 11:44,,11277304.0,,11277304.0,,1,59,python|tensorflow|machine-learning|keras|gpu,19291,84.9414,,3,define max queue size worker use multiprocessing keras fit generator apply transfer learning pre train network use gpu version kera understand define parameter change parameter primarily speed learn unsure whether data still see per epoch maximum size internal training queue use precache sample generator question refer many batch prepare cpu related define optimally number thread generate batch parallel batch compute parallel cpu pass fly onto gpu neural network computation question find many batch cpu generate parallel whether use process base thread question set parameter true change relate cpu usage related question find detailed explanation model fit generator parameter queue size worker use multiprocessing worker mean fit generator kera parameter max q size use model fit generator detailed example use data generator kera use follow spec machine
580,580,49392972,"Error when checking target: expected dense_3 to have shape (3,) but got array with shape (1,)","<p>I am working on training a VGG16-like model in Keras, on a 3 classes subset from Places205, and encountered the following error: </p>

<pre><code>ValueError: Error when checking target: expected dense_3 to have shape (3,) but got array with shape (1,)
</code></pre>

<p>I read multiple similar issues but none helped me so far. The error is on the last layer, where I've put 3 because this is the number of classes I'm trying right now.</p>

<p>The code is the following:</p>

<pre><code>import keras from keras.datasets
import cifar10 from keras.preprocessing.image 
import ImageDataGenerator from keras.models 
import Sequential 
from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D 
from keras import backend as K import os


# Constants used  
img_width, img_height = 224, 224  
train_data_dir='places\\train'  
validation_data_dir='places\\validation'  
save_filename = 'vgg_trained_model.h5'  
training_samples = 15  
validation_samples = 5  
batch_size = 5  
epochs = 5


if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height) else:
    input_shape = (img_width, img_height, 3)

model = Sequential([
    # Block 1
    Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 2
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 3
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 4
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 5
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Top
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(3, activation='softmax') ])

model.summary()

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# no augmentation config train_datagen = ImageDataGenerator() validation_datagen = ImageDataGenerator()
     train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = validation_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(
    train_generator,
    steps_per_epoch=training_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_samples // batch_size)

model.save_weights(save_filename)
</code></pre>",,9,2,,2018/3/20 19:48,14.0,2020/11/14 11:33,2018/3/20 20:55,,4445080.0,,4445080.0,,1,51,python|python-3.x|tensorflow|keras,86634,191.551,,4,error check target expect dense shape get array shape work train vgg like model kera class subset place encounter following error read multiple similar issue none help far error last layer put number class try right code follow
528,528,35543428,Activation function after pooling layer or convolutional layer?,"<p>The theory from these links show that the order of Convolutional Network is: <code>Convolutional Layer - Non-linear Activation - Pooling Layer</code>.</p>

<ol>
<li><a href=""http://neuralnetworksanddeeplearning.com/chap6.html"" rel=""noreferrer"">Neural networks and deep learning (equation (125)</a></li>
<li><a href=""http://www.deeplearningbook.org/contents/convnets.html"" rel=""noreferrer"">Deep learning book (page 304, 1st paragraph)</a></li>
<li><a href=""http://deeplearning.net/tutorial/lenet.html#details-and-notation"" rel=""noreferrer"">Lenet (the equation)</a></li>
<li><a href=""http://deeplearning.net/tutorial/lenet.html#the-convolution-operator"" rel=""noreferrer"">The source in this headline</a></li>
</ol>

<p>But, in the last implementation from those sites, it said that the order is: <code>Convolutional Layer - Pooling Layer - Non-linear Activation</code></p>

<ol>
<li><a href=""http://neuralnetworksanddeeplearning.com/chap6.html#the_code_for_our_convolutional_networks"" rel=""noreferrer"">network3.py</a></li>
<li><a href=""http://deeplearning.net/tutorial/lenet.html#putting-it-all-together"" rel=""noreferrer"">The sourcecode, LeNetConvPoolLayer class</a></li>
</ol>

<p>I've tried too to explore a Conv2D operation syntax, but there is no activation function, it's only convolution with flipped kernel. Can someone help me to explain why is this happen?</p>",35550406.0,3,0,,2016/2/21 23:20,8.0,2021/4/17 14:36,2019/2/25 17:13,,11114701.0,,2147347.0,,1,29,neural-network|theano|convolution,20387,94.6374,,0,activation function pool layer convolutional layer theory link show order convolutional network neural network deep learning equation deep learn book page st paragraph lenet equation source headline last implementation site say order network py sourcecode lenetconvpoollayer class try explore conv operation syntax activation function convolution flipped kernel someone help explain happen
141,141,42480111,Model summary in pytorch,"<p>How do I print the summary of a model in PyTorch like the <code>model.summary()</code> method does in Keras:</p>
<pre><code>Model Summary:
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 1, 15, 27)     0                                            
____________________________________________________________________________________________________
convolution2d_1 (Convolution2D)  (None, 8, 15, 27)     872         input_1[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 8, 7, 27)      0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1512)          0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 1)             1513        flatten_1[0][0]                  
====================================================================================================
Total params: 2,385
Trainable params: 2,385
Non-trainable params: 0
</code></pre>",42616812.0,10,3,,2017/2/27 7:35,41.0,2021/5/19 14:38,2021/3/12 11:22,,9067615.0,,5352399.0,,1,204,python|pytorch,164918,903.869,,5,model summary pytorch print summary model pytorch like method kera
671,671,37911321,Why does prediction needs batch size in Keras?,"<p>In Keras, to predict class of a datatest, the <code>predict_classes()</code> is used.</p>

<p>For example:</p>

<pre><code>classes = model.predict_classes(X_test, batch_size=32)
</code></pre>

<p>My question is, I know the usage of <code>batch_size</code> in training, but why does it need a <code>batch_size</code> for prediction? how does it work?</p>",37912576.0,2,0,,2016/6/19 20:04,14.0,2021/3/21 6:48,2018/7/30 6:59,,2147347.0,,2147347.0,,1,48,neural-network|classification|keras,25898,78.8531,,3,prediction need batch size kera kera predict class datatest use example question know usage training need prediction work
741,741,40866124,Difference between Dense and Activation layer in Keras,"<p>I was wondering what was the difference between Activation Layer and Dense layer in Keras.</p>

<p>Since Activation Layer seems to be a fully connected layer, and Dense have a parameter to pass an activation function, what is the best practice ?</p>

<p>Let's imagine a fictionnal network like this :
Input -> Dense -> Dropout -> Final Layer
Final Layer should be : Dense(activation=softmax) or Activation(softmax) ?
What is the cleanest and why ?</p>

<p>Thanks everyone!</p>",40870126.0,2,0,,2016/11/29 12:37,5.0,2020/10/3 17:03,2017/7/4 10:38,,5974433.0,,3827807.0,,1,30,python|machine-learning|neural-network|deep-learning|keras,9687,73.9448,,3,difference dense activation layer kera wonder difference activation layer dense layer kera since activation layer seem fully connect layer dense parameter pass activation function best practice let imagine fictionnal network like input dense dropout final layer final layer dense activation softmax activation softmax cleanest thanks everyone
260,260,44704435,"Error when checking model input: expected lstm_1_input to have 3 dimensions, but got array with shape (339732, 29)","<p>My input is simply a csv file with 339732 rows and two columns :</p>

<ul>
<li>the first being 29 feature values, i.e. X</li>
<li>the second being a binary label value, i.e. Y</li>
</ul>

<p>I am trying to train my data on a stacked LSTM model:</p>

<pre><code>data_dim = 29
timesteps = 8
num_classes = 2

model = Sequential()
model.add(LSTM(30, return_sequences=True,
               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 30
model.add(LSTM(30, return_sequences=True))  # returns a sequence of vectors of dimension 30
model.add(LSTM(30))  # return a single vector of dimension 30
model.add(Dense(1, activation='softmax'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.summary()
model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1)
</code></pre>

<p>This throws the error:</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""first_approach.py"", line 80, in 
      model.fit(X_train, y_train, batch_size = 400, epochs = 20, verbose = 1)</p>
  
  <p>ValueError: Error when checking model input: expected lstm_1_input to
  have 3 dimensions, but got array with shape (339732, 29)</p>
</blockquote>

<p>I tried reshaping my input using <code>X_train.reshape((1,339732, 29))</code> but it did not work showing error:</p>

<blockquote>
  <p>ValueError: Error when checking model input: expected lstm_1_input to
  have shape (None, 8, 29) but got array with shape (1, 339732, 29)</p>
</blockquote>

<p>How can I feed in my input to the LSTM ?</p>",44704745.0,3,1,,2017/6/22 16:04,8.0,2018/12/7 9:43,,,,,5140684.0,,1,34,python|keras|lstm|recurrent-neural-network|valueerror,71919,87.8274,,3,error check model input expect lstm input dimension get array shape input simply csv file row two columns first feature value e x second binary label value e try train data stacked lstm model throw error traceback recent call last file first approach py line model fit x train train batch size epoch verbose valueerror error check model input expect lstm input dimension get array shape try reshape input use work show error valueerror error check model input expect lstm input shape none get array shape feed input lstm
136,136,42415076,How to insert Keras model into scikit-learn pipeline?,"<p>I'm using a Scikit-Learn custom pipeline (<code>sklearn.pipeline.Pipeline</code>) in conjunction with <code>RandomizedSearchCV</code> for hyper-parameter optimization. This works great.</p>

<p>Now I would like to insert a Keras model as a first step into the pipeline. Parameters of the model should be optimized. The computed (fitted) Keras model should then be used later on in the pipeline by other steps, so I think I have to store the model as a global variable so that the other pipeline steps can use it. Is this right?</p>

<p>I know that Keras offers some wrappers for the Scikit-Learn API but the problem is that these wrappers already do classification / regression but I only want to compute the Keras model and nothing else.</p>

<p>How can this be done?</p>

<p>For example I have a method which returns the model:</p>

<pre><code>def create_model(file_path, argument2,...):
    ...
    return model
</code></pre>

<p>The method needs some fixed parameters like a file path etc. but X and y is not needed (or can be ignored). The parameters of the model should be optimized (number of layers etc.).</p>",47520976.0,2,1,,2017/2/23 11:54,10.0,2019/7/13 17:39,,,,,1684118.0,,1,37,machine-learning|scikit-learn|pipeline|keras|hyperparameters,18325,55.8522,,3,insert kera model scikit learn pipeline use scikit learn custom pipeline conjunction hyper parameter optimization work great would like insert keras model first step pipeline parameter model optimize compute fitted kera model use later pipeline step think store model global variable pipeline step use right know kera offer wrapper scikit learn api problem wrapper already classification regression want compute kera model nothing else example method return model method need fixed parameter like file path etc x need ignore parameter model optimize number layer etc
784,784,52133347,How can I clear a model created with Keras and Tensorflow(as backend)?,"<p>I have a problem when training a neural net with Keras in Jupyter Notebook. I created a sequential model with several hidden layers. After training the model and saving the results, I want to delete this model and create a new model in the same session, as I have a <code>for</code> loop that checks the results for different parameters. But as I understand the errors I get, when changing the parameters, when I loop over, I am just adding layers to the model (even though I initialise it again with <code>network = Sequential()</code> inside the loop). So my question is, how can I completely clear the previous model or how can I initialise a completely new model in the same session?</p>",52141755.0,1,1,,2018/9/2 1:38,2.0,2018/9/2 23:39,,,,,4959244.0,,1,27,python|tensorflow|keras|jupyter-notebook,38593,74.74600000000002,,3,clear model create kera tensorflow backend problem train neural net kera jupyter notebook create sequential model several hidden layer train model save result want delete model create new model session loop check result different parameter understand error get change parameter loop add layer model even though initialise inside loop question completely clear previous model initialise completely new model session
667,667,37674306,What is the difference between 'SAME' and 'VALID' padding in tf.nn.max_pool of tensorflow?,"<p>What is the difference between 'SAME' and 'VALID' padding in <code>tf.nn.max_pool</code> of <code>tensorflow</code>?</p>

<p>In my opinion, 'VALID' means there will be no zero padding outside the edges when we do max pool. </p>

<p>According to <a href=""https://arxiv.org/pdf/1603.07285v1.pdf"">A guide to convolution arithmetic for deep learning</a>, it says that there will be no padding in pool operator, i.e. just use 'VALID' of <code>tensorflow</code>.
But what is 'SAME' padding of max pool in <code>tensorflow</code>?</p>",37675359.0,15,5,,2016/6/7 8:32,192.0,2021/1/20 10:07,2016/6/7 8:40,,5950520.0,,5651936.0,,1,378,python|tensorflow|deep-learning,314392,2542.99,,3,difference valid padding tf nn max pool tensorflow difference valid padding opinion valid mean zero pad outside edge max pool accord guide convolution arithmetic deep learning say padding pool operator e use valid padding max pool
261,261,44716150,How can I assign a class_weight in Keras in a simple way?,"<p>Can anyone tell me what is the simplest way to apply <code>class_weight</code> in Keras when the dataset is unbalanced please?</p>

<p>I only have two classes in my target.</p>

<p>Thanks.</p>",,5,0,,2017/6/23 8:02,9.0,2019/10/7 5:48,2019/6/1 10:06,,7117003.0,,7649569.0,,1,25,python|tensorflow|deep-learning|keras,30306,116.926,,3,assign class weight kera simple way anyone tell simple way apply kera dataset unbalanced please two class target thanks
65,65,55549843,Pytorch doesn't support one-hot vector?,"<p>I am very confused by how Pytorch deals with one-hot vectors. In this <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"" rel=""noreferrer"">tutorial</a>, the neural network will generate a one-hot vector as its output. As far as I understand, the schematic structure of the neural network in the tutorial should be like:</p>

<p><a href=""https://i.stack.imgur.com/1v35k.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1v35k.png"" alt=""enter image description here""></a></p>

<p>However, the <code>labels</code> are not in one-hot vector format. I get the following <code>size</code></p>

<pre><code>print(labels.size())
print(outputs.size())

output&gt;&gt;&gt; torch.Size([4]) 
output&gt;&gt;&gt; torch.Size([4, 10])
</code></pre>

<p>Miraculously, I they pass the <code>outputs</code> and <code>labels</code> to <code>criterion=CrossEntropyLoss()</code>, there's no error at all.</p>

<pre><code>loss = criterion(outputs, labels) # How come it has no error?
</code></pre>

<h2>My hypothesis:</h2>

<p>Maybe pytorch automatically convert the <code>labels</code> to one-hot vector form. So, I try to convert labels to one-hot vector before passing it to the loss function.</p>

<pre><code>def to_one_hot_vector(num_class, label):
    b = np.zeros((label.shape[0], num_class))
    b[np.arange(label.shape[0]), label] = 1

    return b

labels_one_hot = to_one_hot_vector(10,labels)
labels_one_hot = torch.Tensor(labels_one_hot)
labels_one_hot = labels_one_hot.type(torch.LongTensor)

loss = criterion(outputs, labels_one_hot) # Now it gives me error
</code></pre>

<p>However, I got the following error</p>

<blockquote>
  <p>RuntimeError: multi-target not supported at
  /opt/pytorch/pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15</p>
</blockquote>

<p>So, one-hot vectors are not supported in <code>Pytorch</code>? How does <code>Pytorch</code> calculates the <code>cross entropy</code> for the two tensor <code>outputs = [1,0,0],[0,0,1]</code> and <code>labels = [0,2]</code> ? It doesn't make sense to me at all at the moment.</p>",55549952.0,2,0,,2019/4/6 14:02,5.0,2020/8/24 2:51,,,,,9793316.0,,1,22,python|machine-learning|pytorch,29483,67.6783,,4,pytorch support one hot vector confuse pytorch deal one hot vector tutorial neural network generate one hot vector output far understand schematic structure neural network tutorial like however one hot vector format get follow miraculously pass error hypothesis maybe pytorch automatically convert one hot vector form try convert label one hot vector pass loss function however get following error runtimeerror multi target support opt pytorch pytorch aten src thcunn generic classnllcriterion cu one hot vector support calculate two tensor make sense moment
163,163,42861460,How to interpret weights in a LSTM layer in Keras,"<p>I'm currently training a recurrent neural network for weather forecasting, using a LSTM layer. The network itself is pretty simple and looks roughly like this:</p>

<pre><code>model = Sequential()  
model.add(LSTM(hidden_neurons, input_shape=(time_steps, feature_count), return_sequences=False))  
model.add(Dense(feature_count))  
model.add(Activation(""linear""))  
</code></pre>

<p>The weights of the LSTM layer do have the following shapes:</p>

<pre><code>for weight in model.get_weights(): # weights from Dense layer omitted
    print(weight.shape)

&gt; (feature_count, hidden_neurons)
&gt; (hidden_neurons, hidden_neurons)
&gt; (hidden_neurons,)
&gt; (feature_count, hidden_neurons)
&gt; (hidden_neurons, hidden_neurons)
&gt; (hidden_neurons,)
&gt; (feature_count, hidden_neurons)
&gt; (hidden_neurons, hidden_neurons)
&gt; (hidden_neurons,)
&gt; (feature_count, hidden_neurons)
&gt; (hidden_neurons, hidden_neurons)
&gt; (hidden_neurons,)
</code></pre>

<p>In short, it looks like there are four ""elements"" in this LSTM layer. I'm wondering now how to interpret them:</p>

<ul>
<li><p>Where is the <code>time_steps</code> parameter in this representation? How does it influence the weights?</p></li>
<li><p>I've read that a LSTM consists of several blocks, like an input and a forget gate. If those are represented in these weight matrices, which matrix belongs to which gate?</p></li>
<li><p>Is there any way to see what the network has learned? For example, how much does it take from the last time step (<code>t-1</code> if we want to forecast <code>t</code>) and how much from <code>t-2</code> etc? It would be interesting to know if we could read from the weights that the input <code>t-5</code> is completely irrelevant, for example.</p></li>
</ul>

<p>Clarifications and hints would be greatly appreciated.</p>",,2,0,,2017/3/17 15:28,9.0,2019/5/23 3:55,2019/5/23 3:55,,4099593.0,,3367967.0,,1,16,python|keras|neural-network|lstm,12752,65.8223,2019/5/21 0:34,3,interpret weight lstm layer kera currently train recurrent neural network weather forecasting use lstm layer network pretty simple look roughly like weight lstm layer following shape short look like four element lstm layer wonder interpret parameter representation influence weight read lstm consist several block like input forget gate represent weight matrix matrix belongs gate way see network learn example much take last time step want forecast much etc would interest know could read weight input completely irrelevant example clarification hint would greatly appreciated
699,699,39263002,"Calling ""fit"" multiple times in Keras","<p>I've working on a CNN over several hundred GBs of images. I've created a training function that bites off 4Gb chunks of these images and calls <code>fit</code> over each of these pieces. I'm worried that I'm only training on the last piece on not the entire dataset.</p>

<p>Effectively, my pseudo-code looks like this:</p>

<pre><code>DS = lazy_load_400GB_Dataset()
for section in DS:
    X_train = section.images
    Y_train = section.classes

    model.fit(X_train, Y_train, batch_size=16, nb_epoch=30)
</code></pre>

<p>I know that the API and the Keras forums say that this will train over the entire dataset, but I can't intuitively understand why the network wouldn't relearn over just the last training chunk.</p>

<p>Some help understanding this would be much appreciated.</p>

<p>Best,
Joe</p>",39271995.0,2,1,,2016/9/1 5:02,10.0,2018/4/23 3:33,,,,,4652819.0,,1,43,machine-learning|neural-network|theano|conv-neural-network|keras,24041,99.7238,,3,call fit multiple time kera work cnn several hundred gb image create training function bite gb chunk image call piece worried train last piece entire dataset effectively pseudo code look like know api kera forum say train entire dataset intuitively understand network would relearn last training chunk help understanding would much appreciate best joe
577,577,49295311,what is the difference between Flatten() and GlobalAveragePooling2D() in keras,"<p>I want to pass the output of ConvLSTM and Conv2D to a Dense Layer in Keras, what is the difference between using global average pooling and flatten
Both is working in my case.</p>



<pre class=""lang-python prettyprint-override""><code>model.add(ConvLSTM2D(filters=256,kernel_size=(3,3)))
model.add(Flatten())
# or model.add(GlobalAveragePooling2D())
model.add(Dense(256,activation='relu'))
</code></pre>",49299921.0,4,0,,2018/3/15 9:11,15.0,2021/3/26 23:02,2021/3/25 12:22,,10908375.0,,5797699.0,,1,33,python|tensorflow|keras|deep-learning|keras-layer,22657,116.821,,3,difference flatten globalaveragepooling kera want pass output convlstm conv dense layer kera difference use global average pooling flatten work case
184,184,43328632,Pytorch reshape tensor dimension,"<p>For example, I have 1D vector with dimension (5). I would like to reshape it into 2D matrix (1,5).</p>

<p>Here is how I do it with numpy</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; a = np.array([1,2,3,4,5])
&gt;&gt;&gt; a.shape
(5,)
&gt;&gt;&gt; a = np.reshape(a, (1,5))
&gt;&gt;&gt; a.shape
(1, 5)
&gt;&gt;&gt; a
array([[1, 2, 3, 4, 5]])
&gt;&gt;&gt; 
</code></pre>

<p>But how can I do that with Pytorch Tensor (and Variable). I don't want to switch back to numpy and switch to Torch variable again, because it will loss backpropagation information.</p>

<p>Here is what I have in Pytorch</p>

<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; from torch.autograd import Variable
&gt;&gt;&gt; a = torch.Tensor([1,2,3,4,5])
&gt;&gt;&gt; a

 1
 2
 3
 4
 5
[torch.FloatTensor of size 5]

&gt;&gt;&gt; a.size()
(5L,)
&gt;&gt;&gt; a_var = variable(a)
&gt;&gt;&gt; a_var = Variable(a)
&gt;&gt;&gt; a_var.size()
(5L,)
.....do some calculation in forward function
&gt;&gt;&gt; a_var.size()
(5L,)
</code></pre>

<p>Now I want it size to be (1, 5).
How can I resize or reshape the dimension of pytorch tensor in Variable without loss grad information. (because I will feed into another model before backward) </p>",43451081.0,11,0,,2017/4/10 16:38,6.0,2021/4/7 11:52,2017/12/6 5:36,,2956066.0,,5250620.0,,1,57,python|numpy|deep-learning|pytorch|tensor,147143,251.071,,3,pytorch reshape tensor dimension example vector dimension would like reshape matrix numpy pytorch tensor variable want switch back numpy switch torch variable loss backpropagation information pytorch want size resize reshape dimension pytorch tensor variable without loss grad information fee another model backward
573,573,49201236,Check the total number of parameters in a PyTorch model,<p>How to count the total number of parameters in a PyTorch model? Something similar to <code>model.count_params()</code> in Keras.</p>,49201237.0,7,0,,2018/3/9 19:55,35.0,2021/8/23 10:01,2018/9/27 7:24,,604734.0,,604734.0,,1,106,deep-learning|pytorch,56283,402.402,,3,check total number parameter pytorch model count total number parameter pytorch model something similar kera
60,60,55466298,Pytorch: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead,"<p>I have an error in my code which is not getting fixed any which way I try.</p>

<p>The Error is simple, I return a value:</p>

<pre><code>torch.exp(-LL_total/T_total)
</code></pre>

<p>and get the error later in the pipeline:</p>

<pre><code>RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.
</code></pre>

<p>Solutions such as <code>cpu().detach().numpy()</code> give the same error.</p>

<p>How could I fix it? Thanks.</p>",57014852.0,4,1,,2019/4/2 2:54,2.0,2021/3/15 20:44,,,,,10555987.0,,1,28,python|numpy|pytorch,38682,87.75,,3,pytorch ca call numpy variable require grad use var detach numpy instead error code get fix way try error simple return value get error later pipeline solution give error could fix thanks
634,634,51031519,can't import keras.layers.Merge,"<p>I want to merge two LSTM models in Keras. I have seen many examples of importing Merge as:</p>

<pre><code>from keras.layers import Merge
</code></pre>

<p>When I do this, I get an import error.</p>

<p><code>ImportError: cannot import name 'Merge'.</code></p>

<p>Has there been some refactor and now Merge is elsewhere?</p>",51031648.0,3,0,,2018/6/25 20:43,6.0,2021/2/18 6:43,2019/3/4 21:51,,1370986.0,,9555101.0,,1,13,python|keras,27451,54.5542,,3,import kera layer merge want merge two lstm model kera see many example import merge get import error refactor merge elsewhere
475,475,59394947,"How to fix ""ResourceExhaustedError: OOM when allocating tensor""","<p>I wanna make a model with multiple inputs. So, I try to build a model like this.</p>
<pre><code># define two sets of inputs
inputA = Input(shape=(32,64,1))
inputB = Input(shape=(32,1024))
 
# CNN
x = layers.Conv2D(32, kernel_size = (3, 3), activation = 'relu')(inputA)
x = layers.Conv2D(32, (3,3), activation='relu')(x)
x = layers.MaxPooling2D(pool_size=(2,2))(x)
x = layers.Dropout(0.2)(x)
x = layers.Flatten()(x)
x = layers.Dense(500, activation = 'relu')(x)
x = layers.Dropout(0.5)(x)
x = layers.Dense(500, activation='relu')(x)
x = Model(inputs=inputA, outputs=x)
 
# DNN
y = layers.Flatten()(inputB)
y = Dense(64, activation=&quot;relu&quot;)(y)
y = Dense(250, activation=&quot;relu&quot;)(y)
y = Dense(500, activation=&quot;relu&quot;)(y)
y = Model(inputs=inputB, outputs=y)
 
# Combine the output of the two models
combined = concatenate([x.output, y.output])
 

# combined outputs
z = Dense(300, activation=&quot;relu&quot;)(combined)
z = Dense(100, activation=&quot;relu&quot;)(combined)
z = Dense(1, activation=&quot;softmax&quot;)(combined)

model = Model(inputs=[x.input, y.input], outputs=z)

model.summary()

opt = Adam(lr=1e-3, decay=1e-3 / 200)
model.compile(loss = 'sparse_categorical_crossentropy', optimizer = opt,
    metrics = ['accuracy'])
</code></pre>
<p>and the summary
:
_</p>
<p>But, when i try to train this model,</p>
<pre><code>history = model.fit([trainimage, train_product_embd],train_label,
    validation_data=([validimage,valid_product_embd],valid_label), epochs=10, 
    steps_per_epoch=100, validation_steps=10)
</code></pre>
<p>the problem happens....
:</p>
<pre><code> ResourceExhaustedError                    Traceback (most recent call
 last) &lt;ipython-input-18-2b79f16d63c0&gt; in &lt;module&gt;()
 ----&gt; 1 history = model.fit([trainimage, train_product_embd],train_label,
 validation_data=([validimage,valid_product_embd],valid_label),
 epochs=10, steps_per_epoch=100, validation_steps=10)

 4 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py
 in __call__(self, *args, **kwargs)    1470         ret =
 tf_session.TF_SessionRunCallable(self._session._session,    1471      
 self._handle, args,
 -&gt; 1472                                                run_metadata_ptr)    1473         if run_metadata:    1474          
 proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
 
 ResourceExhaustedError: 2 root error(s) found.   (0) Resource
 exhausted: OOM when allocating tensor with shape[800000,32,30,62] and
 type float on /job:localhost/replica:0/task:0/device:GPU:0 by
 allocator GPU_0_bfc     [[{{node conv2d_1/convolution}}]] Hint: If you
 want to see a list of allocated tensors when OOM happens, add
 report_tensor_allocations_upon_oom to RunOptions for current
 allocation info.
 
     [[metrics/acc/Mean_1/_185]] Hint: If you want to see a list of
 allocated tensors when OOM happens, add
 report_tensor_allocations_upon_oom to RunOptions for current
 allocation info.
 
   (1) Resource exhausted: OOM when allocating tensor with
 shape[800000,32,30,62] and type float on
 /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc    
 [[{{node conv2d_1/convolution}}]] Hint: If you want to see a list of
 allocated tensors when OOM happens, add
 report_tensor_allocations_upon_oom to RunOptions for current
 allocation info.
 
 0 successful operations. 0 derived errors ignored.
</code></pre>
<p>Thanks for reading and hopefully helping me :)</p>",59395251.0,3,0,,2019/12/18 15:12,6.0,2021/9/2 14:44,2020/11/9 9:26,user12304080,,,12182110.0,,1,11,python|tensorflow|machine-learning|keras|deep-learning,29948,66.5055,,4,fix resourceexhaustederror oom allocate tensor wan na make model multiple input try build model like summary try train model problem happen thanks reading hopefully help
235,235,44371560,What is the relationship between PyTorch and Torch?,"<p>There are two PyTorch repositories :</p>
<ul>
<li><a href=""https://github.com/hughperkins/pytorch"" rel=""noreferrer"">https://github.com/hughperkins/pytorch</a></li>
<li><a href=""https://github.com/pytorch/pytorch"" rel=""noreferrer"">https://github.com/pytorch/pytorch</a></li>
</ul>
<p>The first clearly requires Torch and lua and is a wrapper, but the second doesn't make any reference to the Torch project except with its name.</p>
<p>How is it related to the <a href=""http://torch.ch/"" rel=""noreferrer"">Lua Torch</a>?</p>",44382388.0,2,1,,2017/6/5 14:46,11.0,2021/3/12 11:47,2021/3/12 11:43,,9067615.0,,5133167.0,,1,41,torch|pytorch,33453,74.4977,,3,relationship pytorch torch two pytorch repositories first clearly require torch lua wrapper second make reference torch project except name related lua torch
359,359,45799474,Keras: model.evaluate vs model.predict accuracy difference in multi-class NLP task,"<p>I am training a simple model in keras for NLP task with following code. Variable names are self explanatory for train, test and validation set. This dataset has 19 classes so final layer of the network has 19 outputs. Labels are also one-hot encoded.</p>



<pre class=""lang-python prettyprint-override""><code>nb_classes = 19
model1 = Sequential()
model1.add(Embedding(nb_words,
                     EMBEDDING_DIM,
                     weights=[embedding_matrix],
                     input_length=MAX_SEQUENCE_LENGTH,
                     trainable=False))
model1.add(LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))
model1.add(Dropout(rate_drop_dense))
model1.add(BatchNormalization())
model1.add(Dense(num_dense, activation=act))
model1.add(Dropout(rate_drop_dense))
model1.add(BatchNormalization())

model1.add(Dense(nb_classes, activation = 'sigmoid'))


model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#One hot encode all labels
ytrain_enc = np_utils.to_categorical(train_labels)
yval_enc = np_utils.to_categorical(val_labels)
ytestenc = np_utils.to_categorical(test_labels)

model1.fit(train_data, ytrain_enc,
             validation_data=(val_data, yval_enc),
             epochs=200,
             batch_size=384,
             shuffle=True,
             verbose=1)
</code></pre>

<p>After first epoch, this gives me these outputs.</p>

<pre class=""lang-python prettyprint-override""><code>Epoch 1/200
216632/216632 [==============================] - 2442s - loss: 0.1427 - acc: 0.9443 - val_loss: 0.0526 - val_acc: 0.9826
</code></pre>

<p>Then I evaluate my model on testing dataset and this also shows me accuracy around 0.98.</p>

<pre class=""lang-python prettyprint-override""><code>model1.evaluate(test_data, y = ytestenc, batch_size=384, verbose=1)
</code></pre>

<p>However, the labels are one-hot encoded, so I need prediction vector of classes so that I can generate confusion matrix etc. So I use,</p>

<pre class=""lang-python prettyprint-override""><code>PREDICTED_CLASSES = model1.predict_classes(test_data, batch_size=384, verbose=1)
temp = sum(test_labels == PREDICTED_CLASSES)
temp/len(test_labels)
0.83
</code></pre>

<p>This shows that total predicted classes were 83% accurate however <code>model1.evaluate</code> shows 98% accuracy!! What am I doing wrong here? Is my loss function okay with categorical class labels? Is my choice of <code>sigmoid</code> activation function for prediction layer okay? or there is difference in the way keras evaluates a model? Please suggest on what can be wrong. This is my first try to make a deep model so I don't have much understanding of what's wrong here.</p>",45834857.0,1,0,,2017/8/21 14:20,16.0,2017/9/5 13:46,2017/9/5 13:46,,4685471.0,,7280300.0,,1,25,machine-learning|deep-learning|keras,26161,82.6706,,5,kera model evaluate v model predict accuracy difference multi class nlp task train simple model kera nlp task follow code variable name self explanatory train test validation set dataset class final layer network output label also one hot encode first epoch give output evaluate model test dataset also show accuracy around however label one hot encode need prediction vector class generate confusion matrix etc use show total predict class accurate however show accuracy wrong loss function okay categorical class label choice activation function prediction layer okay difference way kera evaluate model please suggest wrong first try make deep model much understanding wrong
582,582,49404993,How to use fit_generator with multiple inputs,"<p>Is it possible to have two fit_generator?</p>

<p>I'm creating a model with two inputs,
The model configuration is shown below.</p>

<p><a href=""https://i.stack.imgur.com/FDY0W.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FDY0W.png"" alt=""enter image description here""></a></p>

<p>Label Y uses the same labeling for X1 and X2 data.</p>

<p>The following error will continue to occur.</p>

<blockquote>
  <p><em>Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected
  to see 2 array(s), but instead got the following list of 1 arrays:
  [array([[[[0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           [0.75686276, 0.75686276, 0.75686276],
           ...,
           [0.65882355, 0.65882355, 0.65882355...</em></p>
</blockquote>

<p>My code looks like this:</p>



<pre class=""lang-python prettyprint-override""><code>def generator_two_img(X1, X2, Y,batch_size):
    generator = ImageDataGenerator(rotation_range=15,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   fill_mode='nearest')

    genX1 = generator.flow(X1, Y, batch_size=batch_size)
    genX2 = generator.flow(X2, Y, batch_size=batch_size)

    while True:
        X1 = genX1.__next__()
        X2 = genX2.__next__()
        yield [X1, X2], Y
  """"""
      .................................
  """"""
hist = model.fit_generator(generator_two_img(x_train, x_train_landmark, 
                y_train, batch_size),
                steps_per_epoch=len(x_train) // batch_size, epochs=nb_epoch,
                callbacks = callbacks,
                validation_data=(x_validation, y_validation),
                validation_steps=x_validation.shape[0] // batch_size, 
                `enter code here`verbose=1)
</code></pre>",49405175.0,2,1,,2018/3/21 11:21,13.0,2021/3/6 10:46,2021/3/6 10:46,,11573842.0,,9527246.0,,1,28,python|machine-learning|neural-network|keras|generator,20943,68.4842,,3,use fit generator multiple input possible two fit generator create model two input model configuration show label use labeling x x data following error continue occur error check model input list numpy array pass model size model expect expect see array instead get following list array array code look like
647,647,36986815,"What is the parameter ""max_q_size"" used for in ""model.fit_generator""?","<p>I built a simple generator that yields a <code>tuple(inputs, targets)</code> with only single items in the <code>inputs</code> and <code>targets</code> lists. Basically, it is crawling the data set, one sample item at a time.</p>

<p>I pass this generator into: </p>

<pre><code>  model.fit_generator(my_generator(),
                      nb_epoch=10,
                      samples_per_epoch=1,
                      max_q_size=1  # defaults to 10
                      )
</code></pre>

<p>I get that:</p>

<ul>
<li><code>nb_epoch</code> is the number of times the training batch will be run</li>
<li><code>samples_per_epoch</code> is the number of samples trained with per epoch</li>
</ul>

<p>But what is <code>max_q_size</code> for and why would it default to 10?  I thought the purpose of using a generator was to batch data sets into reasonable chunks, so why the additional queue? </p>",36989864.0,2,0,,2016/5/2 16:07,8.0,2020/8/11 14:23,2018/7/11 19:44,,1079075.0,,722263.0,,1,24,python|machine-learning|generator|keras,20353,60.8345,,3,parameter max q size use model fit generator build simple generator yield single item list basically crawl data set one sample item time pas generator get number time training batch run number sample train per epoch would default think purpose use generator batch data set reasonable chunk additional queue
160,160,42812230,Why plt.imshow() doesn't display the image?,"<p>I am a newbie to keras, and when I tried to run my first keras program on my linux, something just didn't go as I wish.
Here is my python code:</p>

<pre><code>import numpy as np
np.random.seed(123)
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Convolution2D, MaxPooling2D
from keras.utils import np_utils
from keras.datasets import mnist
(X_train,y_train),(X_test,y_test) = mnist.load_data()
print X_train.shape
from matplotlib import pyplot as plt
plt.imshow(X_train[0])
</code></pre>

<p>The last sentence doesn't display anything. I copied those codes from a tutorial with out any modification. And there is nothing wrong with the backend of matplotlib on my computer. I have tested that through the code below.</p>

<pre><code>import matplotlib.pyplot as plt

data = [[0, 0.25], [0.5, 0.75]]

fig, ax = plt.subplots()
im = ax.imshow(data, cmap=plt.get_cmap('hot'), interpolation='nearest',
               vmin=0, vmax=1)
fig.colorbar(im)
plt.show()
</code></pre>

<p>And then I got a image like that:
<a href=""https://i.stack.imgur.com/2r6Lw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2r6Lw.png"" alt=""enter image description here""></a></p>

<p><br>
Moreover, I can get X_train[0] printed and it seems nothing wrong.<br>
So what could be the reason for that? Why the imshow() function in my first code didn't display anything?</p>",42812559.0,4,2,,2017/3/15 14:11,9.0,2020/4/28 16:47,2020/4/27 23:16,,4685471.0,,5511906.0,,1,79,python|matplotlib|keras,175352,320.176,,5,plt imshow display image newbie keras try run first keras program linux something go wish python code last sentence display anything copy code tutorial modification nothing wrong backend matplotlib computer test code get image like moreover get x train print seem nothing wrong could reason imshow function first code display anything
22,22,53953460,how to flatten input in `nn.Sequential` in Pytorch,"<p>how to flatten input inside the <code>nn.Sequential</code> </p>

<pre><code>Model = nn.Sequential(x.view(x.shape[0],-1),
                     nn.Linear(784,256),
                     nn.ReLU(),
                     nn.Linear(256,128),
                     nn.ReLU(),
                     nn.Linear(128,64),
                     nn.ReLU(),
                     nn.Linear(64,10),
                     nn.LogSoftmax(dim=1))
</code></pre>",53953537.0,3,0,,2018/12/28 3:49,1.0,2020/2/24 16:47,2018/12/28 4:02,,3236925.0,,8585699.0,,1,15,python|neural-network|artificial-intelligence|pytorch,20968,57.2862,,3,flatten input nn sequential pytorch flatten input inside
732,732,40564936,Keras: what is the difference between model.evaluate_generator and model.predict_generator,"<p>I used keras data augmentation to perform image classification (ten-class images). The last training epoch gives the results as follows:</p>

<pre><code>Epoch 50/50
4544/4545 [============================&gt;.] - ETA: 0s - loss: 0.7628 - acc: 0.7359 loss:  0.762710434054
New learning rate:  0.00214407973866
4545/4545 [==============================] - 115s - loss: 0.7627 - acc: 0.7360 - val_loss: 0.5563 - val_acc: 0.8124
</code></pre>

<p>Then evaluate the trained model by:</p>

<pre><code>scores = model.evaluate_generator(test_generator,1514) #1514 testing images
print(""Accuracy = "", scores[1])
</code></pre>

<p>It leads to the following results:</p>

<pre><code>('Accuracy = ', 0.80713342132152621)
</code></pre>

<p>The accuracy is not exactly the same as that obtained in the last training epoch. I don't understand the difference, even though it is marginal. </p>

<p>Further, model.predict_generator gives totally different result that is an array shown as follows:</p>

<pre><code>array([[  4.98306963e-06,   1.83774697e-04,   5.49453034e-05, ...,
      9.25193787e-01,   7.74697517e-04,   5.79946618e-06],
   [  2.06657965e-02,   2.35974863e-01,   2.66802781e-05, ...,
      2.16283044e-03,   8.42395966e-05,   2.46680051e-04],
   [  1.40222355e-05,   1.22740224e-03,   7.52218883e-04, ...,
      3.76749843e-01,   3.85622412e-01,   6.47417846e-06],
   ..., 
   [  9.94064331e-01,   1.30184961e-03,   1.08694976e-05, ...,
      1.25828717e-06,   2.29093766e-05,   9.01326363e-04],
   [  7.10375488e-01,   2.01397449e-01,   3.10241080e-06, ...,
      3.66877168e-10,   1.66322934e-05,   1.93767438e-08],
   [  8.13350256e-04,   2.67575349e-04,   6.79878794e-05, ...,
      8.63052785e-01,   9.70983761e-04,   8.54507030e-04]], dtype=float32)
</code></pre>

<p>I don't know what the matrix represents, and what is the difference between model.evaluate_generator and model.predict_generator. </p>

<p>It is noted that the resultant array has a shape of 1514*10. The array should be the prediction probabilities at each class for the set of testing images. If so how to compute a confusion matrix based on the result?</p>",,3,0,,2016/11/12 16:21,2.0,2019/9/2 19:44,2016/11/12 16:28,,6807211.0,,6807211.0,,1,18,keras,26735,58.5083,,4,keras difference model evaluate generator model predict generator use kera data augmentation perform image classification ten class image last training epoch give result follow evaluate trained model lead following result accuracy exactly obtain last training epoch understand difference even though marginal model predict generator give totally different result array show follow know matrix represent difference model evaluate generator model predict generator note resultant array shape array prediction probability class set test image compute confusion matrix base result
216,216,43990046,TensorFlow: Blas GEMM launch failed,"<p>When I'm trying to use TensorFlow with Keras using the gpu, I'm getting this error message:</p>

<pre><code>C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\__main__.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(&lt;keras.pre..., 37800, epochs=2, validation_data=&lt;keras.pre..., validation_steps=4200)`
  from ipykernel import kernelapp as app

Epoch 1/2

InternalError                             Traceback (most recent call last)
C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1038     try:
-&gt; 1039       return fn(*args)
   1040     except errors.OpError as e:

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1020                                  feed_dict, fetch_list, target_list,
-&gt; 1021                                  status, run_metadata)
   1022 

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--&gt; 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InternalError: Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
&lt;ipython-input-13-2a52d1079a66&gt; in &lt;module&gt;()
      1 history=model.fit_generator(batches, batches.n, nb_epoch=2, 
----&gt; 2                     validation_data=val_batches, nb_val_samples=val_batches.n)

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     86                 warnings.warn('Update your `' + object_name +
     87                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 88             return func(*args, **kwargs)
     89         wrapper._legacy_support_signature = inspect.getargspec(func)
     90         return wrapper

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\models.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)
   1108                                         workers=workers,
   1109                                         pickle_safe=pickle_safe,
-&gt; 1110                                         initial_epoch=initial_epoch)
   1111 
   1112     @interfaces.legacy_generator_methods_support

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     86                 warnings.warn('Update your `' + object_name +
     87                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 88             return func(*args, **kwargs)
     89         wrapper._legacy_support_signature = inspect.getargspec(func)
     90         return wrapper

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)
   1888                     outs = self.train_on_batch(x, y,
   1889                                                sample_weight=sample_weight,
-&gt; 1890                                                class_weight=class_weight)
   1891 
   1892                     if not isinstance(outs, list):

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1631             ins = x + y + sample_weights
   1632         self._make_train_function()
-&gt; 1633         outputs = self.train_function(ins)
   1634         if len(outputs) == 1:
   1635             return outputs[0]

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2227         session = get_session()
   2228         updated = session.run(self.outputs + [self.updates_op],
-&gt; 2229                               feed_dict=feed_dict)
   2230         return updated[:len(self.outputs)]
   2231 

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    776     try:
    777       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 778                          run_metadata_ptr)
    779       if run_metadata:
    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    980     if final_fetches or final_targets:
    981       results = self._do_run(handle, final_targets, final_fetches,
--&gt; 982                              feed_dict_string, options, run_metadata)
    983     else:
    984       results = []

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1030     if handle is None:
   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-&gt; 1032                            target_list, options, run_metadata)
   1033     else:
   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1050         except KeyError:
   1051           pass
-&gt; 1052       raise type(e)(node_def, op, message)
   1053 
   1054   def _extend_graph(self):

InternalError: Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]

Caused by op 'dense_1/MatMul', defined at:
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\__main__.py"", line 3, in &lt;module&gt;
    app.launch_new_instance()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2683, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2787, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2847, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-10-1e7a3b259f23&gt;"", line 4, in &lt;module&gt;
    model.add(Dense(10, activation='softmax'))
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\models.py"", line 466, in add
    output_tensor = layer(self.outputs[0])
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\topology.py"", line 585, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\layers\core.py"", line 840, in call
    output = K.dot(inputs, self.kernel)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py"", line 936, in dot
    out = tf.matmul(x, y)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1801, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 1263, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]
</code></pre>

<p>When I'm trying to use TensorFlow with Keras using the cpu, I'm getting this error message:</p>

<pre><code>C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\__main__.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(&lt;keras.pre..., 37800, validation_steps=4200, validation_data=&lt;keras.pre..., epochs=2)`
Epoch 1/2
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1038     try:
-&gt; 1039       return fn(*args)
   1040     except errors.OpError as e:

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1020                                  feed_dict, fetch_list, target_list,
-&gt; 1021                                  status, run_metadata)
   1022 

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\contextlib.py in __exit__(self, type, value, traceback)
     65             try:
---&gt; 66                 next(self.gen)
     67             except StopIteration:

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\errors_impl.py in raise_exception_on_not_ok_status()
    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),
--&gt; 466           pywrap_tensorflow.TF_GetCode(status))
    467   finally:

InternalError: Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]
     [[Node: Assign_3/_84 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_374_Assign_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)
&lt;ipython-input-14-f66b4d3d5b88&gt; in &lt;module&gt;()
      3 with tf.device('/cpu:0'):
      4     history=model.fit_generator(batches, batches.n, nb_epoch=2, 
----&gt; 5                     validation_data=val_batches, nb_val_samples=val_batches.n)

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     86                 warnings.warn('Update your `' + object_name +
     87                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 88             return func(*args, **kwargs)
     89         wrapper._legacy_support_signature = inspect.getargspec(func)
     90         return wrapper

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\models.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)
   1108                                         workers=workers,
   1109                                         pickle_safe=pickle_safe,
-&gt; 1110                                         initial_epoch=initial_epoch)
   1111 
   1112     @interfaces.legacy_generator_methods_support

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\legacy\interfaces.py in wrapper(*args, **kwargs)
     86                 warnings.warn('Update your `' + object_name +
     87                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 88             return func(*args, **kwargs)
     89         wrapper._legacy_support_signature = inspect.getargspec(func)
     90         return wrapper

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)
   1888                     outs = self.train_on_batch(x, y,
   1889                                                sample_weight=sample_weight,
-&gt; 1890                                                class_weight=class_weight)
   1891 
   1892                     if not isinstance(outs, list):

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight)
   1631             ins = x + y + sample_weights
   1632         self._make_train_function()
-&gt; 1633         outputs = self.train_function(ins)
   1634         if len(outputs) == 1:
   1635             return outputs[0]

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py in __call__(self, inputs)
   2227         session = get_session()
   2228         updated = session.run(self.outputs + [self.updates_op],
-&gt; 2229                               feed_dict=feed_dict)
   2230         return updated[:len(self.outputs)]
   2231 

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    776     try:
    777       result = self._run(None, fetches, feed_dict, options_ptr,
--&gt; 778                          run_metadata_ptr)
    779       if run_metadata:
    780         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
    980     if final_fetches or final_targets:
    981       results = self._do_run(handle, final_targets, final_fetches,
--&gt; 982                              feed_dict_string, options, run_metadata)
    983     else:
    984       results = []

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1030     if handle is None:
   1031       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
-&gt; 1032                            target_list, options, run_metadata)
   1033     else:
   1034       return self._do_call(_prun_fn, self._session, handle, feed_dict,

C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1050         except KeyError:
   1051           pass
-&gt; 1052       raise type(e)(node_def, op, message)
   1053 
   1054   def _extend_graph(self):

InternalError: Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]
     [[Node: Assign_3/_84 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_374_Assign_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

Caused by op 'dense_1/MatMul', defined at:
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\__main__.py"", line 3, in &lt;module&gt;
    app.launch_new_instance()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\traitlets\config\application.py"", line 658, in launch_instance
    app.start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\zmq\eventloop\zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tornado\stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\ipykernel\zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2683, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2787, in run_ast_nodes
    if self.run_code(code, result):
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\IPython\core\interactiveshell.py"", line 2847, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""&lt;ipython-input-12-1e7a3b259f23&gt;"", line 4, in &lt;module&gt;
    model.add(Dense(10, activation='softmax'))
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\models.py"", line 466, in add
    output_tensor = layer(self.outputs[0])
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\engine\topology.py"", line 585, in __call__
    output = self.call(inputs, **kwargs)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\layers\core.py"", line 840, in call
    output = K.dot(inputs, self.kernel)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\keras\backend\tensorflow_backend.py"", line 936, in dot
    out = tf.matmul(x, y)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1801, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 1263, in _mat_mul
    transpose_b=transpose_b, name=name)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 768, in apply_op
    op_def=op_def)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 2336, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\nicol\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\ops.py"", line 1228, in __init__
    self._traceback = _extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(64, 784), b.shape=(784, 10), m=64, n=10, k=784
     [[Node: dense_1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=""/job:localhost/replica:0/task:0/gpu:0""](flatten_1/Reshape, dense_1/kernel/read)]]
     [[Node: Assign_3/_84 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_374_Assign_3"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>In both cases, the error is with
    InternalError (see above for traceback): Blas GEMM launch failed
Can you tell me how to get Blas GEMM to launch?
I installed tensorflow and keras in a 3.5 python anaconda environment where I also installed all needed module (numpy, pandas, scipy, scikit-learn). I have a Windows 10 with a NVIDIA gpu that can use CUDA. I downloaded CUDA and cuDNN. I'm using the Jupyter notebook on Chrome.</p>

<p>Sometimes when I run my code, rather than having this error, I get that it starts running and then it crashes. After the crash, I can't do anything on my jupyter notebook and after some time a pop-up asks me if I want to kill the page. This is an image of what I got after the crash. 
!(<a href=""http://www.hostingpics.net/viewer.php?id=647186tensorflowError.png"" rel=""noreferrer"">http://www.hostingpics.net/viewer.php?id=647186tensorflowError.png</a>)</p>

<p>P.S. I know my problem is similar as in this question:
<a href=""https://stackoverflow.com/questions/43768498/tensorflow-basic-example-error-cublas-status-not-initialized"">Tensorflow Basic Example Error: CUBLAS_STATUS_NOT_INITIALIZED</a>
but it has not been solved there and I'm not sure this question is clear enough or is exactly the same problem as I have so I'm posting it with my own error message.
This problem is different of:
<a href=""https://stackoverflow.com/questions/37337728/tensorflow-internalerror-blas-sgemm-launch-failed"">TensorFlow: InternalError: Blas SGEMM launch failed</a>
Since I have a problem with GEMM rather than SGEMM and that my problem is both with gpu and cpu and it is not solved by the answer of this question.</p>",,18,2,,2017/5/15 22:59,17.0,2021/8/29 7:15,2017/5/16 4:39,,681865.0,,7675542.0,,1,54,python|tensorflow|keras|blas,52071,356.266,,1,tensorflow blas gemm launch fail try use tensorflow kera use gpu get error message try use tensorflow kera use cpu get error message case error internalerror see traceback blas gemm launch fail tell get blas gemm launch instal tensorflow kera python anaconda environment also instal need module numpy panda scipy scikit learn window nvidia gpu use cuda download cuda cudnn use jupyter notebook chrome sometimes run code rather error get start run crash crash anything jupyter notebook time pop asks want kill page image get crash p know problem similar question tensorflow basic example error cublas status initialize solve sure question clear enough exactly problem post error message problem different tensorflow internalerror blas sgemm launch fail since problem gemm rather sgemm problem gpu cpu solve answer question
642,642,36864774,Python/Keras - How to access each epoch prediction?,"<p>I'm using Keras to predict a time series. As standard I'm using 20 epochs.
I want to check if my model is learning well, by predicting for each one of the 20 epochs.</p>
<p>By using <code>model.predict()</code> I'm getting only one prediction among all epochs (not sure how Keras selects it). I want all predictions, or at least the 10 best.</p>
<p>Would anyone know how to help me?</p>",36872347.0,3,0,,2016/4/26 12:19,5.0,2021/9/3 6:48,2020/8/28 9:20,,6013016.0,,5606352.0,,1,19,python|machine-learning|keras|deep-learning,11707,50.6738,,5,python keras access epoch prediction use kera predict time series standard use epoch want check model learn well predict one epoch use get one prediction among epoch sure keras select want prediction least best would anyone know help
750,750,41166681,What does global_step mean in Tensorflow?,"<p>In this is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py"" rel=""noreferrer"">tutorial code</a> from TensorFlow website,</p>
<ol>
<li><p>could anyone help explain what does <code>global_step</code> mean?</p>
<p>I found on the Tensorflow website written that <em>global step is used count training steps</em>, but I don't quite get what exactly it means.</p>
</li>
<li><p>Also, what does the number 0 mean when setting up <code>global_step</code>?</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>    def training(loss,learning_rate):
        tf.summary.scalar('loss',loss)
        optimizer = tf.train.GradientDescentOptimizer(learning_rate)
        
        # Why 0 as the first parameter of the global_step tf.Variable?
        global_step = tf.Variable(0, name='global_step',trainable=False)

        train_op = optimizer.minimize(loss, global_step=global_step)
    
        return train_op
</code></pre>
<p>According to Tensorflow doc <em>global_step: increment by one after the variables have been updated</em>. Does that mean after one update <code>global_step</code> becomes 1?</p>",41167447.0,4,0,,2016/12/15 14:32,44.0,2019/10/3 11:27,2020/6/20 9:12,,-1.0,,6733064.0,,1,98,tensorflow|deep-learning,56944,230.422,,3,global step mean tensorflow tutorial code tensorflow website could anyone help explain mean find tensorflow website write global step use count train step quite get exactly mean also number mean set accord tensorflow doc global step increment one variable update mean one update becomes
344,344,45384684,Replace all nonzero values by zero and all zero values by a specific value,<p>I have a 3d tensor which contains some zero and nonzero values. I want to replace all nonzero values by zero and zero values by a specific value. How can I do that?</p>,45386834.0,3,0,,2017/7/29 2:39,3.0,2021/2/1 20:28,,,,,5352399.0,,1,17,pytorch,20033,50.407,,3,replace nonzero value zero zero value specific value tensor contain zero nonzero value want replace nonzero value zero zero value specific value
130,130,42184863,How do you make TensorFlow + Keras fast with a TFRecord dataset?,"<p><strong>What is an example of how to use a TensorFlow TFRecord with a Keras Model and tf.session.run() while keeping the dataset in tensors w/ queue runners?</strong></p>

<p>Below is a snippet that works but it needs the following improvements:</p>

<ul>
<li>Use the <a href=""https://keras.io/models/model/"" rel=""nofollow noreferrer"">Model API</a></li>
<li>specify an Input()</li>
<li>Load a dataset from a TFRecord</li>
<li>Run through a dataset in parallel (such as with a queuerunner)</li>
</ul>

<p>Here is the snippet, there are several TODO lines indicating what is needed:</p>



<pre class=""lang-python prettyprint-override""><code>from keras.models import Model
import tensorflow as tf
from keras import backend as K
from keras.layers import Dense, Input
from keras.objectives import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data

sess = tf.Session()
K.set_session(sess)

# Can this be done more efficiently than placeholders w/ TFRecords?
img = tf.placeholder(tf.float32, shape=(None, 784))
labels = tf.placeholder(tf.float32, shape=(None, 10))

# TODO: Use Input() 
x = Dense(128, activation='relu')(img)
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)
# TODO: Construct model = Model(input=inputs, output=preds)

loss = tf.reduce_mean(categorical_crossentropy(labels, preds))

# TODO: handle TFRecord data, is it the same?
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

sess.run(tf.global_variables_initializer())

# TODO remove default, add queuerunner
with sess.as_default():
    for i in range(1000):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})
    print(loss.eval(feed_dict={img:    mnist_data.test.images, 
                               labels: mnist_data.test.labels}))
</code></pre>

<p><strong>Why is this question relevant?</strong></p>

<ul>
<li>For high performance training without going back to python

<ul>
<li>no <a href=""https://stackoverflow.com/questions/36026892/how-can-i-convert-tfrecords-into-numpy-arrays"">TFRecord to numpy</a> to tensor conversions</li>
</ul></li>
<li><a href=""https://github.com/fchollet/keras/issues/5358"" rel=""nofollow noreferrer"">Keras will soon be part of tensorflow</a></li>
<li>Demonstrate how Keras Model() classes can accept tensors for input data correctly.</li>
</ul>

<p><strong>Here is some starter information for a semantic segmentation problem example:</strong></p>

<ul>
<li>example unet Keras model <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/models/unet.py"" rel=""nofollow noreferrer"">unet.py</a>, happens to be for semantic segmentation.</li>
<li><a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""nofollow noreferrer"">Keras + Tensorflow Blog Post</a></li>
<li>An <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""nofollow noreferrer"">attempt at running the unet model a tf session with TFRecords and a Keras model</a> (not working)</li>
<li>Code to create the TFRecords: <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/utils/tf_records.py"" rel=""nofollow noreferrer"">tf_records.py</a></li>
<li>An attempt at running the unet model a tf session with TFRecords and a Keras model is in <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""nofollow noreferrer"">densenet_fcn.py</a> (not working)</li>
</ul>",44771313.0,2,2,,2017/2/12 6:33,15.0,2019/7/11 7:25,2019/7/11 7:25,,10133797.0,,99379.0,,1,21,machine-learning|tensorflow|deep-learning|keras|keras-layer,15741,63.1881,,3,make tensorflow kera fast tfrecord dataset example use tensorflow tfrecord keras model tf session run keep dataset tensor w queue runner snippet work need following improvement use model api specify input load dataset tfrecord run dataset parallel queuerunner snippet several todo line indicate need question relevant high performance training without go back python tfrecord numpy tensor conversion keras soon part tensorflow demonstrate keras model class accept tensor input data correctly starter information semantic segmentation problem example example unet kera model unet py happen semantic segmentation kera tensorflow blog post attempt run unet model tf session tfrecords kera model work code create tfrecords tf record py attempt run unet model tf session tfrecords keras model densenet fcn py work
786,786,52241680,PyTorch NotImplementedError in forward,"<pre><code>import torch
import torch.nn as nn

device = torch.device('cuda' if torch.cuda.is_available() else 
'cpu')

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2), # 16x16x650
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # 32x16x650
            nn.ReLU(),
            nn.Dropout2d(0.5),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # 64x16x650
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2), # 64x8x325
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU()) # 64x8x325

        self.fc = nn.Sequential(
            nn.Linear(64*8*325, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
        )

        def forward(self, x):
            out = self.layer1(x)
            out = self.layer2(out)
            out = out.reshape(out.size(0), -1)
            out = self.fc(out)
            return out

# HYPERPARAMETER
learning_rate = 0.0001 
num_epochs = 15

import data

def main():
    model = Model().to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), 
lr=learning_rate)

    total_step = len(data.train_loader)
    for epoch in range(num_epochs):
        for i, (images, labels) in enumerate(data.train_loader):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        if (i + 1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))

    model.eval()
    with torch.no_grad():
        correct = 0
        total = 0
        for images, labels in data.test_loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))

if __name__ == '__main__':
    main()
</code></pre>

<p><strong><em>Error:</em></strong></p>

<pre><code>File ""/home/rladhkstn8/Desktop/SWID/tmp/pycharm_project_853/model.py"", line 82, in &lt;module&gt;
    main()
  File ""/home/rladhkstn8/Desktop/SWID/tmp/pycharm_project_853/model.py"", line 56, in main
    outputs = model(images)
  File ""/home/rladhkstn8/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/rladhkstn8/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 83, in forward
    raise NotImplementedError
NotImplementedError
</code></pre>

<p>I do not know where the problem is. I know that <code>NotImplementedError</code> should be implemented, but it happens when there is unimplemented code.</p>",,2,0,,2018/9/9 5:41,1.0,2021/6/17 5:06,2020/5/19 5:05,,1714410.0,,9117119.0,,1,14,python|image-processing|deep-learning|computer-vision|pytorch,13837,79.1642,,4,pytorch notimplementederror forward error know problem know implement happen unimplemented code
791,791,52441877,How does binary cross entropy loss work on autoencoders?,"<p>I wrote a vanilla autoencoder using only <code>Dense</code> layer. 
Below is my code:</p>



<pre class=""lang-py prettyprint-override""><code>iLayer = Input ((784,))
layer1 = Dense(128, activation='relu' ) (iLayer)
layer2 = Dense(64, activation='relu') (layer1)
layer3 = Dense(28, activation ='relu') (layer2)
layer4 = Dense(64, activation='relu') (layer3)
layer5 = Dense(128, activation='relu' ) (layer4)
layer6 = Dense(784, activation='softmax' ) (layer5)
model = Model (iLayer, layer6)
model.compile(loss='binary_crossentropy', optimizer='adam')

(trainX, trainY), (testX, testY) =  mnist.load_data()
print (""shape of the trainX"", trainX.shape)
trainX = trainX.reshape(trainX.shape[0], trainX.shape[1]* trainX.shape[2])
print (""shape of the trainX"", trainX.shape)
model.fit (trainX, trainX, epochs=5, batch_size=100)
</code></pre>

<h2>Questions:</h2>

<p>1) <code>softmax</code> provides probability distribution. Understood. This means, I would have a vector of 784 values with probability between 0 and 1. For example [ 0.02, 0.03..... upto 784 items], summing all 784 elements provides 1. </p>

<p>2) I don't understand how the binary crossentropy works with these values. Binary cross entropy is for two values of output, right?</p>",,1,6,,2018/9/21 10:36,14.0,2019/6/25 7:22,2018/9/21 12:37,,2099607.0,,916439.0,,1,25,machine-learning|neural-network|keras|autoencoder|cross-entropy,14158,54.604,,4,binary cross entropy loss work autoencoders write vanilla autoencoder use layer code question provide probability distribution understood mean would vector value probability example upto item sum element provide understand binary crossentropy work value binary cross entropy two value output right
501,501,32680860,caffe with multi-label images,"<p>I have a dataset of images that have multiple labels; There are 100 classes in the dataset, and each image has 1 to 5 labels associated with them.</p>

<p>I'm following the instruction in the following URL:</p>

<p><a href=""https://github.com/BVLC/caffe/issues/550"" rel=""noreferrer"">https://github.com/BVLC/caffe/issues/550</a></p>

<p>It says that I need to generate a text file listing the images and its labels as in </p>

<blockquote>
<pre><code>/home/my_test_dir/picture-foo.jpg 0
/home/my_test_dir/picture-foo1.jpg 1
</code></pre>
</blockquote>

<p>In my case, since I have multi-label images, does it work to simply add labels as in following?</p>

<blockquote>
<pre><code>/home/my_test_dir/picture-foo.jpg 0 2 5
/home/my_test_dir/picture-foo1.jpg 1 4
</code></pre>
</blockquote>

<p>I have a feeling that it's probably not going to be that simple, and if I'm right, in what step and how should I integrate the multi-label-ness of the dataset in the process of setting up Caffe?</p>",32697800.0,3,0,,2015/9/20 15:09,9.0,2016/6/18 22:05,2015/9/21 5:11,,1714410.0,,639973.0,,1,19,neural-network|deep-learning|caffe|multilabel-classification,15913,57.207,,2,caffe multi label image dataset image multiple label class dataset image label associate follow instruction following url say need generate text file list image label case since multi label image work simply add label follow feeling probably go simple right step integrate multi label ness dataset process set caffe
340,340,45226950,what does arg_scope actually do?,"<p>I am a beginner in neural nets and TensorFlow, and I am trying to understand the role of <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/framework/arg_scope"" rel=""noreferrer""><code>arg_scope</code></a>.</p>

<p>It seems to me that it is a way to put together a dictionary of ""things you want to do"" to a certain layer with certain variables. Please correct me if I am wrong. How would you explain exactly what it is for, to a beginner?</p>",45231064.0,1,0,,2017/7/21 0:01,11.0,2017/7/21 7:05,2017/7/21 7:00,,1735003.0,,5759234.0,,1,21,python|tensorflow|neural-network|conv-neural-network,10200,60.2344,,3,arg scope actually beginner neural net tensorflow try understand role seem way put together dictionary thing want certain layer certain variable please correct wrong would explain exactly beginner
560,560,48822463,How to use PyTorch multiprocessing?,"<p>I'm trying to use python's multiprocessing <code>Pool</code> method in <code>pytorch</code> to process a image. Here's the code:</p>

<pre><code>from multiprocessing import Process, Pool
from torch.autograd import Variable
import numpy as np
from scipy.ndimage import zoom

def get_pred(args):

  img = args[0]
  scale = args[1]
  scales = args[2]
  img_scale = zoom(img.numpy(),
                     (1., 1., scale, scale),
                     order=1,
                     prefilter=False,
                     mode='nearest')

  # feed input data
  input_img = Variable(torch.from_numpy(img_scale),
                     volatile=True).cuda()
  return input_img

scales = [1,2,3,4,5]
scale_list = []
for scale in scales: 
    scale_list.append([img,scale,scales])
multi_pool = Pool(processes=5)
predictions = multi_pool.map(get_pred,scale_list)
multi_pool.close() 
multi_pool.join()
</code></pre>

<p>I'm getting this error:</p>

<pre><code>`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>

<p>`
In this line:</p>

<pre><code>predictions = multi_pool.map(get_pred,scale_list)
</code></pre>

<p>Can anyone tell me what I'm doing wrong ?</p>",48938860.0,2,0,,2018/2/16 8:05,5.0,2018/2/22 23:22,2018/2/16 9:57,,6451244.0,,3336062.0,,1,17,python|computer-vision|multiprocessing|pytorch,38404,57.1375,,4,use pytorch multiprocessing try use python multiprocessing method process image code get error line anyone tell wrong
702,702,39366271,For what reason Convolution 1x1 is used in deep neural networks?,"<p>I'm looking at InceptionV3 (GoogLeNet) architecture and cannot understand why do we need conv1x1 layers?</p>

<p>I know how convolution works, but I see a profit with patch size > 1. </p>",39367644.0,2,0,,2016/9/7 9:33,18.0,2017/9/30 5:02,,,,,1251713.0,,1,23,neural-network|tensorflow|deep-learning|convolution|conv-neural-network,10419,74.2713,,0,reason convolution x use deep neural network look inceptionv googlenet architecture understand need conv x layer know convolution work see profit patch size
621,621,50753477,What does data.norm() < 1000 do in PyTorch?,"<p>I am following the PyTorch tutorial <a href=""https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"" rel=""noreferrer"">here</a>.
It says that</p>

<pre><code>x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() &lt; 1000:
    y = y * 2

print(y)

Out:    
tensor([-590.4467,   97.6760,  921.0221])
</code></pre>

<p>Could someone explain what data.norm() does here?
When I change <code>.randn</code> to <code>.ones</code> its output is  <code>tensor([ 1024.,  1024.,  1024.])</code>.</p>",50753830.0,4,1,,2018/6/8 4:41,6.0,2020/11/30 13:54,2018/6/8 5:22,,2956066.0,,6002424.0,,1,25,python|deep-learning|linear-algebra|pytorch|tensor,9617,74.9322,,3,data norm pytorch follow pytorch tutorial say could someone explain data norm change output
574,574,49206550,pytorch error: multi-target not supported in CrossEntropyLoss(),"<p>I am on a project using acceleration data to predict some activities. 
But I have problems on the loss calculation. I am using <code>CrossEntropyLoss</code> for it.</p>

<p>Data is used for it like below
I use the first 4 data of each rows to predict the index like the last one of each rows.</p>

<pre><code>1 84 84 81 4
81 85 85 80 1
81 82 84 80 1
1 85 84 2 0
81 85 82 80 1
81 82 84 80 1
81 25 84 80 5
</code></pre>

<p>The error messages are like below. </p>

<pre><code>minoh@minoh-VirtualBox:~/cow$ python lec5.py
Traceback (most recent call last):
  File ""lec5.py"", line 97, in &lt;module&gt;
    train(epoch)
  File ""lec5.py"", line 74, in train
    loss = criterion(y_pred, labels)
  File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 357, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py"", line 679, in forward
    self.ignore_index, self.reduce)
  File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1161, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)
  File ""/home/minoh/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py"", line 1052, in nll_loss
    return torch._C._nn.nll_loss(input, target, weight, size_average, ignore_index, reduce)
RuntimeError: multi-target not supported at /opt/conda/conda-bld/pytorch_1518243271935/work/torch/lib/THNN/generic/ClassNLLCriterion.c:22
</code></pre>

<p>My code is based on <a href=""https://github.com/hunkim/PyTorchZeroToAll/blob/master/09_2_softmax_mnist.py"" rel=""noreferrer"">Sung Kim's pytorch</a></p>

<pre><code>import numpy as np
import torch    
from torch.autograd import Variable    
import torch.nn.functional as F    
from torch.utils.data import Dataset, DataLoader    
import torch.nn as nn    
import torch.optim as optim    
from torchvision import datasets, transforms    

class CowDataset(Dataset):    
    def __init__(self):    
        xy_str = np.loadtxt('cow_test', delimiter = ' ', dtype = np.str)    
        xy = xy_str.astype(np.float32)    
        xy_int = xy_str.astype(np.int)    
        self.len = xy.shape[0]    
        self.x_data = torch.from_numpy(xy[:, 0:4])    
        self.y_data = torch.from_numpy(xy_int[:, [4]])    

    def __getitem__(self, index):    
        return self.x_data[index], self.y_data[index]    

    def __len__(self):    
        return self.len    

dataset = CowDataset()    
train_loader = DataLoader(dataset = dataset, batch_size = 32, shuffle = True)    

class CowTestset(Dataset):    
        def __init__(self):    
                xy_str = np.loadtxt('cow_test2', delimiter = ' ', dtype =np.str)    
                xy = xy_str.astype(np.float32)    
                xy_int = xy_str.astype(np.int)    
                self.len = xy.shape[0]    
                self.x_data = torch.from_numpy(xy[:, 0:4])    
                self.y_data = torch.from_numpy(xy_int[:, [4]])    

        def __getitem__(self, index):    
                return self.x_data[index], self.y_data[index]    

        def __len__(self):    
                return self.len    

testset = CowTestset()    
test_loader = DataLoader(dataset = testset, batch_size = 32, shuffle = True)    

class Model(torch.nn.Module):    
    def __init__(self):    
        super(Model, self).__init__()    
        self.l1 = torch.nn.Linear(4,5)    
        self.l2 = torch.nn.Linear(5,7)    
        self.l3 = torch.nn.Linear(7,6)    
        self.sigmoid = torch.nn.Sigmoid()    

    def forward(self, x):    
        out1 = self.sigmoid(self.l1(x))    
        out2 = self.sigmoid(self.l2(out1))    
        y_pred = self.sigmoid(self.l3(out2))    
        return y_pred    

model = Model()    
criterion = nn.CrossEntropyLoss()    
optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum = 0.5)    

def train(epoch):    
    model.train()    
    for batch_idx, (inputs, labels) in enumerate(train_loader):    
        inputs, labels = Variable(inputs), Variable(labels)    
        optimizer.zero_grad()    
        y_pred = model(inputs)    
        loss = criterion(y_pred, labels)    
        loss.backward()    
        optimizer.step()    
        if batch_idx % 10 == 0:    
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))    

def test():    
    model.eval()    
    test_loss = 0    
    correct = 0    
    for data, target in test_loader:    
        data, target = Variable(data, volatile = True), Variable(target)    
        print(target)    
        output = model(data)    
        test_loss += criterion(output, target).data[0]    
        pred = output.data.max(1, keepdim = True)[1]    
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    
    test_loss /= len(test_loader.dataset)    
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(test_loss, correct, len(test_loader.dataset), 100.* correct / len(test_loader.dataset)))    

for epoch in range(1,7):    
    train(epoch)    
    test()    
</code></pre>",49209628.0,1,0,,2018/3/10 7:36,3.0,2018/3/10 15:17,2018/3/10 15:17,,3057587.0,,9471025.0,,1,14,python|python-3.x|machine-learning|neural-network|pytorch,18445,53.8635,,4,pytorch error multi target support crossentropyloss project use acceleration data predict activity problem loss calculation use data use like use first data row predict index like last one row error message like code base sung kim pytorch
372,372,46146757,Very low GPU usage during training in Tensorflow,"<p>I am trying to train a simple multi-layer perceptron for a 10-class image classification task, which is a part of the assignment for the Udacity Deep-Learning course. To be more precise, the task is to classify letters rendered from various fonts (the dataset is called notMNIST).</p>

<p>The code I ended up with looks fairly simple, but no matter what I always get very low GPU usage during training. I measure load with GPU-Z and it shows just 25-30%.</p>

<p>Here is my current code:</p>

<pre><code>graph = tf.Graph()
with graph.as_default():
    tf.set_random_seed(52)

    # dataset definition
    dataset = Dataset.from_tensor_slices({'x': train_data, 'y': train_labels})
    dataset = dataset.shuffle(buffer_size=20000)
    dataset = dataset.batch(128)
    iterator = dataset.make_initializable_iterator()
    sample = iterator.get_next()
    x = sample['x']
    y = sample['y']

    # actual computation graph
    keep_prob = tf.placeholder(tf.float32)
    is_training = tf.placeholder(tf.bool, name='is_training')

    fc1 = dense_batch_relu_dropout(x, 1024, is_training, keep_prob, 'fc1')
    fc2 = dense_batch_relu_dropout(fc1, 300, is_training, keep_prob, 'fc2')
    fc3 = dense_batch_relu_dropout(fc2, 50, is_training, keep_prob, 'fc3')
    logits = dense(fc3, NUM_CLASSES, 'logits')

    with tf.name_scope('accuracy'):
        accuracy = tf.reduce_mean(
            tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1)), tf.float32),
        )
        accuracy_percent = 100 * accuracy

    with tf.name_scope('loss'):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))

    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        # ensures that we execute the update_ops before performing the train_op
        # needed for batch normalization (apparently)
        train_op = tf.train.AdamOptimizer(learning_rate=1e-3, epsilon=1e-3).minimize(loss)

with tf.Session(graph=graph) as sess:
    tf.global_variables_initializer().run()
    step = 0
    epoch = 0
    while True:
        sess.run(iterator.initializer, feed_dict={})
        while True:
            step += 1
            try:
                sess.run(train_op, feed_dict={keep_prob: 0.5, is_training: True})
            except tf.errors.OutOfRangeError:
                logger.info('End of epoch #%d', epoch)
                break

        # end of epoch
        train_l, train_ac = sess.run(
            [loss, accuracy_percent],
            feed_dict={x: train_data, y: train_labels, keep_prob: 1, is_training: False},
        )
        test_l, test_ac = sess.run(
            [loss, accuracy_percent],
            feed_dict={x: test_data, y: test_labels, keep_prob: 1, is_training: False},
        )
        logger.info('Train loss: %f, train accuracy: %.2f%%', train_l, train_ac)
        logger.info('Test loss: %f, test accuracy: %.2f%%', test_l, test_ac)

        epoch += 1
</code></pre>

<p>Here's what I tried so far:</p>

<ol>
<li><p>I changed the input pipeline from simple <code>feed_dict</code> to <code>tensorflow.contrib.data.Dataset</code>. As far as I understood, it is supposed to take care of the efficiency of the input, e.g. load data in a separate thread. So there should not be any bottleneck associated with the input.</p></li>
<li><p>I collected traces as suggested here: <a href=""https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/issues/1824#issuecomment-225754659</a>
However, these traces didn't really show anything interesting. >90% of the train step is matmul operations.</p></li>
<li><p>Changed batch size. When I change it from 128 to 512 the load increases from ~30% to ~38%, when I increase it further to 2048, the load goes to ~45%. I have 6Gb GPU memory and dataset is single channel 28x28 images. Am I really supposed to use such a big batch size? Should I increase it further?</p></li>
</ol>

<p>Generally, should I worry about the low load, is it really a sign that I am training inefficiently?</p>

<p>Here's the GPU-Z screenshots with 128 images in the batch. You can see low load with occasional spikes to 100% when I measure accuracy on the entire dataset after each epoch.</p>

<p><a href=""https://i.stack.imgur.com/0x0MS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0x0MS.png"" alt=""GPU load""></a> </p>",46146814.0,2,0,,2017/9/11 0:26,5.0,2018/1/20 15:42,,,,,1645784.0,,1,22,python|tensorflow|deep-learning|gpu|tensorflow-gpu,28780,55.6364,,4,low gpu usage training tensorflow try train simple multi layer perceptron class image classification task part assignment udacity deep learning course precise task classify letter render various font dataset call notmnist code end look fairly simple matter always get low gpu usage training measure load gpu z show current code try far change input pipeline simple far understood suppose take care efficiency input e g load data separate thread bottleneck associate input collect trace suggest issuecomment however trace really show anything interesting train step matmul operation change batch size change load increase increase far load go gb gpu memory dataset single channel x image really suppose use big batch size increase far generally worry low load really sign train inefficiently gpu z screenshots image batch see low load occasional spike measure accuracy entire dataset epoch
456,456,20923574,What's the difference between convolutional and recurrent neural networks?,"<p>I'm new to the topic of neural networks. I came across the two terms <em>convolutional neural network</em> and <em>recurrent neural network</em>. </p>

<p>I'm wondering if these two terms are referring to the same thing, or, if not, what would be the difference between them?</p>",,8,0,,2014/1/4 16:31,15.0,2020/8/12 14:00,2017/9/17 11:29,,3924118.0,,2450316.0,,1,67,neural-network|difference|recurrent-neural-network,41470,201.671,2020/8/15 12:12,0,difference convolutional recurrent neural network new topic neural network come across two term convolutional neural network recurrent neural network wonder two term refer thing would difference
491,491,31279494,How to install Cudnn from command line,"<p>Cudnn: <a href=""https://developer.nvidia.com/cudnn"" rel=""noreferrer"">https://developer.nvidia.com/cudnn</a></p>

<p>I login and go jump through all the hoops that NVIDIA wants you to do; however, when it comes time to download the file I can't seem to figure out how to do it via wget and the command line. </p>

<p>I was hoping someone has done this. I've copy and pasted the link that they want to click and used this in wget copy-and-pasted-url. But I just get back an html file. </p>",,5,0,,2015/7/7 21:21,3.0,2018/7/26 9:39,,,,,678392.0,,1,17,cuda|nvidia|deep-learning,17793,69.001,,1,install cudnn command line cudnn login go jump hoop nvidia want however come time download file seem figure via wget command line hop someone copy paste link want click use wget copy pasted url get back html file
85,85,14924486,Deep learning for image classification,"<p>After reading a few papers on deep learning and deep belief networks, I got a basic idea of how it works. But still stuck with the last step, i.e, the classification step. 
Most of the implementation I found on the Internet deal with generation. (MNIST digits)</p>

<p>Is there some explanation (or code) available somewhere that talk about classifying images(preferably natural images or objects) using DBNs? </p>

<p>Also some pointers in the direction would be really helpful. </p>",,3,2,,2013/2/17 18:26,7.0,2019/7/11 9:32,2019/7/11 9:32,,1133286.0,,676217.0,,1,18,machine-learning|neural-network|deep-learning|dbn,7800,54.3684,2014/9/16 4:39,0,deep learning image classification read paper deep learning deep belief network get basic idea work still stick last step e classification step implementation find internet deal generation mnist digit explanation code available somewhere talk classifying image preferably natural image object use dbns also pointer direction would really helpful
470,470,59013109,RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same,"<p>I am trying to train the following CNN as follows, but I keep getting the same error regarding .cuda() and I am not sure how to fix it. Here is a chunk of my code so far.</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
import torchvision
from torchvision import datasets, transforms, models
from torch.utils.data.sampler import SubsetRandomSampler


data_dir = ""/home/ubuntu/ML2/ExamII/train2/""
valid_size = .2

# Normalize the test and train sets with torchvision
train_transforms = transforms.Compose([transforms.Resize(224),
                                           transforms.ToTensor(),
                                           ])

test_transforms = transforms.Compose([transforms.Resize(224),
                                          transforms.ToTensor(),
                                          ])

# ImageFolder class to load the train and test images
train_data = datasets.ImageFolder(data_dir, transform=train_transforms)
test_data = datasets.ImageFolder(data_dir, transform=test_transforms)


# Number of train images
num_train = len(train_data)
indices = list(range(num_train))
# Split = 20% of train images
split = int(np.floor(valid_size * num_train))
# Shuffle indices of train images
np.random.shuffle(indices)
# Subset indices for test and train
train_idx, test_idx = indices[split:], indices[:split]
# Samples elements randomly from a given list of indices
train_sampler = SubsetRandomSampler(train_idx)
test_sampler = SubsetRandomSampler(test_idx)
# Batch and load the images
trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=1)
testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=1)


#print(trainloader.dataset.classes)

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = models.resnet50(pretrained=True)

model.fc = nn.Sequential(nn.Linear(2048, 512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512, 10),
                                 nn.LogSigmoid())
                                 # nn.LogSoftmax(dim=1))
# criterion = nn.NLLLoss()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.003)
model.to(device)

#Train the network
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
</code></pre>

<p>However, I keep getting this error in the console: </p>

<blockquote>
  <p>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same`</p>
</blockquote>

<p>Any thoughts on how to fix it? I read that maybe the model hasn't been pushed into my GPU, but not sure how to fix it. Thanks!</p>",59013131.0,5,0,,2019/11/23 23:08,13.0,2020/10/23 9:38,2020/9/30 18:19,,10908375.0,,10168730.0,,1,79,python|python-3.x|machine-learning|deep-learning|pytorch,80877,245.631,,4,runtimeerror input type torch floattensor weight type torch cuda floattensor try train following cnn follow keep get error regard cuda sure fix chunk code far however keep get error console runtimeerror input type torch floattensor weight type torch cuda floattensor thought fix read maybe model push gpu sure fix thanks
567,567,49008074,How to create a neural network for regression?,"<p>I am trying to use Keras to make a neural network. The data I am using is <a href=""https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics"" rel=""noreferrer"">https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics</a>. My code is as follows:</p>

<pre><code>import numpy as np
from keras.layers import Dense, Activation
from keras.models import Sequential
from sklearn.model_selection import train_test_split

data = np.genfromtxt(r""""""file location"""""", delimiter=',')

model = Sequential()
model.add(Dense(32, activation = 'relu', input_dim = 6))
model.add(Dense(1,))
model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])

Y = data[:,-1]
X = data[:, :-1]
</code></pre>

<p>From here I have tried using model.fit(X, Y), but the accuracy of the model appears to remain at 0. I am new to Keras so this is probably an easy solution, apologies in advance.</p>

<p>My question is what is the best way to add regression to the model so that the accuracy increases? Thanks in advance.</p>",49009008.0,1,2,,2018/2/27 11:53,16.0,2020/2/2 18:35,2020/2/2 18:35,,6583140.0,,8788938.0,,1,14,python|numpy|machine-learning|neural-network|keras,22383,63.1997,,3,create neural network regression try use kera make neural network data use hydrodynamics code follow try use model fit x accuracy model appear remain new keras probably easy solution apology advance question best way add regression model accuracy increase thanks advance
532,532,35792278,How to find Number of parameters of a keras model?,"<p>For a Feedforward Network (FFN), it is easy to compute the number of parameters. Given a CNN, LSTM etc is there a quick way to find the number of parameters in a keras model?</p>",35827171.0,4,0,,2016/3/4 9:25,4.0,2021/6/9 3:54,2017/9/5 19:11,,2962001.0,,4497662.0,,1,56,deep-learning|keras,28636,190.628,,3,find number parameter kera model feedforward network ffn easy compute number parameter give cnn lstm etc quick way find number parameter keras model
710,710,39734146,What's the difference between tensorflow dynamic_rnn and rnn?,"<p>There are several classes in <code>tf.nn</code> that relate to RNNs. In the examples I find on the web, <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn</code> seem to be used interchangeably or at least I cannot seem to figure out why one is used in place of the other. What is the difference?</p>",40986014.0,2,1,,2016/9/27 20:54,20.0,2018/12/28 6:57,2018/7/8 23:06,,3924118.0,,302268.0,,1,32,tensorflow|recurrent-neural-network,16047,80.6216,,3,difference tensorflow dynamic rnn rnn several class relate rnns example find web seem use interchangeably least seem figure one use place difference
94,94,41415629,ImportError: No module named 'tensorflow.python',"<p>here i wanna run this code for try neural network with python :</p>

<pre><code>from __future__ import print_function 
from keras.datasets import mnist from 
keras.models import Sequential from 
keras.layers import Activation, Dense 
from keras.utils import np_utils 
import tensorflow as tf


batch_size = 128 nb_classes = 10 nb_epoch = 12

#input image dimensions img_row, img_cols = 28, 28

#the data, Shuffled and split between train and test sets (X_train, y_train), (X_test, y_test) = mnist.load_data()


X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)

X_test = X_test.reshape(X_test.shape[0], img_row * img_cols)

X_train = X_train.astype('float32') X_test = X_test.astype('float32') X_train /= 255 X_text /= 255

print('X_train shape:', X_train.shape) print(X_train_shape[0], 'train samples') print(X_test_shape[0], 'test samples')

#convert class vectors to binary category

Y_train = np_utils.to_categorical(y_train, nb_classes)

Y_test = np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Dense(output_dim = 800, input_dim=X_train.shape[1])) model.add(Activation('sigmoid')) model.add(Dense(nb_classes)) model.add(Actiovation('softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) #crossentropy fungsi galat atau fungsi error dipakai kalo class biner



#model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch = nb_poch, verbose=1, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, verbose = 0) print('Test Score : ', score[0]) print('Test Accuracy : ', score[1])
</code></pre>

<p>at the beginning it must install keras, and success. but when try to run the code at the first the error is :</p>

<blockquote>
  <p>ImportError : No Moduled Name ""tensorflow""</p>
</blockquote>

<p>then i install using pip :</p>

<blockquote>
  <p>pip install tensorflow</p>
</blockquote>

<p>after installation i try to run code again, got another message like this :</p>

<blockquote>
  <p>ImportError : No Moduled Name ""tensorflow.python""</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/tYgfR.png"" rel=""noreferrer"">Message Error</a>
i dont have any idea with the error</p>",,12,1,,2017/1/1 12:58,4.0,2021/6/5 17:28,2019/6/26 12:01,,8171468.0,user7089140,,,1,20,python|tensorflow|keras|neural-network,97137,101.95,,1,importerror module name tensorflow python wan na run code try neural network python begin must install kera success try run code first error importerror moduled name tensorflow install use pip pip install tensorflow installation try run code get another message like importerror moduled name tensorflow python message error dont idea error
41,41,54689096,How to install Keras with gpu support?,"<p>I installed Tensorflow for GPU using: <code>pip install tensorflow-gpu</code> 
But when I tried the same for <strong>Keras</strong> <code>pip install keras-gpu</code>, it pulled me an error: <em>could not find the version that satisfies the requirements</em>.</p>",54689291.0,4,1,,2019/2/14 11:12,5.0,2020/12/7 17:52,2019/10/15 14:13,,3924118.0,,9222818.0,,1,16,tensorflow|keras|pip|anaconda|gpu,53597,72.7166,,1,install kera gpu support instal tensorflow gpu use try kera pull error could find version satisfy requirement
447,447,48226086,Training Loss and Validation Loss in Deep Learning,"<p>Would you please guide me how to interpret the following results?</p>

<p>1) loss &lt; validation_loss
2) loss > validation_loss</p>

<p>It seems that the training loss always should be less than validation loss. But, both of these cases happen when training a model.</p>",,3,2,,2018/1/12 12:13,8.0,2021/2/3 12:24,2019/9/17 6:41,,5884955.0,,6092256.0,,1,8,machine-learning|deep-learning,36489,54.0486,,4,training loss validation loss deep learning would please guide interpret following result loss validation loss loss validation loss seem training loss always less validation loss case happen train model
756,756,51455863,What's the difference between a Tensorflow Keras Model and Estimator?,<p>Both Tensorflow Keras models and Tensorflow Estimators are able to train neural network models and use them to predict new data. They are both high-level APIs that sits on top of the low-level core TensorFlow API. So when should I use one over the other?</p>,51455864.0,2,0,,2018/7/21 12:02,19.0,2019/5/9 23:59,,,,,3966682.0,,1,59,tensorflow|keras|tensorflow-estimator,15484,124.36,,3,difference tensorflow kera model estimator tensorflow kera model tensorflow estimator able train neural network model use predict new data high level apis sit top low level core tensorflow api use one
148,148,42621864,Difference between Keras model.save() and model.save_weights()?,"<p>To save a model in Keras, what are the differences between the output files of:</p>

<ol>
<li><code>model.save()</code> </li>
<li><code>model.save_weights()</code></li>
<li><code>ModelCheckpoint()</code> in the callback</li>
</ol>

<p>The saved file from <code>model.save()</code> is larger than the model from <code>model.save_weights()</code>, but significantly larger than a JSON or Yaml model architecture file.  Why is this?  </p>

<p>Restating this: Why is size(model.save()) + size(something) = size(model.save_weights()) + size(model.to_json()), what is that ""something""?</p>

<p>Would it be more efficient to just <code>model.save_weights()</code> and <code>model.to_json()</code>, and load from these than to just do <code>model.save()</code> and <code>load_model()</code>?  </p>

<p>What are the differences?</p>",42622446.0,3,0,,2017/3/6 9:26,15.0,2020/1/30 15:30,2018/4/14 20:12,,7117003.0,,3993741.0,,1,45,machine-learning|tensorflow|neural-network|keras,24283,90.5412,,3,difference kera model save model save weight save model kera difference output file callback save file large model significantly large json yaml model architecture file restate size model save size something size model save weight size model json something would efficient load difference
117,117,41813665,"Tensorflow Slim: TypeError: Expected int32, got list containing Tensors of type '_Message' instead","<p>I am following <a href=""https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb"" rel=""noreferrer"">this</a> tutorial for learning TensorFlow Slim but upon running the following code for Inception:</p>

<pre><code>import numpy as np
import os
import tensorflow as tf
import urllib2

from datasets import imagenet
from nets import inception
from preprocessing import inception_preprocessing

slim = tf.contrib.slim

batch_size = 3
image_size = inception.inception_v1.default_image_size
checkpoints_dir = '/tmp/checkpoints/'
with tf.Graph().as_default():
    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'
    image_string = urllib2.urlopen(url).read()
    image = tf.image.decode_jpeg(image_string, channels=3)
    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)
    processed_images  = tf.expand_dims(processed_image, 0)

    # Create the model, use the default arg scope to configure the batch norm parameters.
    with slim.arg_scope(inception.inception_v1_arg_scope()):
        logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
    probabilities = tf.nn.softmax(logits)

    init_fn = slim.assign_from_checkpoint_fn(
        os.path.join(checkpoints_dir, 'inception_v1.ckpt'),
        slim.get_model_variables('InceptionV1'))

    with tf.Session() as sess:
        init_fn(sess)
        np_image, probabilities = sess.run([image, probabilities])
        probabilities = probabilities[0, 0:]
        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]

    plt.figure()
    plt.imshow(np_image.astype(np.uint8))
    plt.axis('off')
    plt.show()

    names = imagenet.create_readable_names_for_imagenet_labels()
    for i in range(5):
        index = sorted_inds[i]
        print('Probability %0.2f%% =&gt; [%s]' % (probabilities[index], names[index]))
</code></pre>

<p>I seem to be getting this set of errors:</p>

<pre><code>Traceback (most recent call last):
  File ""DA_test_pred.py"", line 24, in &lt;module&gt;
    logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 290, in inception_v1
    net, end_points = inception_v1_base(inputs, scope=scope)
  File ""/home/deepankar1994/Desktop/MTP/TensorFlowEx/TFSlim/models/slim/nets/inception_v1.py"", line 96, in inception_v1_base
    net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1053, in concat
    dtype=dtypes.int32).get_shape(
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 651, in convert_to_tensor
    as_ref=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 165, in constant
    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 367, in make_tensor_proto
    _AssertCompatible(values, dtype)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 302, in _AssertCompatible
    (dtype.name, repr(mismatch), type(mismatch).__name__))
TypeError: Expected int32, got list containing Tensors of type '_Message' instead.
</code></pre>

<p>This is strange because all of this code is from their official guide. I am new to TF and any help would be appreciated.</p>",41929380.0,4,1,,2017/1/23 18:58,8.0,2020/8/14 15:56,,,,,4482655.0,,1,31,python|machine-learning|tensorflow|computer-vision|deep-learning,32644,124.855,,4,tensorflow slim typeerror expect int get list contain tensor type message instead follow tutorial learn tensorflow slim upon run following code inception seem get set error strange code official guide new tf help would appreciate
777,777,51911749,What is the difference between torch.tensor and torch.Tensor?,"<p>Since version 0.4.0, it is possible to use <code>torch.tensor</code> and <code>torch.Tensor</code></p>

<p>What is the difference? What was the reasoning for providing these two very similar and confusing alternatives?</p>",51912202.0,5,0,,2018/8/18 19:06,10.0,2021/7/22 21:05,2018/8/18 19:13,,157726.0,,157726.0,,1,38,python|pytorch,9503,108.911,,3,difference torch tensor torch tensor since version possible use difference reasoning provide two similar confusing alternative
534,534,36162180,Gradient Descent vs Adagrad vs Momentum in TensorFlow,"<p>I'm studying <em>TensorFlow</em> and how to use it, even if I'm not an expert of neural networks and deep learning (just the basics).</p>

<p>Following tutorials, I don't understand the real and practical differences between the three optimizers for loss. I look at the <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers"" rel=""noreferrer"">API</a> and I understand the principles, but my questions are:</p>

<p><strong>1. When is it preferable to use one instead of the others ?</strong></p>

<p><strong>2. Are there important differences to know ?</strong></p>",44225502.0,3,4,,2016/3/22 18:16,60.0,2019/4/12 17:38,2018/4/12 2:56,,1075708.0,,859453.0,,1,74,tensorflow|deep-learning,34050,289.528,,0,gradient descent v adagrad v momentum tensorflow study tensorflow use even expert neural network deep learning basic follow tutorial understand real practical difference three optimizers loss look api understand principle question preferable use one instead others important difference know
228,228,44232898,"MemoryError in TensorFlow; and ""successful NUMA node read from SysFS had negative value (-1)"" with xen","<p>I am using  tensor flow version :</p>

<blockquote>
  <p>0.12.1</p>
</blockquote>

<p>Cuda tool set version is 8.</p>

<pre><code>lrwxrwxrwx  1 root root   19 May 28 17:27 cuda -&gt; /usr/local/cuda-8.0
</code></pre>

<p>As documented  <a href=""https://www.tensorflow.org/versions/r0.10/get_started/os_setup#optional_install_cuda_gpus_on_linux"" rel=""noreferrer"">here</a> I have downloaded and installed cuDNN. But while execeting following line from my python script I am getting error messages mentioned in header:</p>

<pre><code>  model.fit_generator(train_generator,
   steps_per_epoch= len(train_samples),
   validation_data=validation_generator, 
   validation_steps=len(validation_samples),
   epochs=9)
</code></pre>

<p>Detailed error message is as follows:</p>

<pre><code>Using TensorFlow backend. 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally 
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally 
Epoch 1/9 Exception in thread Thread-1: Traceback (most recent call last):   File "" lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()   File "" lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)   File "" lib/python3.5/site-packages/keras/engine/training.py"", line 612, in data_generator_task
    generator_output = next(self._generator) StopIteration

I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), 
 but there must be at least one NUMA node, so returning NUMA node zero 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] 
Found device 0 with properties: name: GRID K520 major: 3 minor: 0 memoryClockRate (GHz) 0.797 pciBusID 0000:00:03.0 Total memory: 3.94GiB Free memory:
3.91GiB 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] 
 Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GRID K520, pci bus id: 0000:00:03.0) 
Traceback (most recent call last):   File ""model_new.py"", line 82, in &lt;module&gt;
    model.fit_generator(train_generator, steps_per_epoch= len(train_samples),validation_data=validation_generator, validation_steps=len(validation_samples),epochs=9)   File "" lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)   File "" lib/python3.5/site-packages/keras/models.py"", line 1110, in fit_generator
    initial_epoch=initial_epoch)   File "" lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)   File "" lib/python3.5/site-packages/keras/engine/training.py"", line 1890, in fit_generator
    class_weight=class_weight)   File "" lib/python3.5/site-packages/keras/engine/training.py"", line 1633, in train_on_batch
    outputs = self.train_function(ins)   File "" lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2229, in __call__
    feed_dict=feed_dict)   File "" lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)   File "" lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 937, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)   File "" lib/python3.5/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order) MemoryError
</code></pre>

<p>If any suggestion to resolve this error is appreciated.</p>

<p><strong>EDIT:</strong>
Issue is fatal. </p>

<pre><code>uname -a
Linux ip-172-31-76-109 4.4.0-78-generic #99-Ubuntu SMP
Thu Apr 27 15:29:09 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

sudo lshw -short
[sudo] password for carnd:
H/W path    Device  Class      Description
==========================================
                    system     HVM domU
/0                  bus        Motherboard
/0/0                memory     96KiB BIOS
/0/401              processor  Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz
/0/402              processor  CPU
/0/403              processor  CPU
/0/404              processor  CPU
/0/405              processor  CPU
/0/406              processor  CPU
/0/407              processor  CPU
/0/408              processor  CPU
/0/1000             memory     15GiB System Memory
/0/1000/0           memory     15GiB DIMM RAM
/0/100              bridge     440FX - 82441FX PMC [Natoma]
/0/100/1            bridge     82371SB PIIX3 ISA [Natoma/Triton II]
/0/100/1.1          storage    82371SB PIIX3 IDE [Natoma/Triton II]
/0/100/1.3          bridge     82371AB/EB/MB PIIX4 ACPI
/0/100/2            display    GD 5446
/0/100/3            display    GK104GL [GRID K520]
/0/100/1f           generic    Xen Platform Device
/1          eth0    network    Ethernet interface
</code></pre>

<p><strong>EDIT 2:</strong></p>

<p>This is an EC2 instance in Amazon cloud.  And all the files holding value -1.</p>

<pre><code>:/sys$ find . -name numa_node -exec cat '{}' \;
find: 闂?/fs/fuse/connections/39闂? Permission denied
-1
-1
-1
-1
-1
-1
-1
find: 闂?/kernel/debug闂? Permission denied
</code></pre>

<p><strong>EDIT3:</strong>
After updating the numa_nod files NUMA related error is disappeared.  But all other previous errors listed above is remaining. And again I got a fatal error.</p>

<pre><code>Using TensorFlow backend.
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
Epoch 1/9
Exception in thread Thread-1:
Traceback (most recent call last):
  File "" lib/python3.5/threading.py"", line 914, in _bootstrap_inner
    self.run()
  File "" lib/python3.5/threading.py"", line 862, in run
    self._target(*self._args, **self._kwargs)
  File "" lib/python3.5/site-packages/keras/engine/training.py"", line 612, in data_generator_task
    generator_output = next(self._generator)
StopIteration

I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 3.94GiB
Free memory: 3.91GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
Traceback (most recent call last):
  File ""model_new.py"", line 85, in &lt;module&gt;
    model.fit_generator(train_generator, steps_per_epoch= len(train_samples),validation_data=validation_generator, validation_steps=len(validation_samples),epochs=9)
  File "" lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File "" lib/python3.5/site-packages/keras/models.py"", line 1110, in fit_generator
    initial_epoch=initial_epoch)
  File "" lib/python3.5/site-packages/keras/legacy/interfaces.py"", line 88, in wrapper
    return func(*args, **kwargs)
  File "" lib/python3.5/site-packages/keras/engine/training.py"", line 1890, in fit_generator
    class_weight=class_weight)
  File "" lib/python3.5/site-packages/keras/engine/training.py"", line 1633, in train_on_batch
    outputs = self.train_function(ins)
  File "" lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2229, in __call__
    feed_dict=feed_dict)
  File "" lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File "" lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 937, in _run
    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)
  File "" lib/python3.5/site-packages/numpy/core/numeric.py"", line 531, in asarray
    return array(a, dtype, copy=False, order=order)
MemoryError
</code></pre>",,1,7,,2017/5/28 23:16,4.0,2019/5/12 19:35,2017/5/29 4:29,,681865.0,,1144157.0,,1,24,python-3.x|tensorflow|deep-learning,28614,61.6263,,1,memoryerror tensorflow successful numa node read sysfs negative value xen use tensor flow version cuda tool set version document download instal cudnn execeting follow line python script get error message mention header detail error message follow suggestion resolve error appreciate edit issue fatal edit ec instance amazon cloud file hold value edit update numa nod file numa related error disappear previous error list remain get fatal error
639,639,65082243,"dropout(): argument 'input' (position 1) must be Tensor, not str when using Bert with Huggingface","<p>My code was working fine and when I tried to run it today without changing anything I got the following error:</p>
<p>dropout(): argument 'input' (position 1) must be Tensor, not str</p>
<p>Would appreciate if help could be provided. Could be an issue with the data loader?</p>",,3,1,,2020/11/30 22:45,6.0,2021/7/11 22:11,,,,,14738280.0,,1,17,pytorch|bert-language-model,8117,61.8376,,4,dropout argument input position must tensor str use bert huggingface code work fine try run today without change anything get following error dropout argument input position must tensor str would appreciate help could provide could issue data loader
797,797,52673610,Printing all the contents of a tensor,"<p>I came across <a href=""https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py"" rel=""noreferrer"">this</a> PyTorch tutorial (in neural_networks_tutorial.py) where they construct a simple neural network and run an inference. I would like to print the contents of the entire input tensor for debugging purposes. What I get when I try to print the tensor is something like this and not the entire tensor:</p>

<p><a href=""https://i.stack.imgur.com/zsLP1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zsLP1.png"" alt=""enter image description here""></a></p>

<p>I saw a similar <a href=""https://stackoverflow.com/questions/1987694/how-to-print-the-full-numpy-array"">link</a> for numpy but was not sure about what would work for PyTorch. I can convert it to numpy and may be view it, but would like to avoid the extra overhead. Is there a way for me to print the entire tensor?</p>",52675560.0,3,0,,2018/10/5 21:41,7.0,2021/6/22 7:56,,,,,5157633.0,,1,23,python|debugging|machine-learning|pytorch,17643,63.7863,,3,print content tensor come across pytorch tutorial neural network tutorial py construct simple neural network run inference would like print content entire input tensor debug purpose get try print tensor something like entire tensor saw similar link numpy sure would work pytorch convert numpy may view would like avoid extra overhead way print entire tensor
638,638,51136581,How to create a normal distribution in pytorch,"<p>I want to create a random normal distribution in pytorch and mean and std are 4, 0.5 respectively. I didn't find a API for it. Anyone knows? Thanks very much.</p>",,6,1,,2018/7/2 12:46,3.0,2020/7/16 17:12,2019/5/9 11:01,,2956066.0,,9242759.0,,1,19,python|statistics|pytorch|linear-algebra|normal-distribution,41335,76.2653,,3,create normal distribution pytorch want create random normal distribution pytorch mean std respectively find api anyone know thanks much
795,795,52582275,tf.data with multiple inputs / outputs in Keras,"<p>For the application, such as <strong>pair text similarity</strong>, the input data is similar to: <code>pair_1, pair_2</code>. In these problems, we usually have multiple input data. Previously, I implemented my models successfully:</p>

<pre><code>model.fit([pair_1, pair_2], labels, epochs=50)
</code></pre>

<p>I decided to replace my input pipeline with <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""noreferrer"">tf.data</a> API. To this end, I create a Dataset similar to:</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices((pair_1, pair2, labels))
</code></pre>

<p>It compiles successfully but when start to train it throws the following exception:</p>

<pre><code>AttributeError: 'tuple' object has no attribute 'ndim'
</code></pre>

<p>My Keras and Tensorflow version respectively are <code>2.1.6</code> and <code>1.11.0</code>. I found a similar issue in Tensorflow repository:
<a href=""https://github.com/tensorflow/tensorflow/issues/20698"" rel=""noreferrer"">tf.keras multi-input models don't work when using tf.data.Dataset</a>.</p>

<p>Does anyone know how to fix the issue?</p>

<p><strong>Here is some main part of the code</strong>:</p>

<pre class=""lang-py prettyprint-override""><code>(q1_test, q2_test, label_test) = test
(q1_train, q2_train, label_train) = train

    def tfdata_generator(sent1, sent2, labels, is_training):
        '''Construct a data generator using tf.Dataset'''

        dataset = tf.data.Dataset.from_tensor_slices((sent1, sent2, labels))
        if is_training:
            dataset = dataset.shuffle(1000)  # depends on sample size

        dataset = dataset.repeat()
        dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)

        return dataset

train_dataset = tfdata_generator(q1_train, q2_train, label_train, is_training=True, batch_size=_BATCH_SIZE)
test_dataset = tfdata_generator(q1_test, q2_test, label_test, is_training=False, batch_size=_BATCH_SIZE)


inps1 = keras.layers.Input(shape=(50,))
inps2 = keras.layers.Input(shape=(50,))

embed = keras.layers.Embedding(input_dim=nb_vocab, output_dim=300, weights=[embedding], trainable=False)
embed1 = embed(inps1)
embed2 = embed(inps2)

gru = keras.layers.CuDNNGRU(256)
gru1 = gru(embed1)
gru2 = gru(embed2)

concat = keras.layers.concatenate([gru1, gru2])

preds = keras.layers.Dense(1, 'sigmoid')(concat)

model = keras.models.Model(inputs=[inps1, inps2], outputs=preds)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
print(model.summary())

model.fit(
    train_dataset.make_one_shot_iterator(),
    steps_per_epoch=len(q1_train) // _BATCH_SIZE,
    epochs=50,
    validation_data=test_dataset.make_one_shot_iterator(),
    validation_steps=len(q1_test) // _BATCH_SIZE,
    verbose=1)
</code></pre>",52661189.0,2,5,,2018/9/30 21:19,11.0,2020/3/15 22:21,2018/10/7 15:22,,1462770.0,,1462770.0,,1,51,tensorflow|keras|tensorflow-datasets,25330,111.015,,4,tf data multiple input output kera application pair text similarity input data similar problem usually multiple input data previously implement model successfully decide replace input pipeline tf data api end create dataset similar compile successfully start train throw following exception kera tensorflow version respectively find similar issue tensorflow repository tf kera multi input model work use tf data dataset anyone know fix issue main part code
217,217,44054082,keras.utils.to_categorical() - name keras not defined,"<p>I am running the test script from the <a href=""https://keras.io/getting-started/sequential-model-guide/"" rel=""noreferrer"">Keras website</a> for Multilayer Perceptron (MLP) for multi-class softmax classification.  Running in the jupyter notebook I get the error ""name 'keras' is not defined"".  This may be a simple python syntax problem that I am not keen to, however this code comes straight from keras so I expect it should work as is.  I have run other neural nets using keras, so I am pretty sure that I have installed everything (installed keras using anaconda).  Can anyone help?  I have included both the code and the error at the bottom.  Thanks!</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

# Generate dummy data
import numpy as np
x_train = np.random.random((1000, 20))
y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)
x_test = np.random.random((100, 20))
y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)

model = Sequential()
# Dense(64) is a fully-connected layer with 64 hidden units.
# in the first layer, you must specify the expected input data shape:
# here, 20-dimensional vectors.
model.add(Dense(64, activation='relu', input_dim=20))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd,
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs=20,
          batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=128)
</code></pre>

<p>This is the error message:</p>

<pre><code>NameError                                 Traceback (most recent call last)
&lt;ipython-input-1-6d8174e3cf2a&gt; in &lt;module&gt;()
      6 import numpy as np
      7 x_train = np.random.random((1000, 20))
----&gt; 8 y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)
      9 x_test = np.random.random((100, 20))
     10 y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)

NameError: name 'keras' is not defined
</code></pre>",44058174.0,4,2,,2017/5/18 17:13,2.0,2021/7/21 11:51,2017/5/18 17:19,,6846429.0,,6846429.0,,1,8,python|keras,48679,54.1494,,1,kera utils categorical name kera define run test script kera website multilayer perceptron mlp multi class softmax classification run jupyter notebook get error name kera define may simple python syntax problem keen however code come straight kera expect work run neural net use kera pretty sure instal everything instal kera use anaconda anyone help include code error bottom thanks error message
57,57,55324762,The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.engine.input_layer.InputLayer>,"<p>I am new to machine learning. I was following this <a href=""https://www.youtube.com/watch?v=oDHpqu52soI&amp;t=2s"" rel=""nofollow noreferrer"">tutorial</a> on fine-tuning VGG16 models.</p>
<p>The model loaded fine with this code:</p>
<pre><code>vgg_model = tensorflow.keras.applications.vgg16.VGG16()
</code></pre>
<p>but gets this ERROR:</p>
<pre><code>TypeError: The added layer must be an instance of class Layer. Found: &lt;tensorflow.python.keras.engine.input_layer.InputLayer object at 0x000001FA104CBB70&gt;
</code></pre>
<p>When running this code:</p>
<pre><code>model = Sequential()
for layer in vgg_model.layers[:-1]:
    model.add(layer)
</code></pre>
<p>Dependencies:</p>
<ul>
<li>Keras 2.2.3</li>
<li>Tensorflow 1.12.0</li>
<li>tensorflow-gpu1.12.0</li>
<li>Python 3.6.0</li>
</ul>
<p>I am following this <a href=""https://github.com/vbookshelf/Skin-Lesion-Analyzer/blob/master/skin-lesion-analyzer-jupyter-notebook%20version%202.ipynb"" rel=""nofollow noreferrer"">blog</a> but instead, I want to use VGG16.</p>
<p>Any help to fix this would be appreciated. Thank you so much.</p>",55325491.0,3,1,,2019/3/24 14:21,4.0,2021/4/23 9:41,2020/11/13 14:58,,6414102.0,,11037865.0,,1,24,python|tensorflow|keras|transfer-learning|vgg-net,50638,80.2179,,3,added layer must instance class layer find new machine learn follow tutorial fine tune vgg model model load fine code get error run code dependencies kera tensorflow tensorflow gpu python follow blog instead want use vgg help fix would appreciate thank much
92,92,41409248,softmax and sigmoid function for the output layer,"<p>In the deep learning implementations related to object detection and semantic segmentation, I have seen the output layers using either sigmoid or softmax. I am not very clear when to use which? It seems to me both of them can support these tasks. Are there any guidelines for this choice?</p>",41409315.0,4,0,,2016/12/31 14:46,6.0,2019/5/26 3:02,,,,,288609.0,,1,10,tensorflow|computer-vision|deep-learning|theano|keras,18480,51.0668,,0,softmax sigmoid function output layer deep learning implementation relate object detection semantic segmentation see output layer use either sigmoid softmax clear use seem support task guideline choice
185,185,43396572,Dimension of shape in conv1D,"<p>I have tried to build a CNN with one layer, but I have some problem with it.
Indeed, the compilator says me that</p>

<blockquote>
  <p>ValueError: Error when checking model input: expected conv1d_1_input
  to have 3 dimensions, but got array with shape (569, 30)</p>
</blockquote>

<p>This is the code</p>

<pre><code>import numpy
from keras.models import Sequential
from keras.layers.convolutional import Conv1D
numpy.random.seed(7)
datasetTraining = numpy.loadtxt(""CancerAdapter.csv"",delimiter="","")
X = datasetTraining[:,1:31]
Y = datasetTraining[:,0]
datasetTesting = numpy.loadtxt(""CancereEvaluation.csv"",delimiter="","")
X_test = datasetTraining[:,1:31]
Y_test = datasetTraining[:,0]
model = Sequential()
model.add(Conv1D(2,2,activation='relu',input_shape=X.shape))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=150, batch_size=5)
scores = model.evaluate(X_test, Y_test)
print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
</code></pre>",43399308.0,5,0,,2017/4/13 15:44,31.0,2020/1/5 19:06,,,,,7027646.0,,1,64,python|keras|text-classification|keras-layer,53460,222.912,,3,dimension shape conv try build cnn one layer problem indeed compilator say valueerror error check model input expect conv input dimension get array shape code
337,337,45149341,ImportError: cannot import name np_utils,"<p>I'm trying to run the following <a href=""https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py"" rel=""noreferrer"">example</a> from keras</p>

<p>but I get this error:</p>

<pre><code>ImportError
Traceback (most recent call last)
&lt;ipython-input-58-50de27eea0f8&gt; in &lt;module&gt;()   
      8 import numpy as np  
      9 import matplotlib.pyplot as plt  
---&gt; 10 from keras.models import Sequential  
     11 from keras.layers import Dense, LSTM  
     12   

/usr/local/lib/python2.7/dist-packages/keras/__init__.py in &lt;module&gt;()  
      1 from __future__ import absolute_import  
      2   
----&gt; 3 from . import utils  
      4 from . import activations  
      5 from . import applications  

/usr/local/lib/python2.7/dist-packages/keras/utils/__init__.py in &lt;module&gt;()  
      1 from __future__ import absolute_import  
----&gt; 2 from . import np_utils  
      3 from . import generic_utils  
      4 from . import data_utils  
      5 from . import io_utils  

ImportError: cannot import name np_utils  
</code></pre>

<p>I'm using Ubuntu and I installed keras with:</p>

<pre><code>sudo pip install keras 
</code></pre>

<p>This question was already asked but there was no answer:
<a href=""https://stackoverflow.com/questions/45060150/keras-cannot-import-name-np-utils"">Keras: Cannot Import Name np_utils</a> </p>",,15,2,,2017/7/17 16:32,6.0,2021/5/26 3:10,2017/7/18 0:06,,7976758.0,,8320781.0,,1,38,python|keras,90932,218.835,,1,importerror import name np utils try run following example kera get error use ubuntu instal kera question already ask answer kera import name np utils
12,12,62044838,using cuDNN kernel for LSTM,"<p>I want to train my RNN model using Cudnn:</p>

<pre><code>max_length &lt;- 140 
embedding_dim &lt;- 128

model &lt;- keras_model_sequential()

# define model
model %&gt;% 
  # layer input
  layer_embedding(
    name = ""input"",
    input_dim = num_words,
    input_length = max_length,
    output_dim = embedding_dim, 
    embeddings_initializer = initializer_random_uniform(minval = -0.05, maxval = 0.05, seed = 2)
  ) %&gt;%
  # layer dropout
  layer_spatial_dropout_1d(
    name = ""embedding_dropout"",
    rate = 0.2
  ) %&gt;%
  # layer lstm 1
  bidirectional(layer_lstm(
    name = ""lstm"",
    units = 64,
    unroll = FALSE,
    dropout = 0.2,
    use_bias = TRUE,
    recurrent_dropout = 0,
    return_sequences = TRUE
  )) %&gt;% 
  layer_batch_normalization() %&gt;%
  # layer output
  layer_dense(
    name = ""output"",
    units = 3,
    activation = ""softmax""
  )
</code></pre>

<p>when I run this I get this warming:</p>

<blockquote>
  <p>WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it
  doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel
  as fallback when running on GPU</p>
</blockquote>

<p>I think I have followed all the <a href=""https://www.tensorflow.org/guide/keras/rnn#performance_optimization_and_cudnn_kernels_in_tensorflow_20"" rel=""noreferrer"">requirements</a>, not sure what I'm missing. </p>

<p>SessionInfo:</p>

<pre><code>R version 4.0.0 (2020-04-24)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 18363)

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] keras_2.3.0.0

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.4.6     lattice_0.20-41  zeallot_0.1.0    rappdirs_0.3.1  
 [5] grid_4.0.0       R6_2.4.1         jsonlite_1.6.1   magrittr_1.5    
 [9] tfruns_1.4       whisker_0.4      Matrix_1.2-18    reticulate_1.15 
[13] generics_0.0.2   tools_4.0.0      xfun_0.14        compiler_4.0.0  
[17] base64enc_0.1-3  tensorflow_2.2.0 knitr_1.28   
</code></pre>",62307678.0,1,0,,2020/5/27 13:54,3.0,2020/11/2 17:16,2020/5/27 14:06,,2475569.0,,2475569.0,,1,19,r|tensorflow|keras,11941,63.1082,,4,use cudnn kernel lstm want train rnn model use cudnn run get warm warn tensorflow layer lstm use cudnn kernel since meet cudnn kernel criterion use generic gpu kernel fallback run gpu think follow requirement sure miss sessioninfo
133,133,42327543,"Adam optimizer goes haywire after 200k batches, training loss grows","<p>I've been seeing a very strange behavior when training a network, where after a couple of 100k iterations (8 to 10 hours) of learning fine, everything breaks and the training loss <em>grows</em>:</p>

<p><a href=""https://i.stack.imgur.com/47T4x.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/47T4x.jpg"" alt=""Loss explodes""></a></p>

<p>The training data itself is randomized and spread across many <code>.tfrecord</code> files containing <code>1000</code> examples each, then shuffled again in the input stage and batched to <code>200</code> examples.</p>

<h3>The background</h3>

<p>I am designing a network that performs four different regression tasks at the same time, e.g. determining the likelihood of an object appearing in the image and simultanously determining its orientation. The network starts with a couple of convolutional layers, some with residual connections, and then branches into the four fully-connected segments.</p>

<p>Since the first regression results in a probability, I'm using cross entropy for the loss, whereas the others use classical L2 distance. However, due to their nature, the probability loss is around the order of <code>0..1</code>, while the orientation losses can be much larger, say <code>0..10</code>. I already normalized both input and output values and use clipping</p>

<pre><code>normalized = tf.clip_by_average_norm(inferred.sin_cos, clip_norm=2.)
</code></pre>

<p>in cases where things can get really bad.</p>

<p>I've been (successfully) using the Adam optimizer to optimize on the tensor containing all distinct losses (rather than <code>reduce_sum</code>ing them), like so:</p>

<pre><code>reg_loss = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))
loss = tf.pack([loss_probability, sin_cos_mse, magnitude_mse, pos_mse, reg_loss])

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                   epsilon=self.params.adam_epsilon)
op_minimize = optimizer.minimize(loss, global_step=global_step)
</code></pre>

<p>In order to display the results in TensorBoard, I then actually do</p>

<pre><code>loss_sum = tf.reduce_sum(loss)
</code></pre>

<p>for a scalar summary.</p>

<p>Adam is set to learning rate <code>1e-4</code> and epsilon <code>1e-4</code> (I see the same behavior with the default value for epislon and it breaks even faster when I keep the learning rate on <code>1e-3</code>). Regularization also has no influence on this one, it does this sort-of consistently at some point.</p>

<p>I should also add that stopping the training and restarting from the last checkpoint - implying that the training input files are shuffled again as well - results in the same behavior. The training always seems to behave similarly at that point.</p>",42420014.0,2,3,,2017/2/19 13:05,20.0,2017/9/18 20:18,2017/9/18 20:18,,195651.0,,195651.0,,1,35,tensorflow|neural-network|deep-learning|conv-neural-network,11640,90.2638,,4,adam optimizer go haywire k batch train loss grows see strange behavior train network couple k iteration hour learn fine everything break training loss grow training data randomize spread across many file contain example shuffle input stage batch examples background design network perform four different regression task time e g determine likelihood object appearing image simultanously determine orientation network start couple convolutional layer residual connection branch four fully connect segment since first regression result probability use cross entropy loss whereas others use classical l distance however due nature probability loss around order orientation loss much large say already normalize input output value use clipping case thing get really bad successfully use adam optimizer optimize tensor contain distinct loss rather ing like order display result tensorboard actually scalar summary adam set learn rate epsilon see behavior default value epislon break even faster keep learning rate regularization also influence one sort consistently point also add stop training restart last checkpoint imply training input file shuffle well result behavior training always seem behave similarly point
111,111,41708572,TensorFlow: questions regarding tf.argmax() and tf.equal(),"<p>I am learning the TensorFlow, building a multilayer_perceptron model. I am looking into some examples like the one at: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb</a></p>

<p>I then have some questions in the code below:</p>

<pre><code>def multilayer_perceptron(x, weights, biases):
    :
    :

pred = multilayer_perceptron(x, weights, biases)
    :
    :

with tf.Session() as sess:
    sess.run(init)
         :
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))

    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print (""Accuracy:"", accuracy.eval({x: X_test, y: y_test_onehot}))
</code></pre>

<p>I am wondering what do <code>tf.argmax(prod,1)</code> and <code>tf.argmax(y,1)</code> mean and return (type and value) exactly? And is <code>correct_prediction</code> a variable instead of real values?</p>

<p>Finally, how do we get the <code>y_test_prediction</code> array (the prediction result when the input data is <code>X_test</code>) from the tf session? Thanks a lot!</p>",,3,0,,2017/1/17 23:00,14.0,2017/4/21 9:06,,,,,3993270.0,,1,36,tensorflow|neural-network|deep-learning,48278,111.335,,3,tensorflow question regard tf argmax tf equal learn tensorflow build multilayer perceptron model look example like one question code wonder mean return type value exactly variable instead real value finally get array prediction result input data tf session thank lot
654,654,37213388,Keras accuracy does not change,"<p>I have a few thousand audio files and I want to classify them using Keras and Theano. So far, I generated a 28x28 spectrograms (bigger is probably better, but I am just trying to get the algorithm work at this point) of each audio file and read the image into a matrix. So in the end I get this big image matrix to feed into the network for image classification.</p>

<p>In a tutorial I found this mnist classification code:</p>

<pre><code>import numpy as np

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense
from keras.utils import np_utils

batch_size = 128
nb_classes = 10
nb_epochs = 2

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype(""float32"")
X_test = X_test.astype(""float32"")
X_train /= 255
X_test /= 255

print(X_train.shape[0], ""train samples"")
print(X_test.shape[0], ""test samples"")

y_train = np_utils.to_categorical(y_train, nb_classes)
y_test =  np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Dense(output_dim = 100, input_dim = 784, activation= ""relu""))
model.add(Dense(output_dim = 200, activation = ""relu""))
model.add(Dense(output_dim = 200, activation = ""relu""))
model.add(Dense(output_dim = nb_classes, activation = ""softmax""))

model.compile(optimizer = ""adam"", loss = ""categorical_crossentropy"")

model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epochs, show_accuracy = True, verbose = 2, validation_data = (X_test, y_test))
score = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 0)
print(""Test score: "", score[0])
print(""Test accuracy: "", score[1])
</code></pre>

<p>This code runs, and I get the result as expected:</p>

<pre><code>(60000L, 'train samples')
(10000L, 'test samples')
Train on 60000 samples, validate on 10000 samples
Epoch 1/2
2s - loss: 0.2988 - acc: 0.9131 - val_loss: 0.1314 - val_acc: 0.9607
Epoch 2/2
2s - loss: 0.1144 - acc: 0.9651 - val_loss: 0.0995 - val_acc: 0.9673
('Test score: ', 0.099454972004890438)
('Test accuracy: ', 0.96730000000000005)
</code></pre>

<p>Up to this point everything runs perfectly, however when I apply the above algorithm to my dataset, accuracy gets stuck.</p>

<p>My code is as follows:</p>

<pre><code>import os

import pandas as pd

from sklearn.cross_validation import train_test_split

from keras.models import Sequential
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Dense, Activation, Dropout, Flatten
from keras.utils import np_utils

import AudioProcessing as ap
import ImageTools as it

batch_size = 128
nb_classes = 2
nb_epoch = 10  


for i in range(20):
    print ""\n""
# Generate spectrograms if necessary
if(len(os.listdir(""./AudioNormalPathalogicClassification/Image"")) &gt; 0):
    print ""Audio files are already processed. Skipping...""
else:
    print ""Generating spectrograms for the audio files...""
    ap.audio_2_image(""./AudioNormalPathalogicClassification/Audio/"",""./AudioNormalPathalogicClassification/Image/"","".wav"","".png"",(28,28))

# Read the result csv
df = pd.read_csv('./AudioNormalPathalogicClassification/Result/result.csv', header = None)

df.columns = [""RegionName"",""IsNormal""]

bool_mapping = {True : 1, False : 0}

nb_classes = 2

for col in df:
    if(col == ""RegionName""):
        a = 3      
    else:
        df[col] = df[col].map(bool_mapping)

y = df.iloc[:,1:].values

y = np_utils.to_categorical(y, nb_classes)

# Load images into memory
print ""Loading images into memory...""
X = it.load_images(""./AudioNormalPathalogicClassification/Image/"","".png"")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

X_train = X_train.reshape(X_train.shape[0], 784)
X_test = X_test.reshape(X_test.shape[0], 784)
X_train = X_train.astype(""float32"")
X_test = X_test.astype(""float32"")
X_train /= 255
X_test /= 255

print(""X_train shape: "" + str(X_train.shape))
print(str(X_train.shape[0]) + "" train samples"")
print(str(X_test.shape[0]) + "" test samples"")

model = Sequential()


model.add(Dense(output_dim = 100, input_dim = 784, activation= ""relu""))
model.add(Dense(output_dim = 200, activation = ""relu""))
model.add(Dense(output_dim = 200, activation = ""relu""))
model.add(Dense(output_dim = nb_classes, activation = ""softmax""))

model.compile(loss = ""categorical_crossentropy"", optimizer = ""adam"")

print model.summary()

model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = nb_epoch, show_accuracy = True, verbose = 1, validation_data = (X_test, y_test))
score = model.evaluate(X_test, y_test, show_accuracy = True, verbose = 1)
print(""Test score: "", score[0])
print(""Test accuracy: "", score[1])
</code></pre>

<p>AudioProcessing.py</p>

<pre><code>import os
import scipy as sp
import scipy.io.wavfile as wav
import matplotlib.pylab as pylab
import Image

def save_spectrogram_scipy(source_filename, destination_filename, size):
    dt = 0.0005
    NFFT = 1024       
    Fs = int(1.0/dt)  
    fs, audio = wav.read(source_filename)
    if(len(audio.shape) &gt;= 2):
        audio = sp.mean(audio, axis = 1)
    fig = pylab.figure()    
    ax = pylab.Axes(fig, [0,0,1,1])    
    ax.set_axis_off()
    fig.add_axes(ax) 
    pylab.specgram(audio, NFFT = NFFT, Fs = Fs, noverlap = 900, cmap=""gray"")
    pylab.savefig(destination_filename)
    img = Image.open(destination_filename).convert(""L"")
    img = img.resize(size)
    img.save(destination_filename)
    pylab.clf()
    del img

def audio_2_image(source_directory, destination_directory, audio_extension, image_extension, size):
    nb_files = len(os.listdir(source_directory));
    count = 0
    for file in os.listdir(source_directory):
        if file.endswith(audio_extension):        
            destinationName = file[:-4]
            save_spectrogram_scipy(source_directory + file, destination_directory + destinationName + image_extension, size)
            count += 1
            print (""Generating spectrogram for files "" + str(count) + "" / "" + str(nb_files) + ""."")
</code></pre>

<p>ImageTools.py</p>

<pre><code>import os
import numpy as np
import matplotlib.image as mpimg
def load_images(source_directory, image_extension):
    image_matrix = []
    nb_files = len(os.listdir(source_directory));
    count = 0
    for file in os.listdir(source_directory):
        if file.endswith(image_extension):
            with open(source_directory + file,""r+b"") as f:
                img = mpimg.imread(f)
                img = img.flatten()                
                image_matrix.append(img)
                del img
                count += 1
                #print (""File "" + str(count) + "" / "" + str(nb_files) + "" loaded."")
    return np.asarray(image_matrix)
</code></pre>

<p>So I run the above code and recieve:</p>

<pre><code>Audio files are already processed. Skipping...
Loading images into memory...
X_train shape: (2394L, 784L)
2394 train samples
1027 test samples
--------------------------------------------------------------------------------
Initial input shape: (None, 784)
--------------------------------------------------------------------------------
Layer (name)                  Output Shape                  Param #
--------------------------------------------------------------------------------
Dense (dense)                 (None, 100)                   78500
Dense (dense)                 (None, 200)                   20200
Dense (dense)                 (None, 200)                   40200
Dense (dense)                 (None, 2)                     402
--------------------------------------------------------------------------------
Total params: 139302
--------------------------------------------------------------------------------
None
Train on 2394 samples, validate on 1027 samples
Epoch 1/10
2394/2394 [==============================] - 0s - loss: 0.6898 - acc: 0.5455 - val_loss: 0.6835 - val_acc: 0.5716
Epoch 2/10
2394/2394 [==============================] - 0s - loss: 0.6879 - acc: 0.5522 - val_loss: 0.6901 - val_acc: 0.5716
Epoch 3/10
2394/2394 [==============================] - 0s - loss: 0.6880 - acc: 0.5522 - val_loss: 0.6842 - val_acc: 0.5716
Epoch 4/10
2394/2394 [==============================] - 0s - loss: 0.6883 - acc: 0.5522 - val_loss: 0.6829 - val_acc: 0.5716
Epoch 5/10
2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716
Epoch 6/10
2394/2394 [==============================] - 0s - loss: 0.6887 - acc: 0.5522 - val_loss: 0.6832 - val_acc: 0.5716
Epoch 7/10
2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6859 - val_acc: 0.5716
Epoch 8/10
2394/2394 [==============================] - 0s - loss: 0.6882 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716
Epoch 9/10
2394/2394 [==============================] - 0s - loss: 0.6885 - acc: 0.5522 - val_loss: 0.6836 - val_acc: 0.5716
Epoch 10/10
2394/2394 [==============================] - 0s - loss: 0.6877 - acc: 0.5522 - val_loss: 0.6849 - val_acc: 0.5716
1027/1027 [==============================] - 0s
('Test score: ', 0.68490593621422047)
('Test accuracy: ', 0.57156767283349563)
</code></pre>

<p>I tried changing the network, adding more epochs, but I always get the same result no matter what. I don't understand why I am getting the same result.</p>

<p>Any help would be appreciated. Thank you.</p>

<p>Edit:
I found a mistake where pixel values were not read correctly. I fixed the ImageTools.py below as:</p>

<pre><code>import os
import numpy as np
from scipy.misc import imread

def load_images(source_directory, image_extension):
    image_matrix = []
    nb_files = len(os.listdir(source_directory));
    count = 0
    for file in os.listdir(source_directory):
        if file.endswith(image_extension):
            with open(source_directory + file,""r+b"") as f:
                img = imread(f)                
                img = img.flatten()                        
                image_matrix.append(img)
                del img
                count += 1
                #print (""File "" + str(count) + "" / "" + str(nb_files) + "" loaded."")
    return np.asarray(image_matrix)
</code></pre>

<p>Now I actually get grayscale pixel values from 0 to 255, so now my dividing it by 255 makes sense. However, I still get the same result.</p>",37213763.0,14,0,,2016/5/13 15:01,23.0,2021/6/14 7:01,2016/5/15 13:11,,2515942.0,,2515942.0,,1,52,python|audio|machine-learning|theano|keras,115105,281.844,,4,kera accuracy change thousand audio file want classify use kera theano far generate x spectrogram big probably well try get algorithm work point audio file read image matrix end get big image matrix fee network image classification tutorial find mnist classification code code run get result expect point everything run perfectly however apply algorithm dataset accuracy get stuck code follow audioprocessing py imagetools py run code recieve try change network add epoch always get result matter understand get result help would appreciate thank edit find mistake pixel value read correctly fix imagetools py actually get grayscale pixel value divide make sense however still get result
697,697,39114832,Tensorflow TypeError: Fetch argument None has invalid type <type 'NoneType'>?,"<p>I'm building a RNN loosely based on <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html"" rel=""noreferrer"" title=""tutorial"">the TensorFlow tutorial</a>.</p>

<p>The relevant parts of my model are as follows:</p>

<pre><code>input_sequence = tf.placeholder(tf.float32, [BATCH_SIZE, TIME_STEPS, PIXEL_COUNT + AUX_INPUTS])
output_actual = tf.placeholder(tf.float32, [BATCH_SIZE, OUTPUT_SIZE])

lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(CELL_SIZE, state_is_tuple=False)
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * CELL_LAYERS, state_is_tuple=False)

initial_state = state = stacked_lstm.zero_state(BATCH_SIZE, tf.float32)
outputs = []

with tf.variable_scope(""LSTM""):
    for step in xrange(TIME_STEPS):
        if step &gt; 0:
            tf.get_variable_scope().reuse_variables()
        cell_output, state = stacked_lstm(input_sequence[:, step, :], state)
        outputs.append(cell_output)

final_state = state
</code></pre>

<p>And the feeding:</p>

<pre><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(output_actual * tf.log(prediction), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(output_actual, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    numpy_state = initial_state.eval()

    for i in xrange(1, ITERATIONS):
        batch = DI.next_batch()

        print i, type(batch[0]), np.array(batch[1]).shape, numpy_state.shape

        if i % LOG_STEP == 0:
            train_accuracy = accuracy.eval(feed_dict={
                initial_state: numpy_state,
                input_sequence: batch[0],
                output_actual: batch[1]
            })

            print ""Iteration "" + str(i) + "" Training Accuracy "" + str(train_accuracy)

        numpy_state, train_step = sess.run([final_state, train_step], feed_dict={
            initial_state: numpy_state,
            input_sequence: batch[0],
            output_actual: batch[1]
            })
</code></pre>

<p>When I run this, I get the following error: </p>

<pre><code>Traceback (most recent call last):
  File ""/home/agupta/Documents/Projects/Image-Recognition-with-LSTM/RNN/feature_tracking/model.py"", line 109, in &lt;module&gt;
    output_actual: batch[1]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 698, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 838, in _run
    fetch_handler = _FetchHandler(self._graph, fetches)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 355, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 181, in for_fetch
    return _ListFetchMapper(fetch)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 288, in __init__
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 178, in for_fetch
    (fetch, type(fetch)))
TypeError: Fetch argument None has invalid type &lt;type 'NoneType'&gt;
</code></pre>

<p>Perhaps the weirdest part is that this error gets thrown the <strong>second</strong> iteration, and the first works completely fine. I'm ripping my hair trying to fix this, so any help would be greatly appreciated.</p>",39115010.0,2,0,,2016/8/24 5:07,2.0,2018/11/21 5:09,2018/8/2 22:14,,3881403.0,,3102725.0,,1,23,python|artificial-intelligence|tensorflow|typeerror|recurrent-neural-network,37642,68.5027,,4,tensorflow typeerror fetch argument none invalid type build rnn loosely base tensorflow tutorial relevant part model follow feeding run get following error perhaps weird part error get thrown second iteration first work completely fine rip hair try fix help would greatly appreciated
264,264,44732217,Why do we need to explicitly call zero_grad()?,<p>Why do we need to explicitly zero the gradients in PyTorch? Why can't gradients be zeroed when <code>loss.backward()</code> is called? What scenario is served by keeping the gradients on the graph and asking the user to explicitly zero the gradients?</p>,44732271.0,4,0,,2017/6/24 2:39,16.0,2019/11/6 17:42,2019/11/6 17:42,,5352399.0,,5352399.0,,1,53,neural-network|deep-learning|pytorch|gradient-descent,16224,130.241,2019/11/6 12:18,3,need explicitly call zero grad need explicitly zero gradient pytorch gradients zero call scenario serve keep gradient graph ask user explicitly zero gradient
179,179,43235531,Convolutional neural network Conv1d input shape,"<p>I am trying to create a CNN to classify data. My Data is X[N_data, N_features]
I want to create a neural net capable of classifying it. My problem is concerning the input shape of a Conv1D for the keras back end. </p>

<p>I want to repeat a filter over.. let say 10 features and then keep the same weights for the next ten features. 
For each data my convolutional layer would create N_features/10 New neurones.
How can i do so? What should I put in input_shape?  </p>

<pre><code>def cnn_model():
model = Sequential()                                               
model.add(Conv1D(filters=1, kernel_size=10 ,strides=10,     
                  input_shape=(1, 1,N_features),kernel_initializer= 'uniform',      
                  activation= 'relu')) 
model.flatten()
model.add(Dense(N_features/10, init= 'uniform' , activation= 'relu' ))
</code></pre>

<p>Any advice?
thank you!</p>",43236878.0,3,1,,2017/4/5 15:26,11.0,2018/10/19 17:32,2017/7/12 8:35,,5974433.0,,7821557.0,,1,19,input|machine-learning|neural-network|keras|conv-neural-network,29231,60.2634,,0,convolutional neural network conv input shape try create cnn classify data data x n data n feature want create neural net capable classify problem concern input shape conv kera back end want repeat filter let say feature keep weight next ten feature data convolutional layer would create n feature new neurones put input shape advice thank
269,269,44790670,Torch sum a tensor along an axis,"<pre><code>ipdb&gt; outputs.size()
torch.Size([10, 100])
ipdb&gt; print sum(outputs,0).size(),sum(outputs,1).size(),sum(outputs,2).size()
(100L,) (100L,) (100L,)
</code></pre>

<p>How do I sum over the columns instead?</p>",44806271.0,3,0,,2017/6/27 22:02,8.0,2019/7/15 13:36,2019/4/14 0:20,,2956066.0,,3646408.0,,1,41,python|sum|pytorch|torch|tensor,88033,138.379,,3,torch sum tensor along axis sum column instead
93,93,41414489,How to uninstall Keras?,"<p>I have installed Keras using this command:</p>

<pre><code>sudo pip install keras
</code></pre>

<p>It installed properly and worked fine until I tried to import application modules:</p>

<pre><code>from keras.applications.vgg16 import VGG16
Using Theano backend.
Couldn't import dot_parser, loading of dot files will not be possible.
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named applications.vgg16
</code></pre>

<p>I came across <a href=""https://groups.google.com/forum/#!topic/keras-users/3OpfQHzCk64"" rel=""noreferrer"">this link which recommends</a> to uninstall Keras and directly install Keras from GitHub:</p>

<pre><code>sudo pip install git+https://github.com/fchollet/keras.git
</code></pre>

<p>Before reinstalling Keras from GitHub, I tried to unistall Keras using this command but it throws this error:</p>

<pre><code>sudo pip uninstall keras
Can't uninstall 'Keras'. No files were found to uninstall.
</code></pre>",,12,4,,2017/1/1 9:23,4.0,2021/6/9 3:06,2017/9/27 6:31,,7117003.0,,996366.0,,1,21,python|theano|keras,51466,101.246,,1,uninstall kera instal kera use command instal properly work fine try import application module come across link recommend uninstall kera directly install kera github reinstall kera github try unistall kera use command throw error
626,626,50895110,What do I need K.clear_session() and del model for (Keras with Tensorflow-gpu)?,"<p><strong><em>What I am doing</em></strong><br>
I am training and using a convolutional neuron network (CNN) for image-classification using Keras with Tensorflow-gpu as backend.</p>

<p><strong><em>What I am using</em></strong><br>
- PyCharm Community 2018.1.2<br>
- both Python 2.7 and 3.5 (but not both at a time)<br>
- Ubuntu 16.04<br>
- Keras 2.2.0<br>
- Tensorflow-GPU 1.8.0 as backend</p>

<p><strong><em>What I want to know</em></strong><br>
In many codes I see people using</p>

<pre><code>from keras import backend as K 

# Do some code, e.g. train and save model

K.clear_session()
</code></pre>

<p>or deleting the model after using it:</p>

<pre><code>del model
</code></pre>

<p>The keras documentation says regarding <code>clear_session</code>: ""Destroys the current TF graph and creates a new one. Useful to avoid clutter from old models / layers."" - <a href=""https://keras.io/backend/"" rel=""noreferrer"">https://keras.io/backend/</a></p>

<p>What is the point of doing that and should I do it as well? When loading or creating a new model my model gets overwritten anyway, so why bother?</p>",,3,4,,2018/6/17 8:49,18.0,2020/8/13 14:18,2018/6/27 9:09,,9697805.0,,9697805.0,,1,51,python|tensorflow|memory-management|keras,49397,98.3748,,4,need k clear session del model kera tensorflow gpu training use convolutional neuron network cnn image classification use kera tensorflow gpu backend use pycharm community python time ubuntu kera tensorflow gpu backend want know many code see people use delete model use kera documentation say regard destroy current tf graph create new one useful avoid clutter old model layer point well load create new model model get overwritten anyway bother
772,772,51801648,How to apply layer-wise learning rate in Pytorch?,"<p>I know that it is possible to freeze single layers in a network for example to train only the last layers of a pre-trained model. What I闂佺偨鍎查悰?looking for is a way to apply certain learning rates to different layers. </p>

<p>So for example a very low learning rate of 0.000001 for the first layer and then increasing the learning rate gradually for each of the following layers. So that the last layer then ends up with a learning rate of 0.01 or so.</p>

<p>Is this possible in pytorch? Any idea how I can archive this?</p>",51802206.0,1,0,,2018/8/11 16:27,10.0,2019/12/20 9:09,,,,,7483494.0,,1,23,python|neural-network|deep-learning|pytorch,11038,62.7716,,3,apply layer wise learn rate pytorch know possible freeze single layer network example train last layer pre train model look way apply certain learn rate different layer example low learn rate first layer increase learning rate gradually following layer last layer end learn rate possible pytorch idea archive
547,547,48377214,"RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)","<p>Im using a Pytorch Unet model to which i am feeding in a image as input and along with that i am feeding the label as the input image mask and traning the dataset on it.
The Unet model i have picked up from somewhere else, and i am using the cross-entropy loss as a loss function but i get this dimension out of range error,</p>
<pre><code>RuntimeError                              
Traceback (most recent call last)
&lt;ipython-input-358-fa0ef49a43ae&gt; in &lt;module&gt;()
     16 for epoch in range(0, num_epochs):
     17     # train for one epoch
---&gt; 18     curr_loss = train(train_loader, model, criterion, epoch, num_epochs)
     19 
     20     # store best loss and save a model checkpoint

&lt;ipython-input-356-1bd6c6c281fb&gt; in train(train_loader, model, criterion, epoch, num_epochs)
     16         # measure loss
     17         print (outputs.size(),labels.size())
---&gt; 18         loss = criterion(outputs, labels)
     19         losses.update(loss.data[0], images.size(0))
     20 

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     _ _call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
--&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

&lt;ipython-input-355-db66abcdb074&gt; in forward(self, logits, targets)
      9         probs_flat = probs.view(-1)
     10         targets_flat = targets.view(-1)
---&gt; 11         return self.crossEntropy_loss(probs_flat, targets_flat)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in     __call__(self, *input, **kwargs)
    323         for hook in self._forward_pre_hooks.values():
    324             hook(self, input)
  --&gt; 325         result = self.forward(*input, **kwargs)
    326         for hook in self._forward_hooks.values():
    327             hook_result = hook(self, input, result)

/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py in f orward(self, input, target)
    599         _assert_no_grad(target)
    600         return F.cross_entropy(input, target, self.weight, self.size_average,
--&gt; 601                                self.ignore_index, self.reduce)
    602 
    603 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     cross_entropy(input, target, weight, size_average, ignore_index, reduce)
   1138         &gt;&gt;&gt; loss.backward()
   1139     &quot;&quot;&quot;
-&gt; 1140     return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)
   1141 
   1142 

/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py in     log_softmax(input, dim, _stacklevel)
    784     if dim is None:
    785         dim = _get_softmax_dim('log_softmax', input.dim(),      _stacklevel)
--&gt; 786     return torch._C._nn.log_softmax(input, dim)
    787 
    788 

RuntimeError: dimension out of range (expected to be in range of [-1, 0], but got 1)
</code></pre>
<p>Part of my code looks like this</p>
<pre><code>class crossEntropy(nn.Module):
    def __init__(self, weight = None, size_average = True):
        super(crossEntropy, self).__init__()
        self.crossEntropy_loss = nn.CrossEntropyLoss(weight, size_average)
        
    def forward(self, logits, targets):
        probs = F.sigmoid(logits)
        probs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        return self.crossEntropy_loss(probs_flat, targets_flat)


class UNet(nn.Module):
    def __init__(self, imsize):
        super(UNet, self).__init__()
        self.imsize = imsize

        self.activation = F.relu
        
        self.pool1 = nn.MaxPool2d(2)
        self.pool2 = nn.MaxPool2d(2)
        self.pool3 = nn.MaxPool2d(2)
        self.pool4 = nn.MaxPool2d(2)
        self.conv_block1_64 = UNetConvBlock(4, 64)
        self.conv_block64_128 = UNetConvBlock(64, 128)
        self.conv_block128_256 = UNetConvBlock(128, 256)
        self.conv_block256_512 = UNetConvBlock(256, 512)
        self.conv_block512_1024 = UNetConvBlock(512, 1024)

        self.up_block1024_512 = UNetUpBlock(1024, 512)
        self.up_block512_256 = UNetUpBlock(512, 256)
        self.up_block256_128 = UNetUpBlock(256, 128)
        self.up_block128_64 = UNetUpBlock(128, 64)

        self.last = nn.Conv2d(64, 2, 1)


    def forward(self, x):
        block1 = self.conv_block1_64(x)
        pool1 = self.pool1(block1)

        block2 = self.conv_block64_128(pool1)
        pool2 = self.pool2(block2)

        block3 = self.conv_block128_256(pool2)
        pool3 = self.pool3(block3)

        block4 = self.conv_block256_512(pool3)
        pool4 = self.pool4(block4)

        block5 = self.conv_block512_1024(pool4)

        up1 = self.up_block1024_512(block5, block4)

        up2 = self.up_block512_256(up1, block3)

        up3 = self.up_block256_128(up2, block2)

        up4 = self.up_block128_64(up3, block1)

        return F.log_softmax(self.last(up4))
</code></pre>",48389451.0,2,0,,2018/1/22 8:18,5.0,2021/8/2 3:36,2021/8/2 3:36,,4561314.0,,8176285.0,,1,14,machine-learning|pytorch,41554,51.0745,,4,runtimeerror dimension range expect range get im use pytorch unet model feed image input along feed label input image mask traning dataset unet model pick somewhere else use cross entropy loss loss function get dimension range error part code look like
506,506,33962226,Common causes of nans during training,"<p>I've noticed that a frequent occurrence during training is <code>NAN</code>s being introduced.</p>

<p>Often times it seems to be introduced by weights in inner-product/fully-connected or convolution layers blowing up.</p>

<p>Is this occurring because the gradient computation is blowing up? Or is it because of weight initialization (if so, why does weight initialization have this effect)? Or is it likely caused by the nature of the input data?</p>

<p>The overarching question here is simply: <strong>What is the most common reason for NANs to occurring during training?</strong> And secondly, what are some methods for combatting this (and why do they work)?</p>",33980220.0,5,2,,2015/11/27 17:23,65.0,2021/3/28 12:56,2016/12/21 6:50,,1714410.0,,4975126.0,,1,97,machine-learning|neural-network|deep-learning|caffe|gradient-descent,49421,271.776,,4,common cause nan training notice frequent occurrence training introduce often time seem introduce weight inner product fully connect convolution layer blow occur gradient computation blow weight initialization weight initialization effect likely cause nature input data overarch question simply common reason nan occur training secondly method combat work
814,814,53580088,Calculate the Output size in Convolution layer,"<p>What will be the output size, if the input to convolution layer of neural network is an image of size 128X128X3 and 40 filters of size 5X5 are applied to it?</p>",,6,0,,2018/12/2 12:09,24.0,2021/8/31 12:24,,,,,9362939.0,,1,53,python|machine-learning|deep-learning|conv-neural-network,76325,195.131,2021/9/30 9:44,0,calculate output size convolution layer output size input convolution layer neural network image size x x filter size x apply
743,743,40910857,How to interpret increase in both loss and accuracy,"<p>I have run deep learning models(CNN's) using tensorflow. Many times during the epoch, i have observed that both loss and accuracy have increased, or both have decreased. My understanding was that both are always inversely related. What could be scenario where both increase or decrease simultaneously.</p>",40911592.0,4,1,,2016/12/1 12:35,12.0,2021/6/9 6:55,,,,,3898714.0,,1,32,tensorflow|deep-learning|loss,21428,113.924,,4,interpret increase loss accuracy run deep learn model cnn use tensorflow many time epoch observe loss accuracy increase decrease understanding always inversely relate could scenario increase decrease simultaneously
146,146,42596057,Keras error : Expected to see 1 array,"<p>I got the following error when I tried to train an MLP model in keras(I am using keras version <code>1.2.2</code>)</p>

<blockquote>
  <p>Error when checking model input: the list of Numpy arrays that you
  are passing to your model is not the size the model expected. Expected
  to see 1 arrays but instead got the following list of 12859 arrays:</p>
</blockquote>

<p>This is the summary of the model</p>

<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
dense_1 (Dense)                  (None, 20)            4020        dense_input_1[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 2)             42          dense_1[0][0]
====================================================================================================
Total params: 4,062
Trainable params: 4,062
Non-trainable params: 0
____________________________________________________________________________________________________
None
</code></pre>

<p>This is the first line of model</p>

<pre><code> model.add(Dense(20, input_shape=(200,), init='lecun_uniform', activation='tanh'))
</code></pre>

<p>For training:</p>

<pre><code>model.fit(X,Y,nb_epoch=100,verbose=1)
</code></pre>

<p>where X is a list of elements and each element in turn is a list of 200 values.</p>

<p>Edit :</p>

<p>I also tried</p>

<pre><code>model.add(Dense(20, input_shape=(12859,200), init='lecun_uniform', activation='tanh'))
</code></pre>

<p>but I am getting the same error</p>",42596310.0,2,5,,2017/3/4 12:31,6.0,2019/3/21 10:08,2017/3/4 17:43,,5974433.0,,6354442.0,,1,29,python|machine-learning|neural-network|deep-learning|keras,34334,69.7429,,4,kera error expect see array get following error try train mlp model kera use kera version error check model input list numpy array pass model size model expect expect see array instead get following list array summary model first line model train x list element element turn list value edit also try get error
262,262,44717100,Pytorch: Convert FloatTensor into DoubleTensor,"<p>I have 2 numpy arrays, which I convert into tensors to use the TensorDataset object.  </p>

<pre><code>import torch.utils.data as data_utils

X = np.zeros((100,30))
Y = np.zeros((100,30))

train = data_utils.TensorDataset(torch.from_numpy(X).double(), torch.from_numpy(Y))
train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)
</code></pre>

<p>when I do:</p>

<pre><code>for batch_idx, (data, target) in enumerate(train_loader):
    data, target = Variable(data), Variable(target)
    optimizer.zero_grad()
    output = model(data)               # error occurs here
</code></pre>

<p>I get the fallowing error:</p>

<blockquote>
  <p>TypeError: addmm_ received an invalid combination of arguments - got (int, int, torch.DoubleTensor, torch.FloatTensor), but expected one of:
   [...]<br>
   * (float beta, float alpha, torch.DoubleTensor mat1, torch.DoubleTensor mat2)
        didn't match because some of the arguments have invalid types: (int, int, torch.DoubleTensor, torch.FloatTensor)<br>
   * (float beta, float alpha, torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2)
        didn't match because some of the arguments have invalid types: (int, int, torch.DoubleTensor, torch.FloatTensor)</p>
</blockquote>

<p>The last error comes from:</p>

<blockquote>
  <p>output.addmm_(0, 1, input, weight.t())</p>
</blockquote>

<p>As you see in my code I tried converting the tensor by using .double() - but this did not work. Why is he casting one array into a FloatTensor object and the other into a DoubleTensor?
Any ideas?</p>",44719369.0,2,0,,2017/6/23 8:50,4.0,2018/1/2 13:24,2017/6/23 10:16,,2307200.0,,2307200.0,,1,15,python|torch|pytorch,33093,51.0789,,3,pytorch convert floattensor doubletensor numpy array convert tensor use tensordataset object get fallowing error typeerror addmm receive invalid combination argument get int int torch doubletensor torch floattensor expect one float beta float alpha torch doubletensor mat torch doubletensor mat match argument invalid type int int torch doubletensor torch floattensor float beta float alpha torch sparsedoubletensor mat torch doubletensor mat match argument invalid type int int torch doubletensor torch floattensor last error come output addmm input weight see code try convert tensor use double work cast one array floattensor object doubletensor idea
75,75,3345079,Estimating the number of neurons and number of layers of an artificial neural network,"<p>I am looking for a method on how to calculate the number of layers and the number of neurons per layer. As input I only have the size of the input vector, the size of the output vector and the size of the training set.</p>

<p>Usually the best net is determined by trying different net topologies and selecting the one with the least error. Unfortunately I cannot do that.</p>",3345770.0,3,1,,2010/7/27 15:13,52.0,2019/10/1 7:30,2019/10/1 7:30,,815724.0,,391290.0,,1,76,machine-learning|neural-network|deep-learning|artificial-intelligence,40857,211.045,2018/7/20 17:10,3,estimate number neuron number layer artificial neural network look method calculate number layer number neuron per layer input size input vector size output vector size training set usually best net determine try different net topology select one least error unfortunately
239,239,44461772,Creating one hot vector from indices given as a tensor,"<p>I have a tensor of size <code>4 x 6</code> where 4 is batch size and 6 is sequence length. Every element of the sequence vectors are some index (0 to n). I want to create a <code>4 x 6 x n</code> tensor where the vectors in 3rd dimension will be one hot encoding of the index which means I want to put 1 in the specified index and rest of the values will be zero. </p>

<p>For example, I have the following tensor:</p>

<pre><code>[[5, 3, 2, 11, 15, 15],
[1, 4, 6, 7, 3, 3],
[2, 4, 7, 8, 9, 10],
[11, 12, 15, 2, 5, 7]]
</code></pre>

<p>Here, all the values are in between (0 to n) where n = 15. So, I want to convert the tensor to a <code>4 X 6 X 16</code> tensor where the third dimension will represent one hot encoding vector.</p>

<p>How can I do that using PyTorch functionalities? Right now, I am doing this with loop but I want to avoid looping!</p>",44479006.0,3,0,,2017/6/9 15:38,6.0,2019/5/29 19:28,2017/6/10 20:25,,5352399.0,,5352399.0,,1,17,pytorch,14242,66.8143,,3,create one hot vector index give tensor tensor size batch size sequence length every element sequence vector index n want create tensor vector rd dimension one hot encoding index mean want put specified index rest value zero example following tensor value n n want convert tensor tensor third dimension represent one hot encode vector use pytorch functionality right loop want avoid looping
613,613,50393666,How to understand SpatialDropout1D and when to use it?,"<p>Occasionally I see some models are using <code>SpatialDropout1D</code> instead of <code>Dropout</code>. For example, in the Part of speech tagging neural network, they use:</p>



<pre class=""lang-python prettyprint-override""><code>model = Sequential()
model.add(Embedding(s_vocabsize, EMBED_SIZE,
                    input_length=MAX_SEQLEN))
model.add(SpatialDropout1D(0.2)) ##This
model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))
model.add(RepeatVector(MAX_SEQLEN))
model.add(GRU(HIDDEN_SIZE, return_sequences=True))
model.add(TimeDistributed(Dense(t_vocabsize)))
model.add(Activation(""softmax""))
</code></pre>

<p>According to Keras' documentation, it says:</p>

<blockquote>
  <p>This version performs the same function as Dropout, however it drops
  entire 1D feature maps instead of individual elements.</p>
</blockquote>

<p>However, I am unable to understand the meaning of <strong>entrie 1D feature</strong>. More specifically, I am unable to visualize <code>SpatialDropout1D</code> in the same model explained in <a href=""https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network"" rel=""noreferrer"">quora</a>.
Can someone explain this concept by using the same model as in quora?</p>

<p>Also, under what situation we will use <code>SpatialDropout1D</code> instead of <code>Dropout</code>?</p>",50396580.0,2,0,,2018/5/17 14:11,14.0,2021/5/28 12:10,2018/5/17 20:31,,712995.0,,9793316.0,,1,35,machine-learning|keras|deep-learning|conv-neural-network|dropout,15394,97.7494,,3,understand spatialdropout use occasionally see model use instead example part speech tag neural network use accord keras documentation say version perform function dropout however drop entire feature map instead individual element however unable understand meaning entrie feature specifically unable visualize model explain quora someone explain concept use model quora also situation use instead
623,623,50792316,What does -1 mean in pytorch view?,"<p>As the question says, what does <code>-1</code> do in pytorch <code>view</code>?</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; a = torch.arange(1, 17)
&gt;&gt;&gt; a
tensor([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
         11.,  12.,  13.,  14.,  15.,  16.])

&gt;&gt;&gt; a.view(1,-1)
tensor([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,
          11.,  12.,  13.,  14.,  15.,  16.]])

&gt;&gt;&gt; a.view(-1,1)
tensor([[  1.],
        [  2.],
        [  3.],
        [  4.],
        [  5.],
        [  6.],
        [  7.],
        [  8.],
        [  9.],
        [ 10.],
        [ 11.],
        [ 12.],
        [ 13.],
        [ 14.],
        [ 15.],
        [ 16.]])
</code></pre>
<p>Does it (<code>-1</code>) generate additional dimension?
Does it behave the same as numpy <code>reshape</code> <code>-1</code>?</p>",50793899.0,5,1,,2018/6/11 7:21,10.0,2021/6/28 13:29,2021/3/30 9:18,,9067615.0,,3907250.0,,1,25,pytorch|reshape|dimensions,30477,90.9359,,3,mean pytorch view question say pytorch generate additional dimension behave numpy
661,661,37433321,"TensorFlow/TFLearn: ValueError: Cannot feed value of shape (64,) for Tensor u'target/Y:0', which has shape '(?, 10)'","<p>I have been trying to perform regression using <a href=""https://github.com/tflearn/tflearn"" rel=""noreferrer"">tflearn</a> and my own dataset.</p>

<p>Using tflearn I have been trying to implement a convolutional network based off an <a href=""https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_mnist.py"" rel=""noreferrer"">example</a> using the MNIST dataset. Instead of using the MNIST dataset I have tried replacing the training and test data with my own. My data is read in from a csv file and is a different shape to the MNIST data. I have 255 features which represent a 15*15 grid and a target value. In the example I replaced the lines 24-30 with (and included import numpy as np):</p>

<pre><code>#read in train and test csv's where there are 255 features (15*15) and a target
csvTrain = np.genfromtxt('train.csv', delimiter="","")
X = np.array(csvTrain[:, :225]) #225, 15
Y = csvTrain[:,225]

csvTest = np.genfromtxt('test.csv', delimiter="","")
testX = np.array(csvTest[:, :225])
testY = csvTest[:,225]

#reshape features for each instance in to 15*15, targets are just a single number
X = X.reshape([-1,15,15,1])
testX = testX.reshape([-1,15,15,1])

## Building convolutional network
network = input_data(shape=[None, 15, 15, 1], name='input')
</code></pre>

<p>I get the following error:</p>

<blockquote>
  <p>ValueError: Cannot feed value of shape (64,) for Tensor u'target/Y:0',
  which has shape '(?, 10)'</p>
</blockquote>

<p>I have tried various combinations and have seen a <a href=""https://stackoverflow.com/questions/37238653/tensorflow-tflearn-valueerror-cannot-feed-value-of-shape-256-400-400-for-t"">similar question</a> in stackoverflow but have not had success. The example in this page does not work for me and throws a similar error and I do not understand the answer provided or those provided by similar questions.</p>

<p>How do I use my own data?</p>",37435874.0,1,0,,2016/5/25 9:42,7.0,2021/6/16 10:29,2017/5/23 11:47,,-1.0,,728785.0,,1,16,python|tensorflow|deep-learning,22099,52.5775,,4,tensorflow tflearn valueerror fee value shape tensor u target shape try perform regression use tflearn dataset use tflearn try implement convolutional network base example use mnist dataset instead use mnist dataset try replace training test data data read csv file different shape mnist data feature represent grid target value example replace line include import numpy np get following error valueerror fee value shape tensor u target shape try various combination see similar question stackoverflow success example page work throw similar error understand answer provide provide similar question use data
679,679,38189713,What is an Embedding in Keras?,"<p>Keras documentation isn't clear what this actually is. I understand we can use this to compress the input feature space into a smaller one. But how is this done from a neural design perspective? Is it an autoenocder, RBM?</p>",,4,3,,2016/7/4 17:26,21.0,2019/6/10 17:39,,,,user1008537,,,1,102,keras,32515,198.648,,3,embed kera kera documentation clear actually understand use compress input feature space small one neural design perspective autoenocder rbm
298,298,57248777,Backward function in PyTorch,"<p>I have some question about pytorch's backward function I don't think I'm getting the right output :</p>
<pre><code>import numpy as np
import torch
from torch.autograd import Variable
a = Variable(torch.FloatTensor([[1,2,3],[4,5,6]]), requires_grad=True) 
out = a * a
out.backward(a)
print(a.grad)
</code></pre>
<p>the output is</p>
<pre><code>tensor([[ 2.,  8., 18.],
        [32., 50., 72.]])
</code></pre>
<p>maybe it's <code>2*a*a</code></p>
<p>but i think the output suppose to be</p>
<pre><code>tensor([[ 2.,  4., 6.],
        [8., 10., 12.]])
</code></pre>
<p><code>2*a.</code> cause <code>d(x^2)/dx=2x</code></p>",57249287.0,1,0,,2019/7/29 7:13,22.0,2020/9/16 5:28,2020/9/16 5:28,,13328195.0,,11769099.0,,1,19,machine-learning|pytorch|gradient-descent|autograd,13268,56.2912,,1,backward function pytorch question pytorch backward function think get right output output maybe think output suppose cause
636,636,51089334,What is the difference between tf.keras.layers versus tf.layers?,"<p>What is the difference between tf.keras.layers versus tf.layers?<br>
E.g. both of them have Conv2d, do they provide different outputs?<br>
 Is there any benefits if you mix them (something like a tf.keras.layers.Conv2d in one hidden layer and in the next, tf.layers.max_pooling2d)?</p>",54718798.0,3,1,,2018/6/28 18:35,2.0,2019/11/28 17:15,,,,,9262788.0,,1,10,python|tensorflow|keras,7963,57.6043,,3,difference tf kera layer versus tf layer difference tf kera layer versus tf layer e g conv provide different output benefit mix something like tf kera layer conv one hidden layer next tf layer max pool
828,828,47302085,"What is ""metrics"" in Keras?","<p>It is not yet clear for me what <code>metrics</code> are (as given in the code below). What exactly are they evaluating? Why do we need to define them in the <code>model</code>? Why we can have multiple metrics in one model? And more importantly what is the mechanics behind all this? 
Any scientific reference is also appreciated.</p>



<pre class=""lang-python prettyprint-override""><code>model.compile(loss='mean_squared_error',
              optimizer='sgd',
              metrics=['mae', 'acc'])
</code></pre>",47306502.0,5,1,,2017/11/15 7:56,20.0,2021/8/6 6:19,2018/4/19 13:03,,8563649.0,,3705055.0,,1,35,python|machine-learning|neural-network|deep-learning|keras,11295,120.212,,3,metric kera yet clear give code exactly evaluate need define multiple metric one model importantly mechanic behind scientific reference also appreciate
412,412,47435526,What is the meaning of axis=-1 in keras.argmax?,"<p>I am a beginner in Keras and need help to understand <code>keras.argmax(a, axis=-1)</code> and <code>keras.max(a, axis=-1)</code>. What is the meaning of <code>axis=-1</code> when <code>a.shape = (19, 19, 5, 80)</code>? And also what will be the output of <code>keras.argmax(a, axis=-1)</code> and <code>keras.max(a, axis=-1)</code>?</p>",,1,2,,2017/11/22 13:06,40.0,2019/8/13 19:16,2019/8/13 19:16,,2099607.0,,7211427.0,,1,85,keras|axis|argmax,48975,200.76,,3,meaning axis kera argmax beginner kera need help understand meaning also output
492,492,31326015,How to verify CuDNN installation?,"<p>I have searched many places but ALL I get is HOW to install it, not how to verify that it is installed. I can verify my NVIDIA driver is installed, and that CUDA is installed, but I don't know how to verify CuDNN is installed. Help will be much appreciated, thanks!</p>

<p>PS.<br>
This is for a caffe implementation. Currently everything is working without CuDNN enabled.</p>",31349250.0,11,4,,2015/7/9 18:58,69.0,2021/9/4 14:26,2018/5/11 18:27,,1695960.0,,3785114.0,,1,193,cuda|computer-vision|caffe|conv-neural-network|cudnn,437742,957.165,,1,verify cudnn installation search many place get install verify instal verify nvidia driver instal cuda instal know verify cudnn instal help much appreciate thanks p caffe implementation currently everything work without cudnn enable
673,673,37973108,What is the difference between reinforcement learning and deep RL?,"<p>What is the difference between <em>deep</em> reinforcement learning and reinforcement learning? I basically know what reinforcement learning is about, but what does the concrete term <strong>deep</strong> stand for in this context?</p>",37973461.0,2,0,,2016/6/22 16:01,17.0,2019/3/17 2:40,2018/10/31 8:43,,3924118.0,,2916207.0,,1,31,machine-learning|reinforcement-learning|q-learning,13280,101.893,,0,difference reinforcement learning deep rl difference deep reinforcement learning reinforcement learning basically know reinforcement learning concrete term deep stand context
188,188,43451125,"Pytorch, what are the gradient arguments","<p>I am reading through the documentation of PyTorch and found an example where they write </p>

<pre><code>gradients = torch.FloatTensor([0.1, 1.0, 0.0001])
y.backward(gradients)
print(x.grad)
</code></pre>

<p>where x was an initial variable, from which y was constructed (a 3-vector). The question is, what are the 0.1, 1.0 and 0.0001 arguments of the gradients tensor ? The documentation is not very clear on that.</p>",57540363.0,4,0,,2017/4/17 12:04,60.0,2020/7/11 12:51,2019/4/8 16:00,,2956066.0,,5016028.0,,1,125,neural-network|gradient|pytorch|torch|gradient-descent,22063,327.375,,3,pytorch gradient argument read documentation pytorch find example write x initial variable construct vector question argument gradient tensor documentation clear
200,200,43715047,How do I get the weights of a layer in Keras?,"<p>I am using Windows 10, Python 3.5, and tensorflow 1.1.0. I have the following script:</p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.keras.api.keras.backend as K
from tensorflow.contrib.keras.api.keras.layers import Dense

tf.reset_default_graph()
init = tf.global_variables_initializer()
sess =  tf.Session()
K.set_session(sess) # Keras will use this sesssion to initialize all variables

input_x = tf.placeholder(tf.float32, [None, 10], name='input_x')    
dense1 = Dense(10, activation='relu')(input_x)

sess.run(init)

dense1.get_weights()
</code></pre>

<p>I get the error: <code>AttributeError: 'Tensor' object has no attribute 'weights'</code></p>

<p>What am I doing wrong, and how do I get the weights of <code>dense1</code>? I have look at <a href=""https://stackoverflow.com/questions/42053170/keras-how-can-i-get-biasess-weights"">this</a> and <a href=""https://stackoverflow.com/questions/42411891/how-to-extract-bias-weights-in-keras-sequential-model"">this</a> SO post, but I still can't make it work.</p>",43856966.0,4,0,,2017/5/1 6:10,18.0,2021/1/6 10:23,2019/12/7 16:01,,3924118.0,,3747801.0,,1,38,python|tensorflow|deep-learning|keras|keras-layer,76326,182.931,,5,get weight layer kera use window python tensorflow following script get error wrong get weight look post still make work
174,174,43137288,How to determine needed memory of Keras model?,"<p>I am working with Keras 2.0.0 and I'd like to train a deep model with a huge amount of parameters on a GPU. 
Using too large images, I'm running out of memory (OOM). 
Using too low images, the model's accuracy will be worse than possible.
Therefore I'd like to find the biggest possible input size of images that fit to my GPU. 
Is there any functionality calculating the memory (e.g. comparable to <code>model.summary()</code>) given the model and input data?</p>

<p>I appreciate your help.   </p>",,4,5,,2017/3/31 9:32,28.0,2021/8/21 17:50,,,,,7544784.0,,1,46,memory|keras,22271,127.191,,3,determine needed memory kera model work kera like train deep model huge amount parameter gpu use large image run memory oom use low image model accuracy bad possible therefore like find big possible input size image fit gpu functionality calculate memory e g comparable give model input data appreciate help
421,421,47665391,"Keras ValueError: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=5","<p>I have checked all the solutions, but still, I am facing the same error. My training images shape is <code>(26721, 32, 32, 1)</code>, which I believe it is 4 dimension, but I don't know why error shows it is 5 dimension. </p>

<pre><code> model = Sequential()

 model.add(Convolution2D(16, 5, 5, border_mode='same', input_shape= input_shape ))
</code></pre>

<p>So this is how I am defining <code>model.fit_generator</code> </p>

<pre><code>model.fit_generator(train_dataset, train_labels, nb_epoch=epochs, verbose=1,validation_data=(valid_dataset, valid_labels), nb_val_samples=valid_dataset.shape[0],callbacks=model_callbacks)
</code></pre>",47665700.0,6,0,,2017/12/6 1:28,11.0,2021/2/15 17:27,2020/1/3 12:39,,3924118.0,,7732719.0,,1,55,python|tensorflow|deep-learning|keras|conv-neural-network,106940,156.11700000000005,,4,kera valueerror input incompatible layer conv expect ndim find ndim check solution still face error training image shape believe dimension know error show dimension define
422,422,47697622,CNN - Image Resizing VS Padding (keeping aspect ratio or not?),"<p>While usually people tend to simply resize any image into a square while training a CNN (for example resnet takes a 224x224 square image), that looks ugly to me, especially when the aspect ratio  is not around 1.</p>

<p>(In fact that might change ground truth eg the label that an expert might give the distorted image could be different than the original one).</p>

<p>So now I resize the image to,say, 224x160 , keeping the original ratio, and then I pad the image with 0s (paste it into a random location in a totally black 224x224 image).</p>

<p>My approach doesn't seem original to me, and yet I cannot find any information whatsoever about my approach versus the ""usual"" approach.
Funky!</p>

<p>So, which approach is better? Why? (if the answer is data dependent please share your thought regarding when one if preferable over the other.)</p>",49882055.0,2,3,,2017/12/7 14:47,15.0,2020/5/21 8:58,,,,,5889869.0,,1,43,image|machine-learning|neural-network|computer-vision|conv-neural-network,18326,76.2523,,0,cnn image resize v pad keep aspect ratio usually people tend simply resize image square train cnn example resnet take x square image look ugly especially aspect ratio around fact might change ground truth eg label expert might give distorted image could different original one resize image say x keep original ratio pad image paste random location totally black x image approach seem original yet find information whatsoever approach versus usual approach funky approach well answer data dependent please share thought regard one preferable
820,820,67018079,"Error in ""from keras.utils import to_categorical""","<p>I have probem with this code , why ?</p>
<p>the code :</p>
<pre><code>import cv2
import numpy as np
from PIL import Image
import os
import numpy as np
import cv2
import os
import h5py
import dlib
from imutils import face_utils
from keras.models import load_model
import sys
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D,Dropout
from keras.layers import Dense, Activation, Flatten
from keras.utils import to_categorical
from keras import backend as K 
from sklearn.model_selection import train_test_split
from Model import model
from keras import callbacks

# Path for face image database
path = 'dataset'

recognizer = cv2.face.LBPHFaceRecognizer_create()
detector = cv2.CascadeClassifier(&quot;haarcascade_frontalface_default.xml&quot;);


def downsample_image(img):
    img = Image.fromarray(img.astype('uint8'), 'L')
    img = img.resize((32,32), Image.ANTIALIAS)
    return np.array(img)



# function to get the images and label data
def getImagesAndLabels(path):
    
    path = 'dataset'
    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]     
    faceSamples=[]
    ids = []

    for imagePath in imagePaths:
        
        #if there is an error saving any jpegs
        try:
            PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale
        except:
            continue    
        img_numpy = np.array(PIL_img,'uint8')

        id = int(os.path.split(imagePath)[-1].split(&quot;.&quot;)[1])
        faceSamples.append(img_numpy)
        ids.append(id)
    return faceSamples,ids

print (&quot;\n [INFO] Training faces now.&quot;)
faces,ids = getImagesAndLabels(path)

K.clear_session()
n_faces = len(set(ids))
model = model((32,32,1),n_faces)
faces = np.asarray(faces)
faces = np.array([downsample_image(ab) for ab in faces])
ids = np.asarray(ids)
faces = faces[:,:,:,np.newaxis]
print(&quot;Shape of Data: &quot; + str(faces.shape))
print(&quot;Number of unique faces : &quot; + str(n_faces))


ids = to_categorical(ids)

faces = faces.astype('float32')
faces /= 255.

x_train, x_test, y_train, y_test = train_test_split(faces,ids, test_size = 0.2, random_state = 0)

checkpoint = callbacks.ModelCheckpoint('trained_model.h5', monitor='val_acc',
                                           save_best_only=True, save_weights_only=True, verbose=1)
                                    
model.fit(x_train, y_train,
             batch_size=32,
             epochs=10,
             validation_data=(x_test, y_test),
             shuffle=True,callbacks=[checkpoint])
             

# Print the numer of faces trained and end program
print(&quot;enter code here`\n [INFO] &quot; + str(n_faces) + &quot; faces trained. Exiting Program&quot;)
</code></pre>
<hr />
<pre><code>the output:
------------------
File &quot;D:\my hard sam\閻犫晞寮撶粭妤呭闯閳哄啫鐓濆Δ婵婂蔼閹俱倖绋婇崲浼存偉閺勫繐鎯栧☉?閻庢侗鍏涚粭妤冩尙鐎圭姵鎸欏☉鎾垛偓鍕▔閺団剝鐤嗗ù婊劽兼潏鍛偓娑氭偟python\Real-Time-Face-Recognition-Using-CNN-master\Real-Time-Face-Recognition-Using-CNN-master\02_face_training.py&quot;, line 16, in &lt;module&gt;
    from keras.utils import to_categorical
ImportError: cannot import name 'to_categorical' from 'keras.utils' (C:\Users\omar\PycharmProjects\SnakGame\venv\lib\site-packages\keras\utils\__init__.py)
</code></pre>",67018610.0,2,3,,2021/4/9 8:58,3.0,2021/8/6 17:18,2021/4/9 9:01,,9215780.0,,15558831.0,,1,13,python|keras,14631,51.8611,,1,error kera utils import categorical probem code code
191,191,43481490,Keras: class weights (class_weight) for one-hot encoding,"<p>I'd like to use class_weight argument in keras model.fit to handle the imbalanced training data. By looking at some documents, I understood we can pass a dictionary like this:</p>

<pre><code>class_weight = {0 : 1,
    1: 1,
    2: 5}
</code></pre>

<p>(In this example, class-2 will get higher penalty in the loss function.)</p>

<p>The problem is that my network's output has one-hot encoding i.e. class-0 = (1, 0, 0), class-1 = (0, 1, 0), and class-3 = (0, 0, 1).</p>

<p>How can we use the class_weight for one-hot encoded output?</p>

<p>By looking at <a href=""https://github.com/fchollet/keras/blob/d89afdfd82e6e27b850d910890f4a4059ddea331/keras/engine/training.py#L1307"" rel=""noreferrer"">some codes in Keras</a>, it looks like <code>_feed_output_names</code> contain a list of output classes, but in my case, <code>model.output_names</code>/<code>model._feed_output_names</code> returns <code>['dense_1']</code></p>

<p>Related: <a href=""https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"">How to set class weights for imbalanced classes in Keras?</a></p>",43481605.0,4,0,,2017/4/18 20:19,6.0,2018/6/5 8:42,,,,,3109554.0,,1,34,python|keras,26820,99.9138,,3,kera class weight class weight one hot encode like use class weight argument keras model fit handle imbalanced training data look document understood pass dictionary like example class get high penalty loss function problem network output one hot encode e class class class use class weight one hot encode output look code kera look like contain list output class case return relate set class weight imbalanced class kera
32,32,54307225,What's the difference between torch.stack() and torch.cat() functions?,"<p>OpenAI's REINFORCE and actor-critic example for reinforcement learning has the following code:</p>

<p><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""noreferrer"">REINFORCE</a>:</p>

<pre><code>policy_loss = torch.cat(policy_loss).sum()
</code></pre>

<p><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""noreferrer"">actor-critic</a>:</p>

<pre><code>loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()
</code></pre>

<p>One is using <code>torch.cat</code>, the other uses <code>torch.stack</code>.</p>

<p>As far as my understanding goes, <a href=""https://pytorch.org/docs/stable/torch.html"" rel=""noreferrer"">the doc</a> doesn't give any clear distinction between them.</p>

<p><strong>I would be happy to know the differences between the functions.</strong></p>",54307331.0,3,1,,2019/1/22 11:24,16.0,2021/6/8 8:40,2020/10/11 17:26,,913098.0,,913098.0,,1,76,python|machine-learning|deep-learning|pytorch,45187,223.22,,3,difference torch stack torch cat function openai reinforce actor critic example reinforcement learning following code reinforce actor critic one use us far understanding go doc give clear distinction would happy know difference function
442,442,48142181,What's the purpose of keras.backend.function(),"<p>The <a href=""https://keras.io/backend/#function"" rel=""noreferrer"">Keras manual</a> doesn't say too much:</p>

<pre><code>keras.backend.function(inputs, outputs, updates=None)

Instantiates a Keras function.
Arguments
inputs: List of placeholder tensors.
outputs: List of output tensors.
updates: List of update ops.
**kwargs: Passed to tf.Session.run.
Returns
</code></pre>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/backend.py"" rel=""noreferrer"">Tensorflow source code</a>, which is actually quite short, shows that K.function(...) return a Function object which, when called, evaluates the <em>outputs</em> and <em>updates</em> using the <em>inputs</em>. The interesting part is how it handles the updates which I don't follow. Any explanations/examples/pointers to help understanding this K.function(...) is appreciated! Here is the relevant part from <a href=""https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/backend.py"" rel=""noreferrer"">Tensorflow source code</a></p>

<pre><code>class Function(object):
  """"""Runs a computation graph.
  Arguments:
      inputs: Feed placeholders to the computation graph.
      outputs: Output tensors to fetch.
      updates: Additional update ops to be run at function call.
      name: a name to help users identify what this function does.
  """"""

  def __init__(self, inputs, outputs, updates=None, name=None,
               **session_kwargs):
    updates = updates or []
    if not isinstance(inputs, (list, tuple)):
      raise TypeError('`inputs` to a TensorFlow backend function '
                      'should be a list or tuple.')
    if not isinstance(outputs, (list, tuple)):
      raise TypeError('`outputs` of a TensorFlow backend function '
                      'should be a list or tuple.')
    if not isinstance(updates, (list, tuple)):
      raise TypeError('`updates` in a TensorFlow backend function '
                      'should be a list or tuple.')
    self.inputs = list(inputs)
    self.outputs = list(outputs)
    with ops.control_dependencies(self.outputs):
      updates_ops = []
      for update in updates:
        if isinstance(update, tuple):
          p, new_p = update
          updates_ops.append(state_ops.assign(p, new_p))
        else:
          # assumed already an op
          updates_ops.append(update)
      self.updates_op = control_flow_ops.group(*updates_ops)
    self.name = name
    self.session_kwargs = session_kwargs

  def __call__(self, inputs):
    if not isinstance(inputs, (list, tuple)):
      raise TypeError('`inputs` should be a list or tuple.')
    feed_dict = {}
    for tensor, value in zip(self.inputs, inputs):
      if is_sparse(tensor):
        sparse_coo = value.tocoo()
        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),
                                  np.expand_dims(sparse_coo.col, 1)), 1)
        value = (indices, sparse_coo.data, sparse_coo.shape)
      feed_dict[tensor] = value
    session = get_session()
    updated = session.run(
        self.outputs + [self.updates_op],
        feed_dict=feed_dict,
        **self.session_kwargs)
    return updated[:len(self.outputs)]


def function(inputs, outputs, updates=None, **kwargs):
  """"""Instantiates a Keras function.
  Arguments:
      inputs: List of placeholder tensors.
      outputs: List of output tensors.
      updates: List of update ops.
      **kwargs: Passed to `tf.Session.run`.
  Returns:
      Output values as Numpy arrays.
  Raises:
      ValueError: if invalid kwargs are passed in.
  """"""
  if kwargs:
    for key in kwargs:
      if (key not in tf_inspect.getargspec(session_module.Session.run)[0] and
          key not in tf_inspect.getargspec(Function.__init__)[0]):
        msg = ('Invalid argument ""%s"" passed to K.function with Tensorflow '
               'backend') % key
        raise ValueError(msg)
  return Function(inputs, outputs, updates=updates, **kwargs)
</code></pre>",,3,0,,2018/1/7 22:33,7.0,2020/7/25 12:37,2020/1/24 15:49,,3924118.0,,2658138.0,,1,38,tensorflow|keras,17167,75.7388,,3,purpose kera backend function kera manual say much tensorflow source code actually quite short show k function return function object call evaluate output update use input interesting part handle update follow explanation examples pointer help understand k function appreciate relevant part tensorflow source code
707,707,39674713,Neural Network LSTM input shape from dataframe,"<p>I am trying to implement an <a href=""https://keras.io/layers/recurrent/#lstm"" rel=""noreferrer"">LSTM with Keras</a>.</p>

<p>I know that LSTM's in Keras require a 3D tensor with shape <code>(nb_samples, timesteps, input_dim)</code> as an input. However, I am not entirely sure how the input should look like in my case, as I have just one sample of <code>T</code> observations for each input, not multiple samples, i.e. <code>(nb_samples=1, timesteps=T, input_dim=N)</code>. Is it better to split each of my inputs into samples of length <code>T/M</code>? <code>T</code> is around a few million observations for me, so how long should each sample in that case be, i.e., how would I choose <code>M</code>?</p>

<p>Also, am I right in that this tensor should look something like:</p>

<pre><code>[[[a_11, a_12, ..., a_1M], [a_21, a_22, ..., a_2M], ..., [a_N1, a_N2, ..., a_NM]], 
 [[b_11, b_12, ..., b_1M], [b_21, b_22, ..., b_2M], ..., [b_N1, b_N2, ..., b_NM]], 
 ..., 
 [[x_11, x_12, ..., a_1M], [x_21, x_22, ..., x_2M], ..., [x_N1, x_N2, ..., x_NM]]]
</code></pre>

<p>where M and N defined as before and x corresponds to the last sample that I would have obtained from splitting as discussed above? </p>

<p>Finally, given a pandas dataframe with <code>T</code> observations in each column, and <code>N</code> columns, one for each input, how can I create such an input to feed to Keras?</p>",40005797.0,2,2,,2016/9/24 9:21,30.0,2018/5/2 9:48,2018/5/2 9:48,,7483494.0,,2151205.0,,1,40,python|pandas|keras|lstm,25633,91.6352,,3,neural network lstm input shape dataframe try implement lstm kera know lstm kera require tensor shape input however entirely sure input look like case one sample observation input multiple sample e good split input sample length around million observation long sample case e would choose also right tensor look something like n define x corresponds last sample would obtain split discuss finally give pandas dataframe observation column columns one input create input fee keras
95,95,41428868,Image preprocessing in deep learning,"<p>I am experimenting with deep learning on images. I have about ~4000 images from different cameras with different light conditions, image resolutions and view angle. </p>

<p>My question is: <strong>What kind of image preprocessing would be helpful for improving object detection?</strong> (For example: contrast/color normalization, denoising, etc.) </p>",44965153.0,4,6,,2017/1/2 14:44,10.0,2020/3/29 18:02,,,,,1436964.0,,1,20,image-processing|deep-learning|object-detection,19870,67.1928,,0,image preprocessing deep learning experiment deep learning image image different camera different light condition image resolution view angle question kind image preprocessing would helpful improve object detection example contrast color normalization denoising etc
723,723,40310035,How to change Keras backend (where's the json file)?,"<p>I have installed Keras, and wanted to switch the backend to Theano. I checked out <a href=""https://stackoverflow.com/questions/40036748/keras-backend-importerror-cannot-import-name-ctc-ops"">this post</a>, but still have no idea where to put the created json file. Also, below is the error I got when running <code>import keras</code> in Python Shell:</p>

<blockquote>
  <p>Using TensorFlow backend.</p>
  
  <p>Traceback (most recent call last):   File """", line 1, in
  
      import keras   File ""C:\Python27\lib\site-packages\keras__init__.py"", line 2, in 
      from . import backend   File ""C:\Python27\lib\site-packages\keras\backend__init__.py"", line 64, in
  
      from .tensorflow_backend import *   File ""C:\Python27\lib\site-packages\keras\backend\tensorflow_backend.py"",
  line 1, in 
      import tensorflow as tf ImportError: No module named tensorflow</p>
</blockquote>

<p>When running <code>python -c ""import keras; print(keras.__version__)""</code> from Windows command line, I got:</p>

<blockquote>
  <p>Using TensorFlow backend. Traceback (most recent call last):   File
  """", line 1, in    File
  ""C:\Python27\lib\site-packages\keras__init__.py"", line 2, in 
      from . import backend   File ""C:\Python27\lib\site-packages\keras\backend__init__.py"", line 64, in
  
      from .tensorflow_backend import *   File ""C:\Python27\lib\site-packages\keras\backend\tensorflow_backend.py"",
  line 1, in 
      import tensorflow as tf ImportError: No module named tensorflow</p>
</blockquote>

<p>Can someone please help? Thanks!</p>",40310928.0,12,2,,2016/10/28 17:11,6.0,2019/6/20 17:39,2017/5/23 12:18,,-1.0,,4896087.0,,1,35,python|command-line|theano|keras,47557,183.709,,1,change kera backend json file instal kera want switch backend theano check post still idea put created json file also error get run python shell use tensorflow backend traceback recent call last file line import kera file c python lib site package keras init py line import backend file c python lib site package keras backend init py line tensorflow backend import file c python lib site package keras backend tensorflow backend py line import tensorflow tf importerror module name tensorflow run window command line get use tensorflow backend traceback recent call last file line file c python lib site package keras init py line import backend file c python lib site package keras backend init py line tensorflow backend import file c python lib site package keras backend tensorflow backend py line import tensorflow tf importerror module name tensorflow someone please help thanks
203,203,43765381,Where can I find the API documentation of the class Input?,"<p>Where can I find the API documentation of the class <code>keras.layers.Input</code>? I couldn't find it at <a href=""https://keras.io/"" rel=""noreferrer"">https://keras.io/</a>.</p>",43769694.0,2,0,,2017/5/3 16:22,10.0,2019/2/22 20:05,2019/2/22 20:05,,3924118.0,,6521119.0,,1,25,keras,10811,75.1355,,3,find api documentation class input find api documentation class could find
826,826,43490555,how to calculate a net's FLOPs in CNN,"<p>I want to design a convolutional neural network which occupy GPU resource no more than Alexnet.I want to use FLOPs to measure it but I don't know how to calculate it.Is there any tools to do it,please?</p>",43491707.0,4,2,,2017/4/19 8:37,9.0,2020/8/15 12:55,,,,,6799037.0,,1,20,neural-network|deep-learning|caffe|conv-neural-network,20513,56.2481,2020/8/18 4:22,0,calculate net flop cnn want design convolutional neural network occupy gpu resource alexnet want use flop measure know calculate tool please
307,307,57985406,Cannot import name 'tf_utils' when using importing keras,"<p>I'm using Oracle Linux 7.7, and I installed python3.6 using yum (epel repos). Then I install tensorflow 1.5(since if it goes newer ver I got core dumped) and keras. If I'm importing tensorflow, I got nothing.
But when I import keras, I got </p>

<pre><code>ImportError: cannot import name 'tf_utils'
</code></pre>

<p>Here's the full output:</p>

<pre><code>$ python
Python 3.6.8 (default, Aug  7 2019, 08:02:28) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-39.0.1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow
&gt;&gt;&gt; import keras
Using TensorFlow backend.
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/__init__.py"", line 3, in &lt;module&gt;
from . import utils
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/utils/__init__.py"", line 6, in &lt;module&gt;
from . import conv_utils
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/utils/conv_utils.py"", line 9, in &lt;module&gt;
from .. import backend as K
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/__init__.py"", line 1, in &lt;module&gt;
from .load_backend import epsilon
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/load_backend.py"", line 90, in &lt;module&gt;
from .tensorflow_backend import *
  File ""/home/reyhan/project/.virtualenvs/keras_tf/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 13, in &lt;module&gt;
from tensorflow.python.keras.utils import tf_utils
ImportError: cannot import name 'tf_utils'
</code></pre>

<p>I was using python 3.6 by building it from source before and keras worked fine but since I can't install tkinter for pyplot I uninstall it and using the one from yum instead.</p>",57985905.0,2,0,,2019/9/18 5:05,3.0,2020/6/16 8:40,,,,,11332292.0,,1,21,python|tensorflow|keras|oraclelinux,35959,96.6232,,1,import name tf utils use import kera use oracle linux instal python use yum epel repos install tensorflow since go new ver get core dumped keras import tensorflow get nothing import kera get full output use python build source keras worked fine since install tkinter pyplot uninstall use one yum instead
162,162,42821330,Restore original text from Keras闂佺偨鍎查悰?imdb dataset,"<p>Restore original text from Keras闂佺偨鍎查悰?imdb dataset</p>

<p>I want to restore imdb闂佺偨鍎查悰?original text from Keras闂佺偨鍎查悰?imdb dataset.</p>

<p>First, when I load Keras闂佺偨鍎查悰?imdb dataset, it returned sequence of word index.</p>

<p>

<pre><code>&gt;&gt;&gt; (X_train, y_train), (X_test, y_test) = imdb.load_data()
&gt;&gt;&gt; X_train[0]
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
</code></pre>

<p>I found imdb.get_word_index method(), it returns word index dictionary like {闂佺偨鍎查々绱僥ate闂? 984, 闂佺偨鍎茶灃ake闂? 94,闂佺偨鍎哄ú? For converting, I create index word dictionary.


<pre><code>&gt;&gt;&gt; word_index = imdb.get_word_index()
&gt;&gt;&gt; index_word = {v:k for k,v in word_index.items()}
</code></pre>

<p>Then, I tried to restore original text like following.</p>

<p>

<pre><code>&gt;&gt;&gt; ' '.join(index_word.get(w) for w in X_train[5])
""the effort still been that usually makes for of finished sucking ended cbc's an because before if just though something know novel female i i slowly lot of above freshened with connect in of script their that out end his deceptively i i""
</code></pre>

<p>I闂佺偨鍎查悰?not good at English, but I know this sentence is something strange.</p>

<p>Why is this happened? How can I restore original text?</p>",44891281.0,9,0,,2017/3/15 21:49,7.0,2021/3/13 22:24,2017/7/12 8:30,,5974433.0,,7233229.0,,1,39,python|machine-learning|neural-network|nlp|keras,13187,142.681,,2,restore original text kera imdb dataset restore original text kera imdb dataset want restore imdb original text kera imdb dataset first load kera imdb dataset return sequence word index find imdb get word index method return word index dictionary like create make convert create index word dictionary try restore original text like follow good english know sentence something strange happened restore original text
700,700,39274472,"error: function ""atomicAdd(double *, double)"" has already been defined","<p>I get this error while trying to compile the caffe derivative <a href=""https://bitbucket.org/aquariusjay/deeplab-public-ver2"" rel=""noreferrer"">DeepLab_v2</a> on Ubuntu 14.04.5 with Cuda 8.0.</p>

<p>Does anyone know how to solve this?</p>

<p>DeepLab_v2 compiles fine on another computer that has Cuda 7.5, but since in my current computer I have a Pascal Titan X, I probably need to use Cuda 8.0.</p>",39287554.0,1,3,,2016/9/1 14:37,2.0,2019/7/30 21:09,,,,,2191652.0,,1,12,cuda|caffe,7795,53.9673,,1,error function atomicadd double double already define get error try compile caffe derivative deeplab v ubuntu cuda anyone know solve deeplab v compiles fine another computer cuda since current computer pascal titan x probably need use cuda
47,47,55041080,How does Pytorch Dataloader handle variable size data?,"<p>I have a dataset that looks like below. That is the first item is the user id followed by the set of items which is clicked by the user. </p>

<pre><code>0   24104   27359   6684
0   24104   27359
1   16742   31529   31485
1   16742   31529
2   6579    19316   13091   7181    6579    19316   13091
2   6579    19316   13091   7181    6579    19316
2   6579    19316   13091   7181    6579    19316   13091   6579
2   6579    19316   13091   7181    6579
4   19577   21608
4   19577   21608
4   19577   21608   18373
5   3541    9529
5   3541    9529
6   6832    19218   14144
6   6832    19218
7   9751    23424   25067   12606   26245   23083   12606
</code></pre>

<p>I define a custom dataset to handle my click log data.  </p>

<pre class=""lang-py prettyprint-override""><code>import torch.utils.data as data
class ClickLogDataset(data.Dataset):
    def __init__(self, data_path):
        self.data_path = data_path
        self.uids = []
        self.streams = []

        with open(self.data_path, 'r') as fdata:
            for row in fdata:
                row = row.strip('\n').split('\t')
                self.uids.append(int(row[0]))
                self.streams.append(list(map(int, row[1:])))

    def __len__(self):
        return len(self.uids)

    def __getitem__(self, idx):
        uid, stream = self.uids[idx], self.streams[idx]
        return uid, stream
</code></pre>

<p>Then I use a DataLoader to retrieve mini batches from the data for training.</p>

<pre class=""lang-py prettyprint-override""><code>from torch.utils.data.dataloader import DataLoader
clicklog_dataset = ClickLogDataset(data_path)
clicklog_data_loader = DataLoader(dataset=clicklog_dataset, batch_size=16)

for uid_batch, stream_batch in stream_data_loader:
    print(uid_batch)
    print(stream_batch)
</code></pre>

<p>The code above returns differently from what I expected, I want <code>stream_batch</code> to be a 2D tensor of type integer of length <code>16</code>. However, what I get is a list of 1D tensor of length 16, and the list has only one element, like below. Why is that ?</p>

<pre><code>#stream_batch
[tensor([24104, 24104, 16742, 16742,  6579,  6579,  6579,  6579, 19577, 19577,
        19577,  3541,  3541,  6832,  6832,  9751])]
</code></pre>",55041401.0,3,1,,2019/3/7 10:08,15.0,2019/7/25 18:07,,,,,10945214.0,,1,22,python|pytorch|tensor|variable-length,13043,57.6615,,2,pytorch dataloader handle variable size data dataset look like first item user id follow set item click user define custom dataset handle click log data use dataloader retrieve mini batch data train code return differently expect want tensor type integer length however get list tensor length list one element like
316,316,58378374,Why does keras model predict slower after compile?,"<p><a href=""https://i.stack.imgur.com/cCXBx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cCXBx.png"" alt=""prediction speed keras""></a></p>

<p>In theory, the prediction should be constant as the weights have a fixed size. How do I get my speed back after compile (without the need to remove optimizer)?</p>

<p>See associated experiment: <a href=""https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true"" rel=""noreferrer"">https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true</a></p>",58385156.0,2,7,,2019/10/14 13:58,8.0,2020/1/15 6:13,2019/11/5 16:24,,10133797.0,,2593810.0,,1,30,python|performance|tensorflow|keras|jupyter-notebook,12703,84.4156,,5,keras model predict slow compile theory prediction constant weight fix size get speed back compile without need remove optimizer see associate experiment
435,435,47902295,What is the use of verbose in Keras while validating the model?,"<p>I'm running the LSTM model for the first time.
Here is my model:</p>

<pre><code>opt = Adam(0.002)
inp = Input(...)
print(inp)
x = Embedding(....)(inp)
x = LSTM(...)(x)
x = BatchNormalization()(x)
pred = Dense(5,activation='softmax')(x)

model = Model(inp,pred)
model.compile(....)

idx = np.random.permutation(X_train.shape[0])
model.fit(X_train[idx], y_train[idx], nb_epoch=1, batch_size=128, verbose=1)
</code></pre>

<p>What is the use of verbose while training the model?</p>",47905435.0,6,0,,2017/12/20 9:07,35.0,2021/6/18 17:29,2018/7/6 17:19,user10043429,,,3681169.0,,1,128,python|deep-learning|keras|verbose,174099,499.5630000000001,,3,use verbose kera validate model run lstm model first time model use verbose train model
689,689,38543850,How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)?,"<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#image-dashboard"" rel=""noreferrer"">Image Dashboard</a> section of the Tensorboard ReadMe says:</p>

<blockquote>
  <p>Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard.</p>
</blockquote>

<p>I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?  </p>",38676842.0,8,0,,2016/7/23 16:15,13.0,2021/7/6 17:36,2020/12/31 12:13,,913098.0,,5587428.0,,1,38,python|tensorflow|matplotlib|pytorch|tensorboard,34154,147.934,,5,display custom image tensorboard e g matplotlib plot image dashboard section tensorboard readme say since image dashboard support arbitrary pngs use embed custom visualization e g matplotlib scatterplots tensorboard see pyplot image could write file read back tensor use tf image summary write tensorboard statement readme suggest direct way documentation example efficiently
546,546,48373845,Loading model with custom loss + keras,"<p>In Keras, if you need to have a custom loss with additional parameters, we can use it like mentioned on <a href=""https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras"">https://datascience.stackexchange.com/questions/25029/custom-loss-function-with-additional-parameter-in-keras</a></p>

<pre><code>def penalized_loss(noise):
    def loss(y_true, y_pred):
        return K.mean(K.square(y_pred - y_true) - K.square(y_true - noise), axis=-1)
    return loss
</code></pre>

<p>The above method works when I am training the model. However, once the model is trained I am having difficulty in loading the model. When I try to use the custom_objects parameter in load_model like below</p>

<pre><code>model = load_model(modelFile, custom_objects={'penalized_loss': penalized_loss} )
</code></pre>

<p>it complains <code>ValueError: Unknown loss function:loss</code></p>

<p>Is there any way to pass in the loss function as one of the custom losses in <code>custom_objects</code> ? From what I can gather, the inner function is not in the namespace during load_model call. Is there any easier way to load the model or use a custom loss with additional parameters</p>",48373959.0,3,0,,2018/1/22 2:15,8.0,2021/7/2 20:49,,,,,9249320.0,,1,28,python|keras,15458,72.5566,,4,load model custom loss kera kera need custom loss additional parameter use like mention method work train model however model train difficulty load model try use custom object parameter load model like complain way pass loss function one custom loss gather inner function namespace load model call easy way load model use custom loss additional parameter
525,525,35330117,How can I run a loop with a tensor as its range? (in tensorflow),"<p>I want to have a for loop that the number of its iterations is depend on a tensor value. For example:</p>

<pre><code>for i in tf.range(input_placeholder[1,1]):
  # do something
</code></pre>

<p>However I get the following error:</p>

<p><strong>""TypeError: 'Tensor' object is not iterable""</strong></p>

<p>What should I do?</p>",,2,0,,2016/2/11 3:10,10.0,2018/2/1 8:50,2018/2/1 8:50,,3990607.0,,5808490.0,,1,25,python|tensorflow|neural-network|deep-learning,32223,50.0327,,3,run loop tensor range tensorflow want loop number iteration depend tensor value example however get following error typeerror tensor object iterable
168,168,42969779,"Keras error ""You must feed a value for placeholder tensor 'bidirectional_1/keras_learning_phase' with dtype bool""","<p>I get the following error for the code snippet below:</p>

<blockquote>
  <p>You must feed a value for placeholder tensor
  'bidirectional_1/keras_learning_phase' with dtype bool</p>
</blockquote>

<p>If I add the dropout layer <code>model.add(Dropout(dropout))</code>, it works. Anyone knows why? The back-end is Tensorflow, Keras 2.0.1</p>

<pre><code>def prep_model1(embedding_layer1, embedding_layer2, dropout=0.5):

    model0 = Sequential()  
    model0.add(embedding_layer1)
    model0.add(Bidirectional(LSTM(128, return_sequences=False, dropout=dropout)))

    model1 = Sequential() 
    model1.add(embedding_layer2)
    model1.add(Bidirectional(LSTM(128, return_sequences=False, dropout=dropout)))

    model = Sequential()
    model.add(Merge([model0, model1], mode='concat', concat_axis=1))
    #model.add(Dropout(dropout))
    model.add(Dense(1, activation='sigmoid'))

    return model
</code></pre>",,1,0,,2017/3/23 7:33,5.0,2019/8/2 20:37,2017/3/23 9:11,,7755418.0,,7755418.0,,1,21,tensorflow|deep-learning|keras|keras-layer,15377,52.9475,,4,kera error must fee value placeholder tensor bidirectional kera learn phase dtype bool get following error code snippet must fee value placeholder tensor bidirectional kera learn phase dtype bool add dropout layer work anyone know back end tensorflow kera
40,40,54682539,I am not able to import resnet from keras.applications module,"<p>I'm unable to import this module</p>

<pre><code>import keras.applications.resnet
</code></pre>

<blockquote>
  <p>ModuleNotFoundError<br>
   in ()
  ----> 1 import keras.applications.resnet</p>
</blockquote>

<p><code>ModuleNotFoundError: No module named 'keras.applications.resnet'</code></p>

<hr>

<p>keras resnet <a href=""https://keras.io/applications/#resnet"" rel=""noreferrer"">link</a></p>",,5,0,,2019/2/14 2:53,,2020/8/4 14:51,2019/2/14 9:32,,4684861.0,,8435699.0,,1,12,tensorflow|keras|resnet,31606,60.9991,,1,able import resnet kera application module unable import module modulenotfounderror import kera application resnet kera resnet link
612,612,50322001,How to save/load a tensorflow hub module to/from a custom path?,"<p>The <code>tensorflow_hub</code> library maintainers has made it every easy for users to download and use the pre-trained tensorflow modules, e.g.:</p>

<pre><code>import tensorflow_hub as hub

embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder/1"")
</code></pre>

<p>But from the <code>sys.stderr</code> it seemed like it was saving the module locally to a temporary directory, i.e.</p>

<blockquote>
  <p>INFO:tensorflow:Using
  /var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules to
  cache modules. INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_0:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_1:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_10:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_11:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_12:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_13:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_14:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_15:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_16:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_2:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_3:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_4:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_5:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_6:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_7:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_8:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8 INFO:tensorflow:Initialize variable
  module/Embeddings_en/sharded_9:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9 INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection
  INFO:tensorflow:Initialize variable
  module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from
  checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias
  INFO:tensorflow:Initialize variable
  module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0
  from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights
  INFO:tensorflow:Initialize variable
  module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias INFO:tensorflow:Initialize
  variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights INFO:tensorflow:Initialize
  variable module/global_step:0 from checkpoint
  b'/var/folders/j6/xczfl75n3sbfwpg4190gpb104vnlxt/T/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step</p>
</blockquote>

<p>After a machine reboot, the module gets deleted and running the <code>hub.Module('...')</code> code again would re-download the module. </p>

<p><strong>Is is possible to save the module to a custom directory and afterwards load from the custom directory?</strong></p>

<p>If it's possible, <strong>how to save/load a tensorflow hub module to/from a custom path?</strong></p>",50327323.0,6,0,,2018/5/14 1:07,4.0,2020/1/10 15:45,,,,,610569.0,,1,10,python|tensorflow|deep-learning|pre-trained-model|tensorflow-hub,13483,59.5191,,3,save load tensorflow hub module custom path library maintainer make every easy user download use pre train tensorflow module e g seem like save module locally temporary directory e info tensorflow use var folder j xczfl n sbfwpg gpb vnlxt tfhub module cache module info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module embeddings en sharded checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable embeddings en sharded info tensorflow initialize variable module encoder en dnn residualhidden weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable encoder en dnn residualhidden weight info tensorflow initialize variable module encoder en dnn residualhidden weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable encoder en dnn residualhidden weight info tensorflow initialize variable module encoder en dnn residualhidden weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable encoder en dnn residualhidden weight info tensorflow initialize variable module encoder en dnn residualhidden projection checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable encoder en dnn residualhidden projection info tensorflow initialize variable module encoder en dnn residualhidden weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable encoder en dnn residualhidden weight info tensorflow initialize variable module share rank answer response encoder tanh layer bias checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer bias info tensorflow initialize variable module share rank answer response encoder tanh layer weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer weight info tensorflow initialize variable module share rank answer response encoder tanh layer bias checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer bias info tensorflow initialize variable module share rank answer response encoder tanh layer weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer weight info tensorflow initialize variable module share rank answer response encoder tanh layer bias checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer bias info tensorflow initialize variable module share rank answer response encoder tanh layer weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable shared rank answer response encoder tanh layer weight info tensorflow initialize variable module snli classifier linearlayer bias checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable snli classifier linearlayer bias info tensorflow initialize variable module snli classifier linearlayer weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable snli classifier linearlayer weight info tensorflow initialize variable module snli classifier tanh layer bias checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable snli classifier tanh layer bias info tensorflow initialize variable module snli classifier tanh layer weight checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable snli classifier tanh layer weight info tensorflow initialize variable module global step checkpoint b var folder j xczfl n sbfwpg gpb vnlxt tfhub module c f ffa cdb f e e e bf variables variable global step machine reboot module get delete run code would download module possible save module custom directory afterwards load custom directory possible save load tensorflow hub module custom path
3,3,61137954,AttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects',"<p>When I import keras, the error above pops up even though it was working fine yesterday.</p>

<p>How do I resolve this error?</p>

<p>I am working on windows 10
my keras version is: 2.2.4
my tensorflow version is: 2.2.0rc2</p>

<p>complete error traceback is seen below as such:</p>

<pre><code>Traceback (most recent call last):

    from keras import models
  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\__init__.py"", line 3, in &lt;module&gt;

    from . import utils
  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\utils\__init__.py"", line 6, in &lt;module&gt;

    from . import conv_utils

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\utils\conv_utils.py"", line 9, in &lt;module&gt;

    from .. import backend as K

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\backend\__init__.py"", line 1, in &lt;module&gt;

    from .load_backend import epsilon

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\backend\load_backend.py"", line 90, in &lt;module&gt;

    from .tensorflow_backend import *

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in &lt;module&gt;

    import tensorflow as tf

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\__init__.py"", line 41, in &lt;module&gt;

    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\__init__.py"", line 84, in &lt;module&gt;

    from tensorflow.python import keras

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\__init__.py"", line 27, in &lt;module&gt;

    from tensorflow.python.keras import models

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\models.py"", line 24, in &lt;module&gt;

    from tensorflow.python.keras import metrics as metrics_module

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\metrics.py"", line 37, in &lt;module&gt;

    from tensorflow.python.keras.engine import base_layer

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 51, in &lt;module&gt;

    from tensorflow.python.keras import initializers

  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\initializers\__init__.py"", line 127, in &lt;module&gt;

    populate_deserializable_objects()
  File ""C:\Users\world\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\initializers\__init__.py"", line 85, in populate_deserializable_objects
    generic_utils.populate_dict_with_module_objects(
AttributeError: module 'tensorflow.python.keras.utils.generic_utils' has no attribute 'populate_dict_with_module_objects'
</code></pre>",,17,3,,2020/4/10 10:01,3.0,2021/8/3 6:55,2020/4/10 13:20,,7836658.0,,7836658.0,,1,27,python|tensorflow|keras,70000,177.18,,1,attributeerror module tensorflow python kera utils generic utils attribute populate dict module object import keras error pop even though work fine yesterday resolve error work window keras version tensorflow version rc complete error traceback see
719,719,40046619,"Keras + tensorflow gives the error ""no attribute 'control_flow_ops'""","<p>I am trying to run keras for the first time.  I installed the modules with:</p>

<pre><code>pip install keras --user
pip install tensorflow --user
</code></pre>

<p>and then tried to run <a href=""https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py"">https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py</a>.</p>

<p>However it gives me:</p>

<pre><code>AttributeError: 'module' object has no attribute 'control_flow_ops'
</code></pre>

<p>These are the versions I am using.</p>

<pre><code>print tensorflow.__version__
0.11.0rc0
print keras.__version__
1.1.0
</code></pre>

<blockquote>
  <p>What can I do to get keras to run with tensorflow?</p>
</blockquote>",40066895.0,3,6,,2016/10/14 15:17,6.0,2019/8/9 6:54,2016/10/14 15:39,,2179021.0,,2179021.0,,1,13,python|ubuntu|machine-learning|tensorflow|keras,22561,53.2134,,1,kera tensorflow give error attribute control flow ops try run kera first time instal module try run however give version use get kera run tensorflow
220,220,44119207,Is there any way to get variable importance with Keras?,<p>I am looking for a proper or best way to get variable importance in a Neural Network created with Keras. The way I currently do it is I just take the weights (not the biases) of the variables in the first layer with the assumption that more important variables will have higher weights in the first layer. Is there another/better way of doing it?</p>,44120068.0,4,0,,2017/5/22 17:54,16.0,2020/10/9 18:29,2017/10/18 14:40,,562769.0,,1367204.0,,1,26,tensorflow|deep-learning|keras|keras-layer|keras-2,24357,80.3465,,3,way get variable importance kera look proper best way get variable importance neural network create kera way currently take weight bias variable first layer assumption important variable high weight first layer another good way
796,796,52656692,Debugging in Google Colab,"<p>I am running the following code snippet in google colab in a single cell:</p>

<pre><code>%debug
# Create tensors of shape (10, 3) and (10, 2).
x = torch.randn(10, 3)
y = torch.randn(10, 2)

# Build a fully connected layer.
linear = nn.Linear(3, 2)
print ('w: ', linear.weight)
print ('b: ', linear.bias)
</code></pre>

<p>I wish to debug a piece of code (step through it line by line) to understand what is going on. I wish to step inside the function nn.Linear. </p>

<p>However, when I step through, it does not enter the function at  all. Is there a way to step through nn.Linear line by line? Also, how exactly do I set a breakpoint in nn.Linear? Besides, I wish to step though the snippet line by line as well. However, as the picture shows, the step command automatically steps through and executes the print statement as well.</p>

<p><a href=""https://i.stack.imgur.com/PSBvV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PSBvV.png"" alt=""Step_though_collab""></a></p>",,3,0,,2018/10/5 0:15,7.0,2021/8/2 16:00,2018/10/5 17:46,,5157633.0,,5157633.0,,1,22,python|deep-learning|pytorch|tensor|google-colaboratory,18980,52.3132,,3,debug google colab run following code snippet google colab single cell wish debug piece code step line line understand go wish step inside function nn linear however step enter function way step nn linear line line also exactly set breakpoint nn linear besides wish step though snippet line line well however picture show step command automatically step execute print statement well
249,249,44560549,Unbalanced data and weighted cross entropy,"<p>I'm trying to train a network with an unbalanced data. I have A (198 samples), B (436 samples), C (710 samples), D (272 samples) and I have read about the &quot;weighted_cross_entropy_with_logits&quot; but all the examples I found are for binary classification so I'm not very confident in how to set those weights.</p>
<p>Total samples: 1616</p>
<p>A_weight: 198/1616 = 0.12?</p>
<p>The idea behind, if I understood, is to penalize the errors of the majority class and value more positively the hits in the minority one, right?</p>
<p>My piece of code:</p>
<pre><code>weights = tf.constant([0.12, 0.26, 0.43, 0.17])
cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=pred, targets=y, pos_weight=weights))
</code></pre>
<p>I have read <a href=""https://stackoverflow.com/questions/40698709/tensorflow-interpretation-of-weight-in-weighted-cross-entropy"">this one</a> and others examples with binary classification but still not very clear.</p>
<p>Thanks in advance.</p>",44563055.0,3,0,,2017/6/15 6:51,38.0,2021/7/5 20:06,2021/7/5 20:06,,10779185.0,,1405024.0,,1,62,python|machine-learning|tensorflow|deep-learning,52410,157.078,,4,unbalanced data weighted cross entropy try train network unbalanced data sample b sample c sample sample read weighted cross entropy logits example find binary classification confident set weight total sample weight idea behind understood penalize error majority class value positively hit minority one right piece code read one others example binary classification still clear thanks advance
322,322,58636087,Tensorflow - ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float),"<p>Continuation from previous question: <a href=""https://stackoverflow.com/questions/58635521/tensorflow-typeerror-int-object-is-not-iterable/58635565#58635583"">Tensorflow - TypeError: &#39;int&#39; object is not iterable</a></p>
<p>My training data is a list of lists each comprised of 1000 floats. For example, <code>x_train[0] =</code></p>
<pre><code>[0.0, 0.0, 0.1, 0.25, 0.5, ...]
</code></pre>
<p>Here is my model:</p>
<pre><code>model = Sequential()

model.add(LSTM(128, activation='relu',
               input_shape=(1000, 1), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))
</code></pre>
<p>Here is the error I'm getting:</p>
<pre><code>Traceback (most recent call last):
      File &quot;C:\Users\bencu\Desktop\ProjectFiles\Code\Program.py&quot;, line 88, in FitModel
        model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py&quot;, line 728, in fit
        use_multiprocessing=use_multiprocessing)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 224, in fit
        distribution_strategy=strategy)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 547, in _process_training_inputs
        use_multiprocessing=use_multiprocessing)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 606, in _process_inputs
        use_multiprocessing=use_multiprocessing)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py&quot;, line 479, in __init__
        batch_size=batch_size, shuffle=shuffle, **kwargs)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py&quot;, line 321, in __init__
        dataset_ops.DatasetV2.from_tensors(inputs).repeat()
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py&quot;, line 414, in from_tensors
        return TensorDataset(tensors)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py&quot;, line 2335, in __init__
        element = structure.normalize_element(element)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\data\util\structure.py&quot;, line 111, in normalize_element
        ops.convert_to_tensor(t, name=&quot;component_%d&quot; % i))
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1184, in convert_to_tensor
        return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1242, in convert_to_tensor_v2
        as_ref=False)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1296, in internal_convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\tensor_conversion_registry.py&quot;, line 52, in _default_conversion_function
        return constant_op.constant(value, dtype, name=name)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py&quot;, line 227, in constant
        allow_broadcast=True)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py&quot;, line 235, in _constant_impl
        t = convert_to_eager_tensor(value, ctx, dtype)
      File &quot;C:\Users\bencu\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\framework\constant_op.py&quot;, line 96, in convert_to_eager_tensor
        return ops.EagerTensor(value, ctx.device_name, dtype)
    ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
</code></pre>
<p>I've tried googling the error myself, I found something about using the <code>tf.convert_to_tensor</code> function. I tried passing my training and testing lists through this but the function won't take them.</p>",58667409.0,10,4,,2019/10/31 2:18,10.0,2021/8/12 17:11,2021/8/12 17:11,,3750257.0,,6748145.0,,1,78,python|tensorflow|keras|lstm,151013,323.716,,4,tensorflow valueerror fail convert numpy array tensor unsupported object type float continuation previous question tensorflow typeerror int object iterable training data list list comprise float example model error get try google error find something use function try pass training testing list function take
622,622,50776598,TypeError: softmax() got an unexpected keyword argument 'axis',"<p>When I use this it does not give any error</p>

<pre><code>out_layer = tf.add(tf.matmul(layer_4 , weights['out']) , biases['out'])
out_layer = tf.nn.softmax(out_layer)
</code></pre>

<p>But when I use this </p>

<pre><code>model=Sequential()

model.add(Dense(100, input_dim= n_dim, 
activation='tanh',kernel_initializer='uniform'))
keras.layers.core.Dropout(0.3, noise_shape=None, seed=None)

model.add(Dense(50,input_dim=1000,activation='sigmoid'))
keras.layers.core.Dropout(0.4, noise_shape=None, seed=None)

model.add(Dense(15,input_dim=500,activation='sigmoid'))
keras.layers.core.Dropout(0.2, noise_shape=None, seed=None)

model.add(Dense(units=n_class))
model.add(Activation('softmax'))
</code></pre>

<p>I get error as</p>

<blockquote>
  <p>TypeError: softmax() got an unexpected keyword argument 'axis'</p>
</blockquote>

<p>What should I do?
I am using python2
Thanks</p>",50920201.0,6,2,,2018/6/9 16:59,,2019/11/9 10:02,,,,,8804766.0,,1,12,python-2.7|keras|softmax,15071,52.1126,,4,typeerror softmax get unexpected keyword argument axis use give error use get error typeerror softmax get unexpected keyword argument axis use python thanks
254,254,44615147,ValueError: Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel,"<p>This it the code:</p>

<pre><code>X = tf.placeholder(tf.float32, [batch_size, seq_len_1, 1], name='X')
labels = tf.placeholder(tf.float32, [None, alpha_size], name='labels')

rnn_cell = tf.contrib.rnn.BasicLSTMCell(512)
m_rnn_cell = tf.contrib.rnn.MultiRNNCell([rnn_cell] * 3, state_is_tuple=True)
pre_prediction, state = tf.nn.dynamic_rnn(m_rnn_cell, X, dtype=tf.float32)
</code></pre>

<p>This is full error:</p>

<blockquote>
  <p><strong>ValueError:</strong> Trying to share variable rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, but specified shape (1024, 2048) and found shape (513, 2048).</p>
</blockquote>

<p>I'm using a GPU version of tensorflow. </p>",44618354.0,4,0,,2017/6/18 12:49,6.0,2020/9/25 1:25,2017/11/8 11:45,,4304503.0,,6288172.0,,1,20,tensorflow|deep-learning|lstm,10853,82.1422,,4,valueerror try share variable rnn multi rnn cell cell basic lstm cell kernel code full error valueerror try share variable rnn multi rnn cell cell basic lstm cell kernel specified shape find shape use gpu version tensorflow
452,452,48295661,How to pickle Keras model?,"<p>Official documents state that ""It is not recommended to use pickle or cPickle to save a Keras model.""</p>

<p>However, my need for pickling Keras model stems from hyperparameter optimization using sklearn's RandomizedSearchCV (or any other hyperparameter optimizers). It's essential to save the results to a file, since then the script can be executed remotely in a detached session etc.</p>

<p>Essentially, I want to:</p>

<pre><code>trial_search = RandomizedSearchCV( estimator=keras_model, ... )
pickle.dump( trial_search, open( ""trial_search.pickle"", ""wb"" ) )
</code></pre>",,4,10,,2018/1/17 7:23,8.0,2019/2/15 8:59,,,,,2202107.0,,1,19,python|machine-learning|keras|pickle,35237,62.38800000000001,,5,pickle kera model official document state recommend use pickle cpickle save kera model however need pickle kera model stem hyperparameter optimization use sklearn randomizedsearchcv hyperparameter optimizers essential save result file since script execute remotely detached session etc essentially want
71,71,55908188,This model has not yet been built error on model.summary(),"<p>I've keras model defined as follow</p>

<pre class=""lang-py prettyprint-override""><code>class ConvLayer(Layer) :
    def __init__(self, nf, ks=3, s=2, **kwargs):
        self.nf = nf
        self.grelu = GeneralReLU(leak=0.01)
        self.conv = (Conv2D(filters     = nf,
                            kernel_size = ks,
                            strides     = s,
                            padding     = ""same"",
                            use_bias    = False,
                            activation  = ""linear""))
        super(ConvLayer, self).__init__(**kwargs)

    def rsub(self): return -self.grelu.sub
    def set_sub(self, v): self.grelu.sub = -v
    def conv_weights(self): return self.conv.weight[0]

    def build(self, input_shape):
        # No weight to train.
        super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end

    def compute_output_shape(self, input_shape):
        output_shape = (input_shape[0],
                        input_shape[1]/2,
                        input_shape[2]/2,
                        self.nf)
        return output_shape

    def call(self, x):
        return self.grelu(self.conv(x))

    def __repr__(self):
        return f'ConvLayer(nf={self.nf}, activation={self.grelu})'
</code></pre>

<pre class=""lang-py prettyprint-override""><code>class ConvModel(tf.keras.Model):
    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):
        super(ConvModel, self).__init__(name='mlp')
        self.use_bn = use_bn
        self.use_dp = use_dp
        self.num_classes = num_classes

        # backbone layers
        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]
        self.convs += [ConvLayer(nf) for nf in nfs[1:]]
        # classification layers
        self.convs.append(AveragePooling2D())
        self.convs.append(Dense(output_shape, activation='softmax'))

    def call(self, inputs):
        for layer in self.convs: inputs = layer(inputs)
        return inputs
</code></pre>

<p>I'm able to compile this model without any issues</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), 
              loss='categorical_crossentropy',
              metrics=['accuracy'])
</code></pre>

<p>But when I query the summary for this model, I see this error</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; model = ConvModel(nfs, input_shape=(32, 32, 3), output_shape=num_classes)
&gt;&gt;&gt; model.summary()
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-220-5f15418b3570&gt; in &lt;module&gt;()
----&gt; 1 model.summary()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in summary(self, line_length, positions, print_fn)
   1575     """"""
   1576     if not self.built:
-&gt; 1577       raise ValueError('This model has not yet been built. '
   1578                        'Build the model first by calling `build()` or calling '
   1579                        '`fit()` with some data, or specify '

ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.
</code></pre>

<p>I'm providing <code>input_shape</code> for the first layer of my model, why is throwing this error?</p>",55909624.0,6,3,,2019/4/29 17:25,7.0,2021/5/28 8:38,2019/4/29 18:57,,1269281.0,,1269281.0,,1,31,python|tensorflow|keras|tensorflow2.0,49679,124.985,,5,model yet build error model summary keras model define follow able compile model without issue query summary model see error provide first layer model throw error
599,599,49918479,How to install pytorch in Anaconda with conda or pip?,"<p>I am trying to install pytorch in Anaconda to work with Python 3.5 in Windows. Following the instructions in <a href=""http://pytorch.org"" rel=""noreferrer"">pytorch.org</a> I introduced the following code in Anaconda:</p>

<pre><code>pip3 install torch torchvision 
</code></pre>

<p>But the following error came in: </p>

<pre><code>Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\sluis\AppData\Local\Temp\pip-install-qmrvz7b9\torch\
</code></pre>

<p>By searching on the web I found out that it may be because of <code>setuptools</code> being out of date but I checked and have it updated. I also tried:</p>

<pre><code>conda install -c peterjc123 pytorch cuda80
</code></pre>

<p>But the following error arise: </p>

<pre><code>The following specifications were found to be in conflict:
  - pytorch
Use ""conda info &lt;package&gt;"" to see the dependencies for each package.
</code></pre>

<p>I also tried to load the pytorch's tar.bz2 file which I download in the following website: </p>

<p><a href=""http://anaconda.org/peterjc123/pytorch/files"" rel=""noreferrer"">anaconda.org/peterjc123/pytorch/files</a> </p>

<p>And then just do: </p>

<pre><code>$ conda install filename.tar.bz2 
</code></pre>

<p>But I got the following error:</p>

<pre><code>Error: HTTPError: 404 Client Error: None for url: file:///C|/Users/sluis/pytorch-0.3.1-py36_cuda80_cudnn6he774522_2.tar.bz2: file:///C|/Users/sluis/pytorch-0.3.1-py36_cuda80_cudnn6he774522_2.tar.bz2
</code></pre>

<p>I am quite new to this programming world so I don't really know how to dig more on the errors. Anyone knows how to get pytorch installed?</p>

<p><strong>Edit:</strong> As suggested in the comments I tried: </p>

<pre><code>conda install pytorch torchivsion -c pytorch
</code></pre>

<p>And I got the following error: </p>

<pre><code>Error: Packages missing in current win-64 channels:
 - pytorch
 - torchvision
</code></pre>

<p>I did: </p>

<pre><code>anaconda search -t conda torchvision
</code></pre>

<p>And tried to install <code>dericlk/torchvision</code> using the following command: </p>

<pre><code>conda install -c derickl torchvision
</code></pre>

<p>But I am getting the same error: </p>

<pre><code>Error: Package missing in current win-64 channels:
  - torchvision
</code></pre>

<p>I couldn't find any <code>torchvision</code>packages for win-64.</p>

<p><code>conda list</code> is giving me the following: </p>

<pre><code># packages in environment at C:\Users\aaaa\AppData\Local\Continuum\Anaconda3\envs\torchenv2:
#
mkl-include               2018.0.2                      1    anaconda
certifi                   2016.2.28                py35_0
cffi                      1.10.0                   py35_0
cmake                     3.6.3                    vc14_0  [vc14]
openmp                    2018.0.0                intel_8    intel
mkl                       2017.0.3                      0
numpy                     1.13.1                   py35_0
pip                       10.0.0                    &lt;pip&gt;
pip                       9.0.1                    py35_1
pycparser                 2.18                     py35_0
python                    3.5.4                         0
pyyaml                    3.12                     py35_0
setuptools                36.4.0                   py35_1
typing                    3.6.2                    py35_0
vc                        14                            0
vs2015_runtime            14.0.25420                    0
wheel                     0.29.0                   py35_0
wincertstore              0.2                      py35_0
zlib                      1.2.11                   vc14_0  [vc14]
</code></pre>

<p>=======</p>",,11,5,,2018/4/19 10:14,3.0,2021/6/17 1:41,2019/4/13 22:37,,9618242.0,,9618242.0,,1,25,python|pip|pytorch,114767,124.239,,1,install pytorch anaconda conda pip try install pytorch anaconda work python window follow instruction pytorch org introduce following code anaconda following error come search web find may date check update also try following error arise also try load pytorch tar bz file download following website anaconda org peterjc pytorch file get following error quite new programming world really know dig error anyone know get pytorch instal edit suggest comment try get following error try install use following command get error could find package win give follow
602,602,49987261,What is the difference between CuDNNLSTM and LSTM in Keras?,"<p>In <code>Keras</code>, the high-level deep learning library, there are multiple types of recurrent layers; these include <code>LSTM</code> (Long short term memory) and <code>CuDNNLSTM</code>. According to the <a href=""https://keras.io/layers/recurrent/#cudnnlstm"" rel=""noreferrer"">Keras documentation</a>, a <code>CuDNNLSTM</code> is a:</p>

<blockquote>
  <p>Fast LSTM implementation backed by CuDNN.
   Can only be run on GPU, with the TensorFlow backend.</p>
</blockquote>

<p>It is my belief that Keras automatically uses the GPU wherever possible. According to the <a href=""https://www.tensorflow.org/install/install_sources"" rel=""noreferrer"">TensorFlow build instructions</a>, to have a working TensorFlow GPU backend, you will need CuDNN:</p>

<blockquote>
  <p>The following NVIDIA software must be installed on your system:</p>
  
  <ul>
  <li>NVIDIA's Cuda Toolkit (>= 7.0). We recommend version 9.0. For details, see NVIDIA's documentation. Ensure that you append the relevant Cuda pathnames to the LD_LIBRARY_PATH environment variable as described in the NVIDIA documentation.</li>
  <li>The NVIDIA drivers associated with NVIDIA's Cuda Toolkit.</li>
  <li><strong>cuDNN</strong> (>= v3). We recommend version 6.0. For details, see NVIDIA's documentation, particularly the description of appending the appropriate pathname to your LD_LIBRARY_PATH environment variable.</li>
  </ul>
</blockquote>

<p>Therefore, how would a <code>CuDNNLSTM</code> differ in any way from a normal <code>LSTM</code> using a TensorFlow GPU backend? Will <code>CuDNNLSTM</code> be automatically selected and replace the normal <code>LSTM</code> when an available TensorFlow GPU backend is found?</p>",,4,2,,2018/4/23 17:58,17.0,2021/3/8 17:42,,,,,4751706.0,,1,48,tensorflow|keras|lstm,29668,105.289,,3,difference cudnnlstm lstm kera high level deep learn library multiple type recurrent layer include long short term memory accord kera documentation fast lstm implementation back cudnn run gpu tensorflow backend belief keras automatically use gpu wherever possible accord tensorflow build instruction working tensorflow gpu backend need cudnn following nvidia software must instal system nvidia cuda toolkit recommend version detail see nvidia documentation ensure append relevant cuda pathnames ld library path environment variable describe nvidia documentation nvidia driver associate nvidia cuda toolkit cudnn v recommend version detail see nvidia documentation particularly description append appropriate pathname ld library path environment variable therefore would differ way normal use tensorflow gpu backend automatically select replace normal available tensorflow gpu backend find
454,454,48324152,PyTorch: How to change the learning rate of an optimizer at any given moment (no LR schedule),"<p>Is it possible in PyTorch to change the learning rate of the optimizer in the middle of training dynamically (I don't want to define a learning rate schedule beforehand)?</p>

<p>So let's say I have an optimizer:</p>

<pre><code>optim = torch.optim.SGD(model.parameters(), lr=0.01)
</code></pre>

<p>Now due to some tests which I perform during training, I realize my learning rate is too high so I want to change it to say <code>0.001</code>. There doesn't seem to be a method <code>optim.set_lr(0.001)</code> but is there some way to do this? </p>",48324389.0,2,0,,2018/1/18 14:55,22.0,2020/10/20 21:38,2018/1/18 15:09,,3990607.0,,3990607.0,,1,60,python|optimization|neural-network|deep-learning|pytorch,40456,162.428,,3,pytorch change learning rate optimizer give moment lr schedule possible pytorch change learning rate optimizer middle train dynamically want define learning rate schedule beforehand let say optimizer due test perform training realize learning rate high want change say seem method way
19,19,53879727,PyTorch - How to deactivate dropout in evaluation mode,"<p>This is the model I defined it is a simple lstm with 2 fully connect layers.</p>

<pre><code>import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class mylstm(nn.Module):
    def __init__(self,input_dim, output_dim, hidden_dim,linear_dim):
        super(mylstm, self).__init__()
        self.hidden_dim=hidden_dim
        self.lstm=nn.LSTMCell(input_dim,self.hidden_dim)
        self.linear1=nn.Linear(hidden_dim,linear_dim)
        self.linear2=nn.Linear(linear_dim,output_dim)
    def forward(self, input):
        out,_=self.lstm(input)
        out=nn.Dropout(p=0.3)(out)
        out=self.linear1(out)
        out=nn.Dropout(p=0.3)(out)
        out=self.linear2(out)
        return out
</code></pre>

<p><code>x_train</code> and <code>x_val</code> are float dataframe with shape <code>(4478,30)</code>, while <code>y_train</code> and <code>y_val</code> are float df with shape <code>(4478,10)</code>  </p>

<pre><code>    x_train.head()
Out[271]: 
       0       1       2       3    ...        26      27      28      29
0  1.6110  1.6100  1.6293  1.6370   ...    1.6870  1.6925  1.6950  1.6905
1  1.6100  1.6293  1.6370  1.6530   ...    1.6925  1.6950  1.6905  1.6960
2  1.6293  1.6370  1.6530  1.6537   ...    1.6950  1.6905  1.6960  1.6930
3  1.6370  1.6530  1.6537  1.6620   ...    1.6905  1.6960  1.6930  1.6955
4  1.6530  1.6537  1.6620  1.6568   ...    1.6960  1.6930  1.6955  1.7040

[5 rows x 30 columns]

x_train.shape
Out[272]: (4478, 30)
</code></pre>

<p>Define the varible and do one time bp, I can find out the vaildation loss is 1.4941</p>

<pre><code>model=mylstm(30,10,200,100).double()
from torch import optim
optimizer=optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)
criterion=nn.L1Loss()
input_=torch.autograd.Variable(torch.from_numpy(np.array(x_train)))
target=torch.autograd.Variable(torch.from_numpy(np.array(y_train)))
input2_=torch.autograd.Variable(torch.from_numpy(np.array(x_val)))
target2=torch.autograd.Variable(torch.from_numpy(np.array(y_val)))
optimizer.zero_grad()
output=model(input_)
loss=criterion(output,target)
loss.backward()
optimizer.step()
moniter=criterion(model(input2_),target2)

moniter
Out[274]: tensor(1.4941, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>

<p>But I called forward function again I get a different number due to randomness of dropout</p>

<pre><code>moniter=criterion(model(input2_),target2)
moniter
Out[275]: tensor(1.4943, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>

<p>what should I do that I can eliminate all the dropout in predicting phrase?</p>

<p>I tried <code>eval()</code>: </p>

<pre><code>moniter=criterion(model.eval()(input2_),target2)
moniter
Out[282]: tensor(1.4942, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)

moniter=criterion(model.eval()(input2_),target2)
moniter
Out[283]: tensor(1.4945, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)
</code></pre>

<p>And pass an addtional parameter p to control dropout:</p>

<pre><code>import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
class mylstm(nn.Module):
    def __init__(self,input_dim, output_dim, hidden_dim,linear_dim,p):
        super(mylstm, self).__init__()
        self.hidden_dim=hidden_dim
        self.lstm=nn.LSTMCell(input_dim,self.hidden_dim)
        self.linear1=nn.Linear(hidden_dim,linear_dim)
        self.linear2=nn.Linear(linear_dim,output_dim)
    def forward(self, input,p):
        out,_=self.lstm(input)
        out=nn.Dropout(p=p)(out)
        out=self.linear1(out)
        out=nn.Dropout(p=p)(out)
        out=self.linear2(out)
        return out

model=mylstm(30,10,200,100,0.3).double()

output=model(input_)
loss=criterion(output,target)
loss.backward()
optimizer.step()
moniter=criterion(model(input2_,0),target2)
Traceback (most recent call last):

  File ""&lt;ipython-input-286-e49b6fac918b&gt;"", line 1, in &lt;module&gt;
    output=model(input_)

  File ""D:\Users\shan xu\Anaconda3\lib\site-packages\torch\nn\modules\module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)

TypeError: forward() missing 1 required positional argument: 'p'
</code></pre>

<p>But neither of them worked.</p>",53881868.0,3,2,,2018/12/21 5:41,4.0,2019/8/9 9:15,2018/12/25 15:17,,7483494.0,,4469265.0,,1,20,python|deep-learning|lstm|pytorch|dropout,17815,51.0031,,5,pytorch deactivate dropout evaluation mode model define simple lstm fully connect layer float dataframe shape float df shape define varible one time bp find vaildation loss call forward function get different number due randomness dropout eliminate dropout predict phrase try pass addtional parameter p control dropout neither work
33,33,54347963,'tf' is not defined on load_model() - using lambda,"<p>I have a <code>Keras</code> model that I am trying to export and use in a different python code.</p>

<p>Here is my code:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, GRU, Flatten, Dropout, Lambda
from keras.layers.embeddings import Embedding
import tensorflow as tf


EMBEDDING_DIM = 100

model = Sequential()
model.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False))
model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_val_pad, y_val), verbose=2)
model.save('my_model.h5') 
</code></pre>

<hr>

<p>In <strong>another file</strong>, when I import <code>my_model.h5</code> :</p>

<pre><code>from keras.models import load_model
from keras.layers import Lambda
import tensorflow as tf


def learning(test_samples):
    model = load_model('my_model.h5')
    #ERROR HERE
    #rest of the code
</code></pre>

<p>The error is the following:</p>

<pre><code>  in &lt;lambda&gt;
    model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))
NameError: name 'tf' is not defined
</code></pre>

<p>After research, I got that the fact that <strong>I used <code>lambda</code> in my model</strong> is the reason for this problem, but I added these references and it didn't help:</p>

<pre><code>from keras.models import load_model
from keras.layers import Lambda
import tensorflow as tf
</code></pre>

<p>What could be the problem?</p>

<p>Thank you</p>",54348035.0,5,0,,2019/1/24 13:38,2.0,2020/9/2 15:42,,,,,2378622.0,,1,16,python|tensorflow|lambda|keras,22014,66.3708,,3,tf define load model use lambda model try export use different python code code another file import error follow research get fact use model reason problem add reference help could problem thank
45,45,54846905,PyTorch get all layers of model,"<p>What's the easiest way to take a pytorch model and get a list of all the layers without any <code>nn.Sequence</code> groupings? For example, a better way to do this?</p>
<pre><code>import pretrainedmodels

def unwrap_model(model):
    for i in children(model):
        if isinstance(i, nn.Sequential): unwrap_model(i)
        else: l.append(i)

model = pretrainedmodels.__dict__['xception'](num_classes=1000, pretrained='imagenet')
l = []
unwrap_model(model)            
            
print(l)
    
</code></pre>",54849598.0,5,0,,2019/2/23 22:31,2.0,2021/8/25 14:15,2021/3/13 10:55,,9067615.0,,2089899.0,,1,10,python|pytorch,20308,56.2307,,3,pytorch get layer model easy way take pytorch model get list layer without grouping example good way
550,550,48491737,Understanding Keras LSTMs: Role of Batch-size and Statefulness,"<h2>Sources</h2>
<p>There are several sources out there explaining stateful / stateless LSTMs and the role of batch_size which I've read already. I'll refer to them later in my post:</p>
<p>[<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] <a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/</a></p>
<p>[<a href=""https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/"" rel=""noreferrer"">2</a>] <a href=""https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/"" rel=""noreferrer"">https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/</a></p>
<p>[<a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">3</a>] <a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">http://philipperemy.github.io/keras-stateful-lstm/</a></p>
<p>[<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>] <a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/</a></p>
<p>And also other SO threads like <a href=""https://stackoverflow.com/questions/38714959/understanding-keras-lstms"">Understanding Keras LSTMs</a> and <a href=""https://stackoverflow.com/questions/39681046/keras-stateful-vs-stateless-lstms"">Keras - stateful vs stateless LSTMs</a> which didn't fully explain what I'm looking for however.</p>
<hr />
<h2>My Problem</h2>
<p>I am still not sure what is the correct approach for my task regarding statefulness and determining batch_size.</p>
<p>I have about 1000 independent time series (<code>samples</code>) that have a length of about 600 days (<code>timesteps</code>) each (actually variable length, but I thought about trimming the data to a constant timeframe) with 8 features (or <code>input_dim</code>) for each timestep (some of the features are identical to every sample, some individual per sample).</p>
<p><code>Input shape = (1000, 600, 8)</code></p>
<p>One of the features is the one I want to predict, while the others are (supposed to be) supportive for the prediction of this one 闂佺偨鍎茬粭姊恠ter feature闂? I will do that for each of the 1000 time series. What would be the best strategy to model this problem?</p>
<p><code>Output shape = (1000, 600, 1)</code></p>
<hr />
<h2>What is a Batch?</h2>
<p>From [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>]:</p>
<blockquote>
<p>Keras uses fast symbolic mathematical libraries as a backend, such as TensorFlow and Theano.</p>
<p>A downside of using these libraries is that the shape and size of your data must be defined once up front and held constant regardless of whether you are training your network or making predictions.</p>
<p>[闂佺偨鍎哄▔?/p>
<p>This does become a problem when you wish to make fewer predictions than the batch size. For example, you may get the best results with a large batch size, but are required to make predictions for one observation at a time on something like a time series or sequence problem.</p>
</blockquote>
<p>This sounds to me like a 闂佺偨鍎茬粩绂穞ch闂?would be splitting the data along the <code>timesteps</code>-dimension.</p>
<p>However, [<a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""noreferrer"">3</a>] states that:</p>
<blockquote>
<p>Said differently, whenever you train or test your LSTM, you first have to build your input matrix <code>X</code> of shape <code>nb_samples, timesteps, input_dim</code> where your batch size divides <code>nb_samples</code>. For instance, if <code>nb_samples=1024</code> and <code>batch_size=64</code>, it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector.</p>
</blockquote>
<p>When looking deeper into the examples of [<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] and [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>], Jason is always splitting his time series to several samples that only contain 1 timestep (the predecessor that in his example fully determines the next element in the sequence). So I think the batches are really split along the <code>samples</code>-axis. (However his approach of time series splitting doesn闂佺偨鍎查悰?make sense to me for a long-term dependency problem.)</p>
<p><strong>Conclusion</strong></p>
<p>So let闂佺偨鍎查悰?say I pick <code>batch_size=10</code>, that means during one epoch the weights are updated 1000 / 10 = 100 times with 10 randomly picked, complete time series containing 600 x 8 values, and when I later want to make predictions with the model, I闂佺偨鍎查悰鐒?always have to feed it batches of 10 complete time series (or use <em>solution 3</em> from [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>], copying the weights to a new model with different batch_size).</p>
<p>Principles of batch_size understood 闂?<em>however still not knowing what would be a <strong>good value for batch_size.</strong> and how to determine it</em></p>
<hr />
<h2>Statefulness</h2>
<p>The <a href=""https://keras.io/layers/recurrent/#rnn"" rel=""noreferrer"">KERAS documentation</a> tells us</p>
<blockquote>
<p>You can set RNN layers to be 'stateful', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch.</p>
</blockquote>
<p><em><strong>If I闂佺偨鍎查悰?splitting my time series</strong> into several <code>samples</code> (like in the examples of [<a href=""https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">1</a>] and [<a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">4</a>]) so that the dependencies I闂佺偨鍎查悰?like to model span across several batches, <strong>or the batch-spanning samples are otherwise correlated</strong> with each other, I may <strong>need a stateful net</strong>, otherwise not. Is that a correct and complete conclusion?</em></p>
<p>So for my problem I suppose I won闂佺偨鍎查悰?need a stateful net. I闂佺偨鍎查悰?build my training data as a 3D array of the shape <code>(samples, timesteps, features)</code> and then call <code>model.fit</code> with a batch_size yet to determine. Sample code could look like:</p>
<pre><code>model = Sequential()
model.add(LSTM(32, input_shape=(600, 8)))   # (timesteps, features)
model.add(LSTM(32))
model.add(LSTM(32))
model.add(LSTM(32))
model.add(Dense(1, activation='linear'))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)
</code></pre>",48506964.0,1,0,,2018/1/28 21:53,25.0,2020/6/30 11:35,2020/6/30 11:35,,3104974.0,,3104974.0,,1,29,python|keras|lstm|recurrent-neural-network,10850,50.9417,,3,understand kera lstms role batch size statefulness source several source explain stateful stateless lstms role batch size read already refer later post also thread like understand kera lstms keras stateful v stateless lstms fully explain look however problem still sure correct approach task regard statefulness determine batch size independent time series length day actually variable length think trim data constant timeframe feature timestep feature identical every sample individual per sample one feature one want predict others suppose supportive prediction one master feature time series would best strategy model problem batch keras us fast symbolic mathematical library backend tensorflow theano downside use library shape size data must define front hold constant regardless whether train network make prediction become problem wish make prediction batch size example may get best result large batch size require make prediction one observation time something like time series sequence problem sound like batch would split data along dimension however state say differently whenever train test lstm first build input matrix shape batch size divide instance mean model receive block sample compute output whatever number timesteps every sample average gradient propagate update parameter vector look deeply example jason always split time series several sample contain timestep predecessor example fully determine next element sequence think batch really split along axis however approach time series split make sense long term dependency problem conclusion let say pick mean one epoch weight update time randomly pick complete time series contain x value later want make prediction model always fee batch complete time series use solution copy weight new model different batch size principle batch size understood however still know would good value batch size determine statefulness kera documentation tell u set rnn layer stateful mean state compute sample one batch reuse initial state sample next batch split time series several like example dependency like model span across several batch batch spanning sample otherwise correlate may need stateful net otherwise correct complete conclusion problem suppose win need stateful net build training data array shape call batch size yet determine sample code could look like model sequential model add lstm input shape timesteps feature model add lstm model add lstm model add lstm model add dense activation linear model compile loss mean square error optimizer adam model fit x epochs batch size batch size verbose
500,500,32538758,NameError: name 'get_ipython' is not defined,"<p>I am working on Caffe framework and using PyCaffe interface. I am using a Python script obtained from converting the IPython Notebook <strong>00-classification.ipynb</strong> for testing the classification by a trained model for ImageNet. But any <strong>get_ipython()</strong> statement in the script is giving the following error:</p>

<pre><code>$ python python/my_test_imagenet.py 
Traceback (most recent call last):
  File ""python/my_test_imagenet.py"", line 23, in &lt;module&gt;
    get_ipython().magic(u'matplotlib inline')
</code></pre>

<p>In the script, I'm importing the following:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

get_ipython().magic(u'matplotlib inline')

# Make sure that caffe is on the python path:
caffe_root = '/path/to/caffe/'
import sys
sys.path.insert(0, caffe_root + 'python')

import caffe

plt.rcParams['figure.figsize'] = (10, 10)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

import os

# ... Rest of the code...
</code></pre>

<p>Can someone please help me to resolve this error?</p>",32539282.0,3,3,,2015/9/12 12:25,11.0,2020/5/15 16:08,2015/9/12 12:31,,2452617.0,,2452617.0,,1,47,python|ipython|caffe|pycaffe,86389,137.946,,1,nameerror name get ipython define work caffe framework use pycaffe interface use python script obtain convert ipython notebook classification ipynb test classification trained model imagenet get ipython statement script give following error script import following someone please help resolve error
313,313,58216000,Get total amount of free GPU memory and available using pytorch,"<p>I'm using google colab free Gpu's for experimentation and wanted to know how much GPU Memory available to play around, torch.cuda.memory_allocated() returns the current GPU memory occupied, but how do we determine total available memory using PyTorch.</p>",58216793.0,2,0,,2019/10/3 9:18,5.0,2021/8/27 3:21,2019/10/3 18:07,,681865.0,,2809512.0,,1,22,gpu|pytorch|google-colaboratory,25774,74.4447,,3,get total amount free gpu memory available use pytorch use google colab free gpu experimentation want know much gpu memory available play around torch cuda memory allocate return current gpu memory occupy determine total available memory use pytorch
227,227,44226932,Difference between tf.nn_conv2d and tf.nn.depthwise_conv2d,<p>What is the difference between <code>tf.nn_conv2d</code> and <code>tf.nn.depthwise_conv2d</code> in Tensorflow?</p>,45108482.0,3,5,,2017/5/28 11:43,5.0,2018/4/12 8:25,,,,,7994456.0,,1,9,python|tensorflow|deep-learning|conv-neural-network,7978,53.0076,,3,difference tf nn conv tf nn depthwise conv difference tensorflow
108,108,41665799,Keras model.summary() object to string,"<p>I want to write a *.txt file with the neural network hyperparameters and the model architecture. Is it possible to write the object model.summary() to my output file?</p>

<pre><code>(...)
summary = str(model.summary())
(...)
out = open(filename + 'report.txt','w')
out.write(summary)
out.close
</code></pre>

<p>It happens that I'm getting ""None"" as you can see below.</p>

<pre><code>Hyperparameters
=========================

learning_rate: 0.01
momentum: 0.8
decay: 0.0
batch size: 128
no. epochs: 3
dropout: 0.5
-------------------------

None
val_acc: 0.232323229313
val_loss: 3.88496732712
train_acc: 0.0965207634216
train_loss: 4.07161939425
train/val loss ratio: 1.04804469418
</code></pre>

<p>Any idea how to deal with that?</p>",45546663.0,9,1,,2017/1/15 20:14,12.0,2021/6/25 15:03,2021/6/25 15:03,,10375049.0,,3743194.0,,1,67,python|tensorflow|machine-learning|keras|deep-learning,29882,293.502,,5,kera model summary object string want write txt file neural network hyperparameters model architecture possible write object model summary output file happen get none see idea deal
305,305,57874436,Tensorflow Data Adapter Error: ValueError: Failed to find data adapter that can handle input,"<p>While running a sentdex tutorial script of a cryptocurrency RNN, link here</p>

<p><a href=""https://www.youtube.com/watch?v=yWkpRdpOiPY&amp;list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&amp;index=11"" rel=""noreferrer"">YouTube Tutorial: Cryptocurrency-predicting RNN Model</a>,</p>

<p>but have encountered an error when attempting to train the model. My tensorflow version is 2.0.0 and I'm running python 3.6. When attempting to train the model I receive the following error:</p>

<pre class=""lang-py prettyprint-override""><code>File ""C:\python36-64\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 734, in fit
    use_multiprocessing=use_multiprocessing)

File ""C:\python36-64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 224, in fit
    distribution_strategy=strategy)

File ""C:\python36-64\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 497, in _process_training_inputs
    adapter_cls = data_adapter.select_data_adapter(x, y)

File ""C:\python36-64\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py"", line 628, in select_data_adapter
    _type_name(x), _type_name(y)))

ValueError: Failed to find data adapter that can handle input: &lt;class 'numpy.ndarray'&gt;, (&lt;class 'list'&gt; containing values of types {""&lt;class 'numpy.float64'&gt;""})
</code></pre>

<p>Any advice would be greatly appreciated!</p>",57973957.0,6,0,,2019/9/10 15:58,4.0,2021/2/14 15:28,2019/9/10 16:13,,7758804.0,,12047797.0,,1,44,python|tensorflow|keras|lstm,60707,191.933,,2,tensorflow data adapter error valueerror fail find data adapter handle input run sentdex tutorial script cryptocurrency rnn link youtube tutorial cryptocurrency predict rnn model encounter error attempt train model tensorflow version run python attempt train model receive following error advice would greatly appreciated
147,147,42616625,ValueError: Tensor must be from the same graph as Tensor with Bidirectinal RNN in Tensorflow,"<p>I'm doing text tagger using Bidirectional dynamic RNN in tensorflow.
After maching input's dimension, I tried to run a Session. 
this is blstm setting parts:</p>

<pre><code>fw_lstm_cell = BasicLSTMCell(LSTM_DIMS)
bw_lstm_cell = BasicLSTMCell(LSTM_DIMS)

(fw_outputs, bw_outputs), _ = bidirectional_dynamic_rnn(fw_lstm_cell,
                                                        bw_lstm_cell,
                                                        x_place,
                                                        sequence_length=SEQLEN,
                                                        dtype='float32')
</code></pre>

<p>and this is runing part:</p>

<pre><code>  with tf.Graph().as_default():
    # Placehoder Settings
    x_place, y_place = set_placeholder(BATCH_SIZE, EM_DIMS, MAXLEN)

    # BLSTM Model Building
    hlogits = tf_kcpt.build_blstm(x_place)

    # Compute loss
    loss = tf_kcpt.get_loss(log_likelihood)

    # Training
    train_op = tf_kcpt.training(loss)

    # load Eval method
    eval_correct = tf_kcpt.evaluation(logits, y_place)

    # Session Setting &amp; Init
    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init)

    # tensor summary setting
    summary = tf.summary.merge_all()
    summary_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)

    # Save
    saver = tf.train.Saver()

    # Run epoch
    for step in range(EPOCH):
        start_time = time.time()

        feed_dict = fill_feed_dict(KCPT_SET['train'], x_place, y_place)
        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)
</code></pre>

<p>But, it give me the error:</p>

<blockquote>
  <p>ValueError: Tensor(""Shape:0"", shape=(1,), dtype=int32) must be from the same graph as Tensor(""bidirectional_rnn/fw/fw/stack_2:0"", shape=(1,), dtype=int32).</p>
</blockquote>

<p>Help me, please</p>",42616834.0,3,3,,2017/3/6 2:32,2.0,2020/8/12 1:48,2018/4/2 14:43,,6583140.0,,7663887.0,,1,31,python|tensorflow|deep-learning|recurrent-neural-network|bidirectional,45428,83.2293,,4,valueerror tensor must graph tensor bidirectinal rnn tensorflow text tagger use bidirectional dynamic rnn tensorflow maching input dimension try run session blstm set part run part give error valueerror tensor shape shape dtype int must graph tensor bidirectional rnn fw fw stack shape dtype int help please
213,213,43906048,Which parameters should be used for early stopping?,<p>I'm training a neural network for my project using Keras. Keras has provided a function for early stopping. May I know what parameters should be observed to avoid my neural network from overfitting by using early stopping?</p>,43906235.0,2,0,,2017/5/11 3:30,37.0,2021/7/29 11:35,2020/5/4 15:06,,3924118.0,,7343062.0,,1,106,python|keras|deep-learning|conv-neural-network,79779,237.008,,3,parameter use early stopping train neural network project use kera kera provide function early stopping may know parameter observe avoid neural network overfitting use early stopping
311,311,58138071,'tuple' object has no attribute 'layer',"<p>I am having many troubles trying to start training my model (a DCGAN). It is giving me the error:</p>
<pre><code>'tuple' object has no attribute 'layer'
</code></pre>
<p>I read that this could be due to having both the TensorFlow version 1.14.0 and the Keras version 2.2 or higher.
I tried to fix this by downgrading the Keras version to the 2.1.5, but I'm still having the same problem.</p>
<pre><code>from google.colab import drive 
drive.mount('/mntDrive')

import os,sys      #os es para gestionar directorios (trabajar con archivos); sys es para trabajar con variables del sistema

# numpy
import numpy as np

# data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
from sklearn.model_selection import train_test_split

# Charts
import matplotlib.pyplot as plt
from matplotlib.pyplot import imread
#process
from tqdm import tqdm

# Image IO
from PIL import Image
import skimage.io
import skimage.transform
from skimage.transform import resize

# Deep learning
import tensorflow as tf

from tensorflow.keras import layers
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Input
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam
from keras import initializers

from __future__ import absolute_import, division, print_function, unicode_literals


# To make sure that we can reproduce the experiment and get the same results
np.random.seed(21)


# The dimension of our random noise vector.
random_dim = 100
from skimage.color import rgb2gray
import scipy.ndimage
import scipy.misc
import re
images = []
for root, dirnames, filenames in os.walk(&quot;/mntDrive/My Drive/Colab Notebooks/cubism&quot;):
    for filename in filenames:
        if re.search(&quot;\.(jpg|jpeg|png)$&quot;, filename):
            filepath = os.path.join(root, filename)
            image = plt.imread(filepath, )
            image = (image.astype(np.float32) - 127.5)/127.5
            image_resized = resize(image, (112, 112))
            images.append(image_resized)
images = np.array(images)


print('Original image shape: {}'.format(images.shape))
im_gray = rgb2gray(images)
print('New image shape: {}'.format(im_gray.shape))
images_resized = im_gray.reshape(320,12544)

def get_optimizer():
  optimizer=tf.keras.optimizers.Adam(0.001)
  return optimizer


def make_generator_model(optimizer):
    generator = tf.keras.Sequential()
    generator.add(layers.Dense(7*7*256, use_bias=False, input_shape=(random_dim,)))
    generator.add(layers.BatchNormalization())
    generator.add(layers.LeakyReLU())

    generator.add(layers.Reshape((7, 7, 256)))
    assert generator.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    generator.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert generator.output_shape == (None, 14, 14, 128)
    generator.add(layers.BatchNormalization())
    generator.add(layers.LeakyReLU())

    generator.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert generator.output_shape == (None, 28, 28, 64)
    generator.add(layers.BatchNormalization())
    generator.add(layers.LeakyReLU())
    
    generator.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert generator.output_shape == (None, 56, 56, 32)
    generator.add(layers.BatchNormalization())
    generator.add(layers.LeakyReLU())

    generator.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert generator.output_shape == (None, 112, 112, 1)
    generator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001))
    
    return generator    

def make_discriminator_model(optimizer):
    discriminator = tf.keras.Sequential()
    discriminator.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[112, 112, 1]))
    discriminator.add(layers.LeakyReLU())
    discriminator.add(layers.Dropout(0.3))

    discriminator.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    discriminator.add(layers.LeakyReLU())
    discriminator.add(layers.Dropout(0.3))
    
    discriminator.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))   
    discriminator.add(layers.LeakyReLU())
    discriminator.add(layers.Dropout(0.3))
    
    discriminator.add(layers.Flatten())
    discriminator.add(layers.Dense(1, activation='sigmoid' ))
    discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001))    
    
    return discriminator
    
def get_gan_network(discriminator, random_dim, generator, optimizer):
    # We initially set trainable to False since we only want to train either the
    # generator or discriminator at a time
    discriminator.trainable = False
    # gan input (noise) will be 100-dimensional vectors
    gan_input = Input(shape=(random_dim,))
    # the output of the generator (an image)
    x = generator(gan_input)
    # get the output of the discriminator (probability if the image is real or not)
    gan_output = discriminator(x)
    gan = Model(inputs=gan_input, outputs=gan_output)
    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001))
    return gan

def plot_generated_images(epoch, generator, examples=64, dim=(10, 10), figsize=(100, 100)):
    noise = np.random.normal(0, 1, size=[examples, random_dim]) #mean, std deviation, size
    generated_images = generator.predict(noise)
    generated_images = generated_images.reshape(examples, 112, 112)

    plt.figure(figsize=figsize)
    for i in range(generated_images.shape[0]):
        plt.subplot(dim[0], dim[1], i+1)
        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig('dcgan_generated_Originals_2_epoch_%d.png' % epoch)
    

#you can create a function which will save your generated images every 20 epochs
    # Create a wall of generated MNIST images
def train(epochs=15000, batch_size=80):
    # Get the training and testing data
    images_resized
    # Split the training data into batches of size 80
    batch_count = images_resized.shape[0] // batch_size 

    # Build our GAN network
    optimizer=get_optimizer()
    generator = make_generator_model(optimizer)
    discriminator = make_discriminator_model(optimizer)
    gan = get_gan_network(discriminator, random_dim, generator, optimizer)

    for e in range(1, epochs+1):
        print ('-'*15, 'Epoch %d' % e, '-'*15)
        for _ in tqdm(range(batch_count)):
            # Get a random set of input noise and images
            noise = np.random.normal(0, 1, size=[batch_size, random_dim])
            image_batch = images_resized[np.random.randint(0, images_resized.shape[0], size=batch_size)]

            # Generate fake MNIST images
            generated_images = generator.predict(noise)
            X = np.concatenate([image_batch, generated_images])  #128x4096 --- 'a' x 4096

            # Labels for generated and real data
            y_dis = np.zeros(2*batch_size)
            y_dis[:batch_size] = 0.9    # One-sided label smoothing. Se refiere a que la mitad de y_dis seran 0.9 (reales) y la otra mitad seran 0 (falsos)

            
            # Train discriminator
            discriminator.trainable = True
            discriminator.train_on_batch(X, y_dis)      #Se entrena el Discriminador con dos vectores: X(tiene un batch(128) imagenes reales y un batch de imagenes generadas); y_dis(tiene un batch de 128 0.9(serian las reales) y otro batch de 128 ceros(serian las generadas). De esta manera, el algoritmo generador, aunque las genere bien, va a seguir optimizandose el numero de epochs que haga falta(seguimos en la fase de entrenamiento), ya que sus imagenes son continuamente rechazadas, comparandose con las reales y sacando continuamente diferencias. ) 
                                                        #Entra X(imagenes reales, imagenes generadas)---&gt;y_dis(0.9 , 0)
                                                        #train_on_batch(x,y)---&gt; realiza una actualizacion del gradiente en un batch----&gt; x=array de training data (si el modelo tiene varias entradas pueden ser varios); y= array de target data (si el modelo tiene varias salidas pueden ser varios)
            
            # Train generator
            noise = np.random.normal(0, 1, size=[batch_size, random_dim])
            y_gen = np.ones(batch_size)
            discriminator.trainable = False
            gan.train_on_batch(noise, y_gen)            #Entra noise(pixeles desordenados)----&gt;y_gen(todo son 1)

        if e == 1 or e % 50 == 0:
            plot_generated_images(e, generator)
            
            
if __name__ == '__main__':
   train(30000, 80)
</code></pre>
<p>This is the output:</p>
<pre><code>Drive already mounted at /mntDrive; to attempt to forcibly remount, call drive.mount(&quot;/mntDrive&quot;, force_remount=True).
Original image shape: (320, 112, 112, 3)
New image shape: (320, 112, 112)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

    ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    &lt;ipython-input-18-6e3c9ece87ff&gt; in &lt;module&gt;()
        196 
        197 if __name__ == '__main__':
    --&gt; 198    train(30000, 80)
    
    11 frames
    &lt;ipython-input-18-6e3c9ece87ff&gt; in train(epochs, batch_size)
        161     generator = make_generator_model(optimizer)
        162     discriminator = make_discriminator_model(optimizer)
    --&gt; 163     gan = get_gan_network(discriminator, random_dim, generator, optimizer)
        164 
        165     for e in range(1, epochs+1):
    
    &lt;ipython-input-18-6e3c9ece87ff&gt; in get_gan_network(discriminator, random_dim, generator, optimizer)
        128     gan_input = Input(shape=(random_dim,))
        129     # the output of the generator (an image)
    --&gt; 130     x = generator(gan_input)
        131     # get the output of the discriminator (probability if the image is real or not)
        132     gan_output = discriminator(x)
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
        632                     outputs = base_layer_utils.mark_as_return(outputs, acd)
        633                 else:
    --&gt; 634                   outputs = call_fn(inputs, *args, **kwargs)
        635 
        636             except TypeError as e:
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
        245       if not self.built:
        246         self._init_graph_network(self.inputs, self.outputs, name=self.name)
    --&gt; 247       return super(Sequential, self).call(inputs, training=training, mask=mask)
        248 
        249     outputs = inputs  # handle the corner case where self.layers is empty
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in call(self, inputs, training, mask)
        749                                 ' implement a `call` method.')
        750 
    --&gt; 751     return self._run_internal_graph(inputs, training=training, mask=mask)
        752 
        753   def compute_output_shape(self, input_shape):
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask)
        891 
        892           # Compute outputs.
    --&gt; 893           output_tensors = layer(computed_tensors, **kwargs)
        894 
        895           # Update tensor_dict.
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
        661               kwargs.pop('training')
        662             inputs, outputs = self._set_connectivity_metadata_(
    --&gt; 663                 inputs, outputs, args, kwargs)
        664           self._handle_activity_regularization(inputs, outputs)
        665           self._set_mask_metadata(inputs, outputs, previous_mask)
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _set_connectivity_metadata_(self, inputs, outputs, args, kwargs)
       1706     kwargs.pop('mask', None)  # `mask` should not be serialized.
       1707     self._add_inbound_node(
    -&gt; 1708         input_tensors=inputs, output_tensors=outputs, arguments=kwargs)
       1709     return inputs, outputs
       1710 
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _add_inbound_node(self, input_tensors, output_tensors, arguments)
       1793     &quot;&quot;&quot;
       1794     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,
    -&gt; 1795                                         input_tensors)
       1796     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,
       1797                                       input_tensors)
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
        513 
        514   return pack_sequence_as(
    --&gt; 515       structure[0], [func(*x) for x in entries],
        516       expand_composites=expand_composites)
        517 
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in &lt;listcomp&gt;(.0)
        513 
        514   return pack_sequence_as(
    --&gt; 515       structure[0], [func(*x) for x in entries],
        516       expand_composites=expand_composites)
        517 
    
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in &lt;lambda&gt;(t)
       1792             `call` method of the layer at the call that created the node.
       1793     &quot;&quot;&quot;
    -&gt; 1794     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,
       1795                                         input_tensors)
       1796     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,
    AttributeError: 'tuple' object has no attribute 'layer'
    AttributeError: 'tuple' object has no attribute 'layer'
</code></pre>",,3,0,,2019/9/27 16:07,2.0,2020/12/10 23:24,2020/12/10 23:24,,13552470.0,,12131632.0,,1,12,python|tensorflow|keras|version|attributeerror,21211,65.5062,,4,tuple object attribute layer many trouble try start train model dcgan give error read could due tensorflow version keras version high try fix downgrade keras version still problem output
42,42,54716377,How to do gradient clipping in pytorch?,"<p>What is the correct way to perform gradient clipping in pytorch?</p>

<p>I have an exploding gradients problem, and I need to program my way around it.</p>",54816498.0,3,2,,2019/2/15 20:09,22.0,2021/1/19 13:19,2020/10/11 17:27,,913098.0,,913098.0,,1,54,python|machine-learning|deep-learning|pytorch|gradient-descent,70421,237.791,,3,gradient clipping pytorch correct way perform gradient clip pytorch exploding gradient problem need program way around
685,685,38376478,Changing the scale of a tensor in tensorflow,"<p>Sorry if I messed up the title, I didn't know how to phrase this. Anyways, I have a tensor of a set of values, but I want to make sure that every element in the tensor has a range from 0 - 255, (or 0 - 1 works too). However, I don't want to make all the values add up to 1 or 255 like softmax, I just want to down scale the values.</p>

<p>Is there any way to do this?</p>

<p>Thanks!</p>",38377600.0,5,0,,2016/7/14 14:08,4.0,2021/8/21 7:06,,,,,3102725.0,,1,18,python|tensorflow|conv-neural-network,21419,83.3232,,3,change scale tensor tensorflow sorry mess title know phrase anyways tensor set value want make sure every element tensor range work however want make value add like softmax want scale value way thanks
328,328,63008040,How to use AMD GPU for fastai/pytorch?,"<p>I'm using a laptop which has Intel Corporation HD Graphics 5500 (rev 09), and AMD Radeon r5 m255 graphics card.</p>
<p>Does anyone know how to it set up for Deep Learning, specifically fastai/Pytorch?</p>",,2,0,,2020/7/21 5:36,3.0,2021/8/19 9:47,,,,,13954488.0,,1,24,pytorch|gpu|amd|fast-ai,27404,51.3513,,1,use amd gpu fastai pytorch use laptop intel corporation hd graphic rev amd radeon r graphic card anyone know set deep learning specifically fastai pytorch
48,48,55112713,'EarlyStopping' object has no attribute 'on_train_batch_begin',"<p>I want to save the best checkpoint when my model is training, but the callback does not work as I expect. According to <a href=""https://stackoverflow.com/questions/48285129/saving-best-model-in-keras"">Saving best model in Keras</a> this code should work.</p>

<pre><code>model = Sequential()
model.add(Conv1D(filters=32, kernel_size=8, input_shape=(X_train.shape[1], 4)))
model.add(MaxPooling1D(pool_size=4))
model.add(Flatten())
model.add(Dense(16, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.compile(loss='binary_crossentropy', optimizer='adam', 
              metrics=['accuracy'])
model.summary()

stop = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')
save = ModelCheckpoint('./my_model.hdf5', save_best_only=True, monitor='val_loss', mode='min')
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, epsilon=1e-4, mode='min')

history = model.fit(X_train, y_train, epochs=25, verbose=0, callbacks=[stop, save, reduce_lr], validation_split=0.25)
</code></pre>

<p>However it keeps giving me following error:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-28-f86f439eae5a&gt; in &lt;module&gt;()
     17 reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')
     18 
---&gt; 19 history = model.fit(X_train, y_train, batch_size=batch_size, epochs=50, verbose=0, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)
     20 
     21 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
    878           initial_epoch=initial_epoch,
    879           steps_per_epoch=steps_per_epoch,
--&gt; 880           validation_steps=validation_steps)
    881 
    882   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)
    323         # Callbacks batch_begin.
    324         batch_logs = {'batch': batch_index, 'size': len(batch_ids)}
--&gt; 325         callbacks._call_batch_hook(mode, 'begin', batch_index, batch_logs)
    326         progbar.on_batch_begin(batch_index, batch_logs)
    327 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    194     t_before_callbacks = time.time()
    195     for callback in self.callbacks:
--&gt; 196       batch_hook = getattr(callback, hook_name)
    197       batch_hook(batch, logs)
    198     self._delta_ts[hook_name].append(time.time() - t_before_callbacks)

AttributeError: 'EarlyStopping' object has no attribute 'on_train_batch_begin'
</code></pre>

<p>I have successfully used this code for my functional model, but I am not sure what the problem is here with the sequential model.</p>",55114607.0,2,1,,2019/3/12 1:14,1.0,2020/6/19 17:36,,,,,7918602.0,,1,9,python-3.x|keras|deep-learning,15051,60.3103,,4,earlystopping object attribute train batch begin want save best checkpoint model train callback work expect accord save best model kera code work however keep give follow error successfully use code functional model sure problem sequential model
62,62,55496289,"How to fix ""AttributeError: module 'tensorflow' has no attribute 'get_default_graph'""?","<p>I am trying to run some code to create an LSTM model but i get an error:</p>

<p><code>AttributeError: module 'tensorflow' has no attribute 'get_default_graph'</code></p>

<p>My code is as follows:</p>

<pre><code>from keras.models import Sequential

model = Sequential()
model.add(Dense(32, input_dim=784))
model.add(Activation('relu'))
model.add(LSTM(17))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>I have found someone else with a similar problem and they updated tensorflow and it works; but mine is up to date and still does not work. I am new to using keras and machine learning so I apologise if this is something silly!</p>",55496390.0,18,0,,2019/4/3 13:23,4.0,2021/7/24 18:18,2019/4/3 13:34,,10724050.0,,10724050.0,,1,47,python|tensorflow|keras|keras-layer|tf.keras,84703,277.91200000000003,,4,fix attributeerror module tensorflow attribute get default graph try run code create lstm model get error code follow find someone else similar problem update tensorflow work mine date still work new use kera machine learning apologise something silly
336,336,45113245,How to get mini-batches in pytorch in a clean and efficient way?,"<p>I was trying to do a simple thing which was train a linear model with Stochastic Gradient Descent (SGD) using torch:</p>

<pre><code>import numpy as np

import torch
from torch.autograd import Variable

import pdb

def get_batch2(X,Y,M,dtype):
    X,Y = X.data.numpy(), Y.data.numpy()
    N = len(Y)
    valid_indices = np.array( range(N) )
    batch_indices = np.random.choice(valid_indices,size=M,replace=False)
    batch_xs = torch.FloatTensor(X[batch_indices,:]).type(dtype)
    batch_ys = torch.FloatTensor(Y[batch_indices]).type(dtype)
    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)

def poly_kernel_matrix( x,D ):
    N = len(x)
    Kern = np.zeros( (N,D+1) )
    for n in range(N):
        for d in range(D+1):
            Kern[n,d] = x[n]**d;
    return Kern

## data params
N=5 # data set size
Degree=4 # number dimensions/features
D_sgd = Degree+1
##
x_true = np.linspace(0,1,N) # the real data points
y = np.sin(2*np.pi*x_true)
y.shape = (N,1)
## TORCH
dtype = torch.FloatTensor
# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU
X_mdl = poly_kernel_matrix( x_true,Degree )
X_mdl = Variable(torch.FloatTensor(X_mdl).type(dtype), requires_grad=False)
y = Variable(torch.FloatTensor(y).type(dtype), requires_grad=False)
## SGD mdl
w_init = torch.zeros(D_sgd,1).type(dtype)
W = Variable(w_init, requires_grad=True)
M = 5 # mini-batch size
eta = 0.1 # step size
for i in range(500):
    batch_xs, batch_ys = get_batch2(X_mdl,y,M,dtype)
    # Forward pass: compute predicted y using operations on Variables
    y_pred = batch_xs.mm(W)
    # Compute and print loss using operations on Variables. Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape (1,); loss.data[0] is a scalar value holding the loss.
    loss = (1/N)*(y_pred - batch_ys).pow(2).sum()
    # Use autograd to compute the backward pass. Now w will have gradients
    loss.backward()
    # Update weights using gradient descent; w1.data are Tensors,
    # w.grad are Variables and w.grad.data are Tensors.
    W.data -= eta * W.grad.data
    # Manually zero the gradients after updating weights
    W.grad.data.zero_()

#
c_sgd = W.data.numpy()
X_mdl = X_mdl.data.numpy()
y = y.data.numpy()
#
Xc_pinv = np.dot(X_mdl,c_sgd)
print('J(c_sgd) = ', (1/N)*(np.linalg.norm(y-Xc_pinv)**2) )
print('loss = ',loss.data[0])
</code></pre>

<p>the code runs fine and all though my <code>get_batch2</code> method seems really dum/naive, its probably because I am new to pytorch but I have not found a good place where they discuss how to retrieve data batches. I went through their tutorials (<a href=""http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"" rel=""noreferrer"">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a>) and through the data set (<a href=""http://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""noreferrer"">http://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a>) with no luck. The tutorials all seem to assume that one already has the batch and batch-size at the beginning and then proceeds to train with that data without changing it (specifically look at <a href=""http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd"" rel=""noreferrer"">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-variables-and-autograd</a>).</p>

<p>So my question is do I really need to turn my data back into numpy so that I can fetch some random sample of it and then turn it back to pytorch with Variable to be able to train in memory? Is there no way to get mini-batches with torch?</p>

<p>I looked at a few functions torch provides but with no luck:</p>

<pre><code>#pdb.set_trace()
#valid_indices = torch.arange(0,N).numpy()
#valid_indices = np.array( range(N) )
#batch_indices = np.random.choice(valid_indices,size=M,replace=False)
#indices = torch.LongTensor(batch_indices)
#batch_xs, batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)
#batch_xs,batch_ys = torch.index_select(X_mdl, 0, indices), torch.index_select(y, 0, indices)
</code></pre>

<p>even though the code I provided works fine I am worried that its not an efficient implementation AND that if I were to use GPUs that there would be a considerable further slow down (because my guess it putting things in memory and then fetching them back to put them GPU like that is silly).</p>

<hr>

<p>I implemented a new one based on the answer that suggested to use <code>torch.index_select()</code>:</p>

<pre><code>def get_batch2(X,Y,M):
    '''
    get batch for pytorch model
    '''
    # TODO fix and make it nicer, there is pytorch forum question
    #X,Y = X.data.numpy(), Y.data.numpy()
    X,Y = X, Y
    N = X.size()[0]
    batch_indices = torch.LongTensor( np.random.randint(0,N+1,size=M) )
    pdb.set_trace()
    batch_xs = torch.index_select(X,0,batch_indices)
    batch_ys = torch.index_select(Y,0,batch_indices)
    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)
</code></pre>

<p>however, this seems to have issues because it does not work if <code>X,Y</code> are NOT variables...which is really odd. I added this to the pytorch forum: <a href=""https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322"" rel=""noreferrer"">https://discuss.pytorch.org/t/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way/10322</a></p>

<p>Right now what I am struggling with is making this work for gpu. My most current version:</p>

<pre><code>def get_batch2(X,Y,M,dtype):
    '''
    get batch for pytorch model
    '''
    # TODO fix and make it nicer, there is pytorch forum question
    #X,Y = X.data.numpy(), Y.data.numpy()
    X,Y = X, Y
    N = X.size()[0]
    if dtype ==  torch.cuda.FloatTensor:
        batch_indices = torch.cuda.LongTensor( np.random.randint(0,N,size=M) )# without replacement
    else:
        batch_indices = torch.LongTensor( np.random.randint(0,N,size=M) ).type(dtype)  # without replacement
    pdb.set_trace()
    batch_xs = torch.index_select(X,0,batch_indices)
    batch_ys = torch.index_select(Y,0,batch_indices)
    return Variable(batch_xs, requires_grad=False), Variable(batch_ys, requires_grad=False)
</code></pre>

<p>the error:</p>

<pre><code>RuntimeError: tried to construct a tensor from a int sequence, but found an item of type numpy.int64 at index (0)
</code></pre>

<p>I don't get it, do I really have to do:</p>

<pre><code>ints = [ random.randint(0,N) for i i range(M)]
</code></pre>

<p>to get the integers?</p>

<p>It would also be ideal if the data could be a variable. It seems that it <code>torch.index_select</code> does not work for <code>Variable</code> type data.</p>

<p>this list of integers thing still doesn't work:</p>

<pre><code>TypeError: torch.addmm received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor), but expected one of:
 * (torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (float beta, torch.cuda.FloatTensor source, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (float beta, torch.cuda.FloatTensor source, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
 * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)
 * (float beta, torch.cuda.FloatTensor source, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2, *, torch.cuda.FloatTensor out)
      didn't match because some of the arguments have invalid types: (int, torch.cuda.FloatTensor, int, torch.cuda.FloatTensor, torch.FloatTensor, out=torch.cuda.FloatTensor)
</code></pre>",,6,3,,2017/7/15 0:22,20.0,2020/7/8 5:51,2017/11/23 23:59,,1601580.0,,1601580.0,,1,47,python|numpy|machine-learning|deep-learning|pytorch,61158,184.546,,3,get mini batch pytorch clean efficient way try simple thing train linear model stochastic gradient descent sgd use torch code run fine though method seem really dum naive probably new pytorch find good place discuss retrieve data batch go tutorial data set luck tutorial seem assume one already batch batch size beginning proceeds train data without change specifically look pytorch variable autograd question really need turn data back numpy fetch random sample turn back pytorch variable able train memory way get mini batch torch look function torch provide luck even though code provide work fine worried efficient implementation use gpus would considerable slow guess put thing memory fetch back put gpu like silly implement new one base answer suggest use however seem issue work variable really odd add pytorch forum right struggle make work gpu current version error get really get integer would also ideal data could variable seem work type data list integer thing still work
271,271,44806125,AttributeError: 'Model' object has no attribute 'predict_classes',"<p>I'm trying to predict on the validation data with pre-trained and fine-tuned DL models. The code follows the example available in the Keras blog on ""building image classification models using very little data"". Here is the code:</p>

<pre><code>import numpy as np
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.models import Model
from keras.layers import Flatten, Dense
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import roc_auc_score
import itertools
from keras.optimizers import SGD
from sklearn.metrics import roc_curve, auc
from keras import applications
from keras import backend as K
K.set_image_dim_ordering('tf')

# Plotting the confusion matrix
def plot_confusion_matrix(cm, classes,
                          normalize=False, #if true all values in confusion matrix is between 0 and 1
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """"""
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """"""
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print(""Normalized confusion matrix"")
    else:
        print('Confusion matrix, without normalization')

    print(cm)
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment=""center"",
                 color=""white"" if cm[i, j] &gt; thresh else ""black"")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#plot data
def generate_results(validation_labels, y_pred):
    fpr, tpr, _ = roc_curve(validation_labels, y_pred) ##(this implementation is restricted to a binary classification task)
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.05])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('Receiver operating characteristic (ROC) curve')
    plt.show()
    print('Area Under the Curve (AUC): %f' % roc_auc)

img_width, img_height = 100,100
top_model_weights_path = 'modela.h5'
train_data_dir = 'data4/train'
validation_data_dir = 'data4/validation'
nb_train_samples = 20
nb_validation_samples = 20
epochs = 50
batch_size = 10
def save_bottleneck_features():
   datagen = ImageDataGenerator(rescale=1. / 255)
   model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(100,100,3))
   generator = datagen.flow_from_directory(
               train_data_dir,
               target_size=(img_width, img_height),
               batch_size=batch_size,
               class_mode='binary',
               shuffle=False)
   bottleneck_features_train = model.predict_generator(
               generator, nb_train_samples // batch_size)
   np.save(open('bottleneck_features_train', 'wb'),bottleneck_features_train)

   generator = datagen.flow_from_directory(
               validation_data_dir,
               target_size=(img_width, img_height),
               batch_size=batch_size,
               class_mode='binary',
               shuffle=False)
   bottleneck_features_validation = model.predict_generator(
               generator, nb_validation_samples // batch_size)
   np.save(open('bottleneck_features_validation', 'wb'),bottleneck_features_validation)

def train_top_model():
   train_data = np.load(open('bottleneck_features_train', 'rb'))
   train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))
   validation_data = np.load(open('bottleneck_features_validation', 'rb'))
   validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))
   model = Sequential()
   model.add(Flatten(input_shape=train_data.shape[1:]))
   model.add(Dense(512, activation='relu'))
   model.add(Dense(1, activation='sigmoid'))
   sgd = SGD(lr=1e-3, decay=0.00, momentum=0.99, nesterov=False) 
   model.compile(optimizer=sgd,
         loss='binary_crossentropy', metrics=['accuracy'])
   model.fit(train_data, train_labels,
          epochs=epochs,
          batch_size=batch_size,
   validation_data=(validation_data, validation_labels))
   model.save_weights(top_model_weights_path)
   print('Predicting on test data')
   y_pred = model.predict_classes(validation_data)
   print(y_pred.shape)
   print('Generating results')
   generate_results(validation_labels[:,], y_pred[:,])
   print('Generating the ROC_AUC_Scores') #Compute Area Under the Curve (AUC) from prediction scores
   print(roc_auc_score(validation_labels,y_pred)) #this implementation is restricted to the binary classification task or multilabel classification task in label indicator format.
   target_names = ['class 0(Normal)', 'class 1(Abnormal)']
   print(classification_report(validation_labels,y_pred,target_names=target_names))
   print(confusion_matrix(validation_labels,y_pred))
   cnf_matrix = (confusion_matrix(validation_labels,y_pred))
   np.set_printoptions(precision=2)
   plt.figure()
   # Plot non-normalized confusion matrix
   plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix')
   plt.show()
save_bottleneck_features()
train_top_model()

# path to the model weights files.
weights_path = '../keras/examples/vgg16_weights.h5'
top_model_weights_path = 'modela.h5'
# dimensions of our images.
img_width, img_height = 100, 100
train_data_dir = 'data4/train'
validation_data_dir = 'data4/validation'
nb_train_samples = 20
nb_validation_samples = 20
epochs = 50
batch_size = 10

# build the VGG16 network
base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(100,100,3))
print('Model loaded.')

train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1. / 255)
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary')
validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='binary') 
top_model = Sequential()
top_model.add(Flatten(input_shape=base_model.output_shape[1:]))
top_model.add(Dense(512, activation='relu'))
top_model.add(Dense(1, activation='softmax'))
top_model.load_weights(top_model_weights_path)
model = Model(inputs=base_model.input, outputs=top_model(base_model.output))
   # set the first 15 layers (up to the last conv block)
# to non-trainable (weights will not be updated)
for layer in model.layers[:15]: #up to the layer before the last convolution block
        layer.trainable = False
model.summary()
   # fine-tune the model
model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.99), metrics=['accuracy'])
model.fit_generator(train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size,
    verbose=1)
model.save_weights(top_model_weights_path)
bottleneck_features_validation = model.predict_generator(validation_generator, nb_validation_samples // batch_size)
np.save(open('bottleneck_features_validation','wb'), bottleneck_features_validation)
validation_data = np.load(open('bottleneck_features_validation', 'rb'))
y_pred1 = model.predict_classes(validation_data)
</code></pre>

<p>The problem is that the pre-trained model is getting trained on the data and predicts the classes perfectly and gives the confusion matrix as well. As I proceed to fine-tuning the model, I could find that model.predict_classes is not working. Here is the error:</p>

<blockquote>
<pre><code>File ""C:/Users/rajaramans2/codes/untitled12.py"", line 220, in &lt;module&gt;
    y_pred1 = model.predict_classes(validation_data)

AttributeError: 'Model' object has no attribute 'predict_classes'
</code></pre>
</blockquote>

<p>I am confused because, <code>model.predict_classes</code> worked well with the pre-trained model, but not in the fine-tuning stage. The size of validation data is (20,1) and <code>float32</code> type. Any help would be appreciated. </p>",44827730.0,2,1,,2017/6/28 14:59,4.0,2021/8/22 18:46,2017/8/5 20:29,,6626093.0,,7575552.0,,1,31,compiler-errors|keras|prediction,48674,98.1492,,4,attributeerror model object attribute predict class try predict validation data pre train fine tune dl model code follow example available kera blog building image classification model use little data code problem pre trained model get train data predict class perfectly give confusion matrix well proceed fine tune model could find model predict class work error confuse work well pre train model fine tuning stage size validation data type help would appreciate
319,319,58496858,Pytorch RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0,"<p>I use code from <a href=""https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/"" rel=""noreferrer"">here</a> to train a model to predict printed style number from <code>0</code> to <code>9</code>:</p>

<pre><code>idx_to_class = {0: ""0"", 1: ""1"", 2: ""2"", 3: ""3"", 4: ""4"", 5: ""5"", 6: ""6"", 7:""7"", 8: ""8"", 9:""9""}
def predict(model, test_image_name):

    transform = image_transforms['test']

    test_image = Image.open(test_image_name)
    plt.imshow(test_image)

    test_image_tensor = transform(test_image)

    if torch.cuda.is_available():
        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()
    else:
        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)

    with torch.no_grad():
        model.eval()
        # Model outputs log probabilities
        out = model(test_image_tensor)
        ps = torch.exp(out)
        topk, topclass = ps.topk(1, dim=1)
        # print(topclass.cpu().numpy()[0][0])
        print(""Image class:  "", idx_to_class[topclass.cpu().numpy()[0][0]])

predict(model, ""path_of_test_image"")
</code></pre>

<p>But I get an error when try to use <code>predict</code>:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-12-f8636d3ba083&gt;"", line 26, in &lt;module&gt;
    predict(model, ""/home/x/闂佸搫鍊稿ú锕傚Υ?Deep_Learning/pytorch/MNIST/test/2/QQ闂佽鎯屾禍婊兠?0191022093955.png"")

  File ""&lt;ipython-input-12-f8636d3ba083&gt;"", line 9, in predict
    test_image_tensor = transform(test_image)

  File ""/home/x/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py"", line 61, in __call__
    img = t(img)

  File ""/home/x/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py"", line 166, in __call__
    return F.normalize(tensor, self.mean, self.std, self.inplace)

  File ""/home/x/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py"", line 217, in normalize
    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])

RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0
</code></pre>

<p>How could I fix it? Thanks.</p>",58497441.0,1,0,,2019/10/22 3:53,4.0,2021/5/11 5:01,2020/5/27 5:03,,1714410.0,,8410477.0,,1,22,python|image-processing|computer-vision|pytorch,45072,74.0156,,4,pytorch runtimeerror size tensor must match size tensor b non singleton dimension use code train model predict printed style number get error try use could fix thanks
78,78,6699222,When should I use support vector machines as opposed to artificial neural networks?,"<p>I know SVMs are supposedly 'ANN killers' in that they automatically select representation complexity and find a global optimum (see <a href=""http://www.svms.org/anns.html"">here</a> for some SVM praising quotes).</p>

<p>But here is where I'm unclear -- do all of these claims of superiority hold for just the case of a 2 class decision problem or do they go further? (I assume they hold for non-linearly separable classes or else no-one would care) </p>

<p>So a sample of some of the cases I'd like to be cleared up:</p>

<ul>
<li>Are SVMs better than ANNs with many classes? </li>
<li>in an online setting?</li>
<li>What about in a semi-supervised case like reinforcement learning?</li>
<li>Is there a better unsupervised version of SVMs?</li>
</ul>

<p>I don't expect someone to answer all of these lil' subquestions, but rather to give some general bounds for when SVMs are better than the common ANN equivalents (e.g. FFBP, recurrent BP, Boltzmann machines, SOMs, etc.) in practice, and preferably, in theory as well.</p>",6700522.0,5,0,,2011/7/14 20:00,26.0,2019/6/16 12:58,2019/6/16 12:58,,3924118.0,,821806.0,,1,33,machine-learning|neural-network|svm|reinforcement-learning,12661,113.41,,0,use support vector machine oppose artificial neural network know svms supposedly ann killer automatically select representation complexity find global optimum see svm praise quote unclear claim superiority hold case class decision problem go far assume hold non linearly separable class else one would care sample case like clear svms good anns many class online set semi supervise case like reinforcement learning good unsupervised version svms expect someone answer lil subquestions rather give general bound svms good common ann equivalent e g ffbp recurrent bp boltzmann machine som etc practice preferably theory well
489,489,30622805,OpenCL / AMD: Deep Learning,"<p>While ""googl'ing"" and doing some research I were not able to find <strong>any</strong> serious/popular framework/sdk for scientific GPGPU-Computing and <em>OpenCL</em> on <em>AMD</em> hardware. Is there any literature and/or software I missed?</p>

<p>Especially I am interested in <strong>deep learning</strong>.</p>

<p>For all I know <em>deeplearning.net</em> recommends <em>NVIDIA</em> hardware and <em>CUDA</em> frameworks. Additionally all big deep learning frameworks I know, such as <em>Caffe</em>, <em>Theano</em>, <em>Torch</em>, <em>DL4J</em>, ... are focussed on <em>CUDA</em> and do not plan to support <em>OpenCL/AMD</em>.</p>

<p>Furthermore one can find plenty of scientific papers as well as corresponding literature for <em>CUDA</em> based deep learning tasks but nearly nothing for <em>OpenCL/AMD</em> based solutions.</p>

<p>Is there any chance that new or existing scientific frameworks will show up for <em>OpenCL/AMD</em> based solutions in 2015/16?</p>

<p>What is a good start for <em>deep learning</em> with <em>OpenCL/AMD</em>? Any literature? Tutorials? Miscellaneous sources?</p>",30701002.0,8,4,,2015/6/3 14:20,34.0,2019/1/24 2:43,2015/6/5 20:29,,3314143.0,,3314143.0,,1,64,sdk|opencl|neural-network|gpgpu|deep-learning,26851,204.116,2019/1/24 9:25,0,opencl amd deep learning googl ing research able find serious popular framework sdk scientific gpgpu computing opencl amd hardware literature software miss especially interested deep learning know deeplearning net recommends nvidia hardware cuda framework additionally big deep learning frameworks know caffe theano torch dl j focus cuda plan support opencl amd furthermore one find plenty scientific paper well corresponding literature cuda base deep learning task nearly nothing opencl amd base solution chance new exist scientific framework show opencl amd base solution good start deep learning opencl amd literature tutorial miscellaneous source
813,813,53570732,Get single random example from PyTorch DataLoader,"<p>How do I get a single random example from a PyTorch <a href=""https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader"" rel=""nofollow noreferrer""><code>DataLoader</code></a>?</p>
<p>If my <code>DataLoader</code> gives minbatches of multiple images and labels, how do I get a single random image and label?</p>
<p>Note that I don't want a single image and label per minibatch, I want a total of one example.</p>",61389393.0,6,0,,2018/12/1 12:24,2.0,2021/3/15 14:46,2021/3/15 14:27,,9067615.0,,5353461.0,,1,14,python|pytorch,29617,79.6862,,2,get single random example pytorch dataloader get single random example pytorch give minbatches multiple image label get single random image label note want single image label per minibatch want total one example
238,238,44433438,What is the purpose of tf.global_variables_initializer?,"<p>I would like to understand what <code>tf.global_variables_initializer</code> does in a bit more detail.
A <a href=""https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer"" rel=""noreferrer"">sparse description is given here</a>:</p>

<blockquote>
  <p>Returns an Op that initializes global variables.</p>
</blockquote>

<p>But that doesn't really help me. I know that the op is necessary to initialize the graph, but what does that actually mean? Is this the step where the graph is complied? </p>",44434099.0,1,0,,2017/6/8 10:38,15.0,2019/10/27 22:09,2019/10/27 22:09,,3924118.0,,3747801.0,,1,50,tensorflow|deep-learning,33982,79.125,,3,purpose tf global variable initializer would like understand bit detail sparse description give return op initialize global variable really help know op necessary initialize graph actually mean step graph comply
477,477,59560043,What is the difference between model.to(device) and model=model.to(device)?,"<p>Suppose the model is originally stored on CPU, and then I want to move it to GPU0, then I can do:</p>

<pre class=""lang-py prettyprint-override""><code>device = torch.device('cuda:0')
model = model.to(device)
# or
model.to(device)
</code></pre>

<p>What is the difference between those two lines?</p>",,2,1,,2020/1/2 7:57,4.0,2020/12/27 6:37,,,,,10150864.0,,1,10,python|pytorch,15584,58.7707,,3,difference model device model model device suppose model originally store cpu want move gpu difference two line
392,392,46774641,What does the parameter retain_graph mean in the Variable's backward() method?,"<p>I'm going through the <a href=""http://pytorch.org/tutorials/advanced/neural_style_tutorial.html#packages"" rel=""noreferrer"">neural transfer pytorch tutorial</a> and am confused about the use of <code>retain_variable</code>(deprecated, now referred to as <code>retain_graph</code>). The code example show:</p>

<pre><code>class ContentLoss(nn.Module):

    def __init__(self, target, weight):
        super(ContentLoss, self).__init__()
        self.target = target.detach() * weight
        self.weight = weight
        self.criterion = nn.MSELoss()

    def forward(self, input):
        self.loss = self.criterion(input * self.weight, self.target)
        self.output = input
        return self.output

    def backward(self, retain_variables=True):
        #Why is retain_variables True??
        self.loss.backward(retain_variables=retain_variables)
        return self.loss
</code></pre>

<p>From <a href=""http://pytorch.org/docs/master/autograd.html?highlight=backward#torch.autograd.backward"" rel=""noreferrer"">the documentation</a></p>

<blockquote>
  <p>retain_graph (bool, optional) 闂?If False, the graph used to compute
  the grad will be freed. Note that in nearly all cases setting this
  option to True is not needed and often can be worked around in a much
  more efficient way. Defaults to the value of create_graph.</p>
</blockquote>

<p>So by setting <code>retain_graph= True</code>, we're not freeing the memory allocated for the graph on the backward pass. What is the advantage of keeping this memory around, why do we need it?</p>",47174709.0,2,0,,2017/10/16 16:11,29.0,2018/9/5 9:39,2017/12/27 8:11,,2002387.0,,1646823.0,,1,55,neural-network|conv-neural-network|backpropagation|pytorch|automatic-differentiation,35993,146.225,,3,parameter retain graph mean variable backward method go neural transfer pytorch tutorial confuse use deprecate refer code example show documentation retain graph bool optional false graph use compute grad free note nearly case set option true need often work around much efficient way default value create graph set free memory allocate graph backward pas advantage keep memory around need
817,817,53654310,What is the difference between UpSampling2D and Conv2DTranspose functions in keras?,"<p>Here in this code <code>UpSampling2D</code> and <code>Conv2DTranspose</code> seem to be used interchangeably. I want to know why this is happening. </p>

<pre><code># u-net model with up-convolution or up-sampling and weighted binary-crossentropy as loss func

from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout
from keras.optimizers import Adam
from keras.utils import plot_model
from keras import backend as K

def unet_model(n_classes=5, im_sz=160, n_channels=8, n_filters_start=32, growth_factor=2, upconv=True,
               class_weights=[0.2, 0.3, 0.1, 0.1, 0.3]):
    droprate=0.25
    n_filters = n_filters_start
    inputs = Input((im_sz, im_sz, n_channels))
    #inputs = BatchNormalization()(inputs)
    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    #pool1 = Dropout(droprate)(pool1)

    n_filters *= growth_factor
    pool1 = BatchNormalization()(pool1)
    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    pool2 = Dropout(droprate)(pool2)

    n_filters *= growth_factor
    pool2 = BatchNormalization()(pool2)
    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    pool3 = Dropout(droprate)(pool3)

    n_filters *= growth_factor
    pool3 = BatchNormalization()(pool3)
    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool3)
    conv4_0 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_0)
    pool4_1 = MaxPooling2D(pool_size=(2, 2))(conv4_0)
    pool4_1 = Dropout(droprate)(pool4_1)

    n_filters *= growth_factor
    pool4_1 = BatchNormalization()(pool4_1)
    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_1)
    conv4_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv4_1)
    pool4_2 = MaxPooling2D(pool_size=(2, 2))(conv4_1)
    pool4_2 = Dropout(droprate)(pool4_2)

    n_filters *= growth_factor
    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool4_2)
    conv5 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv5)

    n_filters //= growth_factor
    if upconv:
        up6_1 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv5), conv4_1])
    else:
        up6_1 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4_1])
    up6_1 = BatchNormalization()(up6_1)
    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_1)
    conv6_1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_1)
    conv6_1 = Dropout(droprate)(conv6_1)

    n_filters //= growth_factor
    if upconv:
        up6_2 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_1), conv4_0])
    else:
        up6_2 = concatenate([UpSampling2D(size=(2, 2))(conv6_1), conv4_0])
    up6_2 = BatchNormalization()(up6_2)
    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up6_2)
    conv6_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv6_2)
    conv6_2 = Dropout(droprate)(conv6_2)

    n_filters //= growth_factor
    if upconv:
        up7 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv6_2), conv3])
    else:
        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6_2), conv3])
    up7 = BatchNormalization()(up7)
    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up7)
    conv7 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv7)
    conv7 = Dropout(droprate)(conv7)

    n_filters //= growth_factor
    if upconv:
        up8 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])
    else:
        up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2])
    up8 = BatchNormalization()(up8)
    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up8)
    conv8 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv8)
    conv8 = Dropout(droprate)(conv8)

    n_filters //= growth_factor
    if upconv:
        up9 = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])
    else:
        up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1])
    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up9)
    conv9 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv9)

    conv10 = Conv2D(n_classes, (1, 1), activation='sigmoid')(conv9)

    model = Model(inputs=inputs, outputs=conv10)

    def weighted_binary_crossentropy(y_true, y_pred):
        class_loglosses = K.mean(K.binary_crossentropy(y_true, y_pred), axis=[0, 1, 2])
        return K.sum(class_loglosses * K.constant(class_weights))

    model.compile(optimizer=Adam(), loss=weighted_binary_crossentropy)
    return model
</code></pre>",53655426.0,1,0,,2018/12/6 15:08,9.0,2019/10/28 11:33,,,,,6143004.0,,1,37,machine-learning|computer-vision|conv-neural-network|convolution|deconvolution,24462,97.954,,3,difference upsampling conv dtranspose function kera code seem use interchangeably want know happen
79,79,6848828,What is the difference between Q-learning and SARSA?,"<p>Although I know that <a href=""https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action"" rel=""noreferrer"">SARSA</a> is on-policy while <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""noreferrer"">Q-learning</a> is off-policy, when looking at their formulas it's hard (to me) to see any difference between these two algorithms.</p>
<p>According to the book <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""noreferrer"">Reinforcement Learning: An Introduction</a> (by Sutton and Barto). In the SARSA algorithm, given a policy, the corresponding action-value function Q (in the state s and action a, at timestep t), i.e. Q(s<sub>t</sub>, a<sub>t</sub>), can be updated as follows</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) + 濞?(r<sub>t</sub> + 缂?Q(s<sub>t+1</sub>, a<sub>t+1</sub>) - Q(s<sub>t</sub>, a<sub>t</sub>))</p>
</blockquote>
<p>On the other hand, the update step for the Q-learning algorithm is the following</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = Q(s<sub>t</sub>, a<sub>t</sub>) + 濞?(r<sub>t</sub> + 缂?max<sub>a</sub> Q(s<sub>t+1</sub>, a) - Q(s<sub>t</sub>, a<sub>t</sub>))</p>
</blockquote>
<p>which can also be written as</p>
<blockquote>
<p>Q(s<sub>t</sub>, a<sub>t</sub>) = (1 - 濞? * Q(s<sub>t</sub>, a<sub>t</sub>) + 濞?* (r<sub>t</sub> + 缂?max<sub>a</sub> Q(s<sub>t+1</sub>, a))</p>
</blockquote>
<p>where 缂?(gamma) is the discount factor and r<sub>t</sub> is the reward received from the environment at timestep t.</p>
<p>Is the difference between these two algorithms the fact that SARSA only looks up the next policy value while Q-learning looks up the next <em>maximum</em> policy value?</p>
<p><strong>TLDR (and my own answer)</strong></p>
<p>Thanks to all those answering this question since I first asked it. I've made a <a href=""http://alexge233.github.io/relearn/"" rel=""noreferrer"">github repo</a> playing with Q-Learning and empirically understood what the difference is. It all amounts to how <em><strong>you select your next best action</strong></em>, which from an algorithmic standpoint can be a <em>mean</em>, <em>max</em> or <em>best</em> action depending on how you chose to implement it.</p>
<p>The other main difference is <em>when</em> this selection is happening (e.g., <em>online</em> vs <em>offline</em>) and how/why that affects learning. If you are reading this in 2019 and are more of a hands-on person, playing with a RL toy problem is probably the best way to understand the differences.</p>
<p>One last <strong>important</strong> note is that both Suton &amp; Barto as well as Wikipedia often have <em>mixed, confusing</em> or <em>wrong</em> formulaic representations with regards to the <em>next state best/max action and reward</em>:</p>
<blockquote>
<p>r(t+1)</p>
</blockquote>
<p>is in fact</p>
<blockquote>
<p>r(t)</p>
</blockquote>
<p>Hope this helps anyone ever getting stuck at this.</p>",6852935.0,6,0,,2011/7/27 17:46,63.0,2019/10/3 6:41,2020/6/20 9:12,,-1.0,,499699.0,,1,105,artificial-intelligence|reinforcement-learning|q-learning|sarsa,48115,340.7290000000001,,0,difference q learning sarsa although know sarsa policy q learning policy look formula hard see difference two algorithm accord book reinforcement learn introduction sutton barto sarsa algorithm give policy corresponding action value function q state action timestep e q st update follow q st q st rt q st q st hand update step q learn algorithm following q st q st rt maxa q st q st also write q st q st rt maxa q st gamma discount factor rt reward receive environment timestep difference two algorithms fact sarsa look next policy value q learn look next maximum policy value tldr answer thanks answer question since first ask make github repo play q learning empirically understand difference amount select next best action algorithmic standpoint mean max best action depend choose implement main difference selection happen e g online vs offline affect learn read hand person play rl toy problem probably best way understand difference one last important note suton barto well wikipedia often mixed confusing wrong formulaic representation regard next state best max action reward r fact r hope help anyone ever get stick
310,310,58047454,How to fix ' module 'keras.backend.tensorflow_backend' has no attribute '_is_tf_1'',"<p>While training the yolov3 framework, there's always this module error</p>

<p>I have tried reinstalling keras and tensorflow, and the version of keras is 2.3.0 and the version of tensorflow is 1.14.0.</p>

<pre><code>Traceback (most recent call last):
  File ""train.py"", line 6, in &lt;module&gt;
    import keras.backend as K
  File ""F:\Anacoda\lib\site-packages\keras\__init__.py"", line 3, in &lt;module&gt;
    from . import utils
  File ""F:\Anacoda\lib\site-packages\keras\utils\__init__.py"", line 27, in &lt;module&gt;
    from .multi_gpu_utils import multi_gpu_model
  File ""F:\Anacoda\lib\site-packages\keras\utils\multi_gpu_utils.py"", line 7, in &lt;module&gt;
    from ..layers.merge import concatenate
  File ""F:\Anacoda\lib\site-packages\keras\layers\__init__.py"", line 4, in &lt;module&gt;
    from ..engine.base_layer import Layer
  File ""F:\Anacoda\lib\site-packages\keras\engine\__init__.py"", line 8, in &lt;module&gt;
    from .training import Model
  File ""F:\Anacoda\lib\site-packages\keras\engine\training.py"", line 21, in &lt;module&gt;
    from . import training_arrays
  File ""F:\Anacoda\lib\site-packages\keras\engine\training_arrays.py"", line 14, in &lt;module&gt;
    from .. import callbacks as cbks
  File ""F:\Anacoda\lib\site-packages\keras\callbacks\__init__.py"", line 19, in &lt;module&gt;
    if K.backend() == 'tensorflow' and not K.tensorflow_backend._is_tf_1():
AttributeError: module 'keras.backend.tensorflow_backend' has no attribute '_is_tf_1'
</code></pre>",58639231.0,6,0,,2019/9/22 8:34,,2021/7/14 6:30,,,,,12102026.0,,1,12,python-3.x|tensorflow|keras,44426,50.9905,,4,fix module kera backend tensorflow backend attribute tf train yolov framework always module error try reinstall kera tensorflow version kera version tensorflow
418,418,47594861,Predicting a multiple forward time step of a time series using LSTM,"<p>I want to predict certain values that are weekly predictable (low SNR). I need to predict the whole time series of a year formed by the weeks of the year (52 values - Figure 1)</p>

<p><a href=""https://i.stack.imgur.com/5qMLH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5qMLH.png"" alt=""Figure 1: Year time series by week""></a></p>

<p>My first idea was to develop a many-to-many LSTM model (Figure 2) using Keras over TensorFlow. I'm training the model with a 52 input layer (the given time series of previous year) and 52 predicted output layer (the time series of next year). The shape of train_X is (X_examples, 52, 1), in other words, X_examples to train, 52 timesteps of 1 feature each. I understand that Keras will consider the 52 inputs as a time series of the same domain. The shape of the train_Y are the same (y_examples, 52, 1). 
I added a TimeDistributed layer. My thought was that the algorithm will predict the values as a time series instead of isolated values (am I correct?)</p>

<p>The model's code in Keras is:</p>

<pre><code>y = y.reshape(y.shape[0], 52, 1)
X = X.reshape(X.shape[0], 52, 1)
# design network
model = Sequential()
model.add(LSTM(n_neurons, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(TimeDistributed(Dense(1)))
model.compile(loss='mean_squared_error', optimizer='adam')
# fit network
model.fit(X, y, epochs=n_epochs, batch_size=n_batch, verbose=2)
</code></pre>

<p><a href=""https://i.stack.imgur.com/jZfab.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jZfab.jpg"" alt=""Figure 2: Many-to-many LSTM architecture""></a></p>

<p>The problem is that the algorithm is not learning the example. It is predicting values very similar to the attributes' values. Am I modeling the problem correctly? </p>

<p>Second question:
Another idea is to train the algorithm with 1 input and 1 output, but then during the test how will I predict the whole 2015 time series without looking to the '1 input'? The test data will have a different shape than the training data.</p>",,3,3,,2017/12/1 13:49,16.0,2019/4/30 17:41,2018/12/18 14:14,,374437.0,,1445225.0,,1,23,time-series|keras|lstm|prediction|forward,15140,67.5205,,3,predict multiple forward time step time series use lstm want predict certain value weekly predictable low snr need predict whole time series year form week year value figure first idea develop many many lstm model figure use kera tensorflow train model input layer give time series previous year predict output layer time series next year shape train x x example word x example train timesteps feature understand kera consider input time series domain shape train example add timedistributed layer thought algorithm predict value time series instead isolated value correct model code kera problem algorithm learn example predict value similar attribute value model problem correctly second question another idea train algorithm input output test predict whole time series without look input test data different shape training data
672,672,37935920,quantile normalization on pandas dataframe,"<p>Simply speaking, how to apply quantile normalization on a large Pandas dataframe (probably 2,000,000 rows) in Python?</p>

<p>PS. I know that there is a package named rpy2 which could run R in subprocess, using quantile normalize in R. But the truth is that R cannot compute the     correct result when I use the data set as below:</p>

<pre><code>5.690386092696389541e-05,2.051450375415418849e-05,1.963190184049079707e-05,1.258362869906251862e-04,1.503352476021528139e-04,6.881341586355676286e-06
8.535579139044583634e-05,5.128625938538547123e-06,1.635991820040899643e-05,6.291814349531259308e-05,3.006704952043056075e-05,6.881341586355676286e-06
5.690386092696389541e-05,2.051450375415418849e-05,1.963190184049079707e-05,1.258362869906251862e-04,1.503352476021528139e-04,6.881341586355676286e-06
2.845193046348194770e-05,1.538587781561563968e-05,2.944785276073619561e-05,4.194542899687506431e-05,6.013409904086112150e-05,1.032201237953351358e-05
</code></pre>

<p>Edit:</p>

<p>What I want:</p>

<p>Given the data shown above, how to apply quantile normalization following steps in <a href=""https://en.wikipedia.org/wiki/Quantile_normalization"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Quantile_normalization</a>.</p>

<p>I found a piece of code in Python declaring that it could compute the quantile normalization:</p>

<pre><code>import rpy2.robjects as robjects
import numpy as np
from rpy2.robjects.packages import importr
preprocessCore = importr('preprocessCore')


matrix = [ [1,2,3,4,5], [1,3,5,7,9], [2,4,6,8,10] ]
v = robjects.FloatVector([ element for col in matrix for element in col ])
m = robjects.r['matrix'](v, ncol = len(matrix), byrow=False)
Rnormalized_matrix = preprocessCore.normalize_quantiles(m)
normalized_matrix = np.array( Rnormalized_matrix)
</code></pre>

<p>The code works fine with the sample data used in the code, however when I test it with the data given above the result went wrong.</p>

<p>Since ryp2 provides an interface to run R in python subprocess, I test it again in R directly and the result was still wrong. As a result I think the reason is that the method in R is wrong.</p>",37957466.0,8,4,,2016/6/21 5:01,6.0,2021/5/19 5:11,2016/6/21 6:03,,5776838.0,,5776838.0,,1,15,python|deep-learning|data-science,10605,74.102,,3,quantile normalization panda dataframe simply speak apply quantile normalization large panda dataframe probably row python p know package name rpy could run r subprocess use quantile normalize r truth r compute correct result use data set edit want give data show apply quantile normalization follow step find piece code python declare could compute quantile normalization code work fine sample data use code however test data give result go wrong since ryp provide interface run r python subprocess test r directly result still wrong result think reason method r wrong
792,792,52476191,What does initial_epoch in Keras mean?,"<p>I'm a little bit confused about <code>initial_epoch</code> value in <code>fit</code> and <code>fit_generator</code> methods. Here is the <a href=""https://keras.io/api/models/model_training_apis/"" rel=""nofollow noreferrer"">doc</a>:</p>
<blockquote>
<p><strong>initial_epoch</strong>: Integer. Epoch at which to start training (useful for resuming a previous training run).</p>
</blockquote>
<p>I understand, it is not useful if you start training from scratch. It is useful if you trained your dataset and want to improve accuracy or other values (<em>correct me if I'm wrong</em>). But I'm not sure what it really does.</p>
<p>So after all this, I have 2 questions:</p>
<ol>
<li>What does <code>initial_epoch</code> do and what is it for?</li>
<li>When can I use <code>initial_epoch</code>?</li>
</ol>
<ul>
<li>When I change my dataset?</li>
<li>When I change the learning rate, optimizer or loss function?</li>
<li>Both of them?</li>
</ul>",52478034.0,3,0,,2018/9/24 9:23,7.0,2021/5/27 5:37,2021/2/1 10:15,,11256688.0,,2057653.0,,1,21,machine-learning|keras|deep-learning,10607,69.7024,,4,initial epoch kera mean little bit confuse value method doc initial epoch integer epoch start training useful resume previous training run understand useful start train scratch useful train dataset want improve accuracy value correct wrong sure really question use change dataset change learning rate optimizer loss function
202,202,43743593,Keras: How to get layer shapes in a Sequential model,"<p>I would like to access the layer size of all the layers in a <code>Sequential</code> Keras model. My code:</p>

<pre><code>model = Sequential()
model.add(Conv2D(filters=32, 
               kernel_size=(3,3), 
               input_shape=(64,64,3)
        ))
model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2)))
</code></pre>

<p>Then I would like some code like the following to work</p>

<pre><code>for layer in model.layers:
    print(layer.get_shape())
</code></pre>

<p>.. but it doesn't. I get the error: <code>AttributeError: 'Conv2D' object has no attribute 'get_shape'</code></p>",54338259.0,3,0,,2017/5/2 17:08,4.0,2020/3/5 11:58,,,,,3747801.0,,1,34,python|tensorflow|deep-learning|keras|theano,59508,115.498,,3,keras get layer shape sequential model would like access layer size layer keras model code would like code like follow work get error
176,176,43178668,record the computation time for each epoch in Keras during model.fit(),"<p>I want to compare the computation time between different models.
During the fit the computation time per epoch is printed to the console.</p>

<pre><code>Epoch 5/5
160000/160000 [==============================] - **10s** ......
</code></pre>

<p>I'm looking for a way to store these times in a similar way to the model metrics that are saved in each epoch and avaliable through the history object.</p>",43186440.0,3,0,,2017/4/3 7:16,10.0,2020/7/25 8:33,2017/7/12 21:00,,5974433.0,,6293886.0,,1,31,python|machine-learning|neural-network|deep-learning|keras,16143,98.4319,,5,record computation time epoch kera model fit want compare computation time different model fit computation time per epoch print console look way store time similar way model metric save epoch avaliable history object
166,166,42943291,What does Keras.io.preprocessing.sequence.pad_sequences do?,"<p>The Keras documentation could be improved here. After reading through this, I still do not understand what this does exactly: <a href=""https://keras.io/preprocessing/sequence/"" rel=""noreferrer"">Keras.io.preprocessing.sequence.pad_sequences</a></p>

<p>Could someone illuminate what this function does, and ideally provide an example?</p>",42964543.0,2,0,,2017/3/22 5:17,10.0,2021/4/21 10:35,2019/10/7 20:07,,914689.0,,914689.0,,1,46,python|deep-learning|keras,34778,103.565,,3,keras io preprocessing sequence pad sequence kera documentation could improve read still understand exactly keras io preprocessing sequence pad sequence could someone illuminate function ideally provide example
116,116,41789133,What are c_state and m_state in Tensorflow LSTM?,"<p>Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init:</p>

<pre><code>tf.nn.rnn_cell.LSTMCell.__call__(inputs, state, scope=None)
</code></pre>

<p>where <code>state</code> is as follows:</p>

<blockquote>
  <p>state: if state_is_tuple is False, this must be a state Tensor, 2-D, batch x state_size. If state_is_tuple is True, this must be a tuple of state Tensors, both 2-D, with column sizes c_state and m_state.</p>
</blockquote>

<p>What aare <code>c_state</code> and <code>m_state</code> and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation.</p>

<p><a href=""https://web.archive.org/web/20170223030652/https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods"" rel=""noreferrer"">Here is a link to that page in the documentation.</a></p>",41928428.0,4,0,,2017/1/22 9:04,11.0,2018/1/12 2:28,2018/1/12 2:28,,1677912.0,,5299052.0,,1,17,python|tensorflow|deep-learning|lstm,10163,64.6281,,3,c state state tensorflow lstm tensorflow r documentation tf nn rnn cell lstmcell describe init follow state state tuple false must state tensor batch x state size state tuple true must tuple state tensor column size c state state aare fit lstms find reference anywhere documentation link page documentation
201,201,43728235,What is the difference between Keras' MaxPooling1D and GlobalMaxPooling1D functions?,"<p>Both MaxPooling1D and GlobalMaxPooling1D are described as a max pooling operation for temporal data. </p>

<p><code>keras.layers.pooling.MaxPooling1D(pool_size=2, strides=None, padding='valid')</code></p>

<p>I understand that GlobalMaxPooling1D takes no input parameters. 
<code>keras.layers.pooling.GlobalMaxPooling1D()</code></p>

<p>I would just like to visually understand how the two of them differ in the way they work?</p>",43730861.0,2,0,,2017/5/2 0:12,20.0,2019/11/22 16:31,,,,,5755192.0,,1,51,keras|max-pooling,32996,121.474,,3,difference kera maxpooling globalmaxpooling function maxpooling globalmaxpooling describe max pooling operation temporal data understand globalmaxpooling take input parameter would like visually understand two differ way work
768,768,51743214,Is .data still useful in pytorch?,"<p>I'm new to pytorch. I read much pytorch code which heavily uses tensor's <code>.data</code> member. But I search <code>.data</code> in the official document and Google, finding little. I guess <code>.data</code> contains the data in the tensor, but I don't know when we need it and when not? </p>",51744091.0,2,0,,2018/8/8 9:31,8.0,2021/5/27 6:35,2018/8/8 12:10,,1714410.0,,7850499.0,,1,42,python|version|pytorch|tensor,10727,81.9219,,3,data still useful pytorch new pytorch read much pytorch code heavily use tensor member search official document google find little guess contain data tensor know need
533,533,35968973,Keras IndexError: indices are out-of-bounds,"<p>I'm new to Keras and im trying to do Binary MLP on a dataset, and keep getting indices out of bounds with no idea why.</p>

<pre class=""lang-py prettyprint-override""><code>from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD

model = Sequential()
model.add(Dense(64, input_dim=20, init='uniform', activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
          optimizer='rmsprop')
model.fit(trainx, trainy, nb_epoch=20, batch_size=16) # THROWS INDICES ERROR
</code></pre>

<p>Error:</p>

<pre class=""lang-py prettyprint-override""><code>model.fit(trainx, trainy, nb_epoch=20, batch_size=16)

Epoch 1/20
Traceback (most recent call last):

  File ""&lt;ipython-input-6-c81bd7606eb0&gt;"", line 1, in &lt;module&gt;
model.fit(trainx, trainy, nb_epoch=20, batch_size=16)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 646, in fit
shuffle=shuffle, metrics=metrics)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 271, in _fit
ins_batch = slice_X(ins, batch_ids)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 65, in slice_X
return [x[start] for x in X]

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\keras\models.py"", line 65, in &lt;listcomp&gt;
return [x[start] for x in X]

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\frame.py"", line 1963, in __getitem__
return self._getitem_array(key)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\frame.py"", line 2008, in _getitem_array
return self.take(indexer, axis=1, convert=True)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\generic.py"", line 1371, in take
convert=True, verify=True)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\internals.py"", line 3619, in take
indexer = maybe_convert_indices(indexer, n)

  File ""C:\Users\Thiru\Anaconda3\lib\site-packages\pandas\core\indexing.py"", line 1750, in maybe_convert_indices
raise IndexError(""indices are out-of-bounds"")

IndexError: indices are out-of-bounds
</code></pre>

<p>Does anyone have any idea why this is happening? Im able to run other models just fine</p>",36798335.0,3,1,,2016/3/13 10:10,3.0,2018/5/4 11:21,2018/5/2 13:05,,7483494.0,,5766416.0,,1,11,neural-network|theano|keras,9062,67.4289,,4,kera indexerror index bound new keras im try binary mlp dataset keep get index bound idea error anyone idea happen im able run model fine
287,287,56360644,Pytorch RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CUDAType instead,"<p>I am trying to re-execute a GitHub project on my computer for recommendation using embedding, the goal is to first embed the user and item present in the movieLens dataset, and then use the inner product to predict a rating, when I finished the integration of all components, I got an error in the training.</p>

<p>Code:</p>

<pre class=""lang-py prettyprint-override""><code>from lightfm.datasets import fetch_movielens
movielens = fetch_movielens()
ratings_train, ratings_test = movielens['train'], movielens['test']
def _binarize(dataset):

    dataset = dataset.copy()

    dataset.data = (dataset.data &gt;= 0.0).astype(np.float32)
    dataset = dataset.tocsr()
    dataset.eliminate_zeros()

    return dataset.tocoo()
train, test = _binarize(movielens['train']), _binarize(movielens['test'])
class ScaledEmbedding(nn.Embedding):
    """""" Change the scale from normal to [0,1/embedding_dim] """"""
    def reset_parameters(self):
        self.weight.data.normal_(0, 1.0 / self.embedding_dim)
        if self.padding_idx is not None:
            self.weight.data[self.padding_idx].fill_(0)


class ZeroEmbedding(nn.Embedding):

    def reset_parameters(self):
        self.weight.data.zero_()
        if self.padding_idx is not None:
            self.weight.data[self.padding_idx].fill_(0)
class BilinearNet(nn.Module):

    def __init__(self, num_users, num_items, embedding_dim, sparse=False):
        super().__init__()

        self.embedding_dim = embedding_dim

        self.user_embeddings = ScaledEmbedding(num_users, embedding_dim,
                                               sparse=sparse)
        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,
                                               sparse=sparse)
        self.user_biases = ZeroEmbedding(num_users, 1, sparse=sparse)
        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse)

    def forward(self, user_ids, item_ids):

        user_embedding = self.user_embeddings(user_ids)
        item_embedding = self.item_embeddings(item_ids)

        user_embedding = user_embedding.view(-1, self.embedding_dim)
        item_embedding = item_embedding.view(-1, self.embedding_dim)

        user_bias = self.user_biases(user_ids).view(-1, 1)
        item_bias = self.item_biases(item_ids).view(-1, 1)

        dot = (user_embedding * item_embedding).sum(1)

        return dot + user_bias + item_bias

def pointwise_loss(net,users, items, ratings, num_items):

    negatives = Variable(
            torch.from_numpy(np.random.randint(0,
                                               num_items,
                                                  len(users))).cuda()
    )

    positives_loss = (1.0 - torch.sigmoid(net(users, items)))
    negatives_loss = torch.sigmoid(net(users, negatives))

    return torch.cat([positives_loss, negatives_loss]).mean()

embedding_dim = 128
minibatch_size = 1024
n_iter = 10
l2=0.0
sparse = True

num_users, num_items = train.shape
net = BilinearNet(num_users,
                            num_items,
                            embedding_dim,
                            sparse=sparse).cuda()

optimizer = optim.Adagrad(net.parameters(),
                              weight_decay=l2)
for epoch_num in range(n_iter):

    users, items, ratings = shuffle(train)

    user_ids_tensor = torch.from_numpy(users).cuda()
    item_ids_tensor = torch.from_numpy(items).cuda()
    ratings_tensor = torch.from_numpy(ratings).cuda()

    epoch_loss = 0.0

    for (batch_user,
         batch_item,
         batch_ratings) in zip(_minibatch(user_ids_tensor,
                                          minibatch_size),
                               _minibatch(item_ids_tensor,
                                          minibatch_size),
                               _minibatch(ratings_tensor,
                                          minibatch_size)):

        user_var = Variable(batch_user)
        item_var = Variable(batch_item)
        ratings_var = Variable(batch_ratings)
        optimizer.zero_grad()
        loss = pointwise_loss(net,user_var, item_var, ratings_var, num_items)
        epoch_loss += loss.data[0]
        loss.backward()
        optimizer.step()
        print('Epoch {}: loss {}'.format(epoch_num, epoch_loss))
</code></pre>

<p>Error:</p>

<pre><code>RuntimeError Traceback (most recent call last) &lt;ipython-input-87-dcd04440363f&gt; in &lt;module&gt;()
             22         ratings_var = Variable(batch_ratings)
             23         optimizer.zero_grad()
        ---&gt; 24         loss = pointwise_loss(net,user_var, item_var, ratings_var, num_items)
             25         epoch_loss += loss.data[0]
             26         loss.backward()

        &lt;ipython-input-86-679e10f637a5&gt; in pointwise_loss(net, users, items, ratings, num_items)
              8 
              9     positives_loss = (1.0 - torch.sigmoid(net(users, items)))
        ---&gt; 10     negatives_loss = torch.sigmoid(net(users, negatives))
             11 
             12     return torch.cat([positives_loss, negatives_loss]).mean()

        ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in
        __call__(self, *input, **kwargs)
            491             result = self._slow_forward(*input, **kwargs)
            492         else:
        --&gt; 493             result = self.forward(*input, **kwargs)
            494         for hook in self._forward_hooks.values():
            495             hook_result = hook(self, input, result)

        &lt;ipython-input-58-3946abf81d81&gt; in forward(self, user_ids, item_ids)
             16 
             17         user_embedding = self.user_embeddings(user_ids)
        ---&gt; 18         item_embedding = self.item_embeddings(item_ids)
             19 
             20         user_embedding = user_embedding.view(-1, self.embedding_dim)

        ~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in
        __call__(self, *input, **kwargs)
            491             result = self._slow_forward(*input, **kwargs)
            492         else:
        --&gt; 493             result = self.forward(*input, **kwargs)
            494         for hook in self._forward_hooks.values():
            495             hook_result = hook(self, input, result)

        ~\Anaconda3\lib\site-packages\torch\nn\modules\sparse.py in forward(self, input)
            115         return F.embedding(
            116             input, self.weight, self.padding_idx, self.max_norm,
        --&gt; 117             self.norm_type, self.scale_grad_by_freq, self.sparse)
            118 
            119     def extra_repr(self):

        ~\Anaconda3\lib\site-packages\torch\nn\functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)    1504         # remove once script supports set_grad_enabled    1505        
        _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
        -&gt; 1506     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)    1507     1508 

        RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CUDAType instead (while checking arguments for embedding)
</code></pre>

<p>can anyone help me please ?</p>",,3,0,,2019/5/29 12:27,6.0,2021/5/31 20:08,2019/5/29 16:40,,681865.0,,11450077.0,,1,24,python-3.x|pytorch|torch|embedding,21178,81.7035,,4,pytorch runtimeerror expect tensor argument index scalar type long get cudatype instead try execute github project computer recommendation use embed goal first embed user item present movielens dataset use inner product predict rating finish integration component get error training code error anyone help please
365,365,46036522,Defining model in keras (include_top = True),"<p>Can somebody tell me what include_top= True means when defining a model in keras?</p>

<p>I read the meaning of this line in Keras Documentation. It says include_top: whether to include the fully-connected layer at the top of the network.</p>

<p>I am still looking for an intuitive explanation for this line of code. </p>

<pre><code>ResNet50(include_top=True)
</code></pre>

<p>Thanks!</p>",46040835.0,2,0,,2017/9/4 11:55,5.0,2019/7/20 21:40,,,,,4724057.0,,1,19,python|neural-network|keras|convolution,13662,81.1421,,3,define model keras include top true somebody tell include top true mean define model kera read meaning line kera documentation say include top whether include fully connect layer top network still look intuitive explanation line code thanks
458,458,21608025,How to set up theano config,"<p>I'm new to Theano.
Trying to set up a config file.</p>

<p>First of all, I notice that I have no .theanorc file:</p>

<ol>
<li><code>locate .theanorc</code> - returns nothing</li>
<li><code>echo $THEANORC</code> - returns nothing</li>
<li><code>theano.test()</code> - passes ok</li>
</ol>

<p>I'm guessing some default configuration was created wen i installed theano. Where is it?</p>",21703319.0,5,0,,2014/2/6 16:07,5.0,2017/12/3 13:47,,,,,1724926.0,,1,37,theano,44701,115.601,,1,set theano config new theano try set config file first notice theanorc file return nothing return nothing pass ok guess default configuration create wen instal theano
608,608,50283844,"In Keras, how to get the layer name associated with a ""Model"" object contained in my model?","<p>I built a Sequential model with the VGG16 network at the initial base, for example: </p>



<pre class=""lang-python prettyprint-override""><code>from keras.applications import VGG16
conv_base = VGG16(weights='imagenet',
                  # do not include the top, fully-connected Dense layers 
                  include_top=False,
                  input_shape=(150, 150, 3))

from keras import models
from keras import layers

model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
# the 3 corresponds to the three output classes
model.add(layers.Dense(3, activation='sigmoid'))
</code></pre>

<p>My model looks like this: </p>

<pre class=""lang-python prettyprint-override""><code>model.summary()
</code></pre>

<blockquote>
  <hr>

<pre class=""lang-python prettyprint-override""><code>Layer (type)                 Output Shape              Param #   
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688  
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0         
_________________________________________________________________
dense_7 (Dense)              (None, 256)               2097408   
_________________________________________________________________
dense_8 (Dense)              (None, 3)                 771       
=================================================================
Total params: 16,812,867
Trainable params: 16,812,867
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</blockquote>

<p>Now, I want to get the layer names associated with the vgg16 Model portion of my network.  I.e. something like: </p>

<pre class=""lang-python prettyprint-override""><code>layer_name = 'block3_conv1'
filter_index = 0

layer_output = model.get_layer(layer_name).output
loss = K.mean(layer_output[:, :, :, filter_index])
</code></pre>

<p>However, since the vgg16 convolutional is shown as a Model and it's layers are not being exposed, I get the error: </p>

<blockquote>
  <p>ValueError: No such layer: block3_conv1</p>
</blockquote>

<p>How do I do this? </p>",50284846.0,4,0,,2018/5/11 1:48,6.0,2020/5/15 16:30,2018/5/11 7:07,,8563649.0,,4521204.0,,1,36,keras|keras-layer|convolutional-neural-network,62792,129.99200000000002,,5,kera get layer name associate model object contain model build sequential model vgg network initial base example model look like want get layer names associate vgg model portion network e something like however since vgg convolutional show model layer expose get error valueerror layer block conv
659,659,37370015,What is the difference between value iteration and policy iteration?,"<p>In reinforcement learning, what is the difference between <em>policy iteration</em> and <em>value iteration</em>? </p>

<p>As much as I understand, in value iteration, you use the Bellman equation to solve for the optimal policy, whereas, in policy iteration, you randomly select a policy 闁? and find the reward of that policy. </p>

<p>My doubt is that if you are selecting a random policy 闁?in PI, how is it guaranteed to be the optimal policy, even if we are choosing several random policies.</p>",,5,2,,2016/5/22 2:43,65.0,2020/12/29 22:26,2018/7/27 12:35,,2360798.0,,4622664.0,,1,116,machine-learning|reinforcement-learning|markov-models|value-iteration,76935,385.544,2021/2/11 13:08,0,difference value iteration policy iteration reinforcement learn difference policy iteration value iteration much understand value iteration use bellman equation solve optimal policy whereas policy iteration randomly select policy find reward policy doubt select random policy pi guarantee optimal policy even choose several random policy
604,604,50052295,How do you load MNIST images into Pytorch DataLoader?,"<p>The pytorch tutorial for data loading and processing is quite specific to one example, could someone help me with what the function should look like for a more generic simple loading of images?</p>

<p>Tutorial: <a href=""http://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""noreferrer"">http://pytorch.org/tutorials/beginner/data_loading_tutorial.html</a></p>

<p>My Data:</p>

<p>I have the MINST dataset as jpg's in the following folder structure. (I know I can just use the dataset class, but this is purely to see how to load simple images into pytorch without csv's or complex features).</p>

<p>The folder name is the label and the images are 28x28 png's in greyscale, no transformations required.</p>

<pre><code>data
    train
        0
            3.png
            5.png
            13.png
            23.png
            ...
        1
            3.png
            10.png
            11.png
            ...
        2
            4.png
            13.png
            ...
        3
            8.png
            ...
        4
            ...
        5
            ...
        6
            ...
        7
            ...
        8
            ...
        9
            ...
</code></pre>",51698037.0,2,0,,2018/4/26 21:46,19.0,2021/2/11 11:19,2020/6/26 17:34,,5884955.0,,5149399.0,,1,22,python|pytorch,44144,80.3795,,2,load mnist image pytorch dataloader pytorch tutorial data load processing quite specific one example could someone help function look like generic simple loading image tutorial data minst dataset jpg following folder structure know use dataset class purely see load simple image pytorch without csv complex feature folder name label image x png greyscale transformation require
759,759,51541610,Why is TensorFlow's `tf.data` package slowing down my code?,"<p>I'm just learning to use TensorFlow's <code>tf.data</code> API, and I've found that it is slowing my code down a lot, measured in time per epoch. This is the opposite of what it's supposed to do, I thought. I wrote a simple linear regression program to test it out. </p>

<p><strong>Tl;Dr</strong>: With 100,000 training data, <code>tf.data</code> slows time per epoch down by about a factor of ten, if you're using full batch training. Worse if you use smaller batches. The opposite is true with 500 training data.</p>

<p><strong>My question:</strong> What is going on? Is my implementation flawed? Other sources I've read have <code>tf.data</code> improving speeds by about 30%.</p>

<pre><code>import tensorflow as tf 
import numpy as np
import timeit

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.logging.set_verbosity(tf.logging.ERROR)

n_epochs = 10
input_dimensions_list = [10]

def function_to_approximate(x):
    return np.dot(x, random_covector).astype(np.float32) + np.float32(.01) * np.random.randn(1,1).astype(np.float32)

def regress_without_tfData(n_epochs, input_dimension, training_inputs, training_labels):
    tf.reset_default_graph()
    weights = tf.get_variable(""weights"", initializer=np.random.randn(input_dimension, 1).astype(np.float32))

    X = tf.placeholder(tf.float32, shape=(None, input_dimension), name='X')
    Y = tf.placeholder(tf.float32, shape=(None, 1), name='Y')
    prediction = tf.matmul(X,weights)
    loss = tf.reduce_mean(tf.square(tf.subtract(prediction, Y)))
    loss_op = tf.train.AdamOptimizer(.01).minimize(loss)

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
        for _ in range(n_epochs):
            sess.run(loss_op, feed_dict={X: training_inputs, Y:training_labels})

def regress_with_tfData(n_epochs, input_dimension, training_inputs, training_labels, batch_size):
    tf.reset_default_graph()
    weights = tf.get_variable(""weights"", initializer=np.random.randn(input_dimension, 1).astype(np.float32))

    X,Y = data_set.make_one_shot_iterator().get_next()

    prediction = tf.matmul(X, weights)
    loss = tf.reduce_mean(tf.square(tf.subtract(prediction, Y)))
    loss_op = tf.train.AdamOptimizer(.01).minimize(loss)

    init = tf.global_variables_initializer()

    with tf.Session() as sess:
        sess.run(init)
        while True:
            try: 
                sess.run(loss_op)
            except tf.errors.OutOfRangeError:
                break

for input_dimension in input_dimensions_list:
    for data_size in [500, 100000]:

        training_inputs = np.random.randn(data_size, input_dimension).astype(np.float32)
        random_covector = np.random.randint(-5, 5, size=(input_dimension, 1))
        training_labels = function_to_approximate(training_inputs)

        print(""Not using tf.data, with data size ""
        ""{}, input dimension {} and training with ""
        ""a full batch, it took an average of ""
        ""{} seconds to run {} epochs.\n"".
            format(
                data_size,
                input_dimension,
                timeit.timeit(
                    lambda: regress_without_tfData(
                        n_epochs, input_dimension, 
                        training_inputs, training_labels
                    ), 
                    number=3
                ),
                n_epochs))

for input_dimension in input_dimensions_list:
    for data_size, batch_size in [(500, 50), (500, 500), (100000, 50), (100000, 100000)]:

        training_inputs = np.random.randn(data_size, input_dimension).astype(np.float32)
        random_covector = np.random.randint(-5, 5, size=(input_dimension, 1))
        training_labels = function_to_approximate(training_inputs)

        data_set = tf.data.Dataset.from_tensor_slices((training_inputs, training_labels))
        data_set = data_set.repeat(n_epochs)
        data_set = data_set.batch(batch_size)

        print(""Using tf.data, with data size ""
        ""{}, and input dimension {}, and training with ""
        ""batch size {}, it took an average of {} seconds ""
        ""to run {} epochs.\n"".
            format(
                data_size,
                input_dimension,
                batch_size,
                timeit.timeit(
                    lambda: regress_with_tfData(
                        n_epochs, input_dimension, 
                        training_inputs, training_labels, 
                        batch_size
                    ),
                    number=3
                )/3,
                n_epochs
            ))
</code></pre>

<p>This outputs for me:</p>

<blockquote>
  <p>Not using tf.data, with data size 500, input dimension 10 and training
  with a full batch, it took an average of 0.20243382899980134 seconds
  to run 10 epochs.</p>
  
  <p>Not using tf.data, with data size 100000, input dimension 10 and
  training with a full batch, it took an average of 0.2431719040000644
  seconds to run 10 epochs.</p>
  
  <p>Using tf.data, with data size 500, and input dimension 10, and
  training with batch size 50, it took an average of 0.09512088866661846
  seconds to run 10 epochs.</p>
  
  <p>Using tf.data, with data size 500, and input dimension 10, and
  training with batch size 500, it took an average of
  0.07286913600000844 seconds to run 10 epochs.</p>
  
  <p>Using tf.data, with data size 100000, and input dimension 10, and
  training with batch size 50, it took an average of 4.421892363666605
  seconds to run 10 epochs.</p>
  
  <p>Using tf.data, with data size 100000, and input dimension 10, and
  training with batch size 100000, it took an average of
  2.2555197536667038 seconds to run 10 epochs.</p>
</blockquote>

<p><strong>Edit:</strong> Fixed an important issue that Fred Guth pointed out. It didn't much affect the results, though.</p>",51803709.0,5,1,,2018/7/26 14:42,,2020/12/19 19:11,2018/7/29 22:29,,8755792.0,,8755792.0,,1,21,python|python-3.x|tensorflow|machine-learning|deep-learning,2698,61.7242,,4,tensorflow tf data package slow code learn use tensorflow api find slow code lot measure time per epoch opposite suppose think write simple linear regression program test tl dr training data slows time per epoch factor ten use full batch training bad use small batch opposite true training data question go implementation flaw source read improve speed output use tf data data size input dimension training full batch take average second run epochs use tf data data size input dimension training full batch take average second run epochs use tf data data size input dimension training batch size take average second run epochs use tf data data size input dimension training batch size take average second run epochs use tf data data size input dimension training batch size take average second run epochs use tf data data size input dimension training batch size take average second run epochs edit fix important issue fred guth point much affect result though
616,616,50544730,How do I split a custom dataset into training and test datasets?,"<pre><code>import pandas as pd
import numpy as np
import cv2
from torch.utils.data.dataset import Dataset

class CustomDatasetFromCSV(Dataset):
    def __init__(self, csv_path, transform=None):
        self.data = pd.read_csv(csv_path)
        self.labels = pd.get_dummies(self.data['emotion']).as_matrix()
        self.height = 48
        self.width = 48
        self.transform = transform

    def __getitem__(self, index):
        pixels = self.data['pixels'].tolist()
        faces = []
        for pixel_sequence in pixels:
            face = [int(pixel) for pixel in pixel_sequence.split(' ')]
            # print(np.asarray(face).shape)
            face = np.asarray(face).reshape(self.width, self.height)
            face = cv2.resize(face.astype('uint8'), (self.width, self.height))
            faces.append(face.astype('float32'))
        faces = np.asarray(faces)
        faces = np.expand_dims(faces, -1)
        return faces, self.labels

    def __len__(self):
        return len(self.data)
</code></pre>

<p>This is what I could manage to do by using references from other repositories. 
However, I want to split this dataset into train and test. </p>

<p>How can I do that inside this class? Or do I need to make a separate class to do that?</p>",50544887.0,6,0,,2018/5/26 16:16,41.0,2021/7/11 18:27,2019/1/7 14:42,,3924118.0,,4229637.0,,1,70,python|deep-learning|pytorch,101538,386.027,,2,split custom dataset training test datasets could manage use reference repository however want split dataset train test inside class need make separate class
437,437,48001598,Why do we need to call zero_grad() in PyTorch?,"<p>The method <code>zero_grad()</code> needs to be called during training. But the <a href=""https://discuss.pytorch.org/t/minimal-working-example-of-optim-sgd/11623"" rel=""noreferrer"">documentation</a> is not very helpful</p>

<pre><code>|  zero_grad(self)
|      Sets gradients of all model parameters to zero.
</code></pre>

<p>Why do we need to call this method?</p>",48009142.0,3,0,,2017/12/28 4:31,62.0,2021/8/23 4:42,2019/1/18 9:07,,2956066.0,,1424739.0,,1,187,python|neural-network|deep-learning|pytorch|gradient-descent,122959,392.559,,3,need call zero grad pytorch method need call training documentation helpful need call method
660,660,37410996,Scale layer in Caffe,"<p>I am looking through the <a href=""https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt"">Caffe prototxt for deep residual networks</a> and have noticed the appearance of a <code>""Scale""</code> layer.</p>

<pre><code>layer {
    bottom: ""res2b_branch2b""
    top: ""res2b_branch2b""
    name: ""scale2b_branch2b""
    type: ""Scale""
    scale_param {
        bias_term: true
    }
}
</code></pre>

<p>However, this layer is not available in the <a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"">Caffe layer catalogue</a>. Can someone explain the functionality of this layer and the meaning of the parameters or point to a an up-to-date documentation for Caffe?</p>",37412260.0,2,0,,2016/5/24 10:31,5.0,2017/8/2 13:42,2017/6/14 10:03,,1714410.0,,2515628.0,,1,17,neural-network|deep-learning|caffe|conv-neural-network|resnet,19097,52.9239,,3,scale layer caffe look caffe prototxt deep residual network notice appearance layer however layer available caffe layer catalogue someone explain functionality layer meaning parameter point date documentation caffe
394,394,46883606,what is the default kernel_initializer in keras,"<p>In the user manual, it shows the different kernel_initializer below</p>
<p><a href=""https://keras.io/initializers/"" rel=""noreferrer"">https://keras.io/initializers/</a></p>
<p>the main purpose is to initialize the weight matrix in the neural network.</p>
<p>Anyone knows what the default initializer is? the document didn't show the default.</p>",46884086.0,2,0,,2017/10/23 7:03,8.0,2020/6/18 8:15,2020/6/20 9:12,,-1.0,,2753413.0,,1,59,neural-network|keras,34892,151.77100000000004,,3,default kernel initializer kera user manual show different kernel initializer main purpose initialize weight matrix neural network anyone know default initializer document show default
794,794,52578950,Causal padding in keras,"<p>Can someone explain the intuition behind 'causal' padding in Keras. Is there any particular application where this can be used?</p>

<p>The keras manual says this type of padding results in dilated convolution. What exactly it means by 'dilated' convolution?</p>",58815915.0,2,0,,2018/9/30 14:44,1.0,2019/11/12 10:04,,,,,4724057.0,,1,15,python|keras|conv-neural-network,6183,53.1648,,3,causal pad kera someone explain intuition behind causal pad kera particular application use kera manual say type pad result dilated convolution exactly mean dilated convolution
512,512,34642595,Tensorflow Strides Argument,"<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>

<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool"" rel=""noreferrer"">documentation</a> repeatedly says </p>

<blockquote>
  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>What do each of the 4+ integers represent?</li>
<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>
<li>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb"" rel=""noreferrer"">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>
</ol>

<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>",34643081.0,4,0,,2016/1/6 20:56,61.0,2018/11/12 8:51,,,,,3908247.0,,1,121,python|neural-network|convolution|tensorflow|conv-neural-network,43782,371.365,,3,tensorflow stride argument try understand stride argument tf nn avg pool tf nn max pool tf nn conv documentation repeatedly say stride list ints length stride sliding window dimension input tensor question integer represent must stride stride convnets example see sadly example doc reshape use translate well scenario
253,253,44611325,R keras package Error: Python module tensorflow.contrib.keras.python.keras was not found,"<p>I have <code>keras</code> installed with <code>devtools</code> from GitHub in R and TensorFlow installed in Python. </p>

<p>However when I run an example Keras command like:</p>

<pre><code>model &lt;- keras_model_sequential() 
</code></pre>

<p>I get the following:</p>

<blockquote>
  <p>Error: Python module tensorflow.contrib.keras.python.keras was not
  found.</p>

<pre><code>Detected Python configuration:

python:         C:\Python35\python.exe
libpython:      C:/Python35/python35.dll
pythonhome:     C:\Python35
version:        3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)]
Architecture:   64bit
numpy:          C:\Python35\lib\site-packages\numpy
numpy_version:  1.13.0
tensorflow:     C:\Python35\lib\site-packages\tensorflow

python versions found: 
 C:\Python35\python.exe
 C:\Python27\\python.exe
 C:\Python35\\python.exe
 C:\Python36\\python.exe
</code></pre>
</blockquote>",,6,3,,2017/6/18 2:57,,2020/12/1 12:44,,,,,3604745.0,,1,17,r|tensorflow|keras,14197,54.0088,,4,r kera package error python module tensorflow contrib kera python kera find instal github r tensorflow instal python however run example kera command like get following error python module tensorflow contrib kera python kera find
498,498,32379878,Cheat sheet for caffe / pycaffe?,"<p>Does anyone know whether there is a cheat sheet for all important pycaffe commands?
I was so far using caffe only via Matlab interface and terminal + bash scripts.</p>

<p>I wanted to shift towards using ipython and work through the ipython notebook examples. However I find it hard to get an overview of all the functions that are inside the caffe module for python. (I'm also quite new to python).</p>",32509003.0,2,0,,2015/9/3 15:32,40.0,2016/7/18 4:50,,,,,2191652.0,,1,45,ipython-notebook|caffe|pycaffe,26579,143.69799999999995,,3,cheat sheet caffe pycaffe anyone know whether cheat sheet important pycaffe command far use caffe via matlab interface terminal bash script want shift towards use ipython work ipython notebook examples however find hard get overview function inside caffe module python also quite new python
785,785,52162004,I am having trouble with this error (-215:Assertion failed) !ssize.empty() in function 'resize' in OpenCV,"<p>I am trying to apply a Keras' image classifier to my project, but down the road I got stuck with this. Though previously, with the same code I could use OpenCV to read and train images, but after switching to a new batch of images it got caught with the error. So my speculation is that there's something wrong with my file type:</p>
<p>This is from the batch that got the error:</p>
<blockquote>
<p>traf.204.jpg: JPEG image data, JFIF standard 1.01, aspect ratio,
density 1x1, segment length 16, baseline, precision 8, 480x294, frames
1</p>
</blockquote>
<p>This is from the batch that didn't get caught with the error:</p>
<blockquote>
<p>bear.290.jpg: JPEG image data, JFIF standard 1.01, aspect ratio,
density 1x1, segment length 16, baseline, precision 8, 224x224, frames
3</p>
</blockquote>
<p>But the file type seems to be exactly the same (except for the resolution). How can I fix this problem?</p>",52176182.0,9,1,,2018/9/4 8:28,3.0,2021/6/11 13:56,2021/5/18 11:37,,63550.0,,7809118.0,,1,12,python|opencv|keras,58128,65.6575,2021/6/14 3:00,4,trouble error assertion fail ssize empty function aresize opencv try apply keras image classifier project road get stick though previously code could use opencv read train image switch new batch image get catch error speculation something wrong file type batch get error traf jpg jpeg image data jfif standard aspect ratio density x segment length baseline precision x frame batch get catch error bear jpg jpeg image data jfif standard aspect ratio density x segment length baseline precision x frame file type seem exactly except resolution fix problem
138,138,42443936,Keras split train test set when using ImageDataGenerator,"<p>I have a single directory which contains sub-folders (according to labels) of images. I want to split this data into train and test set while using ImageDataGenerator in Keras. Although model.fit() in keras has argument validation_split for specifying the split, I could not find the same for model.fit_generator(). How to do it ?</p>

<pre><code>train_datagen = ImageDataGenerator(rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=32,
    class_mode='binary')

model.fit_generator(
    train_generator,
    samples_per_epoch=nb_train_samples,
    nb_epoch=nb_epoch,
    validation_data=??,
    nb_val_samples=nb_validation_samples)
</code></pre>

<p>I don't have separate directory for validation data, need to split it from the training data</p>",,11,7,,2017/2/24 16:43,39.0,2021/6/26 19:08,,,,,3898714.0,,1,102,keras|train-test-split,85070,492.119,,2,kera split train test set use imagedatagenerator single directory contain sub folder accord label image want split data train test set use imagedatagenerator kera although model fit kera argument validation split specify split could find model fit generator separate directory validation data need split training data
124,124,41957574,Python: rewrite a looping numpy math function to run on GPU,"<p>Can someone help me rewrite this one function <em>(the <code>doTheMath</code> function)</em> to do the calculations on the GPU? I used a few good days now trying to get my head around it but to no result. I wonder maybe somebody can help me rewrite this function in whatever way you may seem fit as log as I gives the same result at the end. I tried to use <code>@jit</code> from <code>numba</code> but for some reason it is actually much slower than running the code as usual. With a huge sample size, the goal is to decrease the execution time considerably so naturally I believe the GPU is the fastest way to do it. </p>

<p>I'll explain a little what is actually happening. The real data, which looks almost identical as the sample data created in the code below is divided into sample sizes of approx 5.000.000 rows each sample or around 150MB per file. In total there are around 600.000.000 rows or 20GB of data. I must loop through this data, sample by sample and then row by row in each sample, take the last 2000 (or another) rows as of each line and run the <code>doTheMath</code> function which returns a result. That result is then saved back to the hardrive where I can do some other things with it with another program. As you can see below, I do not need all of the results of all the rows, only those bigger than a specific amount. If I run my function as it is right now in python I get about 62seconds per 1.000.000 rows. This is a very long time considering all the data and how fast it should be done with.</p>

<p>I must mention that I upload the real data file by file to the RAM with the help of <code>data = joblib.load(file)</code> so uploading the data is not the problem as it takes only about 0.29 seconds per file. Once uploaded I run the entire code below. What takes the longest time is the <code>doTheMath</code> function. I am willing to give all of my 500 reputation points I have on stackoverflow as a reward for somebody willing to help me rewrite this simple code to run on the GPU. My interest is specifically in the GPU, I really want to see how it is done on this problem at hand.</p>

<p><strong>EDIT/UPDATE 1:</strong>
Here is a link to a small sample of the real data: <a href=""https://mab.to/Nx8GvwTjQ"" rel=""noreferrer"">data_csv.zip</a> About 102000 rows of real data1 and 2000 rows for real data2a and data2b. Use <code>minimumLimit = 400</code> on the real sample data </p>

<p><strong>EDIT/UPDATE 2:</strong>
For those following this post here is a short summary of the answers below. Up until now we have 4 answers to the original solution. The one offered by @Divakar are just tweaks to the original code. Of the two tweaks only the first one is actually applicable to this problem, the second one is a good tweak but does not apply here. Out of the other three answers, two of them are CPU based solutions and one tensorflow-GPU try. The Tensorflow-GPU by Paul Panzer seems to be promising but when i actually run it on the GPU it is slower than the original, so the code still needs improvement.</p>

<p>The other two CPU based solutions are submitted by @PaulPanzer (a pure numpy solution) and @MSeifert (a numba solution). Both solutions give very good results and both process data extremely fast compared to the original code. Of the two the one submitted by Paul Panzer is faster. It processes about 1.000.000 rows in about 3 seconds. The only problem is with smaller batchSizes, this can be overcome by either switching to the numba solution offered by MSeifert, or even the original code after all the tweaks that have been discussed below.</p>

<p>I am very happy and thankful to @PaulPanzer and @MSeifert for the work they did on their answers. Still, since this is a question about a GPU based solution, i am waiting to see if anybody is willing to give it a try on a GPU version and see how much faster the data can be processed on the GPU when compared to the current CPU solutions. If there will be no other answers outperforming @PaulPanzer's pure numpy solution then i'll accept his answer as the right one and gets the bounty :) </p>

<p><strong>EDIT/UPDATE 3:</strong>
@Divakar has posted a new answer with a solution for the GPU. After my testings on real data, the speed is not even comparable to the CPU counterpart solutions. The GPU processes about 5.000.000 in about 1,5 seconds. This is incredible :) I am very excited about the GPU solution and i thank @Divakar for posting it. As well as i thank @PaulPanzer and @MSeifert for their CPU solutions :) Now my research continues with an incredible speed due to the GPU :) </p>

<pre><code>import pandas as pd
import numpy as np
import time

def doTheMath(tmpData1, data2a, data2b):
    A = tmpData1[:, 0]
    B = tmpData1[:,1]
    C = tmpData1[:,2]
    D = tmpData1[:,3]
    Bmax = B.max()
    Cmin  = C.min()
    dif = (Bmax - Cmin)
    abcd = ((((A - Cmin) / dif) + ((B - Cmin) / dif) + ((C - Cmin) / dif) + ((D - Cmin) / dif)) / 4)
    return np.where(((abcd &lt;= data2a) &amp; (abcd &gt;= data2b)), 1, 0).sum()

#Declare variables
batchSize = 2000
sampleSize = 5000000
resultArray = []
minimumLimit = 490 #use 400 on the real sample data 

#Create Random Sample Data
data1 = np.matrix(np.random.uniform(1, 100, (sampleSize + batchSize, 4)))
data2a = np.matrix(np.random.uniform(0, 1, (batchSize, 1))) #upper limit
data2b = np.matrix(np.random.uniform(0, 1, (batchSize, 1))) #lower limit
#approx. half of data2a will be smaller than data2b, but that is only in the sample data because it is randomly generated, NOT the real data. The real data2a is always higher than data2b.


#Loop through the data
t0 = time.time()
for rowNr in  range(data1.shape[0]):
    tmp_df = data1[rowNr:rowNr + batchSize] #rolling window
    if(tmp_df.shape[0] == batchSize):
        result = doTheMath(tmp_df, data2a, data2b)
        if (result &gt;= minimumLimit):
            resultArray.append([rowNr , result])
print('Runtime:', time.time() - t0)

#Save data results
resultArray = np.array(resultArray)
print(resultArray[:,1].sum())
resultArray = pd.DataFrame({'index':resultArray[:,0], 'result':resultArray[:,1]})
resultArray.to_csv(""Result Array.csv"", sep=';')
</code></pre>

<p>The PC specs I am working on:</p>

<pre><code>GTX970(4gb) video card; 
i7-4790K CPU 4.00Ghz; 
16GB RAM;
a SSD drive 
running Windows 7; 
</code></pre>

<p>As a side question, would a second video card in SLI help on this problem?</p>",42079546.0,5,8,,2017/1/31 12:26,13.0,2017/2/8 10:01,2017/2/8 10:01,,2480410.0,,2480410.0,,1,23,python|numpy|tensorflow|theano|numba,10485,70.0823,,3,python rewrite looping numpy math function run gpu someone help rewrite one function function calculation gpu use good day try get head around result wonder maybe somebody help rewrite function whatever way may seem fit log give result end try use reason actually much slow run code usual huge sample size goal decrease execution time considerably naturally believe gpu fast way explain little actually happen real data look almost identical sample data create code divide sample size approx row sample around mb per file total around row gb data must loop data sample sample row row sample take last another row line run function return result result save back hardrive thing another program see need result row big specific amount run function right python get second per row long time consider data fast must mention upload real data file file ram help upload data problem take second per file upload run entire code take long time function willing give reputation point stackoverflow reward somebody willing help rewrite simple code run gpu interest specifically gpu really want see problem hand edit update link small sample real data data csv zip row real data row real data data b use real sample data edit update follow post short summary answer answer original solution one offer divakar tweak original code two tweak first one actually applicable problem second one good tweak apply three answer two cpu base solution one tensorflow gpu try tensorflow gpu paul panzer seem promise actually run gpu slow original code still need improvement two cpu base solution submit paulpanzer pure numpy solution mseifert numba solution solution give good result process data extremely fast compare original code two one submit paul panzer faster process row second problem small batchsizes overcome either switch numba solution offer mseifert even original code tweak discuss happy thankful paulpanzer mseifert work answer still since question gpu base solution wait see anybody willing give try gpu version see much fast data process gpu compare current cpu solution answer outperform paulpanzer pure numpy solution accept answer right one get bounty edit update divakar post new answer solution gpu testing real data speed even comparable cpu counterpart solutions gpu process second incredible excited gpu solution thank divakar post well thank paulpanzer mseifert cpu solution research continue incredible speed due gpu pc spec work side question would second video card sli help problem
618,618,50715928,ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer`,"<p>I'm building a model in Keras using some tensorflow function (reduce_sum and l2_normalize) in the last layer while encountered this problem. I have searched for a solution but all of it related to ""Keras tensor"".</p>

<p>Here is my code:</p>

<pre><code>import tensorflow as tf;
from tensorflow.python.keras import backend as K

vgg16_model = VGG16(weights = 'imagenet', include_top = False, input_shape = input_shape);

fire8 = extract_layer_from_model(vgg16_model, layer_name = 'block4_pool');

pool8 = MaxPooling2D((3,3), strides = (2,2), name = 'pool8')(fire8.output);

fc1 = Conv2D(64, (6,6), strides= (1, 1), padding = 'same', name = 'fc1')(pool8);

fc1 = Dropout(rate = 0.5)(fc1);

fc2 = Conv2D(3, (1, 1), strides = (1, 1), padding = 'same', name = 'fc2')(fc1);

fc2 = Activation('relu')(fc2);

fc2 = Conv2D(3, (15, 15), padding = 'valid', name = 'fc_pooling')(fc2);

fc2_norm = K.l2_normalize(fc2, axis = 3);

est = tf.reduce_sum(fc2_norm, axis = (1, 2));
est = K.l2_normalize(est);

FC_model = Model(inputs = vgg16_model.input, outputs = est);
</code></pre>

<p>and then the error: </p>

<blockquote>
  <p>ValueError: Output tensors to a Model must be the output of a
  TensorFlow <code>Layer</code> (thus holding past layer metadata). Found:
  Tensor(""l2_normalize_3:0"", shape=(?, 3), dtype=float32)</p>
</blockquote>

<p>I noticed that without passing fc2 layer to these functions, the model works fine:</p>

<pre><code>FC_model = Model(inputs = vgg16_model.input, outputs = fc2);
</code></pre>

<p>Can someone please explain to me this problem and some suggestion on how to fix it?</p>",50716411.0,2,0,,2018/6/6 8:43,4.0,2019/10/16 11:25,2018/8/1 15:13,,4902934.0,,4902934.0,,1,24,python|tensorflow|machine-learning|keras|tensor,23669,68.0967,,4,valueerror output tensor model must output tensorflow layer build model kera use tensorflow function reduce sum l normalize last layer encounter problem search solution relate keras tensor code error valueerror output tensor model must output tensorflow thus hold past layer metadata find tensor l normalize shape dtype float notice without pass fc layer function model work fine someone please explain problem suggestion fix
590,590,49550182,Keras rename model and layers,"<p>1) I try to rename a model and the layers in Keras with TF backend, since I am using multiple models in one script.
Class Model seem to have the property model.name, but when changing it I get ""AttributeError: can't set attribute"".
What is the Problem here?</p>

<p>2) Additionally, I am using sequential API and I want to give a name to layers, which seems to be possibile with Functional API, but I found no solution for sequential API. Does anonye know how to do it for sequential API?</p>

<p>UPDATE TO 2): Naming the layers works, although it seems to be not documented. Just add the argument name, e.g. model.add(Dense(...,...,name=""hiddenLayer1""). <em>Watch out, Layers with same name share weights!</em></p>",49550620.0,9,1,,2018/3/29 7:20,5.0,2021/8/2 19:09,2018/3/29 7:44,,3921232.0,,3921232.0,,1,23,python|keras,29166,132.26,,3,kera rename model layer try rename model layer kera tf backend since use multiple model one script class model seem property model name change get attributeerror set attribute problem additionally use sequential api want give name layer seem possibile functional api find solution sequential api anonye know sequential api update name layer work although seem document add argument name e g model add dense name hiddenlayer watch layer name share weight
105,105,41625252,Altering trained images to train neural network,"<p>I am currently trying to make a program to differentiate rotten oranges and edible oranges solely based on their external appearance. To do this, I am planning on using a Convolutional Neural Network to train with rotten oranges and normal oranges. After some searching I could only find one database of approx. 150 rotten oranges and 150 normal oranges on a black background (<a href=""http://www.cofilab.com/downloads/"">http://www.cofilab.com/downloads/</a>). Obviously, a machine learning model will need at least few thousand oranges to achieve an accuracy above 90 or so percent. However, can I alter these 150 oranges in some way to produce more photos of oranges? By alter, I mean adding different shades of orange on the citrus fruit to make a ""different orange."" Would this be an effective method of training a neural network?  </p>",41655042.0,4,0,,2017/1/13 0:04,10.0,2017/1/19 5:32,,,,,3763831.0,,1,35,machine-learning|computer-vision|neural-network|conv-neural-network|training-data,2225,56.3893,,2,alter trained image train neural network currently try make program differentiate rotten orange edible orange solely base external appearance plan use convolutional neural network train rotten orange normal orange searching could find one database approx rotten orange normal orange black background obviously machine learn model need least thousand orange achieve accuracy percent however alter orange way produce photo orange alter mean add different shade orange citrus fruit make different orange would effective method train neural network
529,529,35549588,Soft attention vs. hard attention,"<p>In this blog post, <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">The Unreasonable Effectiveness of Recurrent Neural Networks</a>, Andrej Karpathy mentions future directions for neural networks based machine learning:</p>

<blockquote>
  <p>The concept of attention is the most interesting recent architectural innovation in neural networks. [...] soft attention scheme for memory addressing is convenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency because everything that can be attended to is attended to (but softly). Think of this as declaring a pointer in C that doesn't point to a specific address but instead defines an entire distribution over all addresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointed content (that would be an expensive operation!). This has motivated multiple authors to swap soft attention models for hard attention where one samples a particular chunk of memory to attend to (e.g. a read/write action for some memory cell instead of reading/writing from all cells to some degree). This model is significantly more philosophically appealing, scalable and efficient, but unfortunately it is also non-differentiable.</p>
</blockquote>

<p>I think I understood the pointer metaphor, but what is exactly attention and why is the hard one not differentiable?</p>

<p>I found an explanation about attention <a href=""https://www.quora.com/What-is-exactly-the-attention-mechanism-introduced-to-RNN-recurrent-neural-network-It-would-be-nice-if-you-could-make-it-easy-to-understand"" rel=""noreferrer"">here</a>, but still confused about the soft/hard part.</p>",35852153.0,1,1,,2016/2/22 9:08,22.0,2017/11/9 17:17,2016/2/22 10:07,,165753.0,,165753.0,,1,27,machine-learning|neural-network|recurrent-neural-network,19004,66.5154,,0,soft attention v hard attention blog post unreasonable effectiveness recurrent neural network andrej karpathy mention future direction neural network base machine learn concept attention interesting recent architectural innovation neural network soft attention scheme memory addressing convenient keep model fully differentiable unfortunately one sacrifice efficiency everything attend attend softly think declare pointer c point specific address instead define entire distribution address entire memory dereferencing pointer return weighted sum pointed content would expensive operation motivate multiple author swap soft attention model hard attention one sample particular chunk memory attend e g read write action memory cell instead read write cell degree model significantly philosophically appeal scalable efficient unfortunately also non differentiable think understand pointer metaphor exactly attention hard one differentiable find explanation attention still confuse soft hard part
793,793,52548174,How to remove the last FC layer from a ResNet model in PyTorch?,"<p>I am using a ResNet152 model from PyTorch. I'd like to strip off the last FC layer from the model. Here's my code:</p>

<pre><code>from torchvision import datasets, transforms, models
model = models.resnet152(pretrained=True)
print(model)
</code></pre>

<p>When I print the model, the last few lines look like this:</p>

<pre><code>    (2):  Bottleneck(
      (conv1):  Conv2d(2048,  512,  kernel_size=(1,  1),  stride=(1,  1),  bias=False)
      (bn1):  BatchNorm2d(512,  eps=1e-05,  momentum=0.1,  affine=True,  track_running_stats=True)
      (conv2):  Conv2d(512,  512,  kernel_size=(3,  3),  stride=(1,  1),  padding=(1,  1),  bias=False)
      (bn2):  BatchNorm2d(512,  eps=1e-05,  momentum=0.1,  affine=True,  track_running_stats=True)
      (conv3):  Conv2d(512,  2048,  kernel_size=(1,  1),  stride=(1,  1),  bias=False)
      (bn3):  BatchNorm2d(2048,  eps=1e-05,  momentum=0.1,  affine=True,  track_running_stats=True)
      (relu):  ReLU(inplace)
    )
  )
  (avgpool):  AvgPool2d(kernel_size=7,  stride=1,  padding=0)
  (fc):  Linear(in_features=2048,  out_features=1000,  bias=True)
)
</code></pre>

<p>I want to remove that last fc layer from the model.</p>

<p>I found an answer here on SO (<a href=""https://stackoverflow.com/questions/44146655/how-to-convert-pretrained-fc-layers-to-conv-layers-in-pytorch#44410334"">How to convert pretrained FC layers to CONV layers in Pytorch</a>), where <a href=""https://stackoverflow.com/users/7042758/mexmex"">mexmex</a> seems to provide the answer I'm looking for:</p>

<pre><code>list(model.modules()) # to inspect the modules of your model
my_model = nn.Sequential(*list(model.modules())[:-1]) # strips off last linear layer
</code></pre>

<p>So I added those lines to my code like this:</p>

<pre><code>model = models.resnet152(pretrained=True)
list(model.modules()) # to inspect the modules of your model
my_model = nn.Sequential(*list(model.modules())[:-1]) # strips off last linear layer
print(my_model)
</code></pre>

<p>But this code doesn't work as advertised -- as least not for me. The rest of this post is a detailed explanation of why that answer doesn't work so this question doesn't get closed as a duplicate.</p>

<p>First, the printed model is nearly 5x larger than before. I see the same model as before, but followed by what appears to be a repeat of the model, but perhaps flattened.</p>

<pre><code>    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
    )
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
(1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
(2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
(3): ReLU(inplace)
(4): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
(5): Sequential(
  . . . this goes on for ~1600 more lines . . .
  (415): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (416): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (417): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (418): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (419): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (420): ReLU(inplace)
  (421): AvgPool2d(kernel_size=7, stride=1, padding=0)
)
</code></pre>

<p>Second, the fc layer is <strong>still there</strong> -- and the Conv2D layer after it looks just like the first layer of ResNet152. </p>

<p>Third, if I try to invoke <code>my_model.forward()</code>, pytorch complains about a size mismatch. It expects size [1, 3, 224, 224], but the input was [1, 1000]. So it looks like a copy of the entire model (minus the fc layer) is getting appended to the original model.</p>

<p>Bottom line, the only answer I found on SO doesn't actually work.</p>",52548419.0,4,1,,2018/9/28 4:13,8.0,2021/4/13 16:07,,,,,3448136.0,,1,21,python|pytorch|resnet,29308,89.6679,,3,remove last fc layer resnet model pytorch use resnet model pytorch like strip last fc layer model code print model last line look like want remove last fc layer model find answer convert pretrained fc layer conv layer pytorch mexmex seem provide answer look add line code like code work advertised least rest post detailed explanation answer work question get close duplicate first printed model nearly x large see model follow appear repeat model perhaps flatten second fc layer still conv layer look like first layer resnet third try invoke pytorch complains size mismatch expect size input look like copy entire model minus fc layer get append original model bottom line answer find actually work
244,244,44495698,Keras: Difference between Kernel and Activity regularizers,"<p>I have noticed that <em>weight_regularizer</em> is no more available in Keras and that, in its place, there are <em>activity</em> and <em>kernel</em> regularizer. 
I would like to know:</p>

<ul>
<li>What are the main differences between <em>kernel</em> and <em>activity</em> regularizers?</li>
<li>Could I use <em>activity_regularizer</em> in place of <em>weight_regularizer</em>?</li>
</ul>",44496213.0,2,0,,2017/6/12 9:16,21.0,2021/3/17 12:56,2017/12/19 16:13,,2314737.0,,7387749.0,,1,96,machine-learning|keras|keras-layer,42551,211.916,,3,kera difference kernel activity regularizers notice weight regularizer available kera place activity kernel regularizer would like know main difference kernel activity regularizers could use activity regularizer place weight regularizer
588,588,49534421,set_model() missing 1 required positional argument: 'model',"<p>I'm have created a Keras Sequential Model and am using Adam optimizer. I wished to get the learning rate after every epoch. This <a href=""https://stackoverflow.com/questions/47490834/how-can-i-print-the-learning-rate-at-each-epoch-with-adam-optimizer-in-keras"">stackoverflow question</a> seem to answer my question. However, when I followed the solution mentioned, I get the following error</p>

<pre><code>set_model() missing 1 required positional argument: 'model'
</code></pre>

<p>Here's my code to create a model:</p>

<pre><code>model = Sequential()

model.add(Conv2D(64, (5, 5), input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), activation='relu'))

model.add(Conv2D(64, (5, 5), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))

model.add(Conv2D(128, (5, 5), activation='relu'))
model.add(Conv2D(128, (5, 5), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))

model.add(Conv2D(256, (5, 5), activation='relu'))
model.add(Conv2D(256, (5, 5), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(BatchNormalization(axis=3))
model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(256, activation='relu'))

model.add(Dropout(0.5))

model.add(Dense(256, activation='relu'))

model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.4, 
                                            min_lr=0.0001)
csvlogger = CSVLogger(""solution.csv"", separator='\t')
checkpoint = ModelCheckpoint(""models/best_model5.h5"", monitor=""val_acc"", save_best_only=True, mode='max')
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.4, 
                                            min_lr=0.00001)

class MyCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        lr = self.model.optimizer.lr
        decay = self.model.optimizer.decay
        iterations = self.model.optimizer.iterations
        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))
        print(K.eval(lr_with_decay))

model.fit_generator(datagen.flow(x_train, y_train, batch_size=75), 
                           epochs=10, validation_data=(x_validation, y_test),verbose=1, 
                           steps_per_epoch=x_train.shape[0], callbacks=[csvlogger, checkpoint, MyCallback])
</code></pre>

<p>How do I get past this error ""set_model() missing 1 required positional argument: 'model'
""
Below is the stack trace </p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-12-1826a19039cd&gt; in &lt;module&gt;()
    128 model.fit_generator(datagen.flow(x_train, y_train, batch_size=75), 
    129                            epochs=10, validation_data=(x_validation, y_test),verbose=1,
--&gt; 130                            steps_per_epoch=x_train.shape[0], callbacks=[csvlogger, checkpoint, MyCallback])
    131 model.save('trained_model5.h5')
    132 

/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

/usr/local/lib/python3.6/dist-packages/keras/models.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1274                                         use_multiprocessing=use_multiprocessing,
   1275                                         shuffle=shuffle,
-&gt; 1276                                         initial_epoch=initial_epoch)
   1277 
   1278     @interfaces.legacy_generator_methods_support

/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py in wrapper(*args, **kwargs)
     89                 warnings.warn('Update your `' + object_name +
     90                               '` call to the Keras 2 API: ' + signature, stacklevel=2)
---&gt; 91             return func(*args, **kwargs)
     92         wrapper._original_function = func
     93         return wrapper

/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   2131         else:
   2132             callback_model = self
-&gt; 2133         callbacks.set_model(callback_model)
   2134         callbacks.set_params({
   2135             'epochs': epochs,

/usr/local/lib/python3.6/dist-packages/keras/callbacks.py in set_model(self, model)
     50     def set_model(self, model):
     51         for callback in self.callbacks:
---&gt; 52             callback.set_model(model)
     53 
     54     def on_epoch_begin(self, epoch, logs=None):

TypeError: set_model() missing 1 required positional argument: 'model'
</code></pre>

<p>Also, another question that I have is, whether the above solution is correct.<a href=""https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#methods"" rel=""noreferrer"">This tensorflow link about Adam Optimizer</a> suggests learning rate to be calculated as:</p>

<blockquote>
  <p>lr_t &lt;- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)</p>
</blockquote>

<p>This seems quite different from the solution that is mentioned in the other link. Did I miss something?</p>",,3,4,,2018/3/28 12:21,1.0,2019/12/25 14:21,2018/3/28 13:18,,9061899.0,,9061899.0,,1,10,python|tensorflow|deep-learning|keras,11245,61.2038,,4,set model miss require positional argument model create keras sequential model use adam optimizer wish get learning rate every epoch stackoverflow question seem answer question however follow solution mention get following error code create model get past error set model miss require positional argument model stack trace also another question whether solution correct tensorflow link adam optimizer suggest learn rate calculate lr learn rate sqrt beta beta seem quite different solution mention link miss something
747,747,41075993,facenet triplet loss with keras,"<p>I am trying to implement facenet in Keras with Tensorflow backend and I have some problem with the triplet loss.<a href=""https://i.stack.imgur.com/RT3TZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RT3TZ.png"" alt=""enter image description here"" /></a></p>
<p>I call the fit function with 3*n number of images and then I define my custom loss function as follows:</p>
<pre><code>def triplet_loss(self, y_true, y_pred):

    embeddings = K.reshape(y_pred, (-1, 3, output_dim))

    positive_distance = K.mean(K.square(embeddings[:,0] - embeddings[:,1]),axis=-1)
    negative_distance = K.mean(K.square(embeddings[:,0] - embeddings[:,2]),axis=-1)
    return K.mean(K.maximum(0.0, positive_distance - negative_distance + _alpha))

self._model.compile(loss=triplet_loss, optimizer=&quot;sgd&quot;)
self._model.fit(x=x,y=y,nb_epoch=1, batch_size=len(x))
</code></pre>
<p>where y is just a dummy array filled with 0s</p>
<p>The problem is that even after the first iteration with batch size 20 the model starts predicting the same embedding for all the images. So when I first do the prediction on the batch every embedding is different. Then I do the fit and predict again and suddenly all the embeddings becomes almost the same for all the images in the batch</p>
<p>Also notice that there is a Lambda layer at the end of the model. It normalizes the output of the net so all the embeddings has a unit length as it was suggested in the face net study.</p>
<p>Can anybody help me out here?</p>
<p>Model summary</p>
<pre><code>    Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 224, 224, 3)   0                                            
____________________________________________________________________________________________________
convolution2d_1 (Convolution2D)  (None, 112, 112, 64)  9472        input_1[0][0]                    
____________________________________________________________________________________________________
batchnormalization_1 (BatchNormal(None, 112, 112, 64)  128         convolution2d_1[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 56, 56, 64)    0           batchnormalization_1[0][0]       
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 56, 56, 64)    4160        maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
batchnormalization_2 (BatchNormal(None, 56, 56, 64)    128         convolution2d_2[0][0]            
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 56, 56, 192)   110784      batchnormalization_2[0][0]       
____________________________________________________________________________________________________
batchnormalization_3 (BatchNormal(None, 56, 56, 192)   384         convolution2d_3[0][0]            
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 28, 28, 192)   0           batchnormalization_3[0][0]       
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 28, 28, 96)    18528       maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 28, 28, 16)    3088        maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 28, 28, 192)   0           maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 28, 28, 64)    12352       maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 28, 28, 128)   110720      convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 28, 28, 32)    12832       convolution2d_7[0][0]            
____________________________________________________________________________________________________
convolution2d_9 (Convolution2D)  (None, 28, 28, 32)    6176        maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
merge_1 (Merge)                  (None, 28, 28, 256)   0           convolution2d_4[0][0]            
                                                                   convolution2d_6[0][0]            
                                                                   convolution2d_8[0][0]            
                                                                   convolution2d_9[0][0]            
____________________________________________________________________________________________________
convolution2d_11 (Convolution2D) (None, 28, 28, 96)    24672       merge_1[0][0]                    
____________________________________________________________________________________________________
convolution2d_13 (Convolution2D) (None, 28, 28, 32)    8224        merge_1[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 28, 28, 256)   0           merge_1[0][0]                    
____________________________________________________________________________________________________
convolution2d_10 (Convolution2D) (None, 28, 28, 64)    16448       merge_1[0][0]                    
____________________________________________________________________________________________________
convolution2d_12 (Convolution2D) (None, 28, 28, 128)   110720      convolution2d_11[0][0]           
____________________________________________________________________________________________________
convolution2d_14 (Convolution2D) (None, 28, 28, 64)    51264       convolution2d_13[0][0]           
____________________________________________________________________________________________________
convolution2d_15 (Convolution2D) (None, 28, 28, 64)    16448       maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
merge_2 (Merge)                  (None, 28, 28, 320)   0           convolution2d_10[0][0]           
                                                                   convolution2d_12[0][0]           
                                                                   convolution2d_14[0][0]           
                                                                   convolution2d_15[0][0]           
____________________________________________________________________________________________________
convolution2d_16 (Convolution2D) (None, 28, 28, 128)   41088       merge_2[0][0]                    
____________________________________________________________________________________________________
convolution2d_18 (Convolution2D) (None, 28, 28, 32)    10272       merge_2[0][0]                    
____________________________________________________________________________________________________
convolution2d_17 (Convolution2D) (None, 14, 14, 256)   295168      convolution2d_16[0][0]           
____________________________________________________________________________________________________
convolution2d_19 (Convolution2D) (None, 14, 14, 64)    51264       convolution2d_18[0][0]           
____________________________________________________________________________________________________
maxpooling2d_5 (MaxPooling2D)    (None, 14, 14, 320)   0           merge_2[0][0]                    
____________________________________________________________________________________________________
merge_3 (Merge)                  (None, 14, 14, 640)   0           convolution2d_17[0][0]           
                                                                   convolution2d_19[0][0]           
                                                                   maxpooling2d_5[0][0]             
____________________________________________________________________________________________________
convolution2d_21 (Convolution2D) (None, 14, 14, 96)    61536       merge_3[0][0]                    
____________________________________________________________________________________________________
convolution2d_23 (Convolution2D) (None, 14, 14, 32)    20512       merge_3[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_6 (MaxPooling2D)    (None, 14, 14, 640)   0           merge_3[0][0]                    
____________________________________________________________________________________________________
convolution2d_20 (Convolution2D) (None, 14, 14, 256)   164096      merge_3[0][0]                    
____________________________________________________________________________________________________
convolution2d_22 (Convolution2D) (None, 14, 14, 192)   166080      convolution2d_21[0][0]           
____________________________________________________________________________________________________
convolution2d_24 (Convolution2D) (None, 14, 14, 64)    51264       convolution2d_23[0][0]           
____________________________________________________________________________________________________
convolution2d_25 (Convolution2D) (None, 14, 14, 128)   82048       maxpooling2d_6[0][0]             
____________________________________________________________________________________________________
merge_4 (Merge)                  (None, 14, 14, 640)   0           convolution2d_20[0][0]           
                                                                   convolution2d_22[0][0]           
                                                                   convolution2d_24[0][0]           
                                                                   convolution2d_25[0][0]           
____________________________________________________________________________________________________
convolution2d_27 (Convolution2D) (None, 14, 14, 112)   71792       merge_4[0][0]                    
____________________________________________________________________________________________________
convolution2d_29 (Convolution2D) (None, 14, 14, 32)    20512       merge_4[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_7 (MaxPooling2D)    (None, 14, 14, 640)   0           merge_4[0][0]                    
____________________________________________________________________________________________________
convolution2d_26 (Convolution2D) (None, 14, 14, 224)   143584      merge_4[0][0]                    
____________________________________________________________________________________________________
convolution2d_28 (Convolution2D) (None, 14, 14, 224)   226016      convolution2d_27[0][0]           
____________________________________________________________________________________________________
convolution2d_30 (Convolution2D) (None, 14, 14, 64)    51264       convolution2d_29[0][0]           
____________________________________________________________________________________________________
convolution2d_31 (Convolution2D) (None, 14, 14, 128)   82048       maxpooling2d_7[0][0]             
____________________________________________________________________________________________________
merge_5 (Merge)                  (None, 14, 14, 640)   0           convolution2d_26[0][0]           
                                                                   convolution2d_28[0][0]           
                                                                   convolution2d_30[0][0]           
                                                                   convolution2d_31[0][0]           
____________________________________________________________________________________________________
convolution2d_33 (Convolution2D) (None, 14, 14, 128)   82048       merge_5[0][0]                    
____________________________________________________________________________________________________
convolution2d_35 (Convolution2D) (None, 14, 14, 32)    20512       merge_5[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_8 (MaxPooling2D)    (None, 14, 14, 640)   0           merge_5[0][0]                    
____________________________________________________________________________________________________
convolution2d_32 (Convolution2D) (None, 14, 14, 192)   123072      merge_5[0][0]                    
____________________________________________________________________________________________________
convolution2d_34 (Convolution2D) (None, 14, 14, 256)   295168      convolution2d_33[0][0]           
____________________________________________________________________________________________________
convolution2d_36 (Convolution2D) (None, 14, 14, 64)    51264       convolution2d_35[0][0]           
____________________________________________________________________________________________________
convolution2d_37 (Convolution2D) (None, 14, 14, 128)   82048       maxpooling2d_8[0][0]             
____________________________________________________________________________________________________
merge_6 (Merge)                  (None, 14, 14, 640)   0           convolution2d_32[0][0]           
                                                                   convolution2d_34[0][0]           
                                                                   convolution2d_36[0][0]           
                                                                   convolution2d_37[0][0]           
____________________________________________________________________________________________________
convolution2d_39 (Convolution2D) (None, 14, 14, 144)   92304       merge_6[0][0]                    
____________________________________________________________________________________________________
convolution2d_41 (Convolution2D) (None, 14, 14, 32)    20512       merge_6[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_9 (MaxPooling2D)    (None, 14, 14, 640)   0           merge_6[0][0]                    
____________________________________________________________________________________________________
convolution2d_38 (Convolution2D) (None, 14, 14, 160)   102560      merge_6[0][0]                    
____________________________________________________________________________________________________
convolution2d_40 (Convolution2D) (None, 14, 14, 288)   373536      convolution2d_39[0][0]           
____________________________________________________________________________________________________
convolution2d_42 (Convolution2D) (None, 14, 14, 64)    51264       convolution2d_41[0][0]           
____________________________________________________________________________________________________
convolution2d_43 (Convolution2D) (None, 14, 14, 128)   82048       maxpooling2d_9[0][0]             
____________________________________________________________________________________________________
merge_7 (Merge)                  (None, 14, 14, 640)   0           convolution2d_38[0][0]           
                                                                   convolution2d_40[0][0]           
                                                                   convolution2d_42[0][0]           
                                                                   convolution2d_43[0][0]           
____________________________________________________________________________________________________
convolution2d_44 (Convolution2D) (None, 14, 14, 160)   102560      merge_7[0][0]                    
____________________________________________________________________________________________________
convolution2d_46 (Convolution2D) (None, 14, 14, 64)    41024       merge_7[0][0]                    
____________________________________________________________________________________________________
convolution2d_45 (Convolution2D) (None, 7, 7, 256)     368896      convolution2d_44[0][0]           
____________________________________________________________________________________________________
convolution2d_47 (Convolution2D) (None, 7, 7, 128)     204928      convolution2d_46[0][0]           
____________________________________________________________________________________________________
maxpooling2d_10 (MaxPooling2D)   (None, 7, 7, 640)     0           merge_7[0][0]                    
____________________________________________________________________________________________________
merge_8 (Merge)                  (None, 7, 7, 1024)    0           convolution2d_45[0][0]           
                                                                   convolution2d_47[0][0]           
                                                                   maxpooling2d_10[0][0]            
____________________________________________________________________________________________________
convolution2d_49 (Convolution2D) (None, 7, 7, 192)     196800      merge_8[0][0]                    
____________________________________________________________________________________________________
convolution2d_51 (Convolution2D) (None, 7, 7, 48)      49200       merge_8[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_11 (MaxPooling2D)   (None, 7, 7, 1024)    0           merge_8[0][0]                    
____________________________________________________________________________________________________
convolution2d_48 (Convolution2D) (None, 7, 7, 384)     393600      merge_8[0][0]                    
____________________________________________________________________________________________________
convolution2d_50 (Convolution2D) (None, 7, 7, 384)     663936      convolution2d_49[0][0]           
____________________________________________________________________________________________________
convolution2d_52 (Convolution2D) (None, 7, 7, 128)     153728      convolution2d_51[0][0]           
____________________________________________________________________________________________________
convolution2d_53 (Convolution2D) (None, 7, 7, 128)     131200      maxpooling2d_11[0][0]            
____________________________________________________________________________________________________
merge_9 (Merge)                  (None, 7, 7, 1024)    0           convolution2d_48[0][0]           
                                                                   convolution2d_50[0][0]           
                                                                   convolution2d_52[0][0]           
                                                                   convolution2d_53[0][0]           
____________________________________________________________________________________________________
convolution2d_55 (Convolution2D) (None, 7, 7, 192)     196800      merge_9[0][0]                    
____________________________________________________________________________________________________
convolution2d_57 (Convolution2D) (None, 7, 7, 48)      49200       merge_9[0][0]                    
____________________________________________________________________________________________________
maxpooling2d_12 (MaxPooling2D)   (None, 7, 7, 1024)    0           merge_9[0][0]                    
____________________________________________________________________________________________________
convolution2d_54 (Convolution2D) (None, 7, 7, 384)     393600      merge_9[0][0]                    
____________________________________________________________________________________________________
convolution2d_56 (Convolution2D) (None, 7, 7, 384)     663936      convolution2d_55[0][0]           
____________________________________________________________________________________________________
convolution2d_58 (Convolution2D) (None, 7, 7, 128)     153728      convolution2d_57[0][0]           
____________________________________________________________________________________________________
convolution2d_59 (Convolution2D) (None, 7, 7, 128)     131200      maxpooling2d_12[0][0]            
____________________________________________________________________________________________________
merge_10 (Merge)                 (None, 7, 7, 1024)    0           convolution2d_54[0][0]           
                                                                   convolution2d_56[0][0]           
                                                                   convolution2d_58[0][0]           
                                                                   convolution2d_59[0][0]           
____________________________________________________________________________________________________
averagepooling2d_1 (AveragePoolin(None, 1, 1, 1024)    0           merge_10[0][0]                   
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1024)          0           averagepooling2d_1[0][0]         
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 128)           131200      flatten_1[0][0]                  
____________________________________________________________________________________________________
lambda_1 (Lambda)                (None, 128)           0           dense_1[0][0]                    
====================================================================================================
Total params: 7456944
____________________________________________________________________________________________________
None
</code></pre>",,4,13,,2016/12/10 13:18,12.0,2021/7/2 12:21,2021/7/2 12:19,,1021819.0,,2439854.0,,1,24,neural-network|tensorflow|keras,17459,55.1681,,4,facenet triplet loss kera try implement facenet kera tensorflow backend problem triplet loss call fit function n number image define custom loss function follow dummy array fill problem even first iteration batch size model start predict embedding image first prediction batch every embedding different fit predict suddenly embeddings become almost image batch also notice lambda layer end model normalize output net embeddings unit length suggest face net study anybody help model summary
5,5,61473330,CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`,"<p>I got the following error when I ran my pytorch deep learning model in colab</p>

<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1370         ret = torch.addmm(bias, input, weight.t())
   1371     else:
-&gt; 1372         output = input.matmul(weight.t())
   1373         if bias is not None:
   1374             output += bias

RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
</code></pre>

<p>I even reduced batch size from 128 to 64 i.e., reduced to half,  but still, I got this error. Earlier, I ran the same code with a batch size of 128 but didn't get any error like this.</p>",,7,0,,2020/4/28 5:39,2.0,2021/7/23 10:14,2020/4/28 6:01,,681865.0,,6151940.0,,1,18,nlp|pytorch|bert-language-model,41081,76.6546,,4,cuda error cublas status alloc fail call cublascreate handle get following error run pytorch deep learning model colab even reduce batch size e reduce half still get error earlier run code batch size get error like
581,581,49404309,How does keras handle multiple losses?,"<p>If I have something like:</p>



<pre class=""lang-python prettyprint-override""><code>model = Model(inputs = input, outputs = [y1,y2])

l1 = 0.5
l2 = 0.3
model.compile(loss = [loss1,loss2], loss_weights = [l1,l2], ...)
</code></pre>

<p>what does Keras do with the losses to obtain the final loss?
Is it something like:</p>

<pre class=""lang-python prettyprint-override""><code>final_loss = l1*loss1 + l2*loss2
</code></pre>

<p>Also, what does it mean during training? Is the loss2 only used to update the weights on layers where y2 comes from? Or is it used for all the model's layers?</p>",49406231.0,2,0,,2018/3/21 10:48,22.0,2020/1/30 18:02,2020/1/30 18:02,,3924118.0,,9522193.0,,1,58,deep-learning|keras|backpropagation|loss-function,33010,118.275,,4,keras handle multiple loss something like keras loss obtain final loss something like also mean training loss use update weight layer come use model layer
232,232,44328530,"How to get a uniform distribution in a range [r1,r2] in PyTorch?","<p>I want to get a 2-D <code>torch.Tensor</code> with size <code>[a,b]</code> filled with values from a uniform distribution (in range <code>[r1,r2]</code>) in PyTorch.</p>",44375813.0,8,2,,2017/6/2 12:05,5.0,2021/5/9 13:02,2021/5/9 13:02,,9067615.0,,4933403.0,,1,40,python|pytorch|uniform-distribution,45564,168.634,,3,get uniform distribution range r r pytorch want get size fill value uniform distribution range pytorch
645,645,36952763,How to return history of validation loss in Keras,"<p>Using Anaconda Python 2.7 Windows 10.</p>

<p>I am training a language model using the Keras exmaple:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

def sample(a, temperature=1.0):
    # helper function to sample an index from a probability array
    a = np.log(a) / temperature
    a = np.exp(a) / np.sum(np.exp(a))
    return np.argmax(np.random.multinomial(1, a, 1))


# train the model, output generated text after each iteration
for iteration in range(1, 3):
    print()
    print('-' * 50)
    print('Iteration', iteration)
    model.fit(X, y, batch_size=128, nb_epoch=1)
    start_index = random.randint(0, len(text) - maxlen - 1)

    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print()
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + maxlen]
        generated += sentence
        print('----- Generating with seed: ""' + sentence + '""')
        sys.stdout.write(generated)

        for i in range(400):
            x = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(sentence):
                x[0, t, char_indices[char]] = 1.

            preds = model.predict(x, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()
</code></pre>

<p>According to Keras documentation, the <code>model.fit</code> method returns a History callback, which has a history attribute containing the lists of successive losses and other metrics.</p>

<pre><code>hist = model.fit(X, y, validation_split=0.2)
print(hist.history)
</code></pre>

<p>After training my model, if I run <code>print(model.history)</code> I get the error:</p>

<pre><code> AttributeError: 'Sequential' object has no attribute 'history'
</code></pre>

<p>How do I return my model history after training my model with the above code?</p>

<p><strong>UPDATE</strong></p>

<p>The issue was that:</p>

<p>The following had to first be defined:</p>

<pre><code>from keras.callbacks import History 
history = History()
</code></pre>

<p>The callbacks option had to be called</p>

<pre><code>model.fit(X_train, Y_train, nb_epoch=5, batch_size=16, callbacks=[history])
</code></pre>

<p>But now if I print</p>

<pre><code>print(history.History)
</code></pre>

<p>it returns</p>

<pre><code>{}
</code></pre>

<p>even though I ran an iteration. </p>",36956440.0,10,2,,2016/4/30 8:45,13.0,2021/7/6 19:51,2017/3/10 15:21,,5974433.0,,5310324.0,,1,58,python|neural-network|nlp|deep-learning|keras,130988,243.469,,4,return history validation loss kera use anaconda python window train language model use kera exmaple accord keras documentation method return history callback history attribute contain list successive loss metric train model run get error return model history train model code update issue following first define callback option call print return even though run iteration
780,780,52026652,OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable,"<p>I am facing a problem now, unable to run any program in the cluster. It gives error.</p>
<pre><code>OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
OpenBLAS blas_thread_init: RLIMIT_NPROC 64 current, 64 max
OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
OpenBLAS blas_thread_init: RLIMIT_NPROC 64 current, 64 max
OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
OpenBLAS blas_thread_init: RLIMIT_NPROC 64 current, 64 max
OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
OpenBLAS blas_thread_init: RLIMIT_NPROC 64 current, 64 max
Traceback (most recent call last):
  File &quot;hello-world.py&quot;, line 1, in &lt;module&gt;
    from keras.models import Sequential
  File &quot;/home/amalli2s/anaconda3/lib/python3.6/site-packages/keras/__init__.py&quot;, line 3, in &lt;module&gt;
    from . import utils
  File &quot;/home/amalli2s/anaconda3/lib/python3.6/site-packages/keras/utils/__init__.py&quot;, line 2, in &lt;module&gt;
    from . import np_utils
  File &quot;/home/amalli2s/anaconda3/lib/python3.6/site-packages/keras/utils/np_utils.py&quot;, line 6, in &lt;module&gt;
    import numpy as np
  File &quot;/home/amalli2s/.local/lib/python3.6/site-packages/numpy/__init__.py&quot;, line 142, in &lt;module&gt;
    from . import add_newdocs
  File &quot;/home/amalli2s/.local/lib/python3.6/site-packages/numpy/add_newdocs.py&quot;, line 13, in &lt;module&gt;
    from numpy.lib import add_newdoc
  File &quot;/home/amalli2s/.local/lib/python3.6/site-packages/numpy/lib/__init__.py&quot;, line 8, in &lt;module&gt;
    from .type_check import *
  File &quot;/home/amalli2s/.local/lib/python3.6/site-packages/numpy/lib/type_check.py&quot;, line 11, in &lt;module&gt;
    import numpy.core.numeric as _nx
  File &quot;/home/amalli2s/.local/lib/python3.6/site-packages/numpy/core/__init__.py&quot;, line 16, in &lt;module&gt;
    from . import multiarray
SystemError: initialization of multiarray raised unreported exception
</code></pre>
<p>This problem, i assume to be same as this one <a href=""https://stackoverflow.com/questions/51256738/multiple-instances-of-python-running-simultaneously-limited-to-35"">Multiple instances of Python running simultaneously limited to 35</a></p>
<p>So according to the solution when I set
<code>export OPENBLAS_NUM_THREADS=1</code></p>
<p>then I end up getting following error:</p>
<pre><code>terminate called after throwing an instance of 'std::system_error'
  what():  Resource temporarily unavailable
Aborted
</code></pre>
<p>Is there anybody else facing same issue or has idea on how to solve this ? Thank you.</p>
<p>EDIT:
Ok, Seems like this happened because of some config restrictions the admins were trying to implement. Now it works again.</p>",54746150.0,5,0,,2018/8/26 13:20,3.0,2021/6/14 21:12,2020/9/26 16:35,,1578967.0,,1578967.0,,1,11,python-3.x|keras|openblas,11059,70.1749,,4,openblas blas thread init pthread create resource temporarily unavailable face problem unable run program cluster give error problem assume one multiple instance python run simultaneously limit accord solution set end get follow error anybody else face issue idea solve thank edit ok seem like happen config restriction admins try implement work
770,770,51748514,Does ImageDataGenerator add more images to my dataset?,"<p>I'm trying to do image classification with the Inception V3 model. Does <code>ImageDataGenerator</code> from Keras create new images which are added onto my dataset? If I have 1000 images, will using this function double it to 2000 images which are used for training? Is there a way to know how many images were created and now fed into the model?</p>",,7,0,,2018/8/8 13:54,14.0,2021/6/30 13:56,2018/9/6 16:04,,2099607.0,,8926217.0,,1,36,python|tensorflow|machine-learning|keras|computer-vision,15773,163.192,,2,imagedatagenerator add image dataset try image classification inception v model keras create new image add onto dataset image use function double image use training way know many image create feed model
481,481,59823283,Could not load dynamic library 'cudart64_101.dll' on tensorflow CPU-only installation,"<p>I just installed the latest version of Tensorflow via <code>pip install tensorflow</code> and whenever I run a program, I get the log message:</p>

<blockquote>
  <p>W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found</p>
</blockquote>

<p>Is this bad? How do I fix the error?</p>",59823284.0,15,0,,2020/1/20 12:26,46.0,2021/8/3 15:53,2020/5/27 15:25,,10908375.0,,3214872.0,,1,113,python|python-3.x|tensorflow|keras|tensorflow2.0,253535,602.616,,1,could load dynamic library cudart dll tensorflow cpu installation instal late version tensorflow via whenever run program get log message w tensorflow stream executor platform default dso loader cc could load dynamic library cudart dll dlerror cudart dll find bad fix error
263,263,44720822,ValueError with Concatenate Layer (Keras functional API),"<p>After some search here, I still can't find a solution for this. I'm new to Keras, apologies if there is a solution and I actually didn't understand how it was related to my problem.</p>

<p>I am making a small RNN with Keras 2/Functional API, and I have trouble to make the Concatenate Layer work.</p>

<p>Here is my structure :</p>

<pre><code>inputSentence = Input(shape=(30, 91))
sentenceMatrix = LSTM(91, return_sequences=True, input_shape=(30, 91))(inputSentence)

inputDeletion = Input(shape=(30, 1))
deletionMatrix = (LSTM(30, return_sequences=True, input_shape=(30, 1)))(inputDeletion)

fusion = Concatenate([sentenceMatrix, deletionMatrix])
fusion = Dense(122, activation='relu')(fusion)
fusion = Dense(102, activation='relu')(fusion)
fusion = Dense(91, activation='sigmoid')(fusion)

F = Model(inputs=[inputSentence, inputDeletion], outputs=fusion)
</code></pre>

<p>And here is the error:</p>

<pre><code>ValueError: Unexpectedly found an instance of type `&lt;class 'keras.layers.merge.Concatenate'&gt;`. Expected a symbolic tensor instance.
</code></pre>

<p>Full History if it helps a bit more :</p>

<pre><code>Using TensorFlow backend.
    str(inputs) + '. All inputs to the layer '
ValueError: Layer dense_1 was called with an input that isn't a symbolic tensor. Received type: &lt;class 'keras.layers.merge.Concatenate'&gt;. Full input: [&lt;keras.layers.merge.Concatenate object at 0x00000000340DC4E0&gt;]. All inputs to the layer should be tensors.
self.assert_input_compatibility(inputs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 425, in assert_input_compatibility
fusion = Dense(122, activation='relu')(fusion)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 552, in __call__
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 419, in assert_input_compatibility
K.is_keras_tensor(x)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 392, in is_keras_tensor
raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '
ValueError: Unexpectedly found an instance of type `&lt;class 'keras.layers.merge.Concatenate'&gt;`. Expected a symbolic tensor instance.
</code></pre>

<p>I'm using Python 3.6, with Spyder 3.1.4, on Windows 7. I upgraded TensorFlow and Keras with pip this morning.</p>

<p>Thank you for any help provided !</p>",44721656.0,2,0,,2017/6/23 11:52,3.0,2019/1/9 9:11,2017/10/29 19:34,,5974433.0,,8156923.0,,1,9,python|neural-network|concatenation|keras|valueerror,10433,57.6736,,3,valueerror concatenate layer keras functional api search still find solution new keras apology solution actually understand relate problem make small rnn keras functional api trouble make concatenate layer work structure error full history help bit use python spyder window upgraded tensorflow kera pip morning thank help provide
118,118,41855512,How does data normalization work in keras during prediction?,"<p>I see that the imageDataGenerator allows me to specify different styles of data normalization, e.g. featurewise_center, samplewise_center, etc.</p>

<p>I see from the examples that if I specify one of these options, then I need to call the fit method on the generator in order to allow the generator to compute statistics like the mean image on the generator.</p>

<pre><code>(X_train, y_train), (X_test, y_test) = cifar10.load_data()
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True)

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(X_train)

# fits the model on batches with real-time data augmentation:
model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),
                samples_per_epoch=len(X_train), nb_epoch=nb_epoch)
</code></pre>

<p>My question is, how does prediction work if I have specified data normalization during training? I can't see how in the framework I would even pass knowledge of the training set mean/std deviation along to predict to allow me to normalize my test data myself, but I also don't see in the training code where this information is stored.</p>

<p>Are the image statistics needed for normalization stored in the model so that they can be used during prediction?</p>",41855937.0,4,0,,2017/1/25 15:33,17.0,2017/5/12 6:14,2017/1/25 19:38,,5974433.0,,2221270.0,,1,21,python|machine-learning|tensorflow|neural-network|keras,26741,89.5087,,3,data normalization work kera prediction see imagedatagenerator allow specify different style data normalization e g featurewise center samplewise center etc see example specify one option need call fit method generator order allow generator compute statistic like mean image generator question prediction work specify data normalization train see framework would even pass knowledge training set mean std deviation along predict allow normalize test data also see training code information store image statistic need normalization store model use prediction
635,635,51047676,How to get accuracy of model using keras?,"<p>After fitting the model (which was running for a couple of hours), I wanted to get the accuracy with the following code: </p>

<pre><code>train_loss=hist.history['loss']
val_loss=hist.history['val_loss']
train_acc=hist.history['acc']
val_acc=hist.history['val_acc']
xc=range(nb_epoch)
</code></pre>

<p>of the trained model, but was getting an error, which is caused by the deprecated methods I was using. </p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-233-081ed5e89aa4&gt; in &lt;module&gt;()
      3 train_loss=hist.history['loss']
      4 val_loss=hist.history['val_loss']
----&gt; 5 train_acc=hist.history['acc']
      6 val_acc=hist.history['val_acc']
      7 xc=range(nb_epoch)

KeyError: 'acc'
</code></pre>

<p>The code I used to fit the model before trying to read the accuracy, is the following: </p>

<pre><code>hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
            verbose=1, validation_data=(X_test, Y_test))


hist = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, 
            verbose=1, validation_split=0.2)
</code></pre>

<p>Which produces this output when running it:</p>

<pre><code>Epoch 1/20
237/237 [==============================] - 104s 440ms/step - loss: 6.2802 - val_loss: 2.4209
    .....
    .....
    .....
Epoch 19/20
    189/189 [==============================] - 91s 480ms/step - loss: 0.0590 - val_loss: 0.2193
    Epoch 20/20
    189/189 [==============================] - 85s 451ms/step - loss: 0.0201 - val_loss: 0.2312
</code></pre>

<p>I've noticed that I was running deprecated methods &amp; arguments. </p>

<p>So how can I read the accuracy and val_accuracy without having to fit again, and waiting for a couple of hours again? I tried to replace <code>train_acc=hist.history['acc']</code> with <code>train_acc=hist.history['accuracy']</code> but it didn't help.</p>",,4,1,,2018/6/26 16:34,4.0,2020/10/23 7:05,,,,,1324765.0,,1,19,python|tensorflow|machine-learning|keras|deep-learning,48178,72.9314,,4,get accuracy model use kera fit model run couple hour want get accuracy following code trained model get error cause deprecated method use code use fit model try read accuracy follow produce output run notice run deprecate method argument read accuracy val accuracy without fit wait couple hour try replace help
25,25,53980031,PyTorch custom loss function,"<p>How should a custom loss function be implemented ? Using below code is causing error :</p>
<pre><code>import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.utils.data as data_utils
import torch.nn as nn
import torch.nn.functional as F

num_epochs = 20

x1 = np.array([0,0])
x2 = np.array([0,1])
x3 = np.array([1,0])
x4 = np.array([1,1])

num_epochs = 200

class cus2(torch.nn.Module):
    
    def __init__(self):
        super(cus2,self).__init__()
    
    def forward(self, outputs, labels):
        # reshape labels to give a flat vector of length batch_size*seq_len
        labels = labels.view(-1)  

        # mask out 'PAD' tokens
        mask = (labels &gt;= 0).float()

        # the number of tokens is the sum of elements in mask
        num_tokens = int(torch.sum(mask).data[0])

        # pick the values corresponding to labels and multiply by mask
        outputs = outputs[range(outputs.shape[0]), labels]*mask

        # cross entropy loss for all non 'PAD' tokens
        return -torch.sum(outputs)/num_tokens


x = torch.tensor([x1,x2,x3,x4]).float()

y = torch.tensor([0,1,1,0]).long()

train = data_utils.TensorDataset(x,y)
train_loader = data_utils.DataLoader(train , batch_size=2 , shuffle=True)

device = 'cpu'

input_size = 2
hidden_size = 100 
num_classes = 2

learning_rate = .0001

class NeuralNet(nn.Module) : 
    def __init__(self, input_size, hidden_size, num_classes) : 
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(input_size , hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size , num_classes)

    def forward(self, x) : 
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out
        
for i in range(0 , 1) :
        
        model = NeuralNet(input_size, hidden_size, num_classes).to(device)
        
        criterion = nn.CrossEntropyLoss()
#         criterion = Regress_Loss()
#         criterion = cus2()
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        
        total_step = len(train_loader)
        for epoch in range(num_epochs) : 
            for i,(images , labels) in enumerate(train_loader) : 
                images = images.reshape(-1 , 2).to(device)
                labels = labels.to(device)
                
                outputs = model(images)
                loss = criterion(outputs , labels)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
#                 print(loss)
                
        outputs = model(x)
        
        print(outputs.data.max(1)[1])
</code></pre>
<p>makes perfect predictions on training data :</p>
<pre><code>tensor([0, 1, 1, 0])
</code></pre>
<p>Using a custom loss function from <a href=""https://web.archive.org/web/20181129040718/https://cs230-stanford.github.io/pytorch-nlp.html"" rel=""nofollow noreferrer"">here</a>:</p>
<p><a href=""https://i.stack.imgur.com/Eh3bZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Eh3bZ.png"" alt=""image of the code used for the cus2 class"" /></a></p>
<p>is implemented in above code as <code>cus2</code></p>
<p>Un-commenting code <code>#         criterion = cus2()</code> to use this loss function returns :</p>
<pre><code>tensor([0, 0, 0, 0])
</code></pre>
<p>A warning is also returned :</p>
<blockquote>
<p>UserWarning: invalid index of a 0-dim tensor. This will be an error in
PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python
number</p>
</blockquote>
<p>I've not implemented the custom loss function correctly ?</p>",53980730.0,3,0,,2018/12/30 17:52,8.0,2021/7/29 13:21,2021/7/29 13:21,,1476885.0,,470184.0,,1,22,python|deep-learning|pytorch,18179,52.2383,,4,pytorch custom loss function custom loss function implement use code cause error make perfect prediction training data use custom loss function implement code un comment code use loss function return warning also return userwarning invalid index dim tensor error pytorch use tensor item convert dim tensor python number implement custom loss function correctly
761,761,51677788,Data Augmentation in PyTorch,"<p>I am a little bit confused about the data augmentation performed in PyTorch. Now, as far as I know, when we are performing data augmentation, we are KEEPING our original dataset, and then adding other versions of it (Flipping, Cropping...etc). But that doesn't seem like happening in PyTorch. As far as I understood from the references, when we use <code>data.transforms</code> in PyTorch, then it applies them one by one. So for example:</p>

<pre class=""lang-python prettyprint-override""><code>data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
</code></pre>

<p>Here , for the training, we are first randomly cropping the image and resizing it to shape <code>(224,224)</code>. Then we are taking these <code>(224,224)</code> images and horizontally flipping them. Therefore, our dataset is now containing ONLY the horizontally flipped images, so our original images are lost in this case. </p>

<p>Am I right? Is this understanding correct? If not, then where do we tell PyTorch in this code above (taken from Official Documentation) to keep the original images and resize them to the expected shape <code>(224,224)</code>?</p>

<p>Thanks </p>",51678124.0,5,0,,2018/8/3 17:51,26.0,2021/6/25 13:05,2018/8/3 18:16,,9109354.0,,9109354.0,,1,67,python|image-processing|dataset|pytorch|data-augmentation,51475,205.846,,2,data augmentation pytorch little bit confuse data augmentation perform pytorch far know perform data augmentation keep original dataset add version flip crop etc seem like happen pytorch far understood reference use pytorch apply one one example training first randomly crop image resize shape take image horizontally flip therefore dataset contain horizontally flipped image original image lose case right understanding correct tell pytorch code take official documentation keep original image resize expect shape thanks
225,225,44176982,How does the Flatten layer work in Keras?,"<p>I am using the TensorFlow backend.</p>

<p>I am applying a convolution, max-pooling, flatten and a dense layer sequentially. The convolution requires a 3D input (height, width, color_channels_depth).</p>

<p>After the convolution, this becomes (height, width, Number_of_filters).</p>

<p>After applying max-pooling height and width changes. But, after applying the flatten layer, what happens exactly? For example, if the input before flatten is (24, 24, 32), then how it flattens it out?</p>

<p>Is it sequential like (24 * 24) for height, weight for each filter number sequentially, or in some other way? An example would be appreciated with actual values.</p>",44191138.0,3,1,,2017/5/25 9:29,6.0,2019/10/15 16:57,2019/10/15 16:57,,3924118.0,,7990757.0,,1,25,tensorflow|neural-network|keras|keras-layer,45963,94.6496,,3,flatten layer work kera use tensorflow backend apply convolution max pool flatten dense layer sequentially convolution require input height width color channel depth convolution become height width number filter apply max pool height width change apply flatten layer happen exactly example input flatten flatten sequential like height weight filter number sequentially way example would appreciate actual value
457,457,21342931,Error importing Theano,"<p>After installing python, numpy, scipy and theano to ~/.local, I tried to import theano but it threw an error:</p>

<pre><code>&gt;&gt;&gt; import theano
Problem occurred during compilation with the command line below:
g++ -shared -g -march=core2 -mcx16 -msahf --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=4096 -mtune=core2 -D NPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -D NPY_ARRAY_ENSUREARRAY=NPY_ENSUREARRAY -D NPY_ARRAY_ENSURECOPY=NPY_ENSURECOPY -D NPY_ARRAY_ALIGNED=NPY_ALIGNED -D NPY_ARRAY_WRITEABLE=NPY_WRITEABLE -D NPY_ARRAY_UPDATE_ALL=NPY_UPDATE_ALL -D NPY_ARRAY_C_CONTIGUOUS=NPY_C_CONTIGUOUS -D NPY_ARRAY_F_CONTIGUOUS=NPY_F_CONTIGUOUS -m64 -fPIC -I/opt/python/lib/python2.7/site-packages/numpy/core/include -I/home/minh.lengoc/.local/include/python2.7 -o /home/minh.lengoc/.theano/compiledir_Linux-2.6.32-279.14.1.el6.x86_64-x86_64-with-centos-6.3-Final-x86_64-2.7.6-64/lazylinker_ext/lazylinker_ext.so /home/minh.lengoc/.theano/compiledir_Linux-2.6.32-279.14.1.el6.x86_64-x86_64-with-centos-6.3-Final-x86_64-2.7.6-64/lazylinker_ext/mod.cpp -L/home/minh.lengoc/.local/lib -lpython2.7
/usr/bin/ld: /home/minh.lengoc/.local/lib/libpython2.7.a(abstract.o): relocation R_X86_64_32 against `.rodata.str1.8' can not be used when making a shared object; recompile with -fPIC
/home/minh.lengoc/.local/lib/libpython2.7.a: could not read symbols: Bad value
collect2: ld returned 1 exit status

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/home/minh.lengoc/setup/Theano/theano/__init__.py"", line 55, in &lt;module&gt;
    from theano.compile import \
  File ""/home/minh.lengoc/setup/Theano/theano/compile/__init__.py"", line 6, in &lt;module&gt;
    from theano.compile.function_module import *
  File ""/home/minh.lengoc/setup/Theano/theano/compile/function_module.py"", line 18, in &lt;module&gt;
    import theano.compile.mode
  File ""/home/minh.lengoc/setup/Theano/theano/compile/mode.py"", line 11, in &lt;module&gt;
    import theano.gof.vm
  File ""/home/minh.lengoc/setup/Theano/theano/gof/vm.py"", line 516, in &lt;module&gt;
    import lazylinker_c
  File ""/home/minh.lengoc/setup/Theano/theano/gof/lazylinker_c.py"", line 86, in &lt;module&gt;
    preargs=args)
  File ""/home/minh.lengoc/setup/Theano/theano/gof/cmodule.py"", line 1975, in compile_str
    (status, compile_stderr.replace('\n', '. ')))
Exception: Compilation failed (return status=1): /usr/bin/ld: /home/minh.lengoc/.local/lib/libpython2.7.a(abstract.o): relocation R_X86_64_32 against `.rodata.str1.8' can not be used when making a shared object; recompile with -fPIC. /home/minh.lengoc/.local/lib/libpython2.7.a: could not read symbols: Bad value. collect2: ld returned 1 exit status. 
</code></pre>

<p>I'm installing on a Red Hat box:</p>

<pre><code>$ cat /proc/version
Linux version 2.6.32-279.14.1.el6.x86_64 (mockbuild@c6b8.bsys.dev.centos.org) (gcc version 4.4.6 20120305 (Red Hat 4.4.6-4) (GCC) ) #1 SMP Tue Nov 6 23:43:09 UTC 2012
</code></pre>

<p>What should I do...?</p>",21345831.0,3,0,,2014/1/24 21:36,3.0,2019/2/2 12:13,,,,,217802.0,,1,16,python|theano,7549,50.1116,,1,error import theano instal python numpy scipy theano local try import theano throw error instal red hat box
483,483,59864408,tensorflow:Your input ran out of data,"<p>I am working on a seq2seq keras/tensorflow 2.0 model. Every time the user inputs something, my model prints the response perfectly fine. However on the last line of each response I get this:</p>
<blockquote>
<p>You: WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least <code>steps_per_epoch * epochs</code> batches (in this case, 2 batches). You may need to use the repeat() function when building your dataset.</p>
</blockquote>
<p>The &quot;You:&quot; is my last output, before the user is supposed to type something new in. The model works totally fine, but I guess no error is ever good, but I don't quite get this error. It says &quot;interrupting training&quot;, however I am not training anything, this program loads an already trained model. I guess this is why the error is not stopping the program?</p>
<p>In case it helps, my model looks like this:</p>
<pre><code>intent_model = keras.Sequential([
    keras.layers.Dense(8, input_shape=[len(train_x[0])]),  # input layer
    keras.layers.Dense(8),  # hidden layer
    keras.layers.Dense(len(train_y[0]), activation=&quot;softmax&quot;),  # output layer
])

intent_model.compile(optimizer=&quot;adam&quot;, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
intent_model.fit(train_x, train_y, epochs=epochs)

test_loss, test_acc = intent_model.evaluate(train_x, train_y)
print(&quot;Tested Acc:&quot;, test_acc)

intent_model.save(&quot;models/intent_model.h5&quot;)
</code></pre>",,7,1,,2020/1/22 16:40,3.0,2021/6/11 10:52,2020/12/21 17:13,,10908375.0,,12741942.0,,1,19,python|tensorflow|machine-learning|keras|deep-learning,32261,61.6347,,4,tensorflow input run data work seq seq kera tensorflow model every time user input something model print response perfectly fine however last line response get warn tensorflow input run data interrupt train make sure dataset generator generate least batch case batch may need use repeat function build dataset last output user suppose type something new model work totally fine guess error ever good quite get error say interrupt train however train anything program load already trained model guess error stop program case help model look like
563,563,48851558,"Tensorflow estimator ValueError: logits and labels must have the same shape ((?, 1) vs (?,))","<p>I'm relatively new to ML, thought I'll start with keras. Here I'm classifying movie reviews as positive or negative using binary crossentropy. So, when I'm trying to wrap my keras model with tensorflow estimator, I get the error:</p>

<blockquote>
  <p>Tensorflow estimator ValueError: logits and labels must have the same shape ((?, 1) vs (?,))</p>
</blockquote>

<p>I'm using sigmoid activation as my last layer, guess I'm missing something trivial here. Any help? </p>

<pre><code>from tensorflow import keras
import tensorflow as tf
print(""Tensorflow {} loaded"".format(tf.__version__))
import numpy as np

keras.__version__
from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
def vectorize_sequences(sequences, dimension=10000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.  # set specific indices of results[i] to 1s
    return results.astype('float32')

# Our vectorized training data
x_train = vectorize_sequences(train_data)

# Our vectorized test data
x_test = vectorize_sequences(test_data)

# Our vectorized labels
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

model = keras.models.Sequential()
model.add(keras.layers.Dense(16, activation='relu', input_shape=(10000,), name='reviews'))
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
estimator_model = keras.estimator.model_to_estimator(keras_model=model)

def input_function(features,labels=None,shuffle=False,epochs=None,batch_size=None):
    input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""reviews_input"": features},
        y=labels,
        shuffle=shuffle,
        num_epochs=epochs,
        batch_size=batch_size
    )
    return input_fn

estimator_model.train(input_fn=input_function(partial_x_train, partial_y_train, True,20,512))
score = estimator_model.evaluate(input_function(x_val, labels=y_val))
print(score)
</code></pre>",48851837.0,2,0,,2018/2/18 12:15,6.0,2021/3/25 15:48,2018/2/18 13:43,,4282745.0,,8010010.0,,1,23,python|tensorflow|keras,60897,57.3384,,4,tensorflow estimator valueerror logits label must shape v relatively new ml thought start kera classify movie review positive negative use binary crossentropy try wrap kera model tensorflow estimator get error tensorflow estimator valueerror logits label must shape v use sigmoid activation last layer guess miss something trivial help
275,275,44852153,Layer called with an input that isn't a symbolic tensor keras,"<p>I'm trying to pass the output of one layer into two different layers and then join them back together. However, I'm being stopped by this error which is telling me that my input isn't a symbolic tensor.</p>

<pre><code>Received type: &lt;class 'keras.layers.recurrent.LSTM'&gt;. All inputs to the layers should be tensors.
</code></pre>

<p>However, I believe I'm following the documentation quite closely:
<a href=""https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models"" rel=""noreferrer"">https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models</a></p>

<p>and am not entirely sure why this is wrong? </p>

<pre><code>net_input = Input(shape=(maxlen, len(chars)), name='net_input')
lstm_out = LSTM(128, input_shape=(maxlen, len(chars)))

book_out = Dense(len(books), activation='softmax', name='book_output')(lstm_out)
char_out = Dense(len(chars-4), activation='softmax', name='char_output')(lstm_out)

x = keras.layers.concatenate([book_out, char_out])
net_output = Dense(len(chars)+len(books), activation='sigmoid', name='net_output')

model = Model(inputs=[net_input], outputs=[net_output])
</code></pre>

<p>Thanks</p>",,3,0,,2017/6/30 17:38,2.0,2019/7/30 21:23,2017/7/5 14:31,,977098.0,,3853339.0,,1,22,python|neural-network|keras|recurrent-neural-network,59439,66.2963,,3,layer call input symbolic tensor kera try pass output one layer two different layer join back together however stop error tell input symbolic tensor however believe follow documentation quite closely multi input multi output model entirely sure wrong thanks
829,829,17445280,theano - print value of TensorVariable,"<p><strong>How can I print the numerical value of a theano TensorVariable?</strong>
I'm new to theano, so please be patient :)</p>

<p>I have a function where I get <code>y</code> as a parameter.
Now I want to debug-print the shape of this <code>y</code> to the console.
Using</p>

<pre><code>print y.shape
</code></pre>

<p>results in the console output (i was expecting numbers, i.e. <code>(2,4,4)</code>):</p>

<pre><code>Shape.0
</code></pre>

<p>Or how can I print the numerical result of for example the following code (this counts how many values in <code>y</code> are bigger than half the maximum):</p>

<pre><code>errorCount = T.sum(T.gt(T.abs_(y),T.max(y)/2.0))
</code></pre>

<p><code>errorCount</code> should be a single number because <code>T.sum</code> sums up all the values.
But using</p>

<pre><code>print errCount
</code></pre>

<p>gives me (expected something like <code>134</code>):</p>

<pre><code>Sum.0
</code></pre>",17453598.0,5,1,,2013/7/3 10:13,5.0,2020/1/13 1:40,,,,,869402.0,,1,41,python|debugging|theano,31055,116.969,,3,theano print value tensorvariable print numerical value theano tensorvariable new theano please patient function get parameter want debug print shape console use result console output expect number e print numerical result example following code count many value big half maximum single number sum value use give expect something like
404,404,47262955,How to import keras from tf.keras in Tensorflow?,"<pre><code>import tensorflow as tf
import tensorflow 

from tensorflow import keras
from keras.layers import Dense
</code></pre>

<p>I am getting the below error</p>

<pre><code>from keras.layers import Input, Dense
Traceback (most recent call last):

  File ""&lt;ipython-input-6-b5da44e251a5&gt;"", line 1, in &lt;module&gt;
    from keras.layers import Input, Dense

ModuleNotFoundError: No module named 'keras'
</code></pre>

<p>How do I solve this?</p>

<p>Note: I am using Tensorflow version 1.4</p>",47265298.0,8,0,,2017/11/13 11:11,16.0,2021/5/15 15:09,2017/11/24 12:01,,3424990.0,,3424990.0,,1,63,python|tensorflow|deep-learning|keras,140906,245.396,,1,import kera tf kera tensorflow get error solve note use tensorflow version
89,89,41344168,"What is a `""Python""` layer in caffe?","<p>Caffe has a layer type <code>""Python""</code>.  </p>

<p>For instance, this layer type can be used as a <a href=""https://github.com/BVLC/caffe/blob/master/examples/pycaffe/layers/pyloss.py"" rel=""noreferrer"">loss layer</a>.<br>
On other occasions it is used as an <a href=""https://stackoverflow.com/a/34996628/1714410"">input layer</a>.</p>

<p>What is this layer type?<br>
How can this layer be used?</p>",41481539.0,3,0,,2016/12/27 11:17,12.0,2019/8/6 12:19,2019/8/6 12:19,,1714410.0,,1714410.0,,1,19,python|machine-learning|neural-network|deep-learning|caffe,7698,76.9455,,3,python layer caffe caffe layer type instance layer type use loss layer occasion use input layer layer type layer use
536,536,36248056,how to setup cuDnn with theano on Windows 7 64 bit,"<p>I have installed <code>Theano</code> framework and enabled CUDA on my machine, however when I ""import theano"" in my python console, I got the following message:</p>

<pre><code>&gt;&gt;&gt; import theano
Using gpu device 0: GeForce GTX 950 (CNMeM is disabled, CuDNN not available)
</code></pre>

<p>Now that ""CuDNN not available"",  I downloaded <code>cuDnn</code> from Nvidia website. I also updated 'path' in environment, and added 'optimizer_including=cudnn' in '.theanorc.txt' config file. </p>

<p>Then, I tried again, but failed, with:</p>

<pre><code>&gt;&gt;&gt; import theano
Using gpu device 0: GeForce GTX 950 (CNMeM is disabled, CuDNN not available)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Anaconda2\lib\site-packages\theano\__init__.py"", line 111, in &lt;module&gt;
    theano.sandbox.cuda.tests.test_driver.test_nvidia_driver1()
  File ""C:\Anaconda2\lib\site-packages\theano\sandbox\cuda\tests\test_driver.py"", line 31, in test_nvidia_driver1
    profile=False)
  File ""C:\Anaconda2\lib\site-packages\theano\compile\function.py"", line 320, in function
    output_keys=output_keys)
  File ""C:\Anaconda2\lib\site-packages\theano\compile\pfunc.py"", line 479, in pfunc
    output_keys=output_keys)
  File ""C:\Anaconda2\lib\site-packages\theano\compile\function_module.py"", line 1776, in orig_function
    output_keys=output_keys).create(
  File ""C:\Anaconda2\lib\site-packages\theano\compile\function_module.py"", line 1456, in __init__
    optimizer_profile = optimizer(fgraph)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 101, in __call__
    return self.optimize(fgraph)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 89, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 230, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 89, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 230, in apply
    sub_prof = optimizer.optimize(fgraph)
  File ""C:\Anaconda2\lib\site-packages\theano\gof\opt.py"", line 89, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File ""C:\Anaconda2\lib\site-packages\theano\sandbox\cuda\dnn.py"", line 2508, in apply
    dnn_available.msg)
AssertionError: cuDNN optimization was enabled, but Theano was not able to use it. We got this error:
Theano can not compile with cuDNN. We got this error:

&gt;&gt;&gt;
</code></pre>

<p>anyone can help me? Thanks.</p>",36464973.0,2,2,,2016/3/27 13:48,5.0,2017/2/11 0:29,2016/3/27 14:51,,683218.0,,5743949.0,,1,10,python|theano|cudnn,11965,50.3116,,1,setup cudnn theano window bit instal framework enable cuda machine however import theano python console get following message cudnn available download nvidia website also update path environment add optimizer include cudnn theanorc txt config file try fail anyone help thank
405,405,47266383,Save and load weights in keras,"<p>Im trying to save and load weights from the model i have trained.</p>

<p>the code im using to save the model is.</p>

<pre><code>TensorBoard(log_dir='/output')
model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1)
model.save_weights('model.hdf5')
model.save_weights('myModel.h5')
</code></pre>

<p>Let me know if this an incorrect way to do it,or if there is a better way to do it.</p>

<p>but when i try to load them,using this,</p>

<pre><code>from keras.models import load_model
model = load_model('myModel.h5')
</code></pre>

<p>but i get this error:</p>

<hr>

<pre><code>ValueError                                Traceback (most recent call 
last)
&lt;ipython-input-7-27d58dc8bb48&gt; in &lt;module&gt;()
      1 from keras.models import load_model
----&gt; 2 model = load_model('myModel.h5')

/home/decentmakeover2/anaconda3/lib/python3.5/site-
packages/keras/models.py in load_model(filepath, custom_objects, compile)
    235         model_config = f.attrs.get('model_config')
    236         if model_config is None:
--&gt; 237             raise ValueError('No model found in config file.')
    238         model_config = json.loads(model_config.decode('utf-8'))
    239         model = model_from_config(model_config, 
custom_objects=custom_objects)

ValueError: No model found in config file.
</code></pre>

<p>Any suggestions on what i may be doing wrong?
Thank you in advance.</p>",47271117.0,4,0,,2017/11/13 14:15,30.0,2021/3/17 18:27,,,,,8176285.0,,1,65,python|keras,122296,208.35,,3,save load weight kera im try save load weight model train code im use save model let know incorrect way good way try load use get error suggestion may wrong thank advance
694,694,38971293,Get class labels from Keras functional model,"<p>I have a functional model in Keras (Resnet50 from repo examples). I trained it with <code>ImageDataGenerator</code> and <code>flow_from_directory</code> data and saved model to <code>.h5</code> file. When I call <code>model.predict</code> I get an array of class probabilities. But I want to associate them with class labels (in my case - folder names). How can I get them? I found that I could use <code>model.predict_classes</code> and <code>model.predict_proba</code>, but I don't have these functions in Functional model, only in Sequential.</p>",,7,0,,2016/8/16 9:31,24.0,2020/5/4 11:27,,,,,3176441.0,,1,76,python|keras,115859,285.656,,5,get class label keras functional model functional model keras resnet repo example train data save model file call get array class probability want associate class label case folder name get find could use function functional model sequential
90,90,41378461,How to use models from keras.applications for transfer learnig?,"<p>I want to get pretrained VGG16 model in Keras, remove its output layer, and then put a new output layer with the number of classes suited for my problem, and then to fit it on new data. For this reason, I am trying to use the model here: <a href=""https://keras.io/applications/#vgg16"" rel=""noreferrer"">https://keras.io/applications/#vgg16</a>, but since it is not Sequential, I cannot just <code>model.pop()</code>. Popping from layers and adding it also does not work, because in the predictions it still expects the old shape. How would I do that? Is there a way to convert this type of model to <code>Sequential</code>?</p>",41386444.0,2,0,,2016/12/29 11:07,21.0,2018/7/5 10:07,2017/6/9 12:26,,249341.0,,6636290.0,,1,20,python|deep-learning|keras,15151,82.7218,,3,use model kera application transfer learnig want get pretrained vgg model kera remove output layer put new output layer number class suit problem fit new data reason try use model vgg since sequential pop layer add also work prediction still expect old shape would way convert type model
110,110,41695844,Keras showing images from data generator,"<p>I am using image generator for keras like this:</p>

<pre><code>val_generator = datagen.flow_from_directory(
        path+'/valid',
        target_size=(224, 224),
        batch_size=batch_size,)

x,y = val_generator.next()
for i in range(0,1):
    image = x[i]
    plt.imshow(image.transpose(2,1,0))
    plt.show()
</code></pre>

<p>This shows wrong colors:
<a href=""https://i.stack.imgur.com/gwzou.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gwzou.png"" alt=""enter image description here""></a></p>

<p>I have two questions. </p>

<ol>
<li><p>How to fix the problem </p></li>
<li><p>How to get file names of the files (so that I can read it myself from something like matplotlib) </p></li>
</ol>

<p>Edit : this is what my datagen looks like</p>

<pre><code>datagen = ImageDataGenerator(
    rotation_range=3,
#     featurewise_std_normalization=True,
    fill_mode='nearest',
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)
</code></pre>

<p>Edit 2 : </p>

<p>After following Marcin's answer : </p>

<pre><code>image = 255 - image
</code></pre>

<p>I get normal colors , but there are still some weird colors:</p>

<p><a href=""https://i.stack.imgur.com/wAFpa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wAFpa.png"" alt=""enter image description here""></a></p>",41697846.0,6,7,,2017/1/17 11:23,4.0,2020/1/29 16:29,2018/4/24 12:30,,5974433.0,,2670775.0,,1,14,python|image|matplotlib|neural-network|keras,21697,52.1456,,5,kera show image data generator use image generator kera like show wrong color two question fix problem get file name file read something like matplotlib edit datagen look like edit follow marcin answer get normal color still weird color
535,535,36243536,What is the number of filter in CNN?,"<p>I am currently seeing the API of theano,</p>

<pre><code>theano.tensor.nnet.conv2d(input, filters, input_shape=None, filter_shape=None, border_mode='valid', subsample=(1, 1), filter_flip=True, image_shape=None, **kwargs)
</code></pre>

<p>where the <code>filter_shape</code> is a tuple of <code>(num_filter, num_channel, height, width)</code>, I am confusing about this because isn't that the number of filter decided by the stride while sliding the filter window on the image? How can I specify on filter number just like this? It would be reasonable to me if it is calculated by the parameter stride (if there is any).</p>

<p>Also, I am confused with the term feature map as well, is it the neurons at each layer? How about the batch size? How are they correlated?</p>",36243662.0,3,1,,2016/3/27 3:31,30.0,2020/4/16 5:24,2016/12/29 16:33,,3574081.0,,5257450.0,,1,50,machine-learning|neural-network|theano|convolution,59584,134.101,,3,number filter cnn currently see api theano tuple confuse number filter decide stride slide filter window image specify filter number like would reasonable calculate parameter stride also confuse term feature map well neuron layer batch size correlate
127,127,42081257,Why binary_crossentropy and categorical_crossentropy give different performances for the same problem?,"<p>I'm trying to train a CNN to categorize text by topic. When I use binary cross-entropy I get ~80% accuracy, with categorical cross-entropy I get ~50% accuracy.</p>

<p>I don't understand why this is. It's a multiclass problem, doesn't that mean that I have to use categorical cross-entropy and that the results with binary cross-entropy are meaningless?</p>



<pre class=""lang-python prettyprint-override""><code>model.add(embedding_layer)
model.add(Dropout(0.25))
# convolution layers
model.add(Conv1D(nb_filter=32,
                    filter_length=4,
                    border_mode='valid',
                    activation='relu'))
model.add(MaxPooling1D(pool_length=2))
# dense layers
model.add(Flatten())
model.add(Dense(256))
model.add(Dropout(0.25))
model.add(Activation('relu'))
# output layer
model.add(Dense(len(class_id_index)))
model.add(Activation('softmax'))
</code></pre>

<p>Then I compile it either it like this using <code>categorical_crossentropy</code> as the loss function:</p>

<pre class=""lang-python prettyprint-override""><code>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>or </p>

<pre class=""lang-python prettyprint-override""><code>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>Intuitively it makes sense why I'd want to use categorical cross-entropy, I don't understand why I get good results with binary, and poor results with categorical.</p>",46038271.0,12,10,,2017/2/7 3:34,87.0,2021/3/29 20:45,2020/1/20 13:13,,4685471.0,,1291820.0,,1,194,machine-learning|keras|neural-network|deep-learning|conv-neural-network,208918,928.88,,4,binary crossentropy categorical crossentropy give different performance problem try train cnn categorize text topic use binary cross entropy get accuracy categorical cross entropy get accuracy understand multiclass problem mean use categorical cross entropy result binary cross entropy meaningless compile either like use loss function intuitively make sense want use categorical cross entropy understand get good result binary poor result categorical
73,73,56004483,What is a multi-headed model? And what exactly is a 'head' in a model?,"<p>What is a multi-headed model in deep learning?</p>

<p>The only explanation I found so far is this: <em>Every model might be thought of as a backbone plus a head, and if you pre-train backbone and put a random head, you can fine tune it and it is a good idea</em><br>
Can someone please provide a more detailed explanation.</p>",56004582.0,2,0,,2019/5/6 11:39,11.0,2020/11/2 15:41,,,,,11440538.0,,1,31,machine-learning|neural-network|deep-learning,10472,77.4801,,0,multi head model exactly head model multi head model deep learn explanation find far every model might think backbone plus head pre train backbone put random head fine tune good idea someone please provide detailed explanation
257,257,44666390,Max pool layer vs Convolution with stride performance,"<p>In most of the architectures, conv layers are being followed by a pooling layer (max / avg etc.). As those pooling layers are just selecting the output of previous layer (i.e. conv), can we just use convolution with stride 2 and expect the similar accuracy results with reduced process need? </p>",44667825.0,1,1,,2017/6/21 3:30,7.0,2019/6/12 2:08,,,,,2805070.0,,1,34,deep-learning|conv-neural-network|max-pooling,15568,67.5689,,0,max pool layer vs convolution stride performance architecture conv layer follow pooling layer max avg etc pool layer select output previous layer e conv use convolution stride expect similar accuracy result reduced process need
633,633,51030782,"Why do we ""pack"" the sequences in PyTorch?","<p>I was trying to replicate <a href=""https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120"" rel=""noreferrer"">How to use packing for variable-length sequence inputs for rnn</a> but I guess I first need to understand why we need to &quot;pack&quot; the sequence.</p>
<p>I understand why we need to &quot;pad&quot; them but why is &quot;packing&quot; (through <code>pack_padded_sequence</code>) necessary?</p>
<p>Any high-level explanation would be appreciated!</p>",51030945.0,5,1,,2018/6/25 19:40,58.0,2021/3/10 15:39,2021/3/10 15:39,,9067615.0,,3907250.0,,1,126,deep-learning|pytorch|recurrent-neural-network|tensor|zero-padding,43381,418.549,,3,pack sequence pytorch try replicate use pack variable length sequence input rnn guess first need understand need pack sequence understand need pad pack necessary high level explanation would appreciate
584,584,49433936,How to initialize weights in PyTorch?,"<p>How to initialize the weights and biases (for example, with He or Xavier initialization) in a network in PyTorch? </p>",49433937.0,10,2,,2018/3/22 16:34,81.0,2021/8/1 10:41,2021/4/24 1:57,,10908375.0,,604734.0,,1,175,python|machine-learning|deep-learning|neural-network|pytorch,233774,733.475,,3,initialize weight pytorch initialize weight bias example xavier initialization network pytorch
643,643,36886711,"Keras: ""RuntimeError: Failed to import pydot."" after installing graphviz and pydot","<p>I'm using Anaconda Python 2.7 on windows 10</p>

<p>I was planning on doing Keras visualization so (whilst spyder was open) I opened the Anaconda command prompt and pip installed graphviz and pydot. Now when I try run the following:</p>

<pre><code>from keras.models import Sequential
</code></pre>

<p>or any sort of ""from keras."" ,  I get the error:</p>

<pre><code>ImportError: cannot import name gof
</code></pre>

<p>I have uninstalled and reinstalled Keras, Graphviz and pydot. i am using the development version of theano. I cannot find a fix. </p>

<p><strong>P.S</strong></p>

<p>If I uninstall graphviz and pydot, keras works again</p>

<p><strong>EDIT</strong></p>

<p>After uninstalling anaconda and reinstalling it including theano, keras, <strong>graphviz and pydot</strong> I now get the following error:</p>

<pre><code>from keras.utils.visualize_util import plot

Using Theano backend.
Using gpu device 0: GeForce GTX 970M (CNMeM is disabled, cuDNN not available)
Traceback (most recent call last):

  File ""&lt;ipython-input-1-65016ddab3cd&gt;"", line 1, in &lt;module&gt;
  from keras.utils.visualize_util import plot

  File ""C:\Anaconda2\lib\site-packages\keras\utils\visualize_util.py"", line  8, in &lt;module&gt;
  raise RuntimeError('Failed to import pydot. You must install pydot'

RuntimeError: Failed to import pydot. You must install pydot and graphviz  for `pydotprint` to work.
</code></pre>

<p>I used <code>pip install graphviz</code> and <code>pip install git+https://github.com/nlhepler/pydot.git</code></p>",36890526.0,13,4,,2016/4/27 10:08,7.0,2021/4/27 7:29,2016/4/27 12:51,,5310324.0,,5310324.0,,1,29,python|graphviz|theano|keras|pydot,37740,168.707,,1,kera runtimeerror fail import pydot instal graphviz pydot use anaconda python window plan keras visualization whilst spyder open open anaconda command prompt pip instal graphviz pydot try run following sort kera get error uninstalled reinstall kera graphviz pydot use development version theano find fix p uninstall graphviz pydot kera work edit uninstalling anaconda reinstall include theano kera graphviz pydot get following error use
17,17,53775508,How do I install PyTorch v1.0.0+ on Google Colab?,"<p>PyTorch v1.0.0 stable was <a href=""https://github.com/pytorch/pytorch/releases/tag/v1.0.0"" rel=""noreferrer"">released on 8 December 2018</a> after being <a href=""https://code.fb.com/ai-research/announcing-pytorch-1-0-for-both-research-and-production/"" rel=""noreferrer"">announced 7 months earlier</a>.</p>

<p>I want get a version optimised for the hardware that my IPython kernel is running on.</p>

<p>How do I get this version on Google Colab?</p>",53777433.0,6,0,,2018/12/14 7:53,3.0,2020/7/16 11:22,2018/12/17 7:57,,5353461.0,,5353461.0,,1,12,pytorch|google-colaboratory,15938,59.2097,,1,install pytorch v google colab pytorch v stable release december announce month earlier want get version optimise hardware ipython kernel run get version google colab
385,385,46495215,What is the difference between x_train and x_test in Keras?,"<p>I've looked at a few tutorials to crack into Keras for deep learning using Convolutional Neural Networks. In the tutorial (and in Keras' official documentation), the MNIST dataset is loaded like so:</p>

<pre><code>from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
</code></pre>

<p>However, no explanation is offered as to why we have two tuples of data. My question is: <strong>what are <code>x_train</code> and <code>y_train</code> and how do they differ from their <code>x_test</code> and <code>y_test</code> counterparts</strong>?</p>",46496156.0,2,2,,2017/9/29 18:46,7.0,2017/11/23 3:04,,,,,3960852.0,,1,13,python|keras|conv-neural-network,19550,54.3646,,2,difference x train x test kera look tutorial crack kera deep learning use convolutional neural network tutorial kera official documentation mnist dataset load like however explanation offer two tuples data question differ counterpart
197,197,43674411,Training and Loss not changing in Keras CNN model,"<p>I am running a CNN for left and right shoeprint classfication. I have 190,000 training images and I use 10% of it for validation. My model is setup as shown below. I get the paths of all the images, read them in and resize them. I normalize the image, and then fit it to the model. My issue is that I have stuck at a training accuracy of 62.5% and a loss of around 0.6615-0.6619. Is there something wrong that I am doing? How can I stop this from happening?</p>
<p>Just some interesting points to note:</p>
<ol>
<li><p>I first tested this on 10 images I was having the same issue but changing the optimizer to adam and batch size to 4 worked.</p>
</li>
<li><p>I then tested on more and more images, but each time I would need to change the batch size to get improvements in the accuracy and loss. With 10,000 images I had to use a batch size of 500 and optimizer rmsprop. However, the accuracy and loss only really began to change after epoch 10.</p>
</li>
<li><p>I am now training on 190,000 images and I cannot increase the batch size as my GPU is at is max.</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>    imageWidth = 50
    imageHeight = 150
    
    def get_filepaths(directory):
        file_paths = []
        for filename in files:
            filepath = os.path.join(root, filename)
            file_paths.append(filepath) # Add it to the list.
        return file_paths
    
    def cleanUpPaths(fullFilePaths):
        cleanPaths = []
        for f in fullFilePaths:
            if f.endswith(&quot;.png&quot;):
                cleanPaths.append(f)
        return cleanPaths
    
    def getTrainData(paths):
        trainData = []
        for i in xrange(1,190000,2):
            im = image.imread(paths[i])
            im = image.imresize(im, (150,50))
            im = (im-255)/float(255)
            trainData.append(im)
        trainData = np.asarray(trainData)
        right = np.zeros(47500)
        left = np.ones(47500)
        trainLabels = np.concatenate((left, right))
        trainLabels = np_utils.to_categorical(trainLabels)
        return (trainData, trainLabels)

    #create the convnet
    model = Sequential()
    
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(imageWidth,imageHeight,1),strides=1))#32
    model.add(Conv2D(32, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(64, (3, 3), activation='relu',strides=1))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(1, 3)))
    model.add(Dropout(0.25))
    
    model.add(Conv2D(64, (1, 2), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 1)))
    model.add(Dropout(0.25))
    
    model.add(Flatten())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))
    
    sgd = SGD(lr=0.01)
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])
    
    #prepare the training data*/
    
    trainPaths = get_filepaths(&quot;better1/train&quot;)
    trainPaths = cleanUpPaths(trainPaths)
    (trainData, trainLabels) = getTrainData(trainPaths)
    trainData = np.reshape(trainData,(95000,imageWidth,imageHeight,1)).astype('float32')
    trainData = (trainData-255)/float(255)
    
    #train the convnet***
    model.fit(trainData, trainLabels, batch_size=500, epochs=50, validation_split=0.2)
    
    #/save the model and weights*/
    model.save('myConvnet_model5.h5');
    model.save_weights('myConvnet_weights5.h5');
</code></pre>",,5,0,,2017/4/28 7:25,9.0,2021/3/2 16:57,2021/3/2 16:57,,13968097.0,,7935346.0,,1,11,tensorflow|deep-learning|keras|conv-neural-network,18016,61.0226,,4,training loss change kera cnn model run cnn left right shoeprint classfication training image use validation model setup show get path image read resize normalize image fit model issue stick training accuracy loss around something wrong stop happen interesting point note first test image issue change optimizer adam batch size work test image time would need change batch size get improvement accuracy loss image use batch size optimizer rmsprop however accuracy loss really begin change epoch train image increase batch size gpu max
125,125,41971587,How to convert predicted sequence back to text in keras?,"<p>I have a sequence to sequence learning model which works fine and able to predict some outputs. The problem is I have no idea how to convert the output back to text sequence.</p>

<p>This is my code.</p>

<pre><code>from keras.preprocessing.text import Tokenizer,base_filter
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense

txt1=""""""What makes this problem difficult is that the sequences can vary in length,
be comprised of a very large vocabulary of input symbols and may require the model 
to learn the long term context or dependencies between symbols in the input sequence.""""""

#txt1 is used for fitting 
tk = Tokenizer(nb_words=2000, filters=base_filter(), lower=True, split="" "")
tk.fit_on_texts(txt1)

#convert text to sequence
t= tk.texts_to_sequences(txt1)

#padding to feed the sequence to keras model
t=pad_sequences(t, maxlen=10)

model = Sequential()
model.add(Dense(10,input_dim=10))
model.add(Dense(10,activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])

#predicting new sequcenc
pred=model.predict(t)

#Convert predicted sequence to text
pred=??
</code></pre>",,5,4,,2017/2/1 3:51,3.0,2021/9/1 14:27,,,,,996366.0,,1,24,python|keras|keras-layer|sequence-to-sequence,15185,100.726,,3,convert predict sequence back text kera sequence sequence learn model work fine able predict output problem idea convert output back text sequence code
763,763,51700351,ValueError: Unknown metric function when using custom metric in Keras,"<p>Keras 2.x killed off a bunch of useful metrics that I need to use, so I copied the functions from the old metrics.py file into my code, then included them as follows.</p>

<pre><code>def precision(y_true, y_pred): #taken from old keras source code
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
     precision = true_positives / (predicted_positives + K.epsilon())
     return precision
def recall(y_true, y_pred): #taken from old keras source code
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
     recall = true_positives / (possible_positives + K.epsilon())
     return recall
</code></pre>

<p>...</p>

<pre><code>model.compile(loss='categorical_crossentropy', optimizer='adam', 
metrics=['accuracy', precision, recall])
</code></pre>

<p>and this results in </p>

<pre><code>ValueError: Unknown metric function:precision
</code></pre>

<p>What am I doing wrong? I can't see anything I'm doing wrong according to Keras documentation.</p>

<p>edit:</p>

<p>Here is the full Traceback:</p>

<pre><code> Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Python/2.7/site-packages/keras/models.py"", line 274, in 
load_model
    sample_weight_mode=sample_weight_mode)
  File ""/Library/Python/2.7/site-packages/keras/models.py"", line 824, in 
compile
     **kwargs)
  File ""/Library/Python/2.7/site-packages/keras/engine/training.py"", line 
934, in compile
     handle_metrics(output_metrics)
   File ""/Library/Python/2.7/site-packages/keras/engine/training.py"", line 
901, in handle_metrics
    metric_fn = metrics_module.get(metric)
  File ""/Library/Python/2.7/site-packages/keras/metrics.py"", line 75, in get
     return deserialize(str(identifier))
  File ""/Library/Python/2.7/site-packages/keras/metrics.py"", line 67, in 
deserialize
    printable_module_name='metric function')
  File ""/Library/Python/2.7/site-packages/keras/utils/generic_utils.py"", 
line 164, in deserialize_keras_object
    ':' + function_name)
ValueError: Unknown metric function:precision
&lt;FATAL&gt;                         : Failed to load Keras model from file: 
model.h5
***&gt; abort program execution
Traceback (most recent call last):
  File ""classification.py"", line 84, in &lt;module&gt;
    'H:!V:FilenameModel=model.h5:NumEpochs=20:BatchSize=32') 
#:VarTransform=D,G
TypeError: none of the 3 overloaded methods succeeded. Full details:
  TMVA::MethodBase* TMVA::Factory::BookMethod(TMVA::DataLoader* loader, 
TString theMethodName, TString methodTitle, TString theOption = """") =&gt;
    could not convert argument 2
  TMVA::MethodBase* TMVA::Factory::BookMethod(TMVA::DataLoader* loader, 
TMVA::Types::EMVA theMethod, TString methodTitle, TString theOption = """") =&gt;
    FATAL error (C++ exception of type runtime_error)
  TMVA::MethodBase* TMVA::Factory::BookMethod(TMVA::DataLoader*, 
TMVA::Types::EMVA, TString, TString, TMVA::Types::EMVA, TString) =&gt;
    takes at least 6 arguments (4 given)
</code></pre>",,3,0,,2018/8/6 3:50,1.0,2021/5/17 13:52,2018/8/6 5:00,,8384403.0,,8384403.0,,1,15,python|machine-learning|keras,17271,68.9493,,4,valueerror unknown metric function use custom metric kera kera x kill bunch useful metric need use copy function old metric py file code include follow result wrong see anything wrong accord keras documentation edit full traceback
391,391,46717742,Split data directory into training and test directory with sub directory structure preserved,"<p>I am interested in using ImageDataGenerator in Keras for data augmentation. But it requires that training and validation directories with sub directories for classes be fed in separately as below (this is from Keras documentation). I have a single directory with 2 subdirectories for 2 classes (Data/Class1 and Data/Class2). How do I randomly split this into training and validation directories </p>

<pre><code>    train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

    test_datagen = ImageDataGenerator(rescale=1./255)

    train_generator = train_datagen.flow_from_directory(
    'data/train',
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')

   validation_generator = test_datagen.flow_from_directory(
    'data/validation',
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')

   model.fit_generator(
    train_generator,
    steps_per_epoch=2000,
    epochs=50,
    validation_data=validation_generator,
    validation_steps=800)
</code></pre>

<p>I am interested in re-running my algorithm  multiple times with random training and validation data splits.</p>",,7,0,,2017/10/12 19:47,5.0,2019/3/19 14:13,2018/5/18 11:39,,5974433.0,,8767500.0,,1,12,python|machine-learning|neural-network|keras|deep-learning,16424,67.6619,,2,split data directory training test directory sub directory structure preserve interested use imagedatagenerator kera data augmentation require training validation directory sub directory class feed separately keras documentation single directory subdirectory class data class data class randomly split training validation directory interested run algorithm multiple time random training validation data split
714,714,39854390,Nan in summary histogram,"<p>My program will face this some times(not every run will face this..), then if face this I can always reproduce this error loading from the last model I have saved before program crash due to nan.
When rerun from this model, first train process seems fine using the model to generate loss(I have printed loss and shows no problem), but after applying gradients, the values of embedding variables will turn to Nan. </p>

<p>So what is the root cause of the nan problem? Confused as not know how to debug further and this program with same data and params will mostly run ok and only face this problem during some run..</p>

<pre><code>Loading existing model from: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000
Train from restored model: /home/gezi/temp/image-caption//model.flickr.rnn2.nan/model.ckpt-18000
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 5235 get requests, put_count=4729 evicted_count=1000 eviction_rate=0.211461 and unsatisfied allocation rate=0.306781
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:257] Raising pool_size_limit_ from 100 to 110
2016-10-04 21:45:39 epoch:1.87 train_step:18001 duration:0.947 elapsed:0.947 train_avg_metrics:['loss:0.527']  ['loss:0.527']
2016-10-04 21:45:39 epoch:1.87 eval_step: 18001 duration:0.001 elapsed:0.948 ratio:0.001
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1
     [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1
     [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1
     [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1
     [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]]
W tensorflow/core/framework/op_kernel.cc:968] Invalid argument: Nan in summary histogram for: rnn/HistogramSummary_1
     [[Node: rnn/HistogramSummary_1 = HistogramSummary[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](rnn/HistogramSummary_1/tag, rnn/image_text_sim/image_mlp/w_h/read/_309)]]
Traceback (most recent call last):
  File ""./train.py"", line 308, in &lt;module&gt;
    tf.app.run()
</code></pre>",,6,0,,2016/10/4 14:03,1.0,2021/8/16 18:12,2019/12/16 14:20,,10685378.0,,5668239.0,,1,8,python|tensorflow|deep-learning|gradient,19362,57.7478,,4,nan summary histogram program face time every run face face always reproduce error load last model save program crash due nan rerun model first train process seem fine use model generate loss print loss show problem apply gradient value embed variable turn nan root cause nan problem confuse know debug far program data params mostly run ok face problem run
401,401,47183159,How to set weights in Keras with a numpy array?,"<p>I am having trouble with the Keras backend functions for setting values.  I am trying to convert a model from PyTorch to Keras and am trying to set the weights of the Keras model, but the weights do not appear to be getting set.  Note: I am not actually setting with np.ones just using that for an example.</p>

<p>I have tried...</p>

<p>Loading an existing model</p>

<pre><code>import keras
from keras.models import load_model, Model
model = load_model(model_dir+file_name)
keras_layer = [layer for layer in model.layers if layer.name=='conv2d_1'][0]
</code></pre>

<p>Creating a simple model</p>

<pre><code>img_input = keras.layers.Input(shape=(3,3,3))
x = keras.layers.Conv2D(1, kernel_size=1, strides=1, padding=""valid"", 
use_bias=False, name='conv1')(img_input)
model = Model(img_input, x)
keras_layer = [layer for layer in model.layers if layer.name=='conv1'][0]
</code></pre>

<p>Then using set_weights or set_value</p>

<pre><code>keras_layer.set_weights([np.ones((1, 1, 3, 1))])
</code></pre>

<p>or...</p>

<pre><code>K.batch_set_value([(weight,np.ones((1, 1, 3, 1))) for weight in keras_layer.weights])
</code></pre>

<p>afterwards I call either one of the following:</p>

<pre><code>K.batch_get_value([weight for weight in keras_layer.weights])
keras_layer.get_weights()
</code></pre>

<p>And None of the weights appear to have been set.  The same values as before are returned.</p>

<pre><code>[array([[[[  1.61547325e-06],
      [  2.97779252e-06],
      [  1.50160542e-06]]]], dtype=float32)]
</code></pre>

<p>How do I set the weights of a layer in Keras with a numpy array of values?</p>",47183890.0,3,2,,2017/11/8 15:10,9.0,2020/3/5 10:23,2020/3/5 10:23,,6796042.0,,5129113.0,,1,31,python|tensorflow|machine-learning|keras|deep-learning,46061,97.2533,,3,set weight kera numpy array trouble kera backend function set value try convert model pytorch keras try set weight kera model weight appear get set note actually set np one use example try load exist model create simple model use set weight set value afterwards call either one following none weight appear set value return set weight layer kera numpy array value
646,646,36966316,How to get the dimensions of a tensor (in TensorFlow) at graph construction time?,"<p>I am trying an Op that is not behaving as expected.</p>

<pre><code>graph = tf.Graph()
with graph.as_default():
  train_dataset = tf.placeholder(tf.int32, shape=[128, 2])
  embeddings = tf.Variable(
    tf.random_uniform([50000, 64], -1.0, 1.0))
  embed = tf.nn.embedding_lookup(embeddings, train_dataset)
  embed = tf.reduce_sum(embed, reduction_indices=0)
</code></pre>

<p>So I need to know the dimensions of the Tensor <code>embed</code>. I know that it can be done at the run time but it's too much work for such a simple operation. What's the easier way to do it?</p>",36967015.0,6,0,,2016/5/1 11:49,13.0,2018/1/18 20:25,2018/1/18 20:25,,2956066.0,,3274693.0,,1,40,python|tensorflow|deep-learning|tensor,125936,188.401,,3,get dimension tensor tensorflow graph construction time try op behave expect need know dimension tensor know run time much work simple operation easy way
591,591,49603498,Convolution2D + LSTM versus ConvLSTM2D,"<p>Are <code>1</code> and <code>2</code> the same?</p>

<ol>
<li>Use <code>Convolution2D</code> layers and <code>LSTM</code> layers </li>
<li>Use <code>ConvLSTM2D</code></li>
</ol>

<p>If there is any difference, could you explain it for me?</p>",49770553.0,2,0,,2018/4/1 23:14,9.0,2018/4/13 16:40,2018/4/13 16:40,,7224320.0,,4088201.0,,1,23,python|tensorflow|keras,10809,57.3351,,3,convolution lstm versus convlstm use layer layer use difference could explain
549,549,48482787,"PyTorch memory model: ""torch.from_numpy()"" vs ""torch.Tensor()""","<p>I'm trying to have an in-depth understanding of how PyTorch Tensor memory model works.</p>

<pre><code># input numpy array
In [91]: arr = np.arange(10, dtype=float32).reshape(5, 2)

# input tensors in two different ways
In [92]: t1, t2 = torch.Tensor(arr), torch.from_numpy(arr)

# their types
In [93]: type(arr), type(t1), type(t2)
Out[93]: (numpy.ndarray, torch.FloatTensor, torch.FloatTensor)

# ndarray 
In [94]: arr
Out[94]: 
array([[ 0.,  1.],
       [ 2.,  3.],
       [ 4.,  5.],
       [ 6.,  7.],
       [ 8.,  9.]], dtype=float32)
</code></pre>

<hr>

<p>I know that PyTorch tensors <em>share the memory buffer</em> of NumPy ndarrays. Thus, changing one will be reflected in the other. So, here I'm slicing and updating some values in the Tensor <code>t2</code></p>

<pre><code>In [98]: t2[:, 1] = 23.0
</code></pre>

<p>And as expected, it's updated in <code>t2</code> and <code>arr</code> since they share the same memory buffer.</p>

<pre><code>In [99]: t2
Out[99]: 

  0  23
  2  23
  4  23
  6  23
  8  23
[torch.FloatTensor of size 5x2]


In [101]: arr
Out[101]: 
array([[  0.,  23.],
       [  2.,  23.],
       [  4.,  23.],
       [  6.,  23.],
       [  8.,  23.]], dtype=float32)
</code></pre>

<p>But, <strong><code>t1</code> is also updated</strong>. Remember that <code>t1</code> was constructed using <code>torch.Tensor()</code> whereas <code>t2</code> was constructed using <code>torch.from_numpy()</code></p>

<pre><code>In [100]: t1
Out[100]: 

  0  23
  2  23
  4  23
  6  23
  8  23
[torch.FloatTensor of size 5x2]
</code></pre>

<p>So, no matter whether we use <a href=""http://pytorch.org/docs/master/torch.html#torch.from_numpy"" rel=""noreferrer""><code>torch.from_numpy()</code></a> or <a href=""http://pytorch.org/docs/master/tensors.html#torch-tensor"" rel=""noreferrer""><code>torch.Tensor()</code></a> to construct a tensor from an ndarray, <strong>all</strong> such tensors and ndarrays share the same memory buffer.</p>

<p>Based on this understanding, my question is why does a dedicated function <a href=""http://pytorch.org/docs/master/torch.html#torch.from_numpy"" rel=""noreferrer""><code>torch.from_numpy()</code></a> exists when simply <a href=""http://pytorch.org/docs/master/tensors.html#torch-tensor"" rel=""noreferrer""><code>torch.Tensor()</code></a> can do the job?</p>

<p>I looked at the PyTorch documentation but it doesn't mention anything about this? Any ideas/suggestions?</p>",,4,1,,2018/1/28 3:07,12.0,2021/4/15 16:23,2018/8/20 23:26,,2956066.0,,2956066.0,,1,40,python|numpy|multidimensional-array|deep-learning|pytorch,39411,116.382,,3,pytorch memory model torch numpy v torch tensor try depth understanding pytorch tensor memory model work know pytorch tensor share memory buffer numpy ndarrays thus change one reflect slice update value tensor expect update since share memory buffer also updated remember construct use whereas construct use matter whether use construct tensor ndarray tensor ndarrays share memory buffer base understanding question dedicated function exists simply job look pytorch documentation mention anything idea suggestion
427,427,47817424,Loss & accuracy - Are these reasonable learning curves?,"<p>I am learning neural networks and I built a simple one in Keras for the iris dataset classification from the UCI machine learning repository. I used a one hidden layer network with a 8 hidden nodes. Adam optimizer is used with a learning rate of 0.0005 and is run for 200 Epochs. Softmax is used at the output with loss as catogorical-crossentropy. I am getting the following learning curves. </p>

<p><a href=""https://i.stack.imgur.com/Y9bj3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Y9bj3.png"" alt=""Learning Curve""></a></p>

<p>As you can see, the learning curve for the accuracy has a lot of flat regions and I don't understand why. The error seems to be decreasing constantly but the accuracy doesn't seem to be increasing in the same manner. What does the flat regions in the accuracy learning curve imply? Why is the accuracy not increasing at those regions even though error seems to be decreasing? </p>

<p>Is this normal in training or it is more likely that I am doing something wrong here?</p>



<pre class=""lang-python prettyprint-override""><code>dataframe = pd.read_csv(""iris.csv"", header=None)
dataset = dataframe.values
X = dataset[:,0:4].astype(float)
y = dataset[:,4]

scalar = StandardScaler()
X = scalar.fit_transform(X)

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

encoder = OneHotEncoder()
y = encoder.fit_transform(y.reshape(-1,1)).toarray()

# create model
model = Sequential()
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))

# Compile model
adam = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.compile(loss='categorical_crossentropy',
              optimizer=adam, 
              metrics=['accuracy'])

# Fit the model
log = model.fit(X, y, epochs=200, batch_size=5, validation_split=0.2)

fig = plt.figure()
fig.suptitle(""Adam, lr=0.0006, one hidden layer"")

ax = fig.add_subplot(1,2,1)
ax.set_title('Cost')
ax.plot(log.history['loss'], label='Training')
ax.plot(log.history['val_loss'], label='Validation')
ax.legend()

ax = fig.add_subplot(1,2,2)
ax.set_title('Accuracy')
ax.plot(log.history['acc'], label='Training')
ax.plot(log.history['val_acc'], label='Validation')
ax.legend()

fig.show()
</code></pre>",47819022.0,1,0,,2017/12/14 15:55,10.0,2020/7/14 8:29,2017/12/15 17:24,,4685471.0,,5530553.0,,1,18,machine-learning|neural-network|keras|classification|loss,7459,67.0907,,4,loss accuracy reasonable learning curve learn neural network build simple one kera iris dataset classification uci machine learn repository use one hidden layer network hidden node adam optimizer use learn rate run epoch softmax use output loss catogorical crossentropy get follow learning curve see learning curve accuracy lot flat region understand error seem decrease constantly accuracy seem increase manner flat region accuracy learn curve imply accuracy increase region even though error seem decrease normal training likely something wrong
541,541,36720498,Convert Keras model to C++,"<p>I am using Keras (with Theano) to train my CNN model. Does anyone has idea how can I use it in my C++ application? Does anyone tried something similar? I have idea to write some python code that will generate a c++ code with network functions - any suggestion on it?</p>

<p>I found a similar question <a href=""https://stackoverflow.com/questions/36412098/convert-keras-model-to-tensorflow-protobuf"">here</a> how to use Tensorflow Keras model in C++ but without answer.</p>",37379618.0,7,7,,2016/4/19 13:52,25.0,2019/1/31 10:58,2017/5/23 12:10,,-1.0,,5605919.0,,1,60,c++|tensorflow|theano|conv-neural-network|keras,59123,202.087,2019/1/31 11:24,3,convert kera model c use kera theano train cnn model anyone idea use c application anyone try something similar idea write python code generate c code network function suggestion find similar question use tensorflow kera model c without answer
101,101,41531695,"How does keras define ""accuracy"" and ""loss""?","<p>I can't find how Keras defines ""accuracy"" and ""loss"".  I know I can specify different metrics (e.g. mse, cross entropy) - but keras prints out a standard ""accuracy"".  How is that defined? Likewise for loss: I know I can specify different types of regularization -- are those in the loss?</p>

<p>Ideally, I'd like to print out the equation used to define it; if not, I'll settle for an answer here.</p>",41534323.0,1,0,,2017/1/8 10:28,14.0,2018/2/21 15:24,2018/2/21 15:24,,2230844.0,,785494.0,,1,40,python|tensorflow|machine-learning|deep-learning|keras,39824,61.4006,,4,keras define accuracy loss find keras defines accuracy loss know specify different metric e g mse cross entropy keras print standard accuracy defined likewise loss know specify different type regularization loss ideally like print equation use define settle answer
439,439,48029542,Data augmentation in test/validation set?,"<p>It is common practice to augment data (add samples programmatically, such as random crops, etc. in the case of a dataset consisting of images) on both training and test set, or just the training data set?</p>",48031128.0,9,1,,2017/12/29 23:31,4.0,2021/8/11 9:15,2017/12/30 21:42,,774907.0,,774907.0,,1,25,machine-learning|deep-learning,15861,108.801,,2,data augmentation test validation set common practice augment data add sample programmatically random crop etc case dataset consisting image training test set training data set
576,576,49263588,Pytorch beginner : tensor.new method,"<p>everyone, I have a small question. </p>

<p>What is the purpose of the method <code>tensor.new(..)</code> in Pytorch, I didn't find anything in the documentation. It looks like it creates a new Tensor (like the name suggests), but why we don't just use <code>torch.Tensor</code> constructors instead of using this new method that requires an existing tensor.</p>

<p>Thank you in advance.</p>",49264437.0,4,0,,2018/3/13 18:40,1.0,2019/10/21 20:42,2018/3/13 19:25,,5352399.0,,8593338.0,,1,19,pytorch,10664,68.3117,,3,pytorch beginner tensor new method everyone small question purpose method pytorch find anything documentation look like create new tensor like name suggest use constructor instead use new method require exist tensor thank advance
438,438,48001759,What is right batch normalization function in Tensorflow?,"<p>In tensorflow 1.4, I found two functions that do batch normalization and they look same:</p>

<ol>
<li><code>tf.layers.batch_normalization</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""noreferrer"">link</a>)</li>
<li><code>tf.contrib.layers.batch_norm</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm"" rel=""noreferrer"">link</a>)</li>
</ol>

<p>Which function should I use? Which one is more stable?</p>",48006315.0,2,0,,2017/12/28 4:56,22.0,2017/12/28 12:13,2017/12/28 12:13,,712995.0,,6279632.0,,1,22,python|tensorflow|neural-network|deep-learning|batch-normalization,17051,88.727,,3,right batch normalization function tensorflow tensorflow find two function batch normalization look link link function use one stable
52,52,55235212,model.summary() can't print output shape while using subclass model,"<p>This is the two methods for creating a keras model, but the <code>output shapes</code> of the summary results of the two methods are different. Obviously, the former prints more information and makes it easier to check the correctness of the network.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras import Input, layers, Model

class subclass(Model):
    def __init__(self):
        super(subclass, self).__init__()
        self.conv = layers.Conv2D(28, 3, strides=1)

    def call(self, x):
        return self.conv(x)


def func_api():
    x = Input(shape=(24, 24, 3))
    y = layers.Conv2D(28, 3, strides=1)(x)
    return Model(inputs=[x], outputs=[y])

if __name__ == '__main__':
    func = func_api()
    func.summary()

    sub = subclass()
    sub.build(input_shape=(None, 24, 24, 3))
    sub.summary()
</code></pre>

<p>output闂?/p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 24, 24, 3)         0         
_________________________________________________________________
conv2d (Conv2D)              (None, 22, 22, 28)        784       
=================================================================
Total params: 784
Trainable params: 784
Non-trainable params: 0
_________________________________________________________________
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            multiple                  784       
=================================================================
Total params: 784
Trainable params: 784
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>So, how should I use the subclass method to get the <code>output shape</code> at the summary()?</p>",,7,0,,2019/3/19 6:57,5.0,2021/8/26 13:21,,,,,8101774.0,,1,20,python|tensorflow|keras|tf.keras,11803,81.288,,5,model summary print output shape use subclass model two method create keras model summary result two method different obviously former print information make easy check correctness network output use subclass method get summary
790,790,52288635,How to use torch.stack function,"<p>I have a question about torch.stack</p>

<p>I have 2 tensors, a.shape=(2, 3, 4) and b.shape=(2, 3).
<strong>How to stack them</strong> without in-place operation?</p>",52288888.0,3,0,,2018/9/12 6:19,5.0,2020/7/31 15:03,,,,,7789027.0,,1,22,python|pytorch|tensor,41483,77.6715,,3,use torch stack function question torch stack tensor shape b shape stack without place operation
194,194,43529931,How to calculate prediction uncertainty using Keras?,"<p>I would like to calculate NN model certainty/confidence (see <a href=""http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html"" rel=""noreferrer"">What my deep model doesn't know</a>) - when NN tells me an image represents ""8"", I would like to know how certain it is. Is my model 99% certain it is ""8"" or is it 51% it is ""8"", but it could also be ""6""? Some digits are quite ambiguous and I would like to know for which images the model is just ""flipping a coin"".</p>

<p>I have found some theoretical writings about this but I have trouble putting this in code. If I understand correctly, I should evaluate a testing image multiple times while ""killing off"" different neurons (using dropout) and then...?</p>

<p>Working on MNIST dataset, I am running the following model:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Activation, Conv2D, Flatten, Dropout

model = Sequential()
model.add(Conv2D(128, kernel_size=(7, 7),
                 activation='relu',
                 input_shape=(28, 28, 1,)))
model.add(Dropout(0.20))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Dropout(0.20))
model.add(Flatten())
model.add(Dense(units=64, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(units=10, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
model.fit(train_data, train_labels,  batch_size=100, epochs=30, validation_data=(test_data, test_labels,))
</code></pre>

<p><em>How should I predict with this model so that I get its certainty about predictions too?</em> I would appreciate some practical examples (preferably in Keras, but any will do).</p>

<p>To clarify, I am looking for an example of how to get certainty using <a href=""http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html"" rel=""noreferrer"">the method outlined by Yurin Gal</a> (or an explanation of why some other method yields better results). </p>",43668304.0,4,0,,2017/4/20 21:07,17.0,2019/12/4 11:48,2019/11/21 18:02,,3924118.0,,593487.0,,1,43,machine-learning|neural-network|deep-learning|keras|uncertainty,14026,93.9877,,5,calculate prediction uncertainty use kera would like calculate nn model certainty confidence see deep model know nn tell image represent would like know certain model certain could also digit quite ambiguous would like know image model flip coin find theoretical writing trouble put code understand correctly evaluate testing image multiple time kill different neuron use dropout work mnist dataset run follow model predict model get certainty prediction would appreciate practical example preferably kera clarify look example get certainty use method outline yurin gal explanation method yield well result
655,655,37232782,NaN loss when training regression network,"<p>I have a data matrix in ""one-hot encoding"" (all ones and zeros) with 260,000 rows and 35 columns.  I am using Keras to train a simple neural network to predict a continuous variable.  The code to make the network is the following:</p>

<pre><code>model = Sequential()
model.add(Dense(1024, input_shape=(n_train,)))
model.add(Activation('relu'))
model.add(Dropout(0.1))

model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.1))

model.add(Dense(256))
model.add(Activation('relu'))
model.add(Dropout(0.1))
model.add(Dense(1))

sgd = SGD(lr=0.01, nesterov=True);
#rms = RMSprop()
#model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])
model.compile(loss='mean_absolute_error', optimizer=sgd)
model.fit(X_train, Y_train, batch_size=32, nb_epoch=3, verbose=1, validation_data=(X_test,Y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=4)] )
</code></pre>

<p>However, during the training process, I see the loss decrease nicely, but during the middle of the second epoch, it goes to nan:</p>

<pre><code>Train on 260000 samples, validate on 64905 samples
Epoch 1/3
260000/260000 [==============================] - 254s - loss: 16.2775 - val_loss:
 13.4925
Epoch 2/3
 88448/260000 [=========&gt;....................] - ETA: 161s - loss: nan
</code></pre>

<p>I tried using <code>RMSProp</code> instead of <code>SGD</code>, I tried <code>tanh</code> instead of <code>relu</code>, I tried with and without dropout, all to no avail.  I tried with a smaller model, i.e. with only one hidden layer, and same issue (it becomes nan at a different point).  However, it does work with less features, i.e. if there are only 5 columns, and gives quite good predictions. It seems to be there is some kind of overflow, but I can't imagine why--the loss is not unreasonably large at all.  </p>

<p>Python version 2.7.11, running on a linux machine, CPU only.  I tested it with the latest version of Theano, and I also get Nans, so I tried going to Theano 0.8.2 and have the same problem.  With the latest version of Keras has the same problem, and also with the 0.3.2 version.  </p>",37242531.0,22,4,,2016/5/14 23:04,58.0,2021/7/17 19:05,2020/11/9 7:34,,14336011.0,,684979.0,,1,97,python|keras|neural-network|theano|loss-function,119075,739.1030000000002,,4,nan loss train regression network data matrix one hot encode one zero row column use kera train simple neural network predict continuous variable code make network following however training process see loss decrease nicely middle second epoch go nan try use instead try instead try without dropout avail try small model e one hidden layer issue become nan different point however work less feature e columns give quite good prediction seem kind overflow imagine loss unreasonably large python version run linux machine cpu test late version theano also get nan try go theano problem late version kera problem also version
810,810,53500047,Stop Training in Keras when Accuracy is already 1.0,"<p>How will I stop Keras Training when the accuracy already reached 1.0? I tried monitoring loss value, but I haven't tried stopping the training when the accuracy is already 1.</p>

<p>I tried the code below with no luck:</p>

<pre><code>stopping_criterions =[
    EarlyStopping(monitor='loss', min_delta=0, patience = 1000),
    EarlyStopping(monitor='acc', base_line=1.0, patience =0)

]

model.summary()
model.compile(Adam(), loss='binary_crossentropy', metrics=['accuracy']) 
model.fit(scaled_train_samples, train_labels, batch_size=1000, epochs=1000000, callbacks=[stopping_criterions], shuffle = True, verbose=2)
</code></pre>

<p>UPDATE: </p>

<p>The training immediately stops at first epoch, even if the accuracy is still not <code>1.0</code>.</p>

<p><a href=""https://i.stack.imgur.com/29Z3Z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/29Z3Z.png"" alt=""enter image description here""></a></p>

<p>Please help.</p>",,3,1,,2018/11/27 12:45,5.0,2020/12/22 18:41,2020/3/1 3:06,,4780863.0,,4780863.0,,1,14,python|machine-learning|keras|neural-network,8111,52.0363,,4,stop training kera accuracy already stop kera train accuracy already reach tried monitoring loss value try stop training accuracy already try code luck update training immediately stop first epoch even accuracy still please help
291,291,56816241,"Difference between ""detach()"" and ""with torch.nograd()"" in PyTorch?","<p>I know about two ways to exclude elements of a computation from the gradient calculation <code>backward</code></p>

<p><strong>Method 1:</strong> using <code>with torch.no_grad()</code></p>

<pre><code>with torch.no_grad():
    y = reward + gamma * torch.max(net.forward(x))
loss = criterion(net.forward(torch.from_numpy(o)), y)
loss.backward();
</code></pre>

<p><strong>Method 2:</strong> using <code>.detach()</code></p>

<pre><code>y = reward + gamma * torch.max(net.forward(x))
loss = criterion(net.forward(torch.from_numpy(o)), y.detach())
loss.backward();
</code></pre>

<p>Is there a difference between these two? Are there benefits/downsides to either?</p>",,3,0,,2019/6/29 8:47,19.0,2021/1/28 22:44,2021/1/28 22:44,,5884955.0,,10973886.0,,1,50,python|pytorch|autograd,37097,152.27700000000004,,3,difference detach torch nograd pytorch know two way exclude element computation gradient calculation method use method use difference two benefit downside either
339,339,45199047,How to save model.summary() to file in Keras?,"<p>There is <a href=""https://keras.io/models/about-keras-models/"" rel=""noreferrer"">model.summary() method</a> in Keras. It prints table to stdout. Is it possible to save this to file?</p>",45199301.0,2,0,,2017/7/19 19:03,5.0,2021/1/6 11:28,,,,,258483.0,,1,31,python|keras|stdout,14822,81.0836,,5,save model summary file kera model summary method kera print table stdout possible save file
10,10,61706535,Keras - Validation Loss and Accuracy stuck at 0,"<p>I am trying to train a simple 2 layer Fully Connected neural net for Binary Classification in Tensorflow keras. I have split my data into Training and Validation sets with a 80-20 split using sklearn's <code>train_test_split()</code>.</p>

<p>When I call <code>model.fit(X_train, y_train, validation_data=[X_val, y_val])</code>, <strong>it shows 0 validation loss and accuracy for all epochs</strong>, but it trains just fine.</p>

<p><a href=""https://i.stack.imgur.com/aJVoW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aJVoW.png"" alt=""Screenshot of model.fit call and verbose log""></a></p>

<p>Also, when I try to evaluate it on the validation set, the output is non-zero.</p>

<p><a href=""https://i.stack.imgur.com/8C34g.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8C34g.png"" alt=""Screenshot of model.evaluate function call""></a></p>

<p>Can someone please explain why I am facing this 0 loss 0 accuracy error on validation. Thanks for your help.</p>

<p>Here is the complete sample code (MCVE) for this error: <a href=""https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing</a></p>",61707324.0,1,1,,2020/5/10 2:46,4.0,2020/7/15 15:08,2020/5/10 12:17,,4685471.0,,4182274.0,,1,18,python|tensorflow|machine-learning|keras|tf.keras,8285,51.2732,,4,kera validation loss accuracy stuck try train simple layer fully connect neural net binary classification tensorflow kera split data training validation set split use sklearn call show validation loss accuracy epoch train fine also try evaluate validation set output non zero someone please explain face loss accuracy error validation thanks help complete sample code mcve error
753,753,51235118,How to get word vectors from Keras Embedding Layer,"<p>I'm currently working with a Keras model which has a embedding layer as first layer. In order to visualize the relationships and similarity of words between each other I need a function that returns the mapping of words and vectors of every element in the vocabulary (e.g. 'love' - [0.21, 0.56, ..., 0.65, 0.10]).</p>

<p>Is there any way to do it?</p>",51235358.0,1,0,,2018/7/8 18:53,8.0,2021/1/12 11:32,2018/9/5 18:42,,2099607.0,,7714720.0,,1,21,python|dictionary|keras|keras-layer|word-embedding,12148,63.538,,3,get word vector kera embed layer currently work kera model embed layer first layer order visualize relationship similarity word need function return mapping word vector every element vocabulary e g love way
9,9,61705858,Keras: UnboundLocalError: local variable 'logs' referenced before assignment,"<p>I am relatively new to python, and while attempting to train a chatbot I received the error: 闂佺偨鍎查妶绺゜oundLocalError: local variable 'logs' referenced before assignment闂? I used model.fit to train:</p>

<pre><code>model.fit(x_train, y_train, epochs=7)
</code></pre>

<p>And I received the error:</p>

<pre><code>UnboundLocalError                         Traceback (most recent call last)
&lt;ipython-input-10-847c83704a3f&gt; in &lt;module&gt;()
      2           x_train,
      3           y_train,
----&gt; 4           epochs=7
      5           )

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---&gt; 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    854               logs = tmp_logs  # No error, now safe to assign to logs.
    855               callbacks.on_train_batch_end(step, logs)
--&gt; 856         epoch_logs = copy.copy(logs)
    857 
    858         # Run validation.

UnboundLocalError: local variable 'logs' referenced before assignment 

</code></pre>

<p>I ran this in google colab, with the link here: <a href=""https://colab.research.google.com/drive/18uTvvKYDrd8CQi31kg6vX2Dbxg1gD20X?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/18uTvvKYDrd8CQi31kg6vX2Dbxg1gD20X?usp=sharing</a></p>

<p>I used the chatterbot/english dataset on kaggle: <a href=""https://www.kaggle.com/kausr25/chatterbotenglish"" rel=""noreferrer"">https://www.kaggle.com/kausr25/chatterbotenglish</a> </p>",62351374.0,5,1,,2020/5/10 0:59,3.0,2020/12/2 1:12,,,,,13492606.0,,1,12,python|tensorflow|keras,12488,59.386,,4,kera unboundlocalerror local variable log reference assignment relatively new python attempt train chatbot receive error unboundlocalerror local variable log reference assignment use model fit train receive error run google colab link use chatterbot english dataset kaggle
553,553,48547660,AttributeError: module 'PIL.Image' has no attribute 'register_extensions',"<p>I was running lesson1 of fast.ai in google-colab. When I came to the line </p>

<pre><code>img = plt.imread(f'{PATH}valid/cats/{files[0]}')

plt.imshow(img);
</code></pre>

<p>It didn't show an image. Instead I got an error:</p>

<pre>
AttributeError: module 'PIL.Image' has no attribute 'register_extensions'
</pre>

<p>What could be causing this?</p>",,9,0,,2018/1/31 17:06,8.0,2020/6/12 23:37,2018/1/31 17:44,,13531.0,,8814209.0,,1,25,python|deep-learning|google-colaboratory,31231,104.978,,4,attributeerror module pil image attribute aregister extension run lesson fast ai google colab come line show image instead get error attributeerror module pil image attribute aregister extension could cause
346,346,45463778,Instance Normalisation vs Batch normalisation,"<p>I understand that Batch Normalisation helps in faster training by turning the activation towards unit Gaussian distribution and thus tackling vanishing gradients problem. Batch norm acts is applied differently at training(use mean/var from each batch) and test time (use finalized running mean/var from training phase).</p>

<p>Instance normalisation, on the other hand, acts as contrast normalisation as mentioned in this paper <a href=""https://arxiv.org/abs/1607.08022"" rel=""noreferrer"">https://arxiv.org/abs/1607.08022</a> . The authors mention that the output stylised images should be not depend on the contrast of the input content image and hence Instance normalisation helps. </p>

<p>But then should we not also use instance normalisation for image classification where class label should not depend on the contrast of input image. I have not seen any paper using instance normalisation in-place of batch normalisation for classification. What is the reason for that? Also, can and should batch and instance normalisation be used together. I am eager to get an intuitive as well as theoretical understanding of when to use which normalisation. </p>",,4,0,,2017/8/2 14:34,43.0,2021/6/9 3:53,2018/1/5 19:35,,712995.0,,2531306.0,,1,70,machine-learning|neural-network|computer-vision|conv-neural-network|batch-normalization,45307,264.625,,0,instance normalisation v batch normalisation understand batch normalisation help faster training turn activation towards unit gaussian distribution thus tackle vanish gradient problem batch norm act apply differently training use mean var batch test time use finalize run mean var train phase instance normalisation hand act contrast normalisation mention paper author mention output stylise image depend contrast input content image hence instance normalisation help also use instance normalisation image classification class label depend contrast input image see paper use instance normalisation place batch normalisation classification reason also batch instance normalisation use together eager get intuitive well theoretical understanding use normalisation
356,356,45735070,Keras Text Preprocessing - Saving Tokenizer object to file for scoring,"<p>I've trained a sentiment classifier model using Keras library by following the below steps(broadly).</p>

<ol>
<li>Convert Text corpus into sequences using Tokenizer object/class</li>
<li>Build a model using the model.fit() method </li>
<li>Evaluate this model</li>
</ol>

<p>Now for scoring using this model, I was able to save the model to a file and load from a file. However I've not found a way to save the Tokenizer object to file. Without this I'll have to process the corpus every time I need to score even a single sentence. Is there a way around this?</p>",45737582.0,5,0,,2017/8/17 12:25,15.0,2021/1/10 16:00,2017/12/30 13:35,,5974433.0,,5189033.0,,1,59,machine-learning|neural-network|nlp|deep-learning|keras,34480,208.15,,2,kera text preprocessing save tokenizer object file score train sentiment classifier model use kera library follow step broadly convert text corpus sequence use tokenizer object class build model use model fit method evaluate model score use model able save model file load file however find way save tokenizer object file without process corpus every time need score even single sentence way around
299,299,57284345,How to install nvidia apex on Google Colab,"<p>what I did is follow the instruction on the official github site</p>

<pre><code>!git clone https://github.com/NVIDIA/apex
!cd apex
!pip install -v --no-cache-dir ./
</code></pre>

<p>it gives me the error:</p>

<pre><code>ERROR: Directory './' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.
Exception information:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py"", line 178, in main
    status = self.run(options, args)
  File ""/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py"", line 326, in run
    self.name, wheel_cache
  File ""/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/base_command.py"", line 268, in populate_requirement_set
    wheel_cache=wheel_cache
  File ""/usr/local/lib/python3.6/dist-packages/pip/_internal/req/constructors.py"", line 248, in install_req_from_line
    ""nor 'pyproject.toml' found."" % name
pip._internal.exceptions.InstallationError: Directory './' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.
</code></pre>",57903798.0,7,0,,2019/7/31 6:13,,2021/8/3 18:24,,,,,4469265.0,,1,12,python|gpu|pytorch|nvidia|google-colaboratory,7981,77.4082,,1,install nvidia apex google colab follow instruction official github site give error
53,53,55266154,Pytorch preferred way to copy a tensor,"<p>There seems to be several ways to create a copy of a tensor in Pytorch, including</p>

<pre><code>y = tensor.new_tensor(x) #a

y = x.clone().detach() #b

y = torch.empty_like(x).copy_(x) #c

y = torch.tensor(x) #d
</code></pre>

<p><code>b</code> is explicitly preferred over <code>a</code> and <code>d</code> according to a UserWarning I get if I execute either <code>a</code> or <code>d</code>. Why is it preferred? Performance? I'd argue it's less readable. </p>

<p>Any reasons for/against using <code>c</code>? </p>",62496418.0,4,6,,2019/3/20 16:51,21.0,2020/9/23 10:32,,,,,2593878.0,,1,95,copy|pytorch|tensor,62911,195.195,,3,pytorch prefer way copy tensor seem several way create copy tensor pytorch include explicitly prefer accord userwarning get execute either preferred performance argue less readable reason use
23,23,53972159,"How does Pytorch's ""Fold"" and ""Unfold"" work?","<p>I've gone through the <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Fold"" rel=""noreferrer"">official doc</a>. I'm having a hard time understanding what this function is used for and how it works. Can someone explain this in Layman terms? </p>

<p>I get an error for the example they provide, although the Pytorch version I'm using matches the documentation. Perhaps fixing the error, which I did, is supposed to teach me something? The snippet given in the documentation is:</p>

<pre><code>   fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
   input = torch.randn(1, 3 * 2 * 2, 1)
   output = fold(input)
   output.size()
</code></pre>

<p>and the fixed snippet is: </p>

<pre><code>   fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
   input = torch.randn(1, 3 * 2 * 2, 3 * 2 * 2)
   output = fold(input)
   output.size()
</code></pre>

<p>Thanks!</p>",53972525.0,3,0,,2018/12/29 18:14,4.0,2021/6/1 9:50,2021/5/9 12:56,,9067615.0,,5170171.0,,1,26,python|machine-learning|deep-learning|computer-vision|pytorch,15781,91.3925,,3,pytorch fold unfold work go official doc hard time understand function use work someone explain layman term get error example provide although pytorch version use match documentation perhaps fix error suppose teach something snippet give documentation fixed snippet thanks
657,657,37293642,How to tell Keras stop training based on loss value?,"<p>Currently I use the following code:</p>

<pre><code>callbacks = [
    EarlyStopping(monitor='val_loss', patience=2, verbose=0),
    ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),
]
model.fit(X_train.astype('float32'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),
      callbacks=callbacks)
</code></pre>

<p>It tells Keras to stop training when loss didn't improve for 2 epochs. But I want to stop training after loss became smaller than some constant ""THR"":</p>

<pre><code>if val_loss &lt; THR:
    break
</code></pre>

<p>I've seen in documentation there are possibility to make your own callback:
<a href=""http://keras.io/callbacks/"">http://keras.io/callbacks/</a>
But nothing found how to stop training process. I need an advice.</p>",37296168.0,7,0,,2016/5/18 8:02,31.0,2020/8/17 20:37,,,,,2115332.0,,1,84,python|machine-learning|neural-network|conv-neural-network|keras,54332,273.54,,4,tell kera stop training base loss value currently use following code tell kera stop training loss improve epoch want stop training loss become small constant thr see documentation possibility make callback nothing find stop training process need advice
522,522,35050753,How big should batch size and number of epochs be when fitting a model in Keras?,"<p>I am training on 970 samples and validating on 243 samples. </p>

<p>How big should batch size and number of epochs be when fitting a model in Keras to optimize the val_acc? Is there any sort of rule of thumb to use based on data input size?</p>",,5,3,,2016/1/28 0:21,36.0,2020/4/29 21:16,,,,,4984897.0,,1,82,python|machine-learning|keras|data-science,106032,207.102,,3,big batch size number epochs fit model kera train sample validate sample big batch size number epochs fit model kera optimize val acc sort rule thumb use base data input size
152,152,42690721,How to interpret the discriminator's loss and the generator's loss in Generative Adversarial Nets?,"<p>I am reading people's implementation of DCGAN, especially <a href=""https://github.com/carpedm20/DCGAN-tensorflow"" rel=""noreferrer"">this one</a> in tensorflow.</p>

<p>In that implementation, the author draws the losses of the discriminator and of the generator, which is shown below (images come from <a href=""https://github.com/carpedm20/DCGAN-tensorflow"" rel=""noreferrer"">https://github.com/carpedm20/DCGAN-tensorflow</a>):</p>

<p><a href=""https://i.stack.imgur.com/PZm29.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PZm29.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/WRpsu.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WRpsu.png"" alt=""enter image description here""></a></p>

<p>Both the losses of the discriminator and of the generator don't seem to follow any pattern. Unlike general neural networks, whose loss decreases along with the increase of training iteration. How to interpret the loss when training GANs?</p>",44609259.0,1,2,,2017/3/9 8:53,6.0,2020/6/6 8:01,2019/8/3 22:06,,11065347.0,,1177952.0,,1,23,neural-network|deep-learning|generative-adversarial-network,21860,57.9586,,4,interpret discriminator loss generator loss generative adversarial net read people implementation dcgan especially one tensorflow implementation author draw loss discriminator generator show image come loss discriminator generator seem follow pattern unlike general neural network whose loss decrease along increase train iteration interpret loss training gans
251,251,44583254,"ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4","<p>I am trying for multi-class classification and here are the details of my training input and output:</p>

<blockquote>
  <p>train_input.shape= (1, 95000, 360) (95000 length input array with each
  element being an array of 360 length)</p>
  
  <p>train_output.shape = (1, 95000, 22) (22 Classes are there)</p>
</blockquote>

<pre><code>model = Sequential()

model.add(LSTM(22, input_shape=(1, 95000,360)))
model.add(Dense(22, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
model.fit(train_input, train_output, epochs=2, batch_size=500)
</code></pre>

<p>The error is:</p>

<blockquote>
  <p>ValueError: Input 0 is incompatible with layer lstm_13: expected ndim=3, found ndim=4
  in line:
  model.add(LSTM(22, input_shape=(1, 95000,360)))</p>
</blockquote>

<p>Please help me out, I am not able to solve it through other answers.</p>",44583919.0,4,1,,2017/6/16 7:23,6.0,2021/8/25 5:56,,,,,5697891.0,,1,26,python|keras|lstm|recurrent-neural-network,77616,83.3598,,4,valueerror input incompatible layer lstm expect ndim find ndim try multi class classification detail training input output train input shape length input array element array length train output shape class error valueerror input incompatible layer lstm expect ndim find ndim line model add lstm input shape please help able solve answer
629,629,50954479,Using CUDA with pytorch?,"<p>I have searched on here but I found only outdated posts.</p>
<p>I want to run the training on my GPU. I found on some forums that I need to apply <code>.cuda()</code> on anything I want to use CUDA with (I've applied it to everything I could without making the program crash). Surprisingly, this makes the training even slower.</p>
<p>Then, I found that you could use this <code>torch.set_default_tensor_type('torch.cuda.FloatTensor')</code> to use CUDA. With both enabled, nothing changes. What is happening?</p>
<p>Is there a way to reliably enable CUDA on the whole model?</p>
<hr />
<p>Also, what does <code>MyModel()</code> mean? I need more tangible examples, like code examples.  <a href=""https://stackoverflow.com/questions/50495053/if-im-not-specifying-to-use-cpu-gpu-which-one-is-my-script-using"">(This is the post I am referring to)</a></p>",51014127.0,2,3,,2018/6/20 18:02,12.0,2021/5/9 13:00,2021/5/9 13:00,,9067615.0,,9872568.0,,1,20,python|pytorch|torch,56862,65.0193,,1,use cuda pytorch search find outdated post want run training gpu find forum need apply anything want use cuda apply everything could without make program crash surprisingly make train even slow found could use use cuda enable nothing change happen way reliably enable cuda whole model also mean need tangible example like code example post refer
135,135,42411891,How to extract bias weights in Keras sequential model?,"<p>I'm running a simple feed-forward network using <em>Keras</em> . 
Having just one hidden layer I would like to make some inference regarding the relevance of each input to each output and I would like to extract the weights. </p>

<p>This is the model: </p>

<pre><code>def build_model(input_dim, output_dim):
    n_output_layer_1 = 150
    n_output = output_dim
    model = Sequential()
    model.add(Dense(n_output_layer_1, input_dim=input_dim, activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(n_output))
</code></pre>

<p>To extract the weight I wrote: </p>

<pre><code>for layer in model.layers:
    weights = layer.get_weights() 


weights = np.array(weights[0])     #this is hidden to output
first = model.layers[0].get_weights() #input to hidden
first = np.array(first[0])
</code></pre>

<p>Unfortunately I don't get the biases columns in the matrices, which I know Keras automatically puts in it. </p>

<p><strong>Do you know how to retrieve the biases weights?</strong></p>

<p>Thank you in advance for your help !</p>",42412124.0,1,0,,2017/2/23 9:38,5.0,2020/1/20 18:18,,,,,4338302.0,,1,21,python|tensorflow|neural-network|keras,22009,59.5704,2017/7/18 16:11,5,extract bias weight keras sequential model run simple feed forward network use kera one hidden layer would like make inference regard relevance input output would like extract weight model extract weight write unfortunately get bias columns matrix know kera automatically put know retrieve bias weight thank advance help
479,479,59787897,How does TensorFlow SparseCategoricalCrossentropy work?,"<p>I'm trying to understand this loss function in TensorFlow but I don't get it. It's <strong>SparseCategoricalCrossentropy</strong>. All other loss functions need outputs and labels of the same shape, this specific loss function doesn't.</p>

<p>Source code:</p>

<pre><code>import tensorflow as tf;

scce = tf.keras.losses.SparseCategoricalCrossentropy();
Loss = scce(
  tf.constant([ 1,    1,    1,    2   ], tf.float32),
  tf.constant([[1,2],[3,4],[5,6],[7,8]], tf.float32)
);
print(""Loss:"", Loss.numpy());
</code></pre>

<p>The error is:</p>

<pre><code>InvalidArgumentError: Received a label value of 2 which is outside the valid range of [0, 2).  
Label values: 1 1 1 2 [Op:SparseSoftmaxCrossEntropyWithLogits]
</code></pre>

<p>How to provide proper params to the loss function SparseCategoricalCrossentropy?</p>",59788222.0,2,0,,2020/1/17 13:00,5.0,2020/1/23 6:12,,,,,5581893.0,,1,16,tensorflow|machine-learning|deep-learning|loss-function|cross-entropy,11667,59.6678,,4,tensorflow sparsecategoricalcrossentropy work try understand loss function tensorflow get sparsecategoricalcrossentropy loss function need output label shape specific loss function source code error provide proper params loss function sparsecategoricalcrossentropy
115,115,41758385,Resizing images in Keras ImageDataGenerator flow methods,"<p>The Keras <code>ImageDataGenerator</code> class provides the two flow methods <code>flow(X, y)</code> and <code>flow_from_directory(directory)</code> (<a href=""https://keras.io/preprocessing/image/"" rel=""noreferrer"">https://keras.io/preprocessing/image/</a>).</p>

<p>Why is the parameter </p>

<blockquote>
  <p>target_size: tuple of integers, default: (256, 256). The dimensions to which all images found will be resized</p>
</blockquote>

<p>Only provided by <em>flow_from_directory(directory)</em> ? And what is the most concise way to add reshaping of images to the preprocessing pipeline using <em>flow(X, y)</em> ?</p>",41759831.0,5,1,,2017/1/20 7:39,3.0,2021/5/13 14:38,2017/1/20 8:59,,5215538.0,,1934212.0,,1,24,image-processing|keras,30461,65.935,,2,resize image kera imagedatagenerator flow method keras class provide two flow method parameter target size tuple integer default dimension image find resize provide flow directory directory concise way add reshaping image preprocessing pipeline use flow x
681,681,38260113,Implementing contrastive loss and triplet loss in Tensorflow,"<p>I started to play with TensorFlow two days ago and I'm wondering if there is the triplet and the contrastive losses implemented.</p>

<p>I've been looking at <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#losses"" rel=""noreferrer"">the documentation</a>, but I haven't found any example or description about these things.</p>",,3,0,,2016/7/8 6:20,29.0,2018/6/27 8:18,2018/3/20 4:49,,5098368.0,,5913101.0,,1,41,tensorflow|deep-learning,34986,141.776,,4,implement contrastive loss triplet loss tensorflow start play tensorflow two day ago wonder triplet contrastive loss implement look documentation find example description thing
686,686,38441589,Is RNN initial state reset for subsequent mini-batches?,"<p>Could someone please clarify whether the initial state of the RNN in TF is reset for subsequent mini-batches, or the last state of the previous mini-batch is used as mentioned in <a href=""https://arxiv.org/abs/1409.2329"" rel=""noreferrer"">Ilya Sutskever et al., ICLR 2015 </a> ?</p>",38465663.0,2,0,,2016/7/18 16:18,19.0,2018/7/13 11:30,2017/11/12 3:28,,3924118.0,,5724595.0,,1,18,time-series|tensorflow|recurrent-neural-network,15212,53.9287,,3,rnn initial state reset subsequent mini batch could someone please clarify whether initial state rnn tf reset subsequent mini batch last state previous mini batch use mention ilya sutskever et al iclr
326,326,58764619,Why should we use Temperature in softmax?,"<p>I'm recently working on CNN and I want to know what is the function of temperature in softmax formula? and why should we use high temperatures to see a softer norm in probability distribution?<a href=""https://i.stack.imgur.com/HYyQT.jpg"" rel=""noreferrer"">Softmax Formula</a></p>",63471046.0,1,2,,2019/11/8 10:19,6.0,2021/6/11 7:41,2020/12/22 8:13,,11835882.0,,11835882.0,,1,26,machine-learning|deep-learning|conv-neural-network|softmax|densenet,18072,57.228,2020/8/29 18:41,3,use temperature softmax recently work cnn want know function temperature softmax formula use high temperature see soft norm probability distribution softmax formula
266,266,44746507,Keras + TensorFlow Realtime training chart,"<p>I have the following code running inside a Jupyter notebook:</p>

<pre><code># Visualize training history
from keras.models import Sequential
from keras.layers import Dense
import matplotlib.pyplot as plt
import numpy
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load pima indians dataset
dataset = numpy.loadtxt(""pima-indians-diabetes.csv"", delimiter="","")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = Sequential()
model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))
model.add(Dense(8, kernel_initializer='uniform', activation='relu'))
model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))
# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Fit the model
history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)
# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
</code></pre>

<p>The code collects epochs history, then displays the progress history. </p>

<hr>

<p><strong>Q:</strong> How can I make the chart change while training so I can see the changes in real time?</p>",49814021.0,2,0,,2017/6/25 12:47,7.0,2020/7/22 10:43,,,,,1115237.0,,1,18,python|machine-learning|tensorflow|keras|jupyter-notebook,11914,58.5042,,5,kera tensorflow realtime training chart following code run inside jupyter notebook code collect epochs history display progress history q make chart change train see change real time
187,187,43448029,How can I print the values of Keras tensors?,"<p>I am implementing own Keras loss function. How can I access tensor values?</p>

<p>What I've tried</p>

<pre><code>def loss_fn(y_true, y_pred):
    print y_true
</code></pre>

<p>It prints</p>

<pre><code>Tensor(""target:0"", shape=(?, ?), dtype=float32)
</code></pre>

<p>Is there any Keras function to access <code>y_true</code> values?</p>",,8,1,,2017/4/17 8:39,9.0,2021/1/18 17:53,2019/12/7 3:13,,3924118.0,,5203167.0,,1,39,python|neural-network|keras|tensor,30267,117.324,,4,print value kera tensor implement kera loss function access tensor value try print kera function access value
8,8,61586981,"ValueError: Layer sequential_20 expects 1 inputs, but it received 2 input tensors","<p>I am trying to build a simple Autoencoder using the KMNIST dataset from Tensorflow and some sample code from a textbook I'm using, but I keep getting an error when I try to fit the model.</p>
<p>The error says <code>ValueError: Layer sequential_20 expects 1 inputs, but it received 2 input tensors.</code></p>
<p>I'm really new to TensorFlow, and all my research on this error has baffled me since it seems to involve things not in my code.
<a href=""https://stackoverflow.com/questions/61006764/valueerror-layer-model-2-expects-2-inputs-but-it-received-1-input-tensors"">This thread</a> wasn't helpful since I'm only using sequential layers.</p>
<p>Code in full:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds
import pandas as pd
import matplotlib.pyplot as plt

#data = tfds.load(name = 'kmnist')

(img_train, label_train), (img_test, label_test) = tfds.as_numpy(tfds.load(
    name = 'kmnist',
    split=['train', 'test'],
    batch_size=-1,
    as_supervised=True,
))

img_train = img_train.squeeze()
img_test = img_test.squeeze()

## From Hands on Machine Learning Textbook, chapter 17

stacked_encoder = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(100, activation=&quot;selu&quot;),
    keras.layers.Dense(30, activation=&quot;selu&quot;),
])

stacked_decoder = keras.models.Sequential([
    keras.layers.Dense(100, activation=&quot;selu&quot;, input_shape=[30]),
    keras.layers.Dense(28 * 28, activation=&quot;sigmoid&quot;),
    keras.layers.Reshape([28, 28])
])

stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])
stacked_ae.compile(loss=&quot;binary_crossentropy&quot;,
                   optimizer=keras.optimizers.SGD(lr=1.5))

history = stacked_ae.fit(img_train, img_train, epochs=10,
                         validation_data=[img_test, img_test])
</code></pre>",,5,0,,2020/5/4 7:18,1.0,2021/7/5 23:42,2020/8/26 21:56,,10375049.0,,8835445.0,,1,28,python|tensorflow|machine-learning|keras|deep-learning,23714,99.5,,4,valueerror layer sequential expect input receive input tensor try build simple autoencoder use kmnist dataset tensorflow sample code textbook use keep get error try fit model error say really new tensorflow research error baffle since seem involve thing code thread helpful since use sequential layer code full
268,268,44788946,Shuffling training data with LSTM RNN,"<p>Since an LSTM RNN uses previous events to predict current sequences, why do we shuffle the training data? Don't we lose the temporal ordering of the training data? How is it still effective at making predictions after being trained on shuffled training data?</p>",44789322.0,1,0,,2017/6/27 20:04,14.0,2017/6/28 22:48,,,,,1444955.0,,1,37,machine-learning|keras|lstm|recurrent-neural-network,12080,65.7283,,2,shuffle train data lstm rnn since lstm rnn use previous event predict current sequence shuffle training data lose temporal ordering training data still effective make prediction train shuffled training data
592,592,49606482,How to resolve runtime error due to size mismatch in PyTorch?,"<p>I am trying to implement a simple autoencoder using <code>PyTorch</code>. My dataset consists of 256 x 256 x 3 images. I have built a <code>torch.utils.data.dataloader.DataLoader</code> object which has the image stored as tensor. When I run the autoencoder, I get a runtime error:</p>

<blockquote>
  <p>size mismatch, m1: [76800 x 256], m2: [784 x 128] at
  /Users/soumith/minicondabuild3/conda-bld/pytorch_1518371252923/work/torch/lib/TH/generic/THTensorMath.c:1434</p>
</blockquote>

<p>These are my hyperparameters:</p>

<pre><code>batch_size=100,
learning_rate = 1e-3,
num_epochs = 100
</code></pre>

<p>Following is the architecture of my auto-encoder:</p>

<pre><code>class autoencoder(nn.Module):
    def __init__(self):
        super(autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(3*256*256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(True),
            nn.Linear(64, 12),
            nn.ReLU(True),
            nn.Linear(12, 3))

        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(True),
            nn.Linear(12, 64),
            nn.ReLU(True),
            nn.Linear(64, 128),
            nn.Linear(128, 3*256*256),
            nn.ReLU())

def forward(self, x):
    x = self.encoder(x)
    #x = self.decoder(x)
    return x
</code></pre>

<p>This is the code I used to run the model:</p>

<pre><code>for epoch in range(num_epochs):
for data in dataloader:
    img = data['image']
    img = Variable(img)
    # ===================forward=====================
    output = model(img)
    loss = criterion(output, img)
    # ===================backward====================
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
# ===================log========================
print('epoch [{}/{}], loss:{:.4f}'
      .format(epoch+1, num_epochs, loss.data[0]))
if epoch % 10 == 0:
    pic = show_img(output.cpu().data)
    save_image(pic, './dc_img/image_{}.jpg'.format(epoch))
</code></pre>",49607525.0,3,5,,2018/4/2 6:39,6.0,2020/6/18 11:09,2018/4/2 7:36,,3619334.0,,3619334.0,,1,10,python|pytorch|autoencoder,26725,64.7077,,4,resolve runtime error due size mismatch pytorch try implement simple autoencoder use dataset consist x x image build object image store tensor run autoencoder get runtime error size mismatch x x user soumith minicondabuild conda bld pytorch work torch lib th generic thtensormath c hyperparameters follow architecture auto encoder code use run model
420,420,47626254,Changing optimizer in keras during training,<p>I am developing a model using <code>nadam</code> optimizer. I was wondering if there is a way to switch to <code>sgd</code> during training if validation loss does not reduce for two epochs.</p>,47633305.0,3,0,,2017/12/4 3:53,5.0,2020/8/11 6:09,,,,,7280300.0,,1,18,keras,7993,60.4108,,4,change optimizer kera training develop model use optimizer wonder way switch train validation loss reduce two epoch
708,708,39681046,Keras - stateful vs stateless LSTMs,"<p>I'm having a hard time conceptualizing the difference between stateful and stateless LSTMs in Keras. My understanding is that at the end of each batch, the ""state of the network is reset"" in the stateless case, whereas for the stateful case, the state of the network is preserved for each batch, and must then be manually reset at the end of each epoch.</p>

<p>My questions are as follows:
1. In the stateless case, how is the network learning if the state isn't preserved in-between batches?
2. When would one use the stateless vs stateful modes of an LSTM?</p>",43090574.0,2,0,,2016/9/24 21:16,15.0,2017/3/29 10:12,,,,,190894.0,,1,40,tensorflow|deep-learning|keras|lstm,12000,82.3167,,3,kera stateful vs stateless lstms hard time conceptualize difference stateful stateless lstms kera understanding end batch state network reset stateless case whereas stateful case state network preserve batch must manually reset end epoch question follow stateless case network learn state preserve batch would one use stateless v stateful mode lstm
800,800,52922445,"RuntimeError: ""exp"" not implemented for 'torch.LongTensor'","<p>I am following this tutorial: <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""noreferrer"">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>
to implement the Transformer model from the ""Attention Is All You Need"" paper. </p>

<p>However I am getting the following error : 
RuntimeError: ""exp"" not implemented for 'torch.LongTensor'</p>

<p>This is the line, in the PositionalEnconding class, that is causing the error:</p>

<pre><code>div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
</code></pre>

<p>When it is being constructed here: </p>

<pre><code>pe = PositionalEncoding(20, 0)
</code></pre>

<p>Any ideas?? I've already tried converting this to perhaps a Tensor Float type, but this has not worked.</p>

<p>I've even downloaded the whole notebook with accompanying files and the error seems to persist in the original tutorial.</p>

<p>Any ideas what may be causing this error? </p>

<p>Thanks!</p>",52923871.0,5,6,,2018/10/22 4:32,,2021/3/8 2:22,,,,,5833067.0,,1,16,pytorch|tensor|attention-model,9843,68.9725,,4,runtimeerror exp implement torch longtensor follow tutorial implement transformer model attention need paper however get following error runtimeerror exp implement torch longtensor line positionalenconding class cause error construct idea already try convert perhaps tensor float type work even download whole notebook accompany file error seem persist original tutorial idea may cause error thanks
205,205,43784921,How to display custom images in TensorBoard using Keras?,"<p>I'm working on a segmentation problem in Keras and I want to display segmentation results at the end of every training epoch.</p>

<p>I want something similar to <a href=""https://stackoverflow.com/questions/38543850/tensorflow-how-to-display-custom-images-in-tensorboard-e-g-matplotlib-plots""><em>Tensorflow: How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)</em></a>, but using Keras. I know that Keras has the <a href=""https://keras.io/callbacks/#tensorboard"" rel=""noreferrer""><code>TensorBoard</code></a> callback but it seems limited for this purpose.</p>

<p>I know this would break the Keras backend abstraction, but I'm interested in using TensorFlow backend anyway.</p>

<p>Is it possible to achieve that with Keras + TensorFlow?</p>",49363251.0,8,6,,2017/5/4 13:56,17.0,2020/9/29 23:09,2018/9/27 7:24,,604734.0,,604734.0,,1,26,tensorflow|keras|deep-learning|tensorboard,17616,112.584,,5,display custom image tensorboard use kera work segmentation problem kera want display segmentation result end every training epoch want something similar tensorflow display custom image tensorboard e g matplotlib plot use kera know kera callback seem limit purpose know would break kera backend abstraction interested use tensorflow backend anyway possible achieve kera tensorflow
104,104,41617463,Tensorflow Confusion Matrix in TensorBoard,"<p>I want to have a visual of confusion matrix in tensorboard. To do this, I am modifying Evaluation example of Tensorflow Slim: <a href=""https://github.com/tensorflow/models/blob/master/slim/eval_image_classifier.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/slim/eval_image_classifier.py</a></p>

<p>In this example code, Accuracy already provided but it is not possible to add ""confusion matrix"" metric directly because it is not streaming. </p>

<p><strong>What is difference between streaming metrics and non-streaming ones?</strong></p>

<p>Therefore, I tried to add it like this:</p>

<pre><code>c_matrix = slim.metrics.confusion_matrix(predictions, labels)

#These operations needed for image summary
c_matrix = tf.cast(c_matrix, uint8)
c_matrix = tf.expand_dims(c_matrix, 2)
c_matrix = tf.expand_dims(c_matrix, 0)

op = tf.image_summary(""confusion matrix"", c_matrix, collections=[])
tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)
</code></pre>

<p>This creates an image in tensorboard but probably there is a formatting problem. Matrix should be normalized between 0-1 so that It produces meaningful image.</p>

<p><strong>How can I produce a meaningful confusion matrix? How can I deal with multi batch evaluation process?</strong></p>",,4,2,,2017/1/12 15:50,13.0,2018/6/17 6:38,2017/1/12 15:59,,2616232.0,,2616232.0,,1,28,python|tensorflow|deep-learning|tensorboard,24624,108.965,,5,tensorflow confusion matrix tensorboard want visual confusion matrix tensorboard modify evaluation example tensorflow slim example code accuracy already provide possible add confusion matrix metric directly stream difference stream metric non stream one therefore try add like create image tensorboard probably formatting problem matrix normalize produce meaningful image produce meaningful confusion matrix deal multi batch evaluation process
103,103,41563720,"Error when checking model input: expected convolution2d_input_1 to have 4 dimensions, but got array with shape (32, 32, 3)","<p>I want to train a deep network starting with the following layer:</p>

<pre><code>model = Sequential()
model.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3)))
</code></pre>

<p>using </p>

<pre><code>history = model.fit_generator(get_training_data(),
                samples_per_epoch=1, nb_epoch=1,nb_val_samples=5,
                verbose=1,validation_data=get_validation_data()
</code></pre>

<p>with the following generator:</p>

<pre><code>def get_training_data(self):
     while 1:
        for i in range(1,5):
            image = self.X_train[i]
            label = self.Y_train[i]
            yield (image,label)
</code></pre>

<p>(validation generator looks similar).</p>

<p>During training, I get the error: </p>

<pre><code>Error when checking model input: expected convolution2d_input_1 to have 4 
dimensions, but got array with shape (32, 32, 3)
</code></pre>

<p>How can that be, with a first layer</p>

<pre><code> model.add(Conv2D(32, 3, 3, input_shape=(32, 32, 3)))
</code></pre>

<p>?</p>",41595178.0,9,2,,2017/1/10 7:51,12.0,2020/9/22 15:46,,,,,1934212.0,,1,70,deep-learning|keras|keras-layer,155193,228.763,,4,error check model input expect convolution input dimension get array shape want train deep network start follow layer use following generator validation generator look similar training get error first layer
434,434,47877475,keras tensorboard: plot train and validation scalars in a same figure,"<p>So I am using tensorboard within keras. In tensorflow one could use two different summarywriters for train and validation scalars so that tensorboard could plot them in a same figure. Something like the <strong>figure</strong> in </p>

<p><a href=""https://stackoverflow.com/questions/37146614/tensorboard-plot-training-and-validation-losses-on-the-same-graph"">TensorBoard - Plot training and validation losses on the same graph?</a></p>

<p>Is there a way to do this in keras?</p>

<p>Thanks. </p>",48393723.0,2,0,,2017/12/18 22:39,24.0,2019/3/13 19:51,,,,,9050461.0,,1,37,tensorflow|neural-network|keras|tensorboard,15845,109.6,,5,kera tensorboard plot train validation scalar figure use tensorboard within kera tensorflow one could use two different summarywriters train validation scalar tensorboard could plot figure something like figure tensorboard plot training validation loss graph way kera thanks
408,408,47312219,What is the definition of a non-trainable parameter?,"<p>What is the definition of <strong>non-trainable</strong> parameter in a model? </p>

<p>For example, while you are building your own model, its value is 0 as a default, but when you want to use an inception model, it is becoming something else rather than 0. What would be the reason behind it? </p>",47312861.0,4,0,,2017/11/15 16:11,11.0,2019/10/24 1:05,2019/10/24 1:05,,3924118.0,,3013290.0,,1,32,tensorflow|deep-learning|keras|theano|caffe,24288,138.142,,3,definition non trainable parameter definition non trainable parameter model example build model value default want use inception model become something else rather would reason behind
247,247,44544471,How to get the coordinates of the bounding box in YOLO object detection?,"<p><a href=""https://i.stack.imgur.com/snfrZ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/snfrZ.jpg"" alt=""enter image description here""></a></p>

<p>I need to get the bounding box coordinates generated in the above image using YOLO object detection.</p>",44592380.0,5,0,,2017/6/14 12:12,7.0,2021/5/21 23:27,2020/7/18 14:43,,13898969.0,,8160365.0,,1,15,python|deep-learning|computer-vision|object-detection,32344,60.0392,,3,get coordinate bounding box yolo object detection need get bounding box coordinate generate image use yolo object detection
222,222,44132652,Keras - How to perform a prediction using KerasRegressor?,"<p>I am new to machine learning, and I am trying to handle Keras to perform regression tasks. I have implemented this code, based on <a href=""http://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/"" rel=""noreferrer"">this</a> example.</p>

<pre><code>X = df[['full_sq','floor','build_year','num_room','sub_area_2','sub_area_3','state_2.0','state_3.0','state_4.0']]
y = df['price_doc']

X = np.asarray(X)
y = np.asarray(y)

X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.2)
def baseline_model():
    model = Sequential()
    model.add(Dense(13, input_dim=9, kernel_initializer='normal', 
        activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=100, verbose=False)
kfold = KFold(n_splits=10, random_state=seed)
results = cross_val_score(estimator, X_train, Y_train, cv=kfold)
print(""Results: %.2f (%.2f) MSE"" % (results.mean(), results.std()))

prediction = estimator.predict(X_test)
accuracy_score(Y_test, prediction)
</code></pre>

<p>When I run the code I get this error:<br></p>

<blockquote>
  <p><code>AttributeError: 'KerasRegressor' object has no attribute 'model'</code></p>
</blockquote>

<p>How could I correctly 'insert' the model in KerasRegressor?  </p>",44134591.0,3,10,,2017/5/23 10:47,9.0,2018/5/3 19:36,2017/5/23 12:20,,3666197.0,,7387749.0,,1,14,machine-learning|scikit-learn|neural-network|regression|keras,34018,59.5268,,3,keras perform prediction use kerasregressor new machine learning try handle kera perform regression task implement code base example run code get error could correctly insert model kerasregressor
715,715,39890147,"Keras uses way too much GPU memory when calling train_on_batch, fit, etc","<p>I've been messing with Keras, and like it so far. There's one big issue I have been having, when working with fairly deep networks: When calling model.train_on_batch, or model.fit etc., Keras allocates significantly more GPU memory than what the model itself should need. This is not caused by trying to train on some really large images, it's the network model itself that seems to require a lot of GPU memory. I have created this toy example to show what I mean. Here's essentially what's going on:</p>

<p>I first create a fairly deep network, and use model.summary() to get the total number of parameters needed for the network (in this case 206538153, which corresponds to about 826 MB). I then use nvidia-smi to see how much GPU memory Keras has allocated, and I can see that it makes perfect sense (849 MB).</p>

<p>I then compile the network, and can confirm that this does not increase GPU memory usage. And as we can see in this case, I have almost 1 GB of VRAM available at this point.</p>

<p>Then I try to feed a simple 16x16 image and a 1x1 ground truth to the network, and then everything blows up, because Keras starts allocating lots of memory again, for no reason that is obvious to me. Something about training the network seems to require a lot more memory than just having the model, which doesn't make sense to me. I have trained significantly deeper networks on this GPU in other frameworks, so that makes me think that I'm using Keras wrong (or there's something wrong in my setup, or in Keras, but of course that's hard to know for sure).</p>

<p>Here's the code:</p>



<pre class=""lang-python prettyprint-override""><code>from scipy import misc
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Reshape, Flatten, ZeroPadding2D, Dropout
import os

model = Sequential()

model.add(Convolution2D(256, 3, 3, border_mode='same', input_shape=(16,16,1)))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(512, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(Convolution2D(1024, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model.add(Convolution2D(256, 3, 3, border_mode='same'))
model.add(Convolution2D(32, 3, 3, border_mode='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(4))
model.add(Dense(1))

model.summary()

os.system(""nvidia-smi"")
raw_input(""Press Enter to continue..."")    

model.compile(optimizer='sgd',
              loss='mse', 
              metrics=['accuracy'])

os.system(""nvidia-smi"")              
raw_input(""Compiled model. Press Enter to continue..."")

n_batches = 1
batch_size = 1
for ibatch in range(n_batches):
    x = np.random.rand(batch_size, 16,16,1)
    y = np.random.rand(batch_size, 1)

    os.system(""nvidia-smi"")
    raw_input(""About to train one iteration. Press Enter to continue..."")

    model.train_on_batch(x, y)         
    print(""Trained one iteration"")
</code></pre>

<p>Which gives the following output for me:</p>

<pre class=""lang-python prettyprint-override""><code>Using Theano backend.
Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5103)
/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.
  warnings.warn(warn)
____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
convolution2d_1 (Convolution2D)  (None, 16, 16, 256)   2560        convolution2d_input_1[0][0]      
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 8, 8, 256)     0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 8, 8, 512)     1180160     maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 4, 4, 512)     0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 4, 4, 1024)    4719616     maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_3[0][0]            
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_4[0][0]            
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_6[0][0]            
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_7[0][0]            
____________________________________________________________________________________________________
convolution2d_9 (Convolution2D)  (None, 4, 4, 1024)    9438208     convolution2d_8[0][0]            
____________________________________________________________________________________________________
convolution2d_10 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_9[0][0]            
____________________________________________________________________________________________________
convolution2d_11 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_10[0][0]           
____________________________________________________________________________________________________
convolution2d_12 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_11[0][0]           
____________________________________________________________________________________________________
convolution2d_13 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_12[0][0]           
____________________________________________________________________________________________________
convolution2d_14 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_13[0][0]           
____________________________________________________________________________________________________
convolution2d_15 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_14[0][0]           
____________________________________________________________________________________________________
convolution2d_16 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_15[0][0]           
____________________________________________________________________________________________________
convolution2d_17 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_16[0][0]           
____________________________________________________________________________________________________
convolution2d_18 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_17[0][0]           
____________________________________________________________________________________________________
convolution2d_19 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_18[0][0]           
____________________________________________________________________________________________________
convolution2d_20 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_19[0][0]           
____________________________________________________________________________________________________
convolution2d_21 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_20[0][0]           
____________________________________________________________________________________________________
convolution2d_22 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_21[0][0]           
____________________________________________________________________________________________________
convolution2d_23 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_22[0][0]           
____________________________________________________________________________________________________
convolution2d_24 (Convolution2D) (None, 4, 4, 1024)    9438208     convolution2d_23[0][0]           
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 2, 2, 1024)    0           convolution2d_24[0][0]           
____________________________________________________________________________________________________
convolution2d_25 (Convolution2D) (None, 2, 2, 256)     2359552     maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
convolution2d_26 (Convolution2D) (None, 2, 2, 32)      73760       convolution2d_25[0][0]           
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 1, 1, 32)      0           convolution2d_26[0][0]           
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 32)            0           maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 4)             132         flatten_1[0][0]                  
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 1)             5           dense_1[0][0]                    
====================================================================================================
Total params: 206538153
____________________________________________________________________________________________________
None
Thu Oct  6 09:05:42 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   37C    P2    28W / 120W |   1082MiB /  2044MiB |      9%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
Compiled model. Press Enter to continue...
Thu Oct  6 09:05:44 2016       
+------------------------------------------------------+                       
| NVIDIA-SMI 352.63     Driver Version: 352.63         |                       
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 960     Off  | 0000:01:00.0      On |                  N/A |
| 30%   38C    P2    28W / 120W |   1082MiB /  2044MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1796    G   /usr/bin/X                                     155MiB |
|    0      2597    G   compiz                                          65MiB |
|    0      5966    C   python                                         849MiB |
+-----------------------------------------------------------------------------+
About to train one iteration. Press Enter to continue...
Error allocating 37748736 bytes of device memory (out of memory). Driver report 34205696 bytes free and 2144010240 bytes total 
Traceback (most recent call last):
  File ""memtest.py"", line 65, in &lt;module&gt;
    model.train_on_batch(x, y)         
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 712, in train_on_batch
    class_weight=class_weight)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1221, in train_on_batch
    outputs = self.train_function(ins)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py"", line 717, in __call__
    return self.function(*inputs)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 871, in __call__
    storage_map=getattr(self.fn, 'storage_map', None))
  File ""/usr/local/lib/python2.7/dist-packages/theano/gof/link.py"", line 314, in raise_with_op
    reraise(exc_type, exc_value, exc_trace)
  File ""/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.py"", line 859, in __call__
    outputs = self.fn()
MemoryError: Error allocating 37748736 bytes of device memory (out of memory).
Apply node that caused the error: GpuContiguous(GpuDimShuffle{3,2,0,1}.0)
Toposort index: 338
Inputs types: [CudaNdarrayType(float32, 4D)]
Inputs shapes: [(1024, 1024, 3, 3)]
Inputs strides: [(1, 1024, 3145728, 1048576)]
Inputs values: ['not shown']
Outputs clients: [[GpuDnnConv{algo='small', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0}), GpuDnnConvGradI{algo='none', inplace=True}(GpuContiguous.0, GpuContiguous.0, GpuAllocEmpty.0, GpuDnnConvDesc{border_mode='half', subsample=(1, 1), conv_mode='conv', precision='float32'}.0, Constant{1.0}, Constant{0.0})]]

HINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.
HINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.
</code></pre>

<p>A few things to note: </p>

<ul>
<li>I have tried both Theano and TensorFlow backends. Both have the same problems, and run out of memory at the same line. In TensorFlow, it seems that Keras preallocates a lot of memory (about 1.5 GB) so nvidia-smi doesn't help us track what's going on there, but I get the same out-of-memory exceptions. Again, this points towards an error in (my usage of) Keras (although it's hard to be certain about such things, it could be something with my setup).</li>
<li>I tried using CNMEM in Theano, which behaves like TensorFlow: It preallocates a large amount of memory (about 1.5 GB) yet crashes in the same place.</li>
<li>There are some warnings about the CudNN-version. I tried running the Theano backend with CUDA but not CudNN and I got the same errors, so that is not the source of the problem.</li>
<li>If you want to test this on your own GPU, you might want to make the network deeper/shallower depending on how much GPU memory you have to test this.</li>
<li>My configuration is as follows: Ubuntu 14.04, GeForce GTX 960, CUDA 7.5.18, CudNN 5.1.3, Python 2.7, Keras 1.1.0 (installed via pip)</li>
<li>I've tried changing the compilation of the model to use different optimizers and losses, but that doesn't seem to change anything.</li>
<li>I've tried changing the train_on_batch function to use fit instead, but it has the same problem.</li>
<li>I saw one similar question here on StackOverflow - <a href=""https://stackoverflow.com/questions/35757151/why-does-this-keras-model-require-over-6gb-of-memory"">Why does this Keras model require over 6GB of memory?</a> - but as far as I can tell, I don't have those issues in my configuration. I've never had multiple versions of CUDA installed, and I've double checked my PATH, LD_LIBRARY_PATH and CUDA_ROOT variables more times than I can count.</li>
<li>Julius suggested that the activation parameters themselves take up GPU memory. If this is true, can somebody explain it a bit more clearly? I have tried changing the activation function of my convolution layers to functions that are clearly hard-coded with no learnable parameters as far as I can tell, and that doesn't change anything. Also, it seems unlikely that these parameters would take up almost as much memory as the rest of the network itself.</li>
<li>After thorough testing, the largest network I can train is about 453 MB of parameters, out of my ~2 GB of GPU RAM. Is this normal? </li>
<li>After testing Keras on some smaller CNNs that do fit in my GPU, I can see that there are very sudden spikes in GPU RAM usage. If I run a network with about 100 MB of parameters, 99% of the time during training it'll be using less than 200 MB of GPU RAM. But every once in a while, memory usage spikes to about 1.3 GB. It seems safe to assume that it's these spikes that are causing my problems. I've never seen these spikes in other frameworks, but they might be there for a good reason? <strong>If anybody knows what causes them, and if there's a way to avoid them, please chime in!</strong></li>
</ul>",,3,0,,2016/10/6 7:34,11.0,2021/1/27 19:12,2017/10/24 20:00,,4685471.0,,6930321.0,,1,40,memory|tensorflow|keras|theano,19281,75.1405,,4,kera use way much gpu memory call train batch fit etc mess kera like far one big issue work fairly deep network call model train batch model fit etc kera allocate significantly gpu memory model need cause try train really large image network model seem require lot gpu memory create toy example show mean essentially go first create fairly deep network use model summary get total number parameter need network case correspond mb use nvidia smi see much gpu memory kera allocate see make perfect sense mb compile network confirm increase gpu memory usage see case almost gb vram available point try fee simple x image x ground truth network everything blow kera start allocate lot memory reason obvious something train network seem require lot memory model make sense train significantly deep network gpu framework make think use kera wrong something wrong setup kera course hard know sure code give following output thing note try theano tensorflow backends problem run memory line tensorflow seem keras preallocates lot memory gb nvidia smi help u track go get memory exception point towards error usage kera although hard certain thing could something setup try use cnmem theano behave like tensorflow preallocates large amount memory gb yet crash place warning cudnn version try run theano backend cuda cudnn get error source problem want test gpu might want make network deeper shallower depend much gpu memory test configuration follow ubuntu geforce gtx cuda cudnn python kera instal via pip try change compilation model use different optimizers loss seem change anything try change train batch function use fit instead problem saw one similar question stackoverflow keras model require gb memory far tell issue configuration never multiple version cuda instal double check path ld library path cuda root variable time count julius suggest activation parameters take gpu memory true somebody explain bit clearly try change activation function convolution layer function clearly hard cod learnable parameter far tell change anything also seem unlikely parameter would take almost much memory rest network thorough test large network train mb parameter gb gpu ram normal test kera small cnns fit gpu see sudden spike gpu ram usage run network mb parameter time train use less mb gpu ram every memory usage spike gb seem safe assume spike cause problem never see spike framework might good reason anybody know cause way avoid please chime
552,552,48508036,"Sklearn StratifiedKFold: ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead","<p>Working with Sklearn stratified kfold split, and when I attempt to split using multi-class, I received on error (see below).  When I tried and split using binary, it works no problem.</p>



<pre class=""lang-python prettyprint-override""><code>num_classes = len(np.unique(y_train))
y_train_categorical = keras.utils.to_categorical(y_train, num_classes)
kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=999)

# splitting data into different folds
for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train_categorical)):
    x_train_kf, x_val_kf = x_train[train_index], x_train[val_index]
    y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]

ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.
</code></pre>",48512157.0,5,0,,2018/1/29 18:48,4.0,2021/3/10 21:52,2019/4/13 15:46,,4685471.0,,3866549.0,,1,34,python|machine-learning|keras|scikit-learn|cross-validation,46118,101.655,,4,sklearn stratifiedkfold valueerror support target type binary multiclass get multilabel indicator instead work sklearn stratify kfold split attempt split use multi class receive error see try split use binary work problem
144,144,42532386,How to work with multiple inputs for LSTM in Keras?,"<p>I'm trying to predict the water usage of a population.</p>

<p>I have 1 main input:</p>

<ul>
<li>Water volume</li>
</ul>

<p>and 2 secondary inputs:</p>

<ul>
<li>Temperature</li>
<li>Rainfall</li>
</ul>

<p>In theory they have a relation with the water supply. </p>

<p>It must be said that each rainfall and temperature data correspond with the water volume. So this is a time series problem.</p>

<p>The problem is that I don't know how to use 3 inputs from just one .csv file, with 3 columns, each one for each input, as the code below is made. When I have just one input (e.g.water volume) the network works more or less good with this code, but not when I have more than one. (So if you run this code with the csv file below, it will show a dimension error).</p>

<p>Reading some answers from:</p>

<ul>
<li><a href=""http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras</a></li>
<li><a href=""http://machinelearningmastery.com/time-series-forecast-study-python-annual-water-usage-baltimore/"" rel=""noreferrer"">Time Series Forecast Case Study with Python: Annual Water Usage in Baltimore</a></li>
</ul>

<p>it seems to be that many people have the same problem.</p>

<p>The code:</p>

<p><strong>EDIT:</strong> Code has been updated</p>

<pre><code>import numpy
import matplotlib.pyplot as plt
import pandas
import math

from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error


# convert an array of values into a dataset matrix

def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset) - look_back - 1):
        a = dataset[i:(i + look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 2])
    return numpy.array(dataX), numpy.array(dataY)



# fix random seed for reproducibility
numpy.random.seed(7)


# load the dataset
dataframe = pandas.read_csv('datos.csv', engine='python') 
dataset = dataframe.values

# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)

# split into train and test sets
train_size = int(len(dataset) * 0.67) 
test_size = len(dataset) - train_size
train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]

# reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)  
testX, testY = create_dataset(test, look_back)

# reshape input to be  [samples, time steps, features]
trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, 3))
testX = numpy.reshape(testX, (testX.shape[0],look_back, 3))

# create and fit the LSTM network

model = Sequential()
model.add(LSTM(4, input_dim=look_back))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
history= model.fit(trainX, trainY,validation_split=0.33, nb_epoch=200, batch_size=32)

# Plot training
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('p闁肩厧宓恉ida')
plt.xlabel('闁肩厧宓卭ca')
plt.legend(['entrenamiento', 'validaci閻犳劗鐝?], loc='upper right')
plt.show()

# make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)

# Get something which has as many features as dataset
trainPredict_extended = numpy.zeros((len(trainPredict),3))
# Put the predictions there
trainPredict_extended[:,2] = trainPredict[:,0]
# Inverse transform it and select the 3rd column.
trainPredict = scaler.inverse_transform(trainPredict_extended) [:,2]  
print(trainPredict)
# Get something which has as many features as dataset
testPredict_extended = numpy.zeros((len(testPredict),3))
# Put the predictions there
testPredict_extended[:,2] = testPredict[:,0]
# Inverse transform it and select the 3rd column.
testPredict = scaler.inverse_transform(testPredict_extended)[:,2]   


trainY_extended = numpy.zeros((len(trainY),3))
trainY_extended[:,2]=trainY
trainY=scaler.inverse_transform(trainY_extended)[:,2]


testY_extended = numpy.zeros((len(testY),3))
testY_extended[:,2]=testY
testY=scaler.inverse_transform(testY_extended)[:,2]


# calculate root mean squared error
trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY, testPredict))
print('Test Score: %.2f RMSE' % (testScore))

# shift train predictions for plotting
trainPredictPlot = numpy.empty_like(dataset)
trainPredictPlot[:, :] = numpy.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, 2] = trainPredict

# shift test predictions for plotting
testPredictPlot = numpy.empty_like(dataset)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, 2] = testPredict



#plot

 serie,=plt.plot(scaler.inverse_transform(dataset)[:,2])  
prediccion_entrenamiento,=plt.plot(trainPredictPlot[:,2],linestyle='--')  
prediccion_test,=plt.plot(testPredictPlot[:,2],linestyle='--')
plt.title('Consumo de agua')
plt.ylabel('cosumo (m3)')
plt.xlabel('dia')
plt.legend([serie,prediccion_entrenamiento,prediccion_test],['serie','entrenamiento','test'], loc='upper right')
</code></pre>

<p>This is the csv file I have created, if it helps.</p>

<p><a href=""https://pastebin.com/qZv5HygU"" rel=""noreferrer"">datos.csv</a></p>

<p>After changing the code, I fixed all the errors, but I'm not really sure about the results. This is a zoom in the prediction plot:
<a href=""https://i.stack.imgur.com/gEJHZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gEJHZ.png"" alt=""Zoom on plot""></a></p>

<p>which shows that there is a ""displacement"" in the values predicted and in the real ones. When there is a max in the real time series, there is a min in the forecast for the same time, but it seems like it corresponds to the previous time step.</p>",42539325.0,2,3,,2017/3/1 12:56,17.0,2019/1/3 16:54,2017/10/10 22:42,,463213.0,,7553189.0,,1,33,python|neural-network|deep-learning|keras,37696,57.5052,,3,work multiple input lstm kera try predict water usage population main input water volume secondary input temperature rainfall theory relation water supply must say rainfall temperature data correspond water volume time series problem problem know use input one csv file column one input code make one input e g water volume network work less good code one run code csv file show dimension error read answer time series prediction lstm recurrent neural network python keras time series forecast case study python annual water usage baltimore seem many people problem code edit code update csv file create help datos csv change code fix error really sure result zoom prediction plot show displacement value predict real one max real time series min forecast time seem like correspond previous time step
611,611,50319943,"PyTorch : error message ""torch has no [...] member""","<p>Good evening,
I have just installed PyTorch 0.4.0 and I'm trying to carry out the first tutorial ""What is PyTorch?""
I have written a Tutorial.py file which I try to execute with Visual Studio Code</p>

<p>Here is the code :</p>

<pre><code>from __future__ import print_function
import torch

print (torch.__version__)

x = x = torch.rand(5, 3)
print(x)
</code></pre>

<p>Unfortunately, when I try to debug it, i have an error message :
""torch has no rand member""</p>

<p>This is true with any member function of torch I may try</p>

<p>Can anybody help me please?</p>",,4,4,,2018/5/13 19:36,6.0,2021/3/10 11:39,,,,,9501506.0,,1,23,torch|pytorch,29025,98.2511,,1,pytorch error message torch member good evening instal pytorch try carry first tutorial pytorch write tutorial py file try execute visual studio code code unfortunately try debug error message torch rand member true member function torch may try anybody help please
277,277,44889187,AttributeError: 'Tensor' object has no attribute '_keras_history',"<p>I looked for all the ""'Tensor' object has no attribute ***"" but none seems related to Keras (except for <a href=""https://stackoverflow.com/questions/41826531/tensorflow-attributeerror-tensor-object-has-no-attribute-log10"">TensorFlow: AttributeError: &#39;Tensor&#39; object has no attribute &#39;log10&#39;</a> which didn't help)...</p>

<p>I am making a sort of GAN (Generative Adversarial Networks). Here you can find the structure.</p>

<pre><code>Layer (type)                     Output Shape          Param #         Connected to                     
_____________________________________________________________________________
input_1 (InputLayer)             (None, 30, 91)        0                                            
_____________________________________________________________________________
model_1 (Model)                  (None, 30, 1)         12558           input_1[0][0]                    
_____________________________________________________________________________
model_2 (Model)                  (None, 30, 91)        99889           input_1[0][0]                    
                                                                       model_1[1][0]                    
_____________________________________________________________________________
model_3 (Model)                  (None, 1)             456637          model_2[1][0]                    
_____________________________________________________________________________
</code></pre>

<p>I pretrained model_2, and model_3. The thing is I pretrained model_2 with list made of 0 and 1, but model_1 return approached values. So i considered rounding the model1_output, with the following code : the K.round() on model1_out.</p>

<pre><code>import keras.backend as K
[...]
def make_gan(GAN_in, model1, model2, model3):
    model1_out = model1(GAN_in)
    model2_out = model2([GAN_in, K.round(model1_out)])
    GAN_out = model3(model2_out)
    GAN = Model(GAN_in, GAN_out)
    GAN.compile(loss=loss, optimizer=model1.optimizer, metrics=['binary_accuracy'])
    return GAN
[...]
</code></pre>

<p>I have the following error :</p>

<blockquote>
  <p>AttributeError: 'Tensor' object has no attribute '_keras_history'</p>
</blockquote>

<p>Full traceback :</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Asmaa\Documents\BillyValuation\GFD.py"", line 88, in &lt;module&gt;
GAN = make_gan(inputSentence, G, F, D)
  File ""C:\Users\Asmaa\Documents\BillyValuation\GFD.py"", line 61, in make_gan
GAN = Model(GAN_in, GAN_out)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\legacy\interfaces.py"", line 88, in wrapper
return func(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 1705, in __init__
build_map_of_graph(x, finished_nodes, nodes_in_progress)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 1695, in build_map_of_graph
layer, node_index, tensor_index)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 1695, in build_map_of_graph
layer, node_index, tensor_index)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\keras\engine\topology.py"", line 1665, in build_map_of_graph
layer, node_index, tensor_index = tensor._keras_history
AttributeError: 'Tensor' object has no attribute '_keras_history'
</code></pre>

<p>I'm using Python 3.6, with Spyder 3.1.4, on Windows 7. I upgraded TensorFlow and Keras with pip last week.
Thank you for any help provided !</p>",,5,9,,2017/7/3 15:22,2.0,2021/5/13 8:52,,,,,8156923.0,,1,20,python|keras|attributeerror,22463,78.4059,,4,attributeerror tensor object attribute kera history look tensor object attribute none seem relate keras except tensorflow attributeerror tensor object attribute log help make sort gan generative adversarial network find structure pretrained model model thing pretrained model list make model return approach value consider round model output following code k round model following error attributeerror tensor object attribute kera history full traceback use python spyder window upgraded tensorflow kera pip last week thank help provide
255,255,44622857,Why am i getting AttributeError: 'KerasClassifier' object has no attribute 'model'?,"<p>This is the code and I'm getting the error in the last line only which is <code>y_pred = classifier.predict(X_test)</code>. The error I'm getting is <code>AttributeError: 'KerasClassifier' object has no attribute 'model'</code></p>

<pre><code># Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets
from sklearn import preprocessing
from keras.utils import np_utils

# Importing the dataset
dataset = pd.read_csv('Data1.csv',encoding = ""cp1252"")
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_0 = LabelEncoder()
X[:, 0] = labelencoder_X_0.fit_transform(X[:, 0])
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
labelencoder_X_3 = LabelEncoder()
X[:, 3] = labelencoder_X_3.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Creating the ANN!
# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
def build_classifier():
    # Initialising the ANN
    classifier = Sequential()
    # Adding the input layer and the first hidden layer
    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 10))

    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    return classifier

classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 2)
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 1, n_jobs=1)
mean = accuracies.mean()
variance = accuracies.std()

# Predicting the Test set results
import sklearn
y_pred = classifier.predict(X_test)
y_pred = (y_pred &gt; 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Predicting new observations
test = pd.read_csv('test.csv',encoding = ""cp1252"")
test = test.iloc[:, 1:].values
test[:, 0] = labelencoder_X_0.transform(test[:, 0])
test[:, 1] = labelencoder_X_1.transform(test[:, 1])
test[:, 2] = labelencoder_X_2.transform(test[:, 2])
test[:, 3] = labelencoder_X_3.transform(test[:, 3])
test = onehotencoder.transform(test).toarray()
test = test[:, 1:]
new_prediction = classifier.predict_classes(sc.transform(test))
new_prediction1 = (new_prediction &gt; 0.5)
</code></pre>",44623291.0,2,0,,2017/6/19 5:31,1.0,2017/6/19 9:00,2017/6/19 6:14,,3374996.0,,7119128.0,,1,10,python|machine-learning|scikit-learn|deep-learning|keras,19967,50.2013,,4,get attributeerror kerasclassifier object attribute model code get error last line error get
827,827,45361559,Feature Importance Chart in neural network using Keras in Python,"<p>I am using python(3.6) anaconda (64 bit) spyder (3.1.2). I already set a neural network model using keras (2.0.6) for a regression problem(one response, 10 variables). I was wondering how can I generate feature importance chart like so:</p>

<p><img src=""https://i.stack.imgur.com/SWei6.png"" alt=""feature importance chart""></p>

<pre><code>def base_model():
    model = Sequential()
    model.add(Dense(200, input_dim=10, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer = 'adam')
    return model

clf = KerasRegressor(build_fn=base_model, epochs=100, batch_size=5,verbose=0)
clf.fit(X_train,Y_train)
</code></pre>",,3,0,,2017/7/27 21:47,21.0,2020/5/18 3:30,2017/7/27 21:52,,1007939.0,,8286982.0,,1,40,python|neural-network|keras,39299,89.3775,,4,feature importance chart neural network use kera python use python anaconda bit spyder already set neural network model use kera regression problem one response variable wonder generate feature importance chart like
4,4,61425296,Why neural network predicts wrong on its own training data?,"<p>I made a LSTM (RNN) neural network with supervised learning for data stock prediction. The problem is why it predicts wrong on its own training data? (note: <strong>reproducible example</strong> below)</p>

<p>I created simple model to predict next 5 days stock price:</p>

<pre><code>model = Sequential()
model.add(LSTM(32, activation='sigmoid', input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense(y_train.shape[1]))
model.compile(optimizer='adam', loss='mse')

es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.fit(x_train, y_train, batch_size=64, epochs=25, validation_data=(x_test, y_test), callbacks=[es])
</code></pre>

<p>The correct results are in <code>y_test</code> (5 values), so model trains, looking back 90 previous days and then restore weights from best (<code>val_loss=0.0030</code>) result with <code>patience=3</code>:</p>

<pre><code>Train on 396 samples, validate on 1 samples
Epoch 1/25
396/396 [==============================] - 1s 2ms/step - loss: 0.1322 - val_loss: 0.0299
Epoch 2/25
396/396 [==============================] - 0s 402us/step - loss: 0.0478 - val_loss: 0.0129
Epoch 3/25
396/396 [==============================] - 0s 397us/step - loss: 0.0385 - val_loss: 0.0178
Epoch 4/25
396/396 [==============================] - 0s 399us/step - loss: 0.0398 - val_loss: 0.0078
Epoch 5/25
396/396 [==============================] - 0s 391us/step - loss: 0.0343 - val_loss: 0.0030
Epoch 6/25
396/396 [==============================] - 0s 391us/step - loss: 0.0318 - val_loss: 0.0047
Epoch 7/25
396/396 [==============================] - 0s 389us/step - loss: 0.0308 - val_loss: 0.0043
Epoch 8/25
396/396 [==============================] - 0s 393us/step - loss: 0.0292 - val_loss: 0.0056
</code></pre>

<p>Prediction result is pretty awesome, isn't it?</p>

<p><a href=""https://i.stack.imgur.com/5z91q.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5z91q.png"" alt=""enter image description here""></a></p>

<p>That's because algorithm restored best weights from #5 epoch. Okey, let's now save this model to <code>.h5</code> file, move back -10 days and predict last 5 days (at first example we made model and validate on 17-23 April including day off weekends, now let's test on 2-8 April). Result:</p>

<p><a href=""https://i.stack.imgur.com/vXIvp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vXIvp.png"" alt=""enter image description here""></a></p>

<p>It shows absolutely wrong direction. As we see that's because model was trained and took #5 epoch best for validation set on 17-23 April, but not on 2-8. If I try train more, playing with what epoch to choose, whatever I do, there are always a lot of time intervals in the past that have wrong prediction.</p>

<p>Why does model show wrong results on its own trained data? I trained data, it must remember how to predict data on this piece of set, but predicts wrong. What I also tried:</p>

<ul>
<li>Use large data sets with 50k+ rows, 20 years stock prices, adding more or less features</li>
<li>Create different types of model, like adding more hidden layers, different batch_sizes, different layers activations, dropouts, batchnormalization</li>
<li>Create custom EarlyStopping callback, get average val_loss from many validation data sets and choose the best</li>
</ul>

<p>Maybe I miss something? What can I improve?</p>

<p>Here is very simple and <strong>reproducible</strong> example. <code>yfinance</code> downloads S&amp;P 500 stock data.</p>

<pre><code>""""""python 3.7.7
tensorflow 2.1.0
keras 2.3.1""""""


import numpy as np
import pandas as pd
from keras.callbacks import EarlyStopping, Callback
from keras.models import Model, Sequential, load_model
from keras.layers import Dense, Dropout, LSTM, BatchNormalization
from sklearn.preprocessing import MinMaxScaler
import plotly.graph_objects as go
import yfinance as yf
np.random.seed(4)


num_prediction = 5
look_back = 90
new_s_h5 = True # change it to False when you created model and want test on other past dates


df = yf.download(tickers=""^GSPC"", start='2018-05-06', end='2020-04-24', interval=""1d"")
data = df.filter(['Close', 'High', 'Low', 'Volume'])

# drop last N days to validate saved model on past
df.drop(df.tail(0).index, inplace=True)
print(df)


class EarlyStoppingCust(Callback):
    def __init__(self, patience=0, verbose=0, validation_sets=None, restore_best_weights=False):
        super(EarlyStoppingCust, self).__init__()
        self.patience = patience
        self.verbose = verbose
        self.wait = 0
        self.stopped_epoch = 0
        self.restore_best_weights = restore_best_weights
        self.best_weights = None
        self.validation_sets = validation_sets

    def on_train_begin(self, logs=None):
        self.wait = 0
        self.stopped_epoch = 0
        self.best_avg_loss = (np.Inf, 0)

    def on_epoch_end(self, epoch, logs=None):
        loss_ = 0
        for i, validation_set in enumerate(self.validation_sets):
            predicted = self.model.predict(validation_set[0])
            loss = self.model.evaluate(validation_set[0], validation_set[1], verbose = 0)
            loss_ += loss
            if self.verbose &gt; 0:
                print('val' + str(i + 1) + '_loss: %.5f' % loss)

        avg_loss = loss_ / len(self.validation_sets)
        print('avg_loss: %.5f' % avg_loss)

        if self.best_avg_loss[0] &gt; avg_loss:
            self.best_avg_loss = (avg_loss, epoch + 1)
            self.wait = 0
            if self.restore_best_weights:
                print('new best epoch = %d' % (epoch + 1))
                self.best_weights = self.model.get_weights()
        else:
            self.wait += 1
            if self.wait &gt;= self.patience or self.params['epochs'] == epoch + 1:
                self.stopped_epoch = epoch
                self.model.stop_training = True
                if self.restore_best_weights:
                    if self.verbose &gt; 0:
                        print('Restoring model weights from the end of the best epoch')
                    self.model.set_weights(self.best_weights)

    def on_train_end(self, logs=None):
        print('best_avg_loss: %.5f (#%d)' % (self.best_avg_loss[0], self.best_avg_loss[1]))


def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):
    data = []
    labels = []
    start_index = start_index + history_size
    if end_index is None:
        end_index = len(dataset) - target_size
    for i in range(start_index, end_index):
        indices = range(i-history_size, i, step)
        data.append(dataset[indices])
        if single_step:
            labels.append(target[i+target_size])
        else:
            labels.append(target[i:i+target_size])
    return np.array(data), np.array(labels)


def transform_predicted(pr):
    pr = pr.reshape(pr.shape[1], -1)
    z = np.zeros((pr.shape[0], x_train.shape[2] - 1), dtype=pr.dtype)
    pr = np.append(pr, z, axis=1)
    pr = scaler.inverse_transform(pr)
    pr = pr[:, 0]
    return pr


step = 1

# creating datasets with look back
scaler = MinMaxScaler()
df_normalized = scaler.fit_transform(df.values)
dataset = df_normalized[:-num_prediction]
x_train, y_train = multivariate_data(dataset, dataset[:, 0], 0,len(dataset) - num_prediction + 1, look_back, num_prediction, step)
indices = range(len(dataset)-look_back, len(dataset), step)
x_test = np.array(dataset[indices])
x_test = np.expand_dims(x_test, axis=0)
y_test = np.expand_dims(df_normalized[-num_prediction:, 0], axis=0)

# creating past datasets to validate with EarlyStoppingCust
number_validates = 50
step_past = 5
validation_sets = [(x_test, y_test)]
for i in range(1, number_validates * step_past + 1, step_past):
    indices = range(len(dataset)-look_back-i, len(dataset)-i, step)
    x_t = np.array(dataset[indices])
    x_t = np.expand_dims(x_t, axis=0)
    y_t = np.expand_dims(df_normalized[-num_prediction-i:len(df_normalized)-i, 0], axis=0)
    validation_sets.append((x_t, y_t))


if new_s_h5:
    model = Sequential()
    model.add(LSTM(32, return_sequences=False, activation = 'sigmoid', input_shape=(x_train.shape[1], x_train.shape[2])))
    # model.add(Dropout(0.2))
    # model.add(BatchNormalization())
    # model.add(LSTM(units = 16))
    model.add(Dense(y_train.shape[1]))
    model.compile(optimizer = 'adam', loss = 'mse')

    # EarlyStoppingCust is custom callback to validate each validation_sets and get average
    # it takes epoch with best ""best_avg"" value
    # es = EarlyStoppingCust(patience = 3, restore_best_weights = True, validation_sets = validation_sets, verbose = 1)

    # or there is keras extension with built-in EarlyStopping, but it validates only 1 set that you pass through fit()
    es = EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True)

    model.fit(x_train, y_train, batch_size = 64, epochs = 25, shuffle = True, validation_data = (x_test, y_test), callbacks = [es])
    model.save('s.h5')
else:
    model = load_model('s.h5')



predicted = model.predict(x_test)
predicted = transform_predicted(predicted)
print('predicted', predicted)
print('real', df.iloc[-num_prediction:, 0].values)
print('val_loss: %.5f' % (model.evaluate(x_test, y_test, verbose=0)))


fig = go.Figure()
fig.add_trace(go.Scatter(
    x = df.index[-60:],
    y = df.iloc[-60:,0],
    mode='lines+markers',
    name='real',
    line=dict(color='#ff9800', width=1)
))
fig.add_trace(go.Scatter(
    x = df.index[-num_prediction:],
    y = predicted,
    mode='lines+markers',
    name='predict',
    line=dict(color='#2196f3', width=1)
))
fig.update_layout(template='plotly_dark', hovermode='x', spikedistance=-1, hoverlabel=dict(font_size=16))
fig.update_xaxes(showspikes=True)
fig.update_yaxes(showspikes=True)
fig.show()
</code></pre>",61578364.0,9,8,,2020/4/25 12:02,7.0,2020/5/4 13:15,2020/4/25 12:24,,4685471.0,,1802225.0,,1,26,python|tensorflow|machine-learning|keras|neural-network,3606,85.0281,,5,neural network predicts wrong training data make lstm rnn neural network supervised learn data stock prediction problem predict wrong training data note reproducible example create simple model predict next day stock price correct result value model train look back previous day restore weight best result prediction result pretty awesome algorithm restore best weight epoch okey let save model file move back day predict last day first example make model validate april include day weekend let test april result show absolutely wrong direction see model train take epoch best validation set april try train playing epoch choose whatever always lot time interval past wrong prediction model show wrong result trained data train data must remember predict data piece set predict wrong also try use large data set k row year stock price add less feature create different type model like add hidden layer different batch size different layer activation dropout batchnormalization create custom earlystopping callback get average val loss many validation data set choose best maybe miss something improve simple reproducible example downloads p stock data
149,149,42633644,Using Keras for video prediction (time series),"<p>I want to predict the next frame of a (greyscale) video given <code>N</code> previous frames - using CNNs or RNNs in Keras. Most tutorials and other information regarding time series prediction and Keras use a 1-dimensional input in their network but mine would be 3D <code>(N frames x rows x cols)</code></p>

<p>I'm currently really unsure what a good approach for this problem would be. My ideas include:</p>

<ul>
<li><p>Using one or more LSTM layers. The problem here is that I'm not sure whether they're suited to take a series of images instead a series of scalars as input. Wouldn't the memory consumption explode? If it is okay to use them: How can I use them in Keras for higher dimensions?</p></li>
<li><p>Using 3D convolution on the input (the stack of previous video frames). This raises other questions: Why would this help when I'm not doing a classification but a prediction? How can I stack the layers in such a way that the input of the network has dimensions <code>(N x cols x rows)</code> and the output <code>(1 x cols x rows)</code>?</p></li>
</ul>

<p>I'm pretty new to CNNs/RNNs and Keras and would appreciate any hint into the right direction.</p>",42635571.0,2,0,,2017/3/6 19:08,9.0,2017/7/12 23:32,2017/7/12 23:32,,5974433.0,,3367967.0,,1,17,machine-learning|neural-network|time-series|keras|lstm,9764,58.7585,,3,use kera video prediction time series want predict next frame greyscale video give previous frame use cnns rnns kera tutorial information regard time series prediction kera use dimensional input network mine would currently really unsure good approach problem would idea include use one lstm layer problem sure whether suit take series image instead series scalar input would memory consumption explode okay use use kera high dimension use convolution input stack previous video frame raise question would help classification prediction stack layer way input network dimension output pretty new cnns rnns kera would appreciate hint right direction
783,783,52126539,Using pretrained gensim Word2vec embedding in keras,"<p>I have trained word2vec in gensim. In Keras, I want to use it to make matrix of sentence using that word embedding. As storing the matrix of all the sentences is very space and memory inefficient. So, I want to make embedding layer in Keras to achieve this so that It can be used in further layers(LSTM). Can you tell me in detail how to do this?</p>

<p>PS: It is different from other questions because I am using gensim for word2vec training instead of keras.</p>",52126699.0,3,1,,2018/9/1 8:53,2.0,2019/8/6 11:30,2018/9/1 9:25,,7290240.0,,7290240.0,,1,9,python|keras|gensim|word2vec|word-embedding,12000,53.7167,,3,use pretrained gensim word vec embed kera train word vec gensim kera want use make matrix sentence use word embedding store matrix sentence space memory inefficient want make embed layer kera achieve use layer lstm tell detail p different question use gensim word vec train instead kera
598,598,49886369,KL Divergence for two probability distributions in PyTorch,<p>I have two probability distributions. How should I find the KL-divergence between them in PyTorch? The regular cross entropy only accepts integer labels.</p>,49892425.0,4,0,,2018/4/17 19:50,2.0,2021/3/16 3:33,,,,,6084324.0,,1,13,machine-learning|pytorch,25029,66.9938,,4,kl divergence two probability distribution pytorch two probability distribution find kl divergence pytorch regular cross entropy accept integer label
362,362,45961428,Make a custom loss function in keras,"<p>Hi I have been trying to make a custom loss function in keras for dice_error_coefficient. It has its implementations in <strong>tensorboard</strong> and I tried using the same function in keras with tensorflow but it keeps returning a <strong>NoneType</strong> when I used <strong>model.train_on_batch</strong> or <strong>model.fit</strong> where as it gives proper values when used in metrics in the model. Can please someone help me out with what should i do? I have tried following libraries like Keras-FCN by ahundt where he has used custom loss functions but none of it seems to work. The target and output in the code are y_true and y_pred respectively as used in the losses.py file in keras.</p>

<pre><code>def dice_hard_coe(target, output, threshold=0.5, axis=[1,2], smooth=1e-5):
    """"""References
    -----------
    - `Wiki-Dice &lt;https://en.wikipedia.org/wiki/S闂佹壆顫渆nsen闂佺偨鍎插纾嘽e_coefficient&gt;`_
    """"""

    output = tf.cast(output &gt; threshold, dtype=tf.float32)
    target = tf.cast(target &gt; threshold, dtype=tf.float32)
    inse = tf.reduce_sum(tf.multiply(output, target), axis=axis)
    l = tf.reduce_sum(output, axis=axis)
    r = tf.reduce_sum(target, axis=axis)
    hard_dice = (2. * inse + smooth) / (l + r + smooth)
    hard_dice = tf.reduce_mean(hard_dice)
    return hard_dice
</code></pre>",45963039.0,2,0,,2017/8/30 13:11,28.0,2021/1/4 18:18,2017/8/30 14:04,,2523237.0,,8538125.0,,1,43,python|machine-learning|tensorflow|keras,61668,132.36,,4,make custom loss function kera hi try make custom loss function kera dice error coefficient implementation tensorboard try use function kera tensorflow keep return nonetype use model train batch model fit give proper value use metric model please someone help try follow library like kera fcn ahundt use custom loss function none seem work target output code true pred respectively use loss py file kera
206,206,43818584,Custom loss function in Keras,"<p>I'm working on a image class-incremental classifier approach using a CNN as a feature extractor and a fully-connected block for classifying.</p>

<p>First, I did a fine-tuning of a VGG per-trained network to do a new task. Once the net is trained for the new task, i store some examples for every class in order to avoid forgetting when new classes are available.</p>

<p>When some classes are available, i have to compute every output of the exemplars included the exemplars for the new classes. Now adding zeros to the outputs for old classes and adding the label corresponding to each new class on the new classes outputs i have my new labels, i.e:
if 3 new classes enter....</p>

<p>Old class type output: <code>[0.1, 0.05, 0.79, ..., 0 0 0]</code></p>

<p>New class type output: <code>[0.1, 0.09, 0.3, 0.4, ..., 1 0 0]</code> **the last outputs correspond to the class.</p>

<p>My question is, how i can change the loss function for a custom one to train for the new classes?
The loss function that i want to implement is defined as:</p>

<p><a href=""https://i.stack.imgur.com/0kImf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0kImf.png"" alt=""loss function""></a></p>

<p>where distillation loss corresponds to the outputs for old classes to avoid forgetting, and classification loss corresponds to the new classes.</p>

<p>If you can provide me a sample of code to change the loss function in keras would be nice.</p>

<p>Thanks!!!!!</p>",43821374.0,2,0,,2017/5/6 8:55,18.0,2021/2/17 13:34,2020/9/16 14:22,,10908375.0,,6729283.0,,1,53,python|tensorflow|keras|deep-learning|computer-vision,58475,127.268,,4,custom loss function kera work image class incremental classifier approach use cnn feature extractor fully connect block classify first fine tuning vgg per train network new task net train new task store example every class order avoid forget new class available class available compute every output exemplar include exemplar new class add zero output old class add label correspond new class new class output new label e new class enter old class type output new class type output last outputs correspond class question change loss function custom one train new class loss function want implement define distillation loss correspond output old class avoid forgetting classification loss correspond new class provide sample code change loss function kera would nice thanks
688,688,38536788,g++ error on import of Theano on Windows 7,"<p>I'm attempting to get setup with a proper g++ installation according to <a href=""http://deeplearning.net/software/theano/install_windows.html#install-windows"" rel=""nofollow noreferrer"">the theano installation guide</a>. I've previously had theano working with the python only implementation. I'm using the bleeding edge version of theano from their git repo on python 3.4. I've tried using the theano suggested TDM-GCC-64 method as well as MinGW, and both result in the exact same error. (copied as readable as possible)</p>

<pre><code>Problem occurred during compilation with the command line below:
C:\MinGW\bin\g++.exe -shared -g -march=skylake -mmmx -mno-3dnow -msse -msse2 -msse3 
-mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt 
-mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx 
-mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase 
-mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mno-avx512f 
-mno-avx512er -mno-avx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt 
-mxsavec -mxsaves -mno-avx512dq -mno-avx512bw -mno-avx512vl 
-mno-avx512ifma -mno-avx512vbmi -mno-clwb -mno-pcommit -mno-mwaitx 
-mno-clzero -mno-pku --param l1-cache-size=32 --param 
l1-cache-line-size=64 --param l2-cache-size=8192 -mtune=skylake 
-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -DMS_WIN64 
-IC:\Python34_64bit\lib\site-packages\numpy\core\include
IC:\Python34_64bit\include -IC:\Python34_64bit\lib\site-packages\theano\gof
-o C:\Users\Jwely\AppData\Local\Theano\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.4.4-64\lazylinker_ext\lazylinker_ext.pyd 
C:\Users\Jwely\AppData\Local\Theano\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.4.4-64\lazylinker_ext\mod.cpp 
-LC:\Python34_64bit\libs -LC:\Python34_64bit -lpython34


In file included from c:\mingw\include\c++\6.1.0\math.h:36:0,
from C:\Python34_64bit\include/pyport.h:328,
from C:\Python34_64bit\include/Python.h:50,
from C:\Users\Jwely\AppData\Local\Theano\compiledir_Windows-7-6.1.7601-SP1-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.4.4-64\lazylinker_ext\mod.cpp:1:
c:\mingw\include\c++\6.1.0\cmath:1133:11: error: '::hypot' has not been declared
    using ::hypot;
            ^~~~~
</code></pre>

<p>It may be worth noting that before it prints this error, it prints an entire file worth of code, you can find the entire error output here</p>

<p>I'm not sure what to try next, I've followed the directions twice, used a couple different installation methods for some dependencies, and made sure to clean up my system path between each attempt and reboot.</p>",39844430.0,6,0,,2016/7/22 23:58,6.0,2018/10/23 8:16,2017/10/20 15:37,,4058635.0,,4058635.0,,1,8,g++|theano|importerror,5848,56.668,,1,g error import theano window attempt get setup proper g installation accord theano installation guide previously theano work python implementation use bleeding edge version theano git repo python try use theano suggest tdm gcc method well mingw result exact error copy readable possible may worth note print error print entire file worth code find entire error output sure try next follow direction twice use couple different installation method dependency make sure clean system path attempt reboot
204,204,43782409,How to use ModelCheckpoint with custom metrics in Keras?,"<p>Is it possible to use custom <a href=""https://keras.io/metrics/"" rel=""noreferrer"">metrics</a> in the <a href=""https://keras.io/callbacks/#modelcheckpoint"" rel=""noreferrer""><code>ModelCheckpoint</code></a> callback?</p>",43782410.0,1,0,,2017/5/4 12:04,5.0,2018/10/18 15:40,2018/9/27 7:25,,604734.0,,604734.0,,1,22,keras|deep-learning,12317,57.762,,4,use modelcheckpoint custom metric kera possible use custom metric callback
373,373,46154189,What is the difference of static Computational Graphs in tensorflow and dynamic Computational Graphs in Pytorch?,"<p>When I was learning tensorflow, one basic concept of tensorflow was computational graphs, and the graphs was said to be static.
And I found in Pytorch, the graphs was said to be dynamic.
What's the difference of static Computational Graphs in tensorflow and dynamic Computational Graphs in Pytorch?</p>",46154721.0,3,0,,2017/9/11 11:04,14.0,2017/9/11 18:15,,,,,8590947.0,,1,30,tensorflow|deep-learning|torch,9477,73.9067,,0,difference static computational graph tensorflow dynamic computational graph pytorch learn tensorflow one basic concept tensorflow computational graph graph say static find pytorch graph say dynamic difference static computational graph tensorflow dynamic computational graph pytorch
329,329,63068639,ValueError: Unknown layer: Functional,"<p>I made a CNN in colab and saved the models at every epoch. I exported the h5 file and now am trying to run the model on some test images. Here's the main error:</p>
<pre><code>ValueError: Unknown layer: Functional
</code></pre>
<p>Here's the code I used to run the model and save at each epoch:</p>
<pre><code>epochs = 50

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    keras.callbacks.ModelCheckpoint(&quot;save_at_{epoch}.h5&quot;),
]
model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss=&quot;binary_crossentropy&quot;,
    metrics=[&quot;accuracy&quot;],
)
model.fit(
    train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,
)
</code></pre>
<p>After the model ran I just downloaded the h5 file from the colab sidebar locally. I re-uploaded the file from the local disk, and here's how I'm trying to load the model:</p>
<pre><code># load and evaluate a saved model
from tensorflow.keras.models import load_model

# load model#
loaded_model = load_model('save_at_47.h5')
loaded_model.layers[0].input_shape
</code></pre>
<p>Here's the full traceback:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-4-6af7396280fa&gt; in &lt;module&gt;()
      3 
      4 # load model#
----&gt; 5 loaded_model = load_model('save_at_47.h5')
      6 loaded_model.layers[0].input_shape

5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
    182     if (h5py is not None and (
    183         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--&gt; 184       return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
    185 
    186     if sys.version_info &gt;= (3, 4) and isinstance(filepath, pathlib.Path):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    176     model_config = json.loads(model_config.decode('utf-8'))
    177     model = model_config_lib.model_from_config(model_config,
--&gt; 178                                                custom_objects=custom_objects)
    179 
    180     # set weights

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---&gt; 55   return deserialize(config, custom_objects=custom_objects)
     56 
     57 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
    107       module_objects=globs,
    108       custom_objects=custom_objects,
--&gt; 109       printable_module_name='layer')

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    360     config = identifier
    361     (cls, cls_config) = class_and_config_for_serialized_keras_object(
--&gt; 362         config, module_objects, custom_objects, printable_module_name)
    363 
    364     if hasattr(cls, 'from_config'):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)
    319   cls = get_registered_object(class_name, custom_objects, module_objects)
    320   if cls is None:
--&gt; 321     raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)
    322 
    323   cls_config = config['config']

ValueError: Unknown layer: Functional
</code></pre>
<p>It seems there have been several similar <a href=""https://stackoverflow.com/questions/53051274/i-trained-a-keras-model-on-google-colab-now-not-able-to-load-it-locally-on-my-s"">questions here</a>,and <a href=""https://stackoverflow.com/questions/53183865/unknown-initializer-glorotuniform-when-loading-keras-model"">here</a>. Changing the import method hasn't helped yet, and trying to make some <a href=""https://stackoverflow.com/questions/54286368/valueerror-unknown-layername-when-loading-a-keras-model"">kind of custom</a> object has not worked either.</p>",63085142.0,6,8,,2020/7/24 7:14,5.0,2021/7/23 7:53,2020/7/24 7:33,,10777776.0,,10777776.0,,1,19,python|tensorflow|keras,24880,74.3834,,4,valueerror unknown layer functional make cnn colab save model every epoch export h file try run model test image main error code use run model save epoch model run download h file colab sidebar locally upload file local disk try load model full traceback seem several similar question change import method help yet try make kind custom object work either
658,658,37340129,TensorFlow: training on my own image,"<p>I am new to TensorFlow. I am looking for the help on the image recognition where I can <strong>train my own image</strong> dataset.</p>

<p>Is there any example for training the new dataset?</p>",37343690.0,3,2,,2016/5/20 7:07,53.0,2020/2/10 9:51,2018/2/5 4:42,,6053728.0,,6343552.0,,1,44,python|tensorflow|conv-neural-network|tensorflow-datasets,54742,143.35299999999995,,3,tensorflow training image new tensorflow look help image recognition train image dataset example train new dataset
371,371,46141690,How to write a PyTorch sequential model?,"<p>So far, I wrote my MLP, RNN and CNN in Keras, but now PyTorch is gaining popularity inside deep learning communities, and so I also started to learn this framework. I am a big fan of sequential models in Keras, which allow us to make simple models very fast. I also saw that PyTorch has this functionality, but I don't know how to code one. I tried this way</p>

<pre><code>import torch
import torch.nn as nn

net = nn.Sequential()
net.add(nn.Linear(3, 4))
net.add(nn.Sigmoid())
net.add(nn.Linear(4, 1))
net.add(nn.Sigmoid())
net.float()

print(net)
</code></pre>

<p>but it is giving this error</p>

<blockquote>
  <p>AttributeError: 'Sequential' object has no attribute 'add'</p>
</blockquote>

<p>Also, if possible, can you give simple examples for RNN and CNN models in PyTorch sequential model?</p>",46149960.0,4,0,,2017/9/10 14:16,5.0,2021/6/22 14:25,2018/10/20 13:47,,3924118.0,,996366.0,,1,20,python|sequential|pytorch,42475,93.5125,,3,write pytorch sequential model far write mlp rnn cnn kera pytorch gain popularity inside deep learning community also start learn framework big fan sequential model kera allow u make simple model fast also saw pytorch functionality know code one try way give error attributeerror sequential object attribute add also possible give simple example rnn cnn model pytorch sequential model
698,698,39124676,Show progress bar for each epoch during batchwise training in Keras,"<p>When I load the whole dataset in memory and train the network in Keras using following code:</p>

<pre><code>model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1)
</code></pre>

<p>This generates a progress bar per epoch with metrics like ETA, accuracy, loss, etc</p>

<p>When I train the network in batches, I'm using the following code</p>

<pre><code>for e in range(40):
        for X, y in data.next_batch():
            model.fit(X, y, nb_epoch=1, batch_size=data.batch_size, verbose=1)
</code></pre>

<p>This will generate a progress bar for each batch instead of each epoch. Is it possible to generate a progress bar for each epoch during batchwise training? </p>",39192224.0,3,1,,2016/8/24 13:26,16.0,2021/6/25 19:48,,,,,3138699.0,,1,55,python|machine-learning|keras,61615,158.159,,5,show progress bar epoch batchwise training kera load whole dataset memory train network kera use follow code generate progress bar per epoch metric like eta accuracy loss etc train network batch use following code generate progress bar batch instead epoch possible generate progress bar epoch batchwise training
308,308,58015489,Flask and Keras model Error ''_thread._local' object has no attribute 'value''?,"<p>I am using the following:
python 3.6.4</p>

<p>Flask = 1.1.1,</p>

<p>Keras = 2.3.0,</p>

<p>TensorFlow = 1.14.0,
I have a Flask server that gets pictures from the clients. using Keras model with a TensorFlow back-end I try to get a prediction from a pre-trained model.</p>

<p>I am using the following function to upload the model( as part of a class)</p>

<pre><code>
 model_path = self.conf[""model_path""] // path in conf to model
 self.model = load_model(model_path)  // uploading the model
 self.model._make_predict_function()
 p_log.info(""model had been upload successfully "")
</code></pre>

<p>and I use the following line for prediction:</p>

<pre><code>cm_prediction = self.model.predict([face, reye, leye, fg])[0]
</code></pre>

<p>Until today I didn't have any problem, always got a prediction.
<strong>now I get the following error</strong>: </p>

<pre><code>Traceback (most recent call last):
  File ""D:\code_project\path to project"", line 75, in predict
    cm_prediction = self.model.predict([face, reye, leye, fg])[0]
  File ""D:\code_project\path to project"", line 1462, in predict
    callbacks=callbacks)
  File ""D:\code_project\predictserver\venv\lib\site-packages\keras\engine\training_arrays.py"", line 276, in predict_loop
    callbacks.model.stop_training = False
  File ""D:\code_project\predictserver\venv\lib\site-packages\keras\engine\network.py"", line 323, in __setattr__
    super(Network, self).__setattr__(name, value)
  File ""D:\code_project\predictserver\venv\lib\site-packages\keras\engine\base_layer.py"", line 1215, in __setattr__
    if not _DISABLE_TRACKING.value:
AttributeError: '_thread._local' object has no attribute 'value'
</code></pre>

<p>I have a simple Flask server running:</p>

<pre><code>if __name__ == '__main__':
    pre = predictor()
    # app.run(debug=True)
    app.run(host='0.0.0.0', port=12345)
</code></pre>

<p>The model is always being uploaded.</p>

<p>If I am running the program <strong>without the Flask server</strong>, hence giving manually input, <strong>I get a prediction</strong>, but as soon as the <strong>server is on</strong> the error appears and I <strong>stop getting a predictions</strong></p>

<p>I tried to look on the web for some similar problem but didnt found any, if someone knows what the problem and how to solve it, I will appreciate sharing it.</p>",58023399.0,19,2,,2019/9/19 16:38,5.0,2021/1/21 21:01,2019/9/26 23:34,,12127207.0,,11306005.0,,1,18,python|tensorflow|flask|keras,12433,135.778,,4,flask keras model error thread local object attribute value use following python flask keras tensorflow flask server get picture client use kera model tensorflow back end try get prediction pre trained model use following function upload model part class use following line prediction today problem always get prediction get following error simple flask server run model always uploaded run program without flask server hence give manually input get prediction soon server error appear stop get prediction try look web similar problem didnt find someone know problem solve appreciate share
301,301,57540745,What is the difference between register_parameter and register_buffer in PyTorch?,"<p>Module's <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer"" rel=""noreferrer"">parameters</a> get changed during training, that is, they are what is learnt during training of a neural network, but what is a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_parameter"" rel=""noreferrer"">buffer</a>?</p>
<p>and is it learnt during neural network training?</p>",,2,0,,2019/8/18 0:04,8.0,2021/7/22 20:47,2020/11/2 15:47,,1714410.0,,10855529.0,,1,20,machine-learning|deep-learning|neural-network|pytorch,7975,82.6069,,3,difference register parameter register buffer pytorch module parameter get change training learnt training neural network buffer learnt neural network training
692,692,38810424,How does one debug NaN values in TensorFlow?,"<p>I was running TensorFlow and I happen to have something yielding a NaN. I'd like to know what it is but I do not know how to do this. The main issue is that in a ""normal"" procedural program I would just write a print statement just before the operation is executed. The issue with TensorFlow is that I cannot do that because I first declare (or define) the graph, so adding print statements to the graph definition does not help. Are there any rules, advice, heuristics, anything to track down what might be causing the NaN?</p>

<hr>

<p>In this case I know more precisely what line to look at because I have the following:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>when this line is present I have it that it returns NaN as declared by my summary writers. Why is this? Is there a way to at least explore what value Z has after its being square rooted?</p>

<hr>

<p>For the specific example I posted, I tried <code>tf.Print(0,Z)</code> but with no success it printed nothing. As in:</p>

<pre><code>Delta_tilde = 2.0*tf.matmul(x,W) - tf.add(WW, XX) #note this quantity should always be positive because its pair-wise euclidian distance
Z = tf.sqrt(Delta_tilde)
tf.Print(0,[Z]) # &lt;-------- TF PRINT STATMENT
Z = Transform(Z) # potentially some transform, currently I have it to return Z for debugging (the identity)
Z = tf.pow(Z, 2.0)
A = tf.exp(Z) 
</code></pre>

<p>I actually don't understand what <code>tf.Print</code> is suppose to do. Why does it need two arguments? If I want to print 1 tensor why would I need to pass 2? Seems bizarre to me.</p>

<hr>

<p>I was looking at the function <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/control_flow_ops.html#add_check_numerics_ops"" rel=""noreferrer"">tf.add_check_numerics_ops()</a> but it doesn't say how to use it (plus the docs seem to not be super helpful). Does anyone know how to use this?</p>

<hr>

<p>Since I've had comments addressing the data might be bad, I am using standard MNIST. However, I am computing a quantity that is positive (pair-wise eucledian distance) and then square rooting it. Thus, I wouldn't see how the data specifically would be an issue.</p>",,9,0,,2016/8/7 2:47,16.0,2020/8/22 7:21,2016/8/9 15:20,,1601580.0,,1601580.0,,1,56,python|machine-learning|neural-network|tensorflow|conv-neural-network,26645,193.502,,4,one debug nan value tensorflow run tensorflow happen something yield nan like know know main issue normal procedural program would write print statement operation execute issue tensorflow first declare define graph add print statement graph definition help rule advice heuristic anything track might cause nan case know precisely line look follow line present return nan declare summary writer way least explore value z square root specific example post try success print nothing actually understand suppose need two argument want print tensor would need pass seem bizarre look function tf add check numerics ops say use plus doc seem super helpful anyone know use since comment address data might bad use standard mnist however compute quantity positive pair wise eucledian distance square root thus would see data specifically would issue
113,113,41715025,Keras flowFromDirectory get file names as they are being generated,"<p>Is it possible to get the file names that were loaded using <code>flow_from_directory</code> ? 
I have :</p>

<pre><code>datagen = ImageDataGenerator(
    rotation_range=3,
#     featurewise_std_normalization=True,
    fill_mode='nearest',
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

train_generator = datagen.flow_from_directory(
        path+'/train',
        target_size=(224, 224),
        batch_size=batch_size,)
</code></pre>

<p>I have a custom generator for my multi output model like:</p>

<pre><code>a = np.arange(8).reshape(2, 4)
# print(a)

print(train_generator.filenames)

def generate():
    while 1:
        x,y = train_generator.next()
        yield [x] ,[a,y]
</code></pre>

<p>Node that at the moment I am generating random numbers for <code>a</code> but for real training , I wish to load up a <code>json</code> file that contains the bounding box coordinates for my images. For that I will need to get the file names that were generated using <code>train_generator.next()</code> method. After I have that , I can load the file, parse the <code>json</code> and pass it instead of <code>a</code>. It is also necessary that the ordering of the <code>x</code> variable and the list of the file names that I get is the same. </p>",,6,2,,2017/1/18 8:59,4.0,2020/12/13 5:13,2017/1/23 23:35,,5974433.0,,2670775.0,,1,32,python|machine-learning|neural-network|keras,22902,102.839,,5,kera flowfromdirectory get file name generate possible get file name load use custom generator multi output model like node moment generate random number real training wish load file contain bounding box coordinate image need get file name generate use method load file parse pass instead also necessary ordering variable list file name get
297,297,57237381,"""RuntimeError: Expected 4-dimensional input for 4-dimensional weight 32 3 3, but got 3-dimensional input of size [3, 224, 224] instead""?","<p>I am trying to use a pre-trained model. Here's where the problem occurs</p>

<p>Isn't the model supposed to take in a simple colored image? Why is it expecting a 4-dimensional input?</p>

<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-51-d7abe3ef1355&gt; in &lt;module&gt;()
     33 
     34 # Forward pass the data through the model
---&gt; 35 output = model(data)
     36 init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
     37 

5 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)
    336                             _pair(0), self.dilation, self.groups)
    337         return F.conv2d(input, self.weight, self.bias, self.stride,
--&gt; 338                         self.padding, self.dilation, self.groups)
    339 
    340 

RuntimeError: Expected 4-dimensional input for 4-dimensional weight 32 3 3, but got 3-dimensional input of size [3, 224, 224] instead
</code></pre>

<p>Where </p>

<pre><code>inception = models.inception_v3()
model = inception.to(device)
</code></pre>",57238103.0,2,1,,2019/7/28 1:52,4.0,2021/6/9 5:04,2021/6/9 5:04,,1714410.0,,6458245.0,,1,29,machine-learning|computer-vision|pytorch|conv-neural-network|torchvision,40647,81.0361,,4,runtimeerror expect dimensional input dimensional weight get dimensional input size instead try use pre train model problem occur model suppose take simple colored image expect dimensional input
132,132,42327006,How to calculate top5 accuracy in keras?,"<p>I want to calculate top5 in imagenet2012 dataset, but i don't know how to do it in keras.
fit function just can calculate top 1 accuracy.</p>",,3,1,,2017/2/19 12:05,3.0,2021/2/26 3:44,,,,,6222724.0,,1,15,deep-learning|keras,13800,59.5595,,4,calculate top accuracy kera want calculate top imagenet dataset know keras fit function calculate top accuracy
730,730,40511562,TensorFlow 'module' object has no attribute 'global_variables_initializer',"<p>I'm new to Tensorflow
I'm running a Deep learning Assignment from Udacity on iPython notebook.
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb"">link</a></p>

<p>And it has an error.</p>

<pre><code>AttributeError                            Traceback (most recent call last)
`&lt;ipython-input-18-3446420b5935&gt;` in `&lt;module&gt;`()
  2 
  3 with tf.Session(graph=graph) as session:
----&gt; 4   tf.global_variables_initializer().run()

AttributeError: 'module' object has no attribute 'global_variables_initializer'
</code></pre>

<p>Please help! How can I fix this? Thank you.</p>",40512652.0,4,0,,2016/11/9 16:20,5.0,2020/8/13 10:52,2016/11/10 22:21,,434217.0,,6477475.0,,1,36,python|tensorflow|deep-learning|word2vec,28386,110.612,,4,tensorflow module object attribute global variable initializer new tensorflow run deep learning assignment udacity ipython notebook link error please help fix thank
733,733,40601975,Tensorflow Assign requires shapes of both tensors to match. lhs shape= [20] rhs shape= [48],"<p>I am a TensorFlow noob. I have trained a TensorFlow model from the open source implementation of deeppose and have to now run the model against a new set of images.</p>

<p>The model was trained on images of size <code>100 * 100</code> so I have resized the new set of images to the same size. I have <code>149</code> new images to run the model against. When I run the model, I get the following error.</p>

<pre><code>InvalidArgumentError (see above for traceback): Assign requires shapes
of both tensors to match. lhs shape= [20] rhs shape= [48]
</code></pre>

<p>At the line</p>

<pre><code>saver = tf.train.Saver(tf.all_variables())
</code></pre>

<p>I suspect the trained model size and the test image sizes are not matching. I am not clear how to fix this issue. I printed out the list of variables from the <code>tf.all_variables()</code> call. Here it is</p>

<pre><code>Tensor(""Placeholder:0"", shape=(128, 100, 100, 3), dtype=float32)
(11, 11, 3, 20)
conv1/weights:0
(20,)
conv1/biases:0
(5, 5, 20, 35)
conv2/weights:0
(35,)
conv2/biases:0
(3, 3, 35, 50)
conv4/weights:0
(50,)
conv4/biases:0
(3, 3, 50, 75)
conv5/weights:0
(75,)
conv5/biases:0
(300, 1024)
local1/weights:0
(1024,)
local1/biases:0
(1024, 1024)
local2/weights:0
(1024,)
local2/biases:0
(1024, 0)
softmax_linear/weights:0
(0,)
softmax_linear/biases:0
</code></pre>

<p>I am not sure where the RHS parameter is coming from. I've looked at all config files and there doesn't seem to be any parameter specifying this config.</p>

<p>Any help in resolving this will be greatly appreciated.</p>",43422479.0,6,3,,2016/11/15 4:19,6.0,2020/8/25 10:10,2016/11/15 9:07,,1951176.0,,1374228.0,,1,15,python|tensorflow|deep-learning,26005,102.66,,4,tensorflow assign require shape tensor match lhs shape rh shape tensorflow noob train tensorflow model open source implementation deeppose run model new set image model train image size resize new set image size new image run model run model get following error line suspect trained model size test image size match clear fix issue print list variable call sure rh parameter come look config file seem parameter specify config help resolve greatly appreciated
670,670,37901047,What is num_units in tensorflow BasicLSTMCell?,"<p>In MNIST LSTM examples, I don't understand what ""hidden layer"" means. Is it the imaginary-layer formed when you represent an unrolled RNN over time? </p>

<p>Why is the <code>num_units = 128</code> in most cases ? </p>",39440218.0,11,2,,2016/6/18 19:51,43.0,2020/10/22 2:23,2020/4/29 8:39,,6253584.0,,6253584.0,,1,56,tensorflow|neural-network|lstm|recurrent-neural-network,41047,300.653,,3,num unit tensorflow basiclstmcell mnist lstm example understand hide layer mean imaginary layer form represent unrolled rnn time case
314,314,58255821,How to use K.get_session in Tensorflow 2.0 or how to migrate it?,"<pre><code>def __init__(self, **kwargs):
    self.__dict__.update(self._defaults) # set up default values
    self.__dict__.update(kwargs) # and update with user overrides
    self.class_names = self._get_class()
    self.anchors = self._get_anchors()
    self.sess = K.get_session()
</code></pre>

<p>RuntimeError: <code>get_session</code> is not available when using TensorFlow 2.0.</p>",,4,0,,2019/10/6 9:13,3.0,2020/12/28 21:23,,,,,12171649.0,,1,21,machine-learning|keras|deep-learning|real-time|tensorflow2.0,27677,69.5685,,4,use k get session tensorflow migrate runtimeerror available use tensorflow
416,416,47555829,preprocess_input() method in keras,"<p>I am trying out sample <code>keras</code> code from the below <code>keras</code> documentation page,
<a href=""https://keras.io/applications/"" rel=""noreferrer"">https://keras.io/applications/</a></p>

<p>What <code>preprocess_input(x)</code> function of <code>keras</code> module does in the below code? Why do we have to do <code>expand_dims(x, axis=0)</code> before that is passed to the <code>preprocess_input()</code> method?</p>

<pre><code>from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input
import numpy as np

model = ResNet50(weights='imagenet')

img_path = 'elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
</code></pre>

<p>Is there any documentation with a good explanation of these functions?</p>

<p>Thanks!</p>",47556342.0,3,5,,2017/11/29 15:03,16.0,2021/4/4 22:16,2019/2/2 10:56,,10455534.0,,4724057.0,,1,51,python|keras,72722,153.047,,2,preprocess input method kera try sample code documentation page function module code pass method documentation good explanation function thanks
82,82,10565868,multi-layer perceptron (MLP) architecture: criteria for choosing number of hidden layers and size of the hidden layer?,<p>If we have 10 eigenvectors then we can have 10 neural nodes in input layer.If we have 5 output classes then we can have 5 nodes in output layer.But what is the criteria for choosing number of hidden layer in a MLP and how many neural nodes in 1 hidden layer?</p>,10568938.0,4,3,,2012/5/12 17:18,88.0,2020/4/9 0:29,2015/9/16 21:42,,66549.0,,1001658.0,,1,106,machine-learning|neural-network|deep-learning|perceptron,54351,328.741,2021/2/10 21:19,3,multi layer perceptron mlp architecture criterion choose number hidden layer size hidden layer eigenvectors neural node input layer output class node output layer criterion choose number hidden layer mlp many neural node hidden layer
609,609,50304156,Tensorflow Allocation Memory: Allocation of 38535168 exceeds 10% of system memory,"<p>Using ResNet50 pre-trained Weights I am trying to build a classifier. The code base is fully implemented in Keras high-level Tensorflow API. The complete code is posted in the below GitHub Link.</p>
<p>Source Code: <a href=""https://gist.github.com/Madhivarman/676650f71ec35a5f2802631fcfa0ff73"" rel=""noreferrer"">Classification Using RestNet50 Architecture</a></p>
<p>The file size of the pre-trained model is <strong>94.7mb</strong>.</p>
<p>I loaded the pre-trained file</p>
<pre><code>new_model = Sequential()

new_model.add(ResNet50(include_top=False,
                pooling='avg',
                weights=resnet_weight_paths))
</code></pre>
<p>and fit the model</p>
<pre><code>train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 12,
    class_mode = 'categorical'
    )

validation_generator = data_generator.flow_from_directory(
    'path_to_the_validation_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    class_mode = 'categorical'
    )

#compile the model

new_model.fit_generator(
    train_generator,
    steps_per_epoch = 3,
    validation_data = validation_generator,
    validation_steps = 1
)
</code></pre>
<p>and in the Training dataset, I have two folders dog and cat, each holder almost 10,000 images. When  I compiled the script, I get the following error</p>
<blockquote>
<p>Epoch 1/1 2018-05-12 13:04:45.847298: W
tensorflow/core/framework/allocator.cc:101] Allocation of 38535168
exceeds 10% of system memory. 2018-05-12 13:04:46.845021: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:47.552176: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.199240: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.918930: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:49.274137: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:49.647061: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.028839: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.413735: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory.</p>
</blockquote>
<p>Any ideas to optimize the way to load the pre-trained model (or) get rid of this warning message?</p>
<p>Thanks!</p>",51367588.0,7,3,,2018/5/12 8:09,13.0,2021/3/2 11:08,2020/6/20 9:12,,-1.0,,9044016.0,,1,40,python|tensorflow|memory|keras-layer|resnet,70499,138.393,,4,tensorflow allocation memory allocation exceeds system memory use resnet pre train weight try build classifier code base fully implement kera high level tensorflow api complete code post github link source code classification use restnet architecture file size pre trained model mb load pre train file fit model training dataset two folder dog cat holder almost image compile script get following error epoch w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory w tensorflow core framework allocator cc allocation exceeds system memory idea optimize way load pre train model get rid warning message thanks
754,754,51299836,What values are returned from model.evaluate() in Keras?,"<p>I've got multiple outputs from my model from multiple Dense layers. My model has <code>'accuracy'</code> as the only metric in compilation. I'd like to know the loss and accuracy for each output. This is some part of my code.</p>

<pre><code>scores = model.evaluate(X_test, [y_test_one, y_test_two], verbose=1)
</code></pre>

<p>When I printed out the scores, this is the result.</p>

<pre><code>[0.7185557290413819, 0.3189622712272771, 0.39959345855771927, 0.8470299135229717, 0.8016634374641469]
</code></pre>

<p>What are these numbers represent?</p>

<p>I'm new to Keras and this might be a trivial question. However, I have read the docs from Keras but I'm still not sure.</p>",51303340.0,1,0,,2018/7/12 7:36,9.0,2020/12/20 22:43,2018/7/12 11:02,,2099607.0,,8822850.0,,1,44,python|keras,71029,82.2057,,5,value return model evaluate kera get multiple output model multiple dense layer model metric compilation like know loss accuracy output part code print score result number represent new keras might trivial question however read doc kera still sure
334,334,45063602,"Attempting to reset tensorflow graph when using keras, failing","<p>I'm spinning up a Python 3 API w/gunicorn that uses keras to calculate vectors for an image, pretty straightforward.</p>

<p>How can I reset the data stored in memory for each request? Slowly over time the requests increase in the time it takes to respond. I've run a profiler and it's specifically this line in tensorflow (also memory usage goes up slowly over time per process):</p>

<pre><code>#tensorflow/python/framework/ops.py:2317:_as_graph_def
graph.node.extend([op.node_def])
</code></pre>

<p>It takes longer as more data is in the node. Here is the code I execute:</p>

<pre><code># We have 11439MiB of GPU memory, lets only use 2GB of it:
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.22
sess = tf.Session(config=config)
set_session(sess)
sess.graph.as_default()

# Get the vector for the image
img_size = (224,224)
vgg = VGG16(include_top=False, weights='imagenet')
img = kimage.load_img(tmpfile.name, target_size=img_size)
x = kimage.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
pred = vgg.predict(x)
vectors = pred.ravel().tolist()
</code></pre>

<p>I thought <code>as_default()</code> would help, but it doesn't. I also tried closing the session after I get the list of vectors, and that fails.</p>",45067789.0,1,0,,2017/7/12 16:56,1.0,2017/7/13 1:18,,,,,524481.0,,1,12,python|tensorflow|keras,11299,52.6122,,4,attempt reset tensorflow graph use kera fail spin python api w gunicorn use kera calculate vector image pretty straightforward reset data store memory request slowly time request increase time take respond run profiler specifically line tensorflow also memory usage go slowly time per process take long data node code execute thought would help also try close session get list vector fails
762,762,51691563,CUDA runtime error (59) : device-side assert triggered,"<p>I have access to Tesla K20c, I am running ResNet50 on CIFAR10 dataset... 
Then I get the error as: <br/><code>THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu line=265 error=59 : device-side assert triggered</code> <br/>
<code>Traceback (most recent call last):</code> <br/>
<code>File ""main.py"", line 109, in &lt;module&gt;</code><br/>
<code>train(loader_train, model, criterion, optimizer)</code>
<br/><code>File ""main.py"", line 54, in train</code>
<code>optimizer.step()</code>
 <br/><code>File ""/usr/local/anaconda35/lib/python3.6/site-packages/torch/optim/sgd.py"", line 93, in step</code><br/>
<code>d_p.add_(weight_decay, p.data)</code>
 <code>RuntimeError: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:265</code>
<br/> How to resolve this error</p>",51701698.0,6,6,,2018/8/5 5:10,7.0,2021/8/20 22:32,,,,,7295290.0,,1,43,gpu|pytorch,85189,170.322,,4,cuda runtime error device side assert trigger access tesla k c run resnet cifar dataset get error resolve error
121,121,41908379,"Keras - Plot training, validation and test set accuracy","<p>I want to plot the output of this simple neural network: </p>

<pre><code>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(x_test, y_test, nb_epoch=10, validation_split=0.2, shuffle=True)

model.test_on_batch(x_test, y_test)
model.metrics_names
</code></pre>

<p>I have plotted <em>accuracy</em> and <em>loss</em> of training and validation:</p>

<pre><code>print(history.history.keys())
#  ""Accuracy""
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
# ""Loss""
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()
</code></pre>

<p>Now I want to add and plot test set's accuracy from <code>model.test_on_batch(x_test, y_test)</code>, but from <code>model.metrics_names</code> I obtain the same value <em>'acc'</em> utilized for plotting accuracy on training data <code>plt.plot(history.history['acc'])</code>. How could I plot test set's accuracy?</p>",41909089.0,4,0,,2017/1/28 9:46,20.0,2021/5/23 14:34,,,,,7387749.0,,1,52,keras,108500,174.74200000000005,,5,kera plot train validation test set accuracy want plot output simple neural network plot accuracy loss training validation want add plot test set accuracy obtain value acc utilize plot accuracy train data could plot test set accuracy
517,517,34870614,What does tf.nn.embedding_lookup function do?,"<pre><code>tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None)
</code></pre>

<p>I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)?</p>

<p>For instance, in the <code>skip-gram</code> model if we use <code>tf.nn.embedding_lookup(embeddings, train_inputs)</code>, then for each <code>train_input</code> it finds the correspond embedding?</p>",34877590.0,8,1,,2016/1/19 7:14,61.0,2018/8/5 11:52,2018/7/13 12:19,,2956066.0,,5808490.0,,1,167,python|tensorflow|deep-learning|word-embedding|natural-language-processing,72014,737.63,,3,tf nn embed lookup function understand duty function like lookup table mean return parameter correspond id id instance model use find correspond embedding
295,295,57144586,"Tensorflow GradientTape ""Gradients does not exist for variables"" intermittently","<p>When training my network I am occasionally met with the warning: </p>

<p><code>W0722 11:47:35.101842 140641577297728 optimizer_v2.py:928] Gradients does not exist for variables ['model/conv1d_x/Variable:0'] when minimizing the loss.
</code></p>

<p>This happens sporadically at infrequent intervals (maybe once in every 20 successful steps). My model basically has two paths which join together with concatenations at various positions in the network. To illustrate this, here is a simplified example of what I mean.</p>

<pre><code>class myModel(tf.keras.Model):

  def __init__(self):

    self.conv1 = Conv2D(32)
    self.conv2 = Conv2D(32)
    self.conv3 = Conv2D(16)

  def call(self, inputs):

    net1 = self.conv1(inputs)
    net2 = self.conv2(inputs)
    net = tf.concat([net1, net2], axis=2)
    net = self.conv3(net)
    end_points = tf.nn.softmax(net)

model = myModel()

with tf.GradientTape() as tape:

  predicition = model(image)
  loss = myloss(labels, prediction)

gradients = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre>

<p>In reality my network is much larger, but the variables that generally don't have gradients tend to be the ones at the top of the network. Before each <code>Conv2D</code> layer I also have a custom gradient. Sometimes when I the error appears I can notice that the gradient function for that layer has not been called.</p>

<p>My question is how can the gradient tape sometimes take what appears to be different paths when propagating backwards through my network. My secondary question, is this caused by having two separate routes through my network (i.e. conv1 AND conv2). Is there a fundamental flaw in this network architecture?</p>

<p>Ideally, could I define to the <code>GradientTape()</code> that it must find the gradients for each of the top layers? </p>",,5,4,,2019/7/22 10:59,2.0,2021/8/1 13:06,,,,,8058705.0,,1,20,python|tensorflow|keras,15095,60.7153,,4,tensorflow gradienttape gradient exist variable intermittently train network occasionally meet warning happen sporadically infrequent interval maybe every successful step model basically two path join together concatenation various position network illustrate simplified example mean reality network much large variable generally gradient tend one top network layer also custom gradient sometimes error appear notice gradient function layer call question gradient tape sometimes take appear different path propagating backwards network secondary question cause two separate route network e conv conv fundamental flaw network architecture ideally could define must find gradient top layer
764,764,51704808,"What is the difference between Loss, accuracy, validation loss, Validation accuracy?","<p>At the end of each epoch, I am getting for example the following output:</p>

<pre><code>Epoch 1/25
2018-08-06 14:54:12.555511: 
2/2 [==============================] - 86s 43s/step - loss: 6.0767 - acc: 0.0469 - val_loss: 4.1037 - val_acc: 0.2000
Epoch 2/25
2/2 [==============================] - 26s 13s/step - loss: 3.6901 - acc: 0.0938 - val_loss: 2.5610 - val_acc: 0.0000e+00
Epoch 3/25
2/2 [==============================] - 66s 33s/step - loss: 3.1491 - acc: 0.1406 - val_loss: 2.4793 - val_acc: 0.0500
Epoch 4/25
2/2 [==============================] - 44s 22s/step - loss: 3.0686 - acc: 0.0694 - val_loss: 2.3159 - val_acc: 0.0500
Epoch 5/25
2/2 [==============================] - 62s 31s/step - loss: 2.5884 - acc: 0.1094 - val_loss: 2.4601 - val_acc: 0.1500
Epoch 6/25
2/2 [==============================] - 41s 20s/step - loss: 2.7708 - acc: 0.1493 - val_loss: 2.2542 - val_acc: 0.4000
.
.
.
.
</code></pre>

<p>Can anyone explain me what's the difference between loss, accuracy, validation loss and validation accuracy?</p>",51705639.0,3,0,,2018/8/6 9:47,5.0,2019/8/26 9:34,2018/8/7 18:40,,7017416.0,,7017416.0,,1,15,tensorflow|keras,13646,55.54,,4,difference loss accuracy validation loss validation accuracy end epoch get example following output anyone explain difference loss accuracy validation loss validation accuracy
59,59,55445712,Custom loss function in Keras based on the input data,"<p>I am trying to create the custom loss function using Keras. I want to compute the loss function based on the input and predicted the output of the neural network. </p>

<p>I tried using the customloss function in Keras. I think y_true is the output that we give for training and y_pred is the predicted output of the neural network. The below loss function is same as ""mean_squared_error"" loss in Keras.   </p>

<pre><code>def customloss(y_true, y_pred):
    return K.mean(K.square(y_pred - y_true), axis=-1)
</code></pre>

<p>I would like to use the input to the neural network also to compute the custom loss function in addition to mean_squared_error loss. Is there a way to send an input to the neural network as an argument to the customloss function. </p>

<p>Thank you.</p>",55530654.0,2,1,,2019/3/31 21:41,13.0,2019/4/5 12:03,,,,,3443033.0,,1,15,keras,9805,52.9658,,4,custom loss function kera base input data try create custom loss function use kera want compute loss function base input predict output neural network try use customloss function kera think true output give training pred predicted output neural network loss function mean square error loss kera would like use input neural network also compute custom loss function addition mean squared error loss way send input neural network argument customloss function thank
551,551,48493755,Keras AttributeError: 'list' object has no attribute 'ndim',"<p>I'm running a Keras neural network model in Jupyter Notebook (Python 3.6)</p>

<p>I get the following error</p>

<blockquote>
  <p>AttributeError: 'list' object has no attribute 'ndim'</p>
</blockquote>

<p>after calling the .fit() method from Keras.model</p>

<pre><code>model  = Sequential()
model.add(Dense(5, input_dim=len(X_data[0]), activation='sigmoid' ))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
model.fit(X_data, y_data, epochs=20, batch_size=10)
</code></pre>

<p>I checked the requirements.txt file for Keras (in Anaconda3) and the numpy, scipy, and six module versions are all up to date.</p>

<p>What can explain this AttributeError?</p>

<p>The full error message is the following (seems to be somewhat related to Numpy):</p>

<blockquote>
  <p>--------------------------------------------------------------------------- AttributeError                            Traceback (most recent call
  last)  in ()
        3 model.add(Dense(1, activation = 'sigmoid'))
        4 model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])
  ----> 5 model.fit(X_data, y_data, epochs=20, batch_size=10)</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\models.py in fit(self, x, y,
  batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)
      963                               initial_epoch=initial_epoch,
      964                               steps_per_epoch=steps_per_epoch,
  --> 965                               validation_steps=validation_steps)
      966 
      967     def evaluate(self, x=None, y=None,</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x,
  y, batch_size, epochs, verbose, callbacks, validation_split,
  validation_data, shuffle, class_weight, sample_weight, initial_epoch,
  steps_per_epoch, validation_steps, **kwargs)    1591<br>
  class_weight=class_weight,    1592             check_batch_axis=False,
  -> 1593             batch_size=batch_size)    1594         # Prepare validation data.    1595         do_validation = False</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)    1424<br>
  self._feed_input_shapes,    1425<br>
  check_batch_axis=False,
  -> 1426                                     exception_prefix='input')    1427         y = _standardize_input_data(y, self._feed_output_names,<br>
  1428                                     output_shapes,</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
       68     elif isinstance(data, list):
       69         data = [x.values if x.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else data</p>
  
  <p>~\Anaconda3\lib\site-packages\keras\engine\training.py in
  (.0)
       68     elif isinstance(data, list):
       69         data = [x.values if x.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else x for x in data]
  ---> 70         data = [np.expand_dims(x, 1) if x is not None and x.ndim == 1 else x for x in data]
       71     else:
       72         data = data.values if data.<strong>class</strong>.<strong>name</strong> == 'DataFrame' else data</p>
  
  <p>AttributeError: 'list' object has no attribute 'ndim'</p>
</blockquote>",48494469.0,3,0,,2018/1/29 2:55,3.0,2019/2/15 10:55,2018/1/29 3:22,,7201349.0,,7201349.0,,1,24,python|tensorflow|machine-learning|keras|jupyter-notebook,57173,80.4288,,4,kera attributeerror list object attribute ndim run keras neural network model jupyter notebook python get following error attributeerror list object attribute ndim call fit method kera model check requirement txt file kera anaconda numpy scipy six module version date explain attributeerror full error message follow seem somewhat related numpy attributeerror traceback recent call last model add dense activation sigmoid model compile loss mean square error optimizer adam metric acc model fit x data data epoch batch size anaconda lib site package keras model py fit self x batch size epoch verbose callback validation split validation data shuffle class weight sample weight initial epoch step per epoch validation step kwargs initial epoch initial epoch step per epoch step per epoch validation step validation step def evaluate self x none none anaconda lib site package keras engine training py fit self x batch size epoch verbose callback validation split validation data shuffle class weight sample weight initial epoch step per epoch validation step kwargs class weight class weight check batch axis false batch size batch size prepare validation data validation false anaconda lib site package keras engine training py standardize user data self x sample weight class weight check batch axis batch size self fee input shape check batch axis false exception prefix input standardize input data self fee output name output shape anaconda lib site package keras engine training py standardize input data data name shape check batch axis exception prefix elif isinstance data list data x value x class name dataframe else x x data data np expand dims x x none x ndim else x x data else data data value data class name dataframe else data anaconda lib site package keras engine training py elif isinstance data list data x value x class name dataframe else x x data data np expand dims x x none x ndim else x x data else data data value data class name dataframe else data attributeerror list object attribute ndim
545,545,48373685,Keras ImageDataGenerator() how to get all labels from data,"<p>I am using the ImageDataGenerator() in Keras and I would like to get the labels of my entire test data.</p>

<p>Currently I am using the following code to accomplish this task:</p>

<pre><code>test_batches = ImageDataGenerator().flow_from_directory(...)

test_labels = []

for i in range(0,3):
    test_labels.extend(np.array(test_batches[i][1]))
</code></pre>

<p>This code however only works because I know I have a total of 150 images and my batch size is defined to be 50.</p>

<p>Moreover using:</p>

<pre><code>imgs, labels = next(test_batches)
</code></pre>

<p>as suggested in similar posts on this topic only returns labels for one batch and not the entire dataset. As such I wonder if there is a more efficient way of doing this than the method I am using above.</p>",48376379.0,4,0,,2018/1/22 1:45,1.0,2019/7/3 16:47,2018/1/22 1:50,,6341510.0,,6341510.0,,1,11,python|keras,13692,53.3459,,2,kera imagedatagenerator get label data use imagedatagenerator kera would like get label entire test data currently use following code accomplish task code however work know total image batch size define moreover use suggest similar post topic return label one batch entire dataset wonder efficient way method use
423,423,47731935,Using multiple validation sets with keras,"<p>I am training a model with keras using the <code>model.fit()</code> method. 
I would like to use multiple validation sets that should be validated on separately after each training epoch so that i get one loss value for each validation set. If possible they should be both displayed during training and as well be returned by the <code>keras.callbacks.History()</code> callback.</p>

<p>I am thinking of something like this:</p>

<pre><code>history = model.fit(train_data, train_targets,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_data=[
                        (validation_data1, validation_targets1), 
                        (validation_data2, validation_targets2)],
                    shuffle=True)
</code></pre>

<p>I currently have no idea how to implement this. Is it possible to achieve this by writing my own <code>Callback</code>? Or how else would you approach this problem?</p>",47738812.0,3,0,,2017/12/9 18:45,7.0,2021/2/5 15:04,,,,,9077324.0,,1,21,validation|keras|monitoring,5938,58.6946,,4,use multiple validation set kera train model kera use method would like use multiple validation set validate separately training epoch get one loss value validation set possible display training well return callback think something like currently idea implement possible achieve write else would approach problem
145,145,42586475,Is it possible to automatically infer the class_weight from flow_from_directory in Keras?,"<p>I have an imbalanced multi-class dataset and I want to use the <code>class_weight</code> argument from <code>fit_generator</code> to give weights to the classes according to the number of images of each class. I'm using <code>ImageDataGenerator.flow_from_directory</code> to load the dataset from a directory.</p>

<p>Is it possible to directly infer the <code>class_weight</code> argument from the <code>ImageDataGenerator</code> object?</p>",42587192.0,5,1,,2017/3/3 18:57,5.0,2021/5/24 20:30,2018/9/27 7:24,,604734.0,,604734.0,,1,16,keras|deep-learning,8981,82.8133,,2,possible automatically infer class weight flow directory kera imbalanced multi class dataset want use argument give weight class accord number image class use load dataset directory possible directly infer argument object
341,341,45271344,ImportError: No module named 'keras',"<p>So basically, I am fairly new to programming and using python. I am trying to build an ANN model for which I have to use Tensor flow, Theano and Keras library. I have Anaconda 4.4.1 with Python 3.5.2 on Windows 10 x64 and I have installed these libraries by following method.</p>

<ol>
<li>Create a new environment with Anaconda and Python 3.5:
conda create -n tensorflow python=3.5 anaconda</li>
<li>Activate the environment: 
activate tensorflow</li>
<li>After this you can install Theano, TensorFlow and Keras:
conda install theano,
conda install mingw libpython,
pip install tensorflow,
pip install keras,</li>
<li>Update the packages:
conda update --all</li>
</ol>

<p>All these packages are installed correctly and I have check them with conda list.
However, when I am trying to import any of these 3 libraries (i.e. Tensor flow, Theano and Keras), it is giving me the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;ipython-input-3-c74e2bd4ca71&gt;"", line 1, in &lt;module&gt;
import keras
ImportError: No module named 'keras'
</code></pre>",,10,5,,2017/7/24 1:11,0.0,2021/6/23 15:49,2017/7/25 18:35,,2449192.0,,8354975.0,,1,19,python-3.x|tensorflow|anaconda|keras,102559,94.0439,,1,importerror module name kera basically fairly new program use python try build ann model use tensor flow theano keras library anaconda python window x instal library follow method create new environment anaconda python conda create n tensorflow python anaconda activate environment activate tensorflow install theano tensorflow keras conda install theano conda install mingw libpython pip install tensorflow pip install kera update package conda update package instal correctly check conda list however try import library e tensor flow theano keras give following error
508,508,34181056,Q-learning vs temporal-difference vs model-based reinforcement learning,"<p>I'm in a course called ""Intelligent Machines"" at the university. We were introduced with 3 methods of reinforced learning, and with those we were given the intuition of when to use them, and I quote:</p>

<ol>
<li>Q-Learning - Best when MDP can't be solved. </li>
<li>Temporal Difference Learning - best when MDP is known or can be learned but can't be solved.</li>
<li>Model-based - best when MDP can't be learned.</li>
</ol>

<p>Are there any good examples explaining when to choose one method over the other?</p>",34263310.0,1,5,,2015/12/9 14:17,10.0,2018/11/22 15:47,2018/11/22 15:39,,3924118.0,,2204803.0,,1,22,machine-learning|reinforcement-learning|q-learning|temporal-difference,17183,57.3404,,0,q learn vs temporal difference v model base reinforcement learn course call intelligent machine university introduce method reinforced learning give intuition use quote q learning best mdp solve temporal difference learn best mdp know learn solve model base best mdp learn good example explain choose one method
86,86,15917849,How can I assign/update subset of tensor shared variable in Theano?,"<p>When compiling a function in <code>theano</code>, a shared variable(say X) can be updated by specifying <code>updates=[(X, new_value)]</code>.
Now I am trying to update only subset of a shared variable:</p>

<pre><code>from theano import tensor as T
from theano import function
import numpy

X = T.shared(numpy.array([0,1,2,3,4]))
Y = T.vector()
f = function([Y], updates=[(X[2:4], Y)] # error occur:
                                        # 'update target must 
                                        # be a SharedVariable'
</code></pre>

<p>The codes will raise a error ""update target must be a SharedVariable"", I guess that means update targets can't be non-shared variables. So is there any way to compile a function to just udpate subset of shared variables?</p>",,2,0,,2013/4/10 5:42,5.0,2015/11/10 8:57,2013/5/24 16:15,,380038.0,,2168406.0,,1,25,python|numpy|theano,15080,58.7136,,3,assign update subset tensor share variable theano compile function shared variable say x update specify try update subset share variable code raise error update target must sharedvariable guess mean update target non share variable way compile function udpate subset share variable
668,668,37849322,How to understand the term `tensor` in TensorFlow?,"<p>I am new to TensorFlow. While I am reading the existing documentation, I found the term <code>tensor</code> really confusing. Because of it, I need to clarify the following questions:</p>

<ol>
<li>What is the relationship between <code>tensor</code> and <code>Variable</code>, <code>tensor</code><br>
vs. <code>tf.constant</code>, 'tensor' vs. <code>tf.placeholder</code>?</li>
<li>Are they all types of tensors?</li>
</ol>",37870634.0,4,2,,2016/6/16 3:18,36.0,2018/5/27 15:18,2018/5/20 9:09,,606664.0,,2452761.0,,1,37,python|tensorflow|machine-learning|deep-learning|tensor,19558,135.765,,3,understand term tensor tensorflow new tensorflow read exist documentation find term really confusing need clarify following question relationship vs tensor v type tensor
755,755,51337558,How to import keras.engine.topology in Tensorflow?,"<p>I want to import keras.engine.topology in Tensorflow.
I used to add the word tensorflow at the beginning of every Keras import if I want to use the Tensorflow  version of Keras.</p>

<p>For example: instead of writing:</p>

<pre><code>from keras.layers import Dense, Dropout, Input
</code></pre>

<p>I just write the following code and it works fine :</p>

<pre><code>from tensorflow.keras.layers import Dense, Dropout, Input
</code></pre>

<p>But that's not the case for this specific import:</p>

<pre><code>from tensorflow.keras.engine.topology import Layer, InputSpec
</code></pre>

<p>And I m getting the following error message:</p>

<pre><code>No module named 'tensorflow.keras.engine'
</code></pre>",51337967.0,2,0,,2018/7/14 10:24,5.0,2019/11/13 11:09,,,,,4713057.0,,1,18,python|tensorflow|keras,21541,57.5331,,1,import kera engine topology tensorflow want import kera engine topology tensorflow use add word tensorflow beginning every kera import want use tensorflow version kera example instead write write following code work fine case specific import get following error message
523,523,35074549,How to load a model from an HDF5 file in Keras?,"<p>How to load a model from an HDF5 file in Keras?</p>

<p>What I tried:</p>

<pre><code>model = Sequential()

model.add(Dense(64, input_dim=14, init='uniform'))
model.add(LeakyReLU(alpha=0.3))
model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))
model.add(Dropout(0.5))

model.add(Dense(64, init='uniform'))
model.add(LeakyReLU(alpha=0.3))
model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))
model.add(Dropout(0.5))

model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))


sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)

checkpointer = ModelCheckpoint(filepath=""/weights.hdf5"", verbose=1, save_best_only=True)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2, callbacks=[checkpointer])
</code></pre>

<p>The above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so:</p>

<pre><code>model2 = Sequential()
model2.load_weights(""/Users/Desktop/SquareSpace/weights.hdf5"")
</code></pre>

<p>This is the error I get:</p>

<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-101-ec968f9e95c5&gt; in &lt;module&gt;()
      1 model2 = Sequential()
----&gt; 2 model2.load_weights(""/Users/Desktop/SquareSpace/weights.hdf5"")

/Applications/anaconda/lib/python2.7/site-packages/keras/models.pyc in load_weights(self, filepath)
    582             g = f['layer_{}'.format(k)]
    583             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]
--&gt; 584             self.layers[k].set_weights(weights)
    585         f.close()
    586 

IndexError: list index out of range
</code></pre>",35196851.0,5,0,,2016/1/29 0:03,46.0,2020/3/20 15:36,2016/2/3 20:46,,4984897.0,,4984897.0,,1,105,python|machine-learning|keras|data-science,228122,481.43300000000005,,3,load model hdf file kera load model hdf file kera try code successfully save best model file name weight hdf want load model code show try error get
751,751,41175401,What is a batch in TensorFlow?,"<p>The introductory documentation, which I am reading (<a href=""https://www.tensorflow.org/get_started/"" rel=""noreferrer"">TOC here</a>) uses the term &quot;batch&quot; (<a href=""https://www.tensorflow.org/tutorials/keras/classification"" rel=""noreferrer"">for instance here</a>) without having defined it.</p>",,1,1,,2016/12/16 0:11,24.0,2020/9/3 1:08,2020/9/3 1:08,,916142.0,,916142.0,,1,48,tensorflow|machine-learning|neural-network|deep-learning|tensor,43849,102.168,,3,batch tensorflow introductory documentation read toc use term batch instance without define
20,20,53900396,What are Torch Scripts in PyTorch?,"<p>I've just found that PyTorch docs expose something that is called <a href=""https://pytorch.org/docs/stable/jit.html?highlight=model%20features"" rel=""noreferrer"">Torch Scripts</a>. However, I do not know:</p>

<ul>
<li>When they should be used?</li>
<li>How they should be used?</li>
<li>What are their benefits?</li>
</ul>",53903128.0,1,1,,2018/12/23 0:49,15.0,2020/9/2 9:15,,,,,7347631.0,,1,43,pytorch|jit,17295,83.5517,,0,torch script pytorch find pytorch doc expose something call torch script however know use use benefit
722,722,40243753,"Exception: ""dot"" not found in path in python on mac","<p>I want to use <code>caffe.draw</code> to draw the caffe net by anaconda python on mac. But I got the error like this:</p>

<pre><code>File ""python/draw_net.py"", line 45, in &lt;module&gt;
    main()
  File ""python/draw_net.py"", line 41, in main
    caffe.draw.draw_net_to_file(net, args.output_image_file, args.rankdir)
  File ""/Users/xxh/caffe/distribute/python/caffe/draw.py"", line 222, in draw_net_to_file
    fid.write(draw_net(caffe_net, rankdir, ext))
  File ""/Users/xxh/caffe/distribute/python/caffe/draw.py"", line 204, in draw_net
    return get_pydot_graph(caffe_net, rankdir).create(format=ext)
  File ""/Users/xxh/anaconda2/lib/python2.7/site-packages/pydot.py"", line 1883, in create
    prog=prog))
Exception: ""dot"" not found in path.
</code></pre>

<p>I have installed pydot and graphviz ,and how can I add the dot's path to python path?</p>",,7,1,,2016/10/25 15:19,5.0,2021/4/23 17:05,,,,,4814899.0,,1,15,python|macos|path|pycaffe,15192,85.7265,,1,exception dot find path python mac want use draw caffe net anaconda python mac get error like instal pydot graphviz add dot path python path
516,516,34717241,How to use advanced activation layers in Keras?,"<p>This is my code that works if I use other activation layers like tanh:</p>

<pre><code>model = Sequential()
act = keras.layers.advanced_activations.PReLU(init='zero', weights=None)
model.add(Dense(64, input_dim=14, init='uniform'))
model.add(Activation(act))
model.add(Dropout(0.15))
model.add(Dense(64, init='uniform'))
model.add(Activation('softplus'))
model.add(Dropout(0.15))
model.add(Dense(2, init='uniform'))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='binary_crossentropy', optimizer=sgd)
model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)
</code></pre>

<p>In this case, it doesn't work and says ""TypeError: 'PReLU' object is not callable"" and the error is called at the model.compile line. Why is this the case? All the non-advanced activation functions works. However, neither of the advanced activation functions, including this one, works.</p>",34721948.0,3,0,,2016/1/11 8:44,10.0,2018/2/1 17:09,,,,,4984897.0,,1,33,python|machine-learning|neural-network|keras|data-science,24973,96.3899,,3,use advanced activation layer kera code work use activation layer like tanh case work say typeerror prelu object callable error call model compile line case non advance activation function work however neither advanced activation function include one work
54,54,55268762,How to accumulate gradients for large batch sizes in Keras,"<p>I am working with a very memory demanding CNN model for a task of classification.
This poses a big limit on the batch size that I can use during training.</p>

<p>One solution is to accumulate the gradients during training, meaning that the weights of the model are not updated after every single batch. Instead the same weights are used for several batches, while the gradients from each batch are accumulated and than averaged for a single weight-update action.</p>

<p>I'm using a Tensorflow backend Keras and I'm pretty sure that Keras has no off-the-shelf function/method to achieve this.</p>

<p>How can it be done for a Keras/tensorflow model?</p>",55281501.0,3,3,,2019/3/20 19:26,8.0,2020/1/23 9:18,,,,,9673730.0,,1,17,python|tensorflow|machine-learning|keras,7228,52.6361,,3,accumulate gradient large batch size kera work memory demand cnn model task classification pose big limit batch size use train one solution accumulate gradient train meaning weight model update every single batch instead weight use several batch gradient batch accumulate average single weight update action use tensorflow backend kera pretty sure kera shelf function method achieve keras tensorflow model
485,485,60018578,What does model.eval() do in pytorch?,"<p>I am using <a href=""https://github.com/natanielruiz/deep-head-pose/blob/master/code/train_hopenet.py"" rel=""noreferrer"">this code</a>, and saw <code>model.eval()</code> in some cases.</p>
<p>I understand it is supposed to allow me to &quot;evaluate my model&quot;, but I don't understand when I should and shouldn't use it, or how to turn if off.</p>
<p>I would like to run the above code to train the network, and also be able to run validation every epoch. I wasn't able to do it still.</p>",60018731.0,4,2,,2020/2/1 15:58,28.0,2021/8/19 17:45,2020/8/18 17:38,,10908375.0,,913098.0,,1,93,python|machine-learning|deep-learning|pytorch,76431,269.933,,5,model eval pytorch use code saw case understand suppose allow evaluate model understand use turn would like run code train network also able run validation every epoch able still
539,539,36548736,TensorFlow: Unpooling,"<p>Is there TensorFlow native function that does unpooling for Deconvolutional Networks ? </p>

<p>I have written this in normal python, but it is getting complicated when want to translate it to TensorFlow as it's objects does not even support item assignment at the moment, and I think this is a great inconvenience with TF.</p>",36557995.0,5,3,,2016/4/11 12:29,7.0,2021/3/18 16:01,,,,,5724595.0,,1,24,tensorflow|conv-neural-network|deconvolution,14142,74.602,,3,tensorflow unpooling tensorflow native function unpooling deconvolutional network write normal python get complicate want translate tensorflow object even support item assignment moment think great inconvenience tf
766,766,51730880,Where do I get a CPU-only version of PyTorch?,"<p>I'm trying to get a basic app running with Flask + PyTorch, and host it on Heroku. However, I run into the issue that the maximum slug size is 500mb on the free version, and PyTorch itself is ~500mb. </p>

<p>After some google searching, someone wrote about finding a cpu-only version of PyTorch, and using that, which is much smaller <a href=""https://www.codementor.io/akshaysharma17/how-and-why-i-built-an-ml-based-python-api-hosted-on-heroku-j74qbfwn1"" rel=""noreferrer"">here</a>.</p>

<p>However, I'm pretty lost as to how this is done, and the person didn't document this at all. Any advice is appreciated, thanks. </p>

<p>EDIT: </p>

<p>To be more specific about my problem, I tried installing torch by (as far as I understand), including a requirements.txt which listed torch as a dependency. Current I have: torch==0.4.1. However this doesn't work bc of size.</p>

<p>My question is, do you know what I could write in the requirements file to get the cpu-only version of torch that is smaller, or alternatively, if the requirements.txt doesn't work for this, what I would do instead, to get the cpu version. </p>",51781950.0,6,2,,2018/8/7 15:51,4.0,2021/7/20 17:17,2018/8/10 18:15,,699168.0,,699168.0,,1,24,heroku|pytorch,23497,98.284,,1,get cpu version pytorch try get basic app run flask pytorch host heroku however run issue maximum slug size mb free version pytorch mb google search someone write find cpu version pytorch use much small however pretty lose person document advice appreciated thanks edit specific problem try instal torch far understand include requirement txt list torch dependency current torch however work bc size question know could write requirement file get cpu version torch small alternatively requirement txt work would instead get cpu version
83,83,10722064,Training a Neural Network with Reinforcement learning,"<p>I know the basics of feedforward neural networks, and how to train them using the backpropagation algorithm, but I'm looking for an algorithm than I can use for training an ANN online with reinforcement learning.</p>

<p>For example, the <a href=""http://www.google.com/search?q=cart%20pole%20swing%20up"" rel=""noreferrer"">cart pole swing up</a> problem is one I'd like to solve with an ANN. In that case, I don't know what should be done to control the pendulum, I only know how close I am to the ideal position. I need to have the ANN learn based on reward and punishment. Thus, supervised learning isn't an option.</p>

<p>Another situation is something like the <a href=""http://en.wikipedia.org/wiki/Snake_%28video_game%29"" rel=""noreferrer"">snake game</a>, where feedback is delayed, and limited to goals and anti-goals, rather than reward.</p>

<p>I can think of some algorithms for the first situation, like hill-climbing or genetic algorithms, but I'm guessing they would both be slow. They might also be applicable in the second scenario, but incredibly slow, and not conducive to online learning.</p>

<p>My question is simple: <strong>Is there a simple algorithm for training an artificial neural network with reinforcement learning?</strong> I'm mainly interested in real-time reward situations, but if an algorithm for goal-based situations is available, even better.</p>",10722300.0,2,1,,2012/5/23 14:27,42.0,2017/11/8 9:33,,,,,785745.0,,1,66,algorithm|language-agnostic|machine-learning|neural-network|reinforcement-learning,28687,83.2307,,0,train neural network reinforcement learning know basic feedforward neural network train use backpropagation algorithm look algorithm use train ann online reinforcement learn example cart pole swing problem one like solve ann case know control pendulum know close ideal position need ann learn base reward punishment thus supervise learning option another situation something like snake game feedback delay limit goal anti goal rather reward think algorithm first situation like hill climb genetic algorithm guess would slow might also applicable second scenario incredibly slow conducive online learn question simple simple algorithm train artificial neural network reinforcement learn mainly interested real time reward situation algorithm goal base situation available even well
411,411,47432168,Taking subsets of a pytorch dataset,"<p>I have a network which I want to train on some dataset (as an example, say <code>CIFAR10</code>). I can create data loader object via</p>

<pre><code>trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)
</code></pre>

<p>My question is as follows: Suppose I want to make several different training iterations. Let's say I want at first to train the network on all images in odd positions, then on all images in even positions and so on. In order to do that, I need to be able to access to those images. Unfortunately, it seems that <code>trainset</code> does not allow such access. That is, trying to do <code>trainset[:1000]</code> or more generally <code>trainset[mask]</code> will throw an error.</p>

<p>I could do instead </p>

<pre><code>trainset.train_data=trainset.train_data[mask]
trainset.train_labels=trainset.train_labels[mask]
</code></pre>

<p>and then</p>

<pre><code>trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                              shuffle=True, num_workers=2)
</code></pre>

<p>However, that will force me to create a new copy of the full dataset in each iteration (as I already changed <code>trainset.train_data</code> so I will need to redefine <code>trainset</code>). Is there some way to avoid it?</p>

<p>Ideally, I would like to have something ""equivalent"" to</p>

<pre><code>trainloader = torch.utils.data.DataLoader(trainset[mask], batch_size=4,
                                              shuffle=True, num_workers=2)
</code></pre>",47434173.0,2,0,,2017/11/22 10:22,8.0,2019/11/5 2:03,2017/11/22 10:39,,7214344.0,,7214344.0,,1,22,python|machine-learning|neural-network|torch|pytorch,33139,97.8814,,2,take subset pytorch dataset network want train dataset example say create data loader object via question follow suppose want make several different train iteration let say want first train network image odd position image even position order need able access image unfortunately seem allow access try generally throw error could instead however force create new copy full dataset iteration already change need redefine way avoid ideally would like something equivalent
742,742,40879504,How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"<p>Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the <a href=""https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/index.html"" rel=""noreferrer"">docs of tensorflow</a> on how to call the <code>tf.nn.dropout</code>. And here is my code</p>

<pre class=""lang-py prettyprint-override""><code># before proceeding further.
from __future__ import print_function
import numpy as np  
import tensorflow as tf
from six.moves import cPickle as pickle


pickle_file = 'notMNIST.pickle'

with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)


image_size = 28
num_labels = 10

def reformat(dataset, labels):
    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]
    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)
    return dataset, labels

    train_dataset, train_labels = reformat(train_dataset, train_labels)
    valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
    test_dataset, test_labels = reformat(test_dataset, test_labels)
    print('Training set', train_dataset.shape, train_labels.shape)
    print('Validation set', valid_dataset.shape, valid_labels.shape)
    print('Test set', test_dataset.shape, test_labels.shape)

    def accuracy(predictions, labels):
        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))  / predictions.shape[0])


# ReLU neuron
# param
training_epochs = 30
batch_size = 521
display_step = 1
n_input = 784 # img shape: 28*28
n_classes = 10 # MNIST total classes (0-9 digits)

# hyper-parameter
n_hidden_1 = 256 
learning_rate = 0.05
lambda_term = 0.01


graph = tf.Graph()
with graph.as_default():
    # init weights
    weights_hiden =  tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=np.sqrt(n_input)))
    weights_out = tf.Variable(tf.random_normal([n_hidden_1, n_classes], stddev=np.sqrt(n_hidden_1)))

    biases_hidden = tf.Variable(tf.random_normal([n_hidden_1]))
    biases_out = tf.Variable(tf.random_normal([n_classes]))

    x = tf.placeholder(""float"", [None, n_input])
    y = tf.placeholder(""float"", [None, n_classes])

    def model(x, weights_hiden, weights_out, biases_hidden, biases_out):
        # hidden layer with RELU activation
        layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights_hiden), biases_hidden))
        # apply DropOut to hidden layer
        keep_prob = tf.placeholder(tf.float32)  # DROP-OUT here
        drop_out = tf.nn.dropout(layer_1, keep_prob)  # DROP-OUT here
        # output layer with linear activation
        out_layer = tf.matmul(layer_1, weights_out) + biases_out
        return out_layer

    # Construct model
    pred = model(x, weights_hiden, weights_out, biases_hidden, biases_out)

    # Define loss and optimizer
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y) +
                          lambda_term * tf.nn.l2_loss(weights_hiden) + 
                          lambda_term * tf.nn.l2_loss(weights_out) +
                          lambda_term * tf.nn.l2_loss(biases_hidden) + 
                          lambda_term * tf.nn.l2_loss(biases_out))
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)


# run the graph
with tf.Session(graph=graph) as sess:
    tf.initialize_all_variables().run()
    print('Initialized')
    # Training cycle
    for epoch in range(training_epochs):
        avg_cost = 0.
        total_batch = int(train_dataset.shape[0]/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x = train_dataset[(i*batch_size):((i*batch_size) + batch_size), :]
            batch_y = train_labels[(i*batch_size):((i*batch_size) + batch_size), :]
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            # Compute average loss
            avg_cost += c / total_batch
        # Display logs per epoch step
        if epoch % display_step == 0:
            print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(avg_cost))
    print(""Optimization Finished!"")

    # Test model
    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
    # Calculate accuracy
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    print(""Test data accuracy:"", accuracy.eval({x: test_dataset, y: test_labels}))
    print(""Valid data accuracy:"", accuracy.eval({x: valid_dataset, y: valid_labels}))
</code></pre>

<p>The <code>tf.nn.dropout</code> is called in function <code>model()</code>, but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result:</p>

<pre><code>Epoch: 0001 cost= 579980.086977807
Epoch: 0002 cost= 238859.802382506
Epoch: 0003 cost= 90672.733752856
Epoch: 0004 cost= 32649.040985028
Epoch: 0005 cost= 11325.878361874
Epoch: 0006 cost= 3866.805511076
Epoch: 0007 cost= 1357.785540469
Epoch: 0008 cost= 519.381747333
Epoch: 0009 cost= 225.359804119
Epoch: 0010 cost= 110.099476707
Epoch: 0011 cost= 55.212384386
Epoch: 0012 cost= 28.469241683
Epoch: 0013 cost= 14.511494627
Epoch: 0014 cost= 6.567228943
Epoch: 0015 cost= 3.186372240
Epoch: 0016 cost= 1.701917576
Epoch: 0017 cost= 1.041632473
Epoch: 0018 cost= 0.843376874
Epoch: 0019 cost= 0.786183911
Epoch: 0020 cost= 0.775412846
Epoch: 0021 cost= 0.782965020
Epoch: 0022 cost= 0.796788171
Epoch: 0023 cost= 0.814522117
Epoch: 0024 cost= 0.832090579
Epoch: 0025 cost= 0.849197715
Epoch: 0026 cost= 0.867473578
Epoch: 0027 cost= 0.889561496
Epoch: 0028 cost= 0.921837020
Epoch: 0029 cost= 16.655304543
Epoch: 0030 cost= 1.421570476
Optimization Finished!
Test data accuracy: 0.8775
Valid data accuracy: 0.8069
</code></pre>

<p>How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!</p>",40881670.0,2,1,,2016/11/30 3:00,18.0,2017/8/1 13:54,2017/7/19 1:08,,5046896.0,,5046896.0,,1,41,neural-network|tensorflow|deep-learning,56616,104.412,,4,apply drop tensorflow improve accuracy neural network drop regularization technique want apply notmnist data reduce fitting finish udacity deep learning course assignment read doc tensorflow call code call function apply dropout technique neural network accuracy seem change result apply dropout tensorflow improve accuracy network thank
627,627,50920908,Get Confusion Matrix From a Keras Multiclass Model,"<p>I am building a multiclass model with Keras.</p>

<pre><code>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training
</code></pre>

<p>Here is how my test data looks like (it's text data).</p>

<pre><code>X_test
Out[25]: 
array([[621, 139, 549, ...,   0,   0,   0],
       [621, 139, 543, ...,   0,   0,   0]])

y_test
Out[26]: 
array([[0, 0, 1],
       [0, 1, 0]])
</code></pre>

<p>After generating predictions...</p>

<pre><code>predictions = model.predict(X_test)
predictions
Out[27]: 
array([[ 0.29071924,  0.2483743 ,  0.46090645],
       [ 0.29566404,  0.45295066,  0.25138539]], dtype=float32)
</code></pre>

<p>I did the following to get the confusion matrix.</p>

<pre><code>y_pred = (predictions &gt; 0.5)

confusion_matrix(y_test, y_pred)
Traceback (most recent call last):

  File ""&lt;ipython-input-38-430e012b2078&gt;"", line 1, in &lt;module&gt;
    confusion_matrix(y_test, y_pred)

  File ""/Users/abrahammathew/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py"", line 252, in confusion_matrix
    raise ValueError(""%s is not supported"" % y_type)

ValueError: multilabel-indicator is not supported
</code></pre>

<p>However, I am getting the above error.</p>

<p>How can I get a confusion matrix when doing a multiclass neural network in Keras?</p>",50921588.0,1,0,,2018/6/19 4:58,11.0,2020/6/22 4:32,2020/6/22 4:32,,6013016.0,,2725751.0,,1,33,python|keras|scikit-learn|multiclass-classification,79027,75.1911,2019/5/27 13:22,5,get confusion matrix keras multiclass model build multiclass model kera test data look like text data generate prediction following get confusion matrix however get error get confusion matrix multiclass neural network kera
27,27,53998282,"How does the ""number of workers"" parameter in PyTorch dataloader actually work?","<ol>
<li>If <code>num_workers</code> is 2, Does that mean that it will put 2 batches in the RAM and send 1 of them to the GPU or Does it put 3 batches in the RAM then sends 1 of them to the GPU?</li>
<li>What does actually happen when the number of workers is higher than the number of CPU cores? I tried it and it worked fine but How does it work? (I thought that the maximum number of workers I can choose is the number of cores).</li>
<li>If I set <code>num_workers</code> to 3 and during the training there were no batches in the memory for the GPU, Does the main process waits for its workers to read the batches or Does it read a single batch (without waiting for the workers)?</li>
</ol>",54002191.0,1,1,,2019/1/1 19:23,16.0,2020/10/12 18:14,2020/10/12 18:14,,1333610.0,,4931135.0,,1,49,python|memory-management|deep-learning|pytorch|ram,32123,86.8273,,3,number worker parameter pytorch dataloader actually work mean put batch ram send gpu put batch ram send gpu actually happen number worker high number cpu core try work fine work think maximum number worker choose number core set training batch memory gpu main process wait worker read batch read single batch without wait worker
510,510,34518656,How to interpret loss and accuracy for a machine learning model,"<p>When I trained my neural network with Theano or Tensorflow, they will report a variable called ""loss"" per epoch.</p>

<p>How should I interpret this variable? Higher loss is better or worse, or what does it mean for the final performance (accuracy) of my neural network?</p>",34519264.0,3,1,,2015/12/29 20:33,92.0,2021/3/28 11:44,2021/3/28 11:44,,4685471.0,,5492392.0,,1,245,machine-learning|neural-network|mathematical-optimization|deep-learning|objective-function,213941,535.321,2021/2/10 12:34,0,interpret loss accuracy machine learning model train neural network theano tensorflow report variable call loss per epoch interpret variable high loss well bad mean final performance accuracy neural network
234,234,44369938,Openai gym environment for multi-agent games,"<p>Is it possible to use <a href=""https://openai.com/"" rel=""noreferrer"">openai</a>'s <a href=""https://gym.openai.com/docs"" rel=""noreferrer"">gym environments</a> for multi-agent games? Specifically, I would like to model a card game with four players (agents). The player scoring a turn starts the next turn. How would I model the necessary coordination between the players (e.g. who's turn it is next)? Ultimately, I would like to use reinforcement learning on four agents that play against each other.</p>",57329702.0,3,2,,2017/6/5 13:19,7.0,2019/8/2 19:07,,,,,1228765.0,,1,24,reinforcement-learning|openai-gym,8741,52.1662,,3,openai gym environment multi agent game possible use openai gym environment multi agent game specifically would like model card game four player agents player score turn start next turn would model necessary coordination player e g turn next ultimately would like use reinforcement learn four agent play
142,142,42504669,Keras + Tensorflow and Multiprocessing in Python,"<p>I'm using Keras with Tensorflow as backend.</p>

<p>I am trying to save a model in my main process and then load/run (i.e. call <code>model.predict</code>) within another process.</p>

<p>I'm currently just trying the naive approach from the docs to save/load the model: <a href=""https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"" rel=""noreferrer"">https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model</a>.<br>
So basically:</p>

<ol>
<li><code>model.save()</code> in main process</li>
<li><code>model = load_model()</code> in child process</li>
<li><code>model.predict()</code> in child process</li>
</ol>

<p>However, it simply hangs on the <code>load_model</code> call.</p>

<p>Searching around I've discovered this potentially related answer suggesting that Keras can only be utilized in one process: <a href=""https://stackoverflow.com/questions/38176827/using-multiprocessing-with-theano"">using multiprocessing with theano</a> but am unsure if this is true (can't seem to find much on this).</p>

<p>Is there a way to accomplish my goal?  A high level description or short example is greatly appreciated.</p>

<p>Note: I've attempted approaches along the lines of passing a graph to the process but failed since it seems tensorflow graphs aren't pickable (related SO post for that here: <a href=""https://stackoverflow.com/questions/34900246/tensorflow-passing-a-session-to-a-python-multiprocess"">Tensorflow: Passing a session to a python multiprocess</a>).  If there is indeed a way to pass the tensorflow graph/model to the child process then I am open to that as well.</p>

<p>Thanks!</p>",42506478.0,3,0,,2017/2/28 9:20,24.0,2018/12/18 19:27,2017/5/23 11:47,,-1.0,,3431836.0,,1,41,python|tensorflow|neural-network|keras|python-multiprocessing,32129,105.628,,3,kera tensorflow multiprocessing python use kera tensorflow backend try save model main process load run e call within another process currently try naive approach doc save load model save keras model basically main process child process child process however simply hang call search around discover potentially related answer suggest kera utilize one process use multiprocessing theano unsure true seem find much way accomplish goal high level description short example greatly appreciated note attempt approach along line pass graph process fail since seem tensorflow graph pickable relate post tensorflow pass session python multiprocess indeed way pass tensorflow graph model child process open well thanks
370,370,46135499,How to Properly Combine TensorFlow's Dataset API and Keras?,"<p>Keras' <code>fit_generator()</code> model method expects a generator which produces tuples of the shape (input, targets), where both elements are NumPy arrays. <a href=""https://keras.io/models/model/"" rel=""noreferrer"">The documentation</a> seems to imply that if I simply wrap a <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer""><code>Dataset</code> iterator</a> in a generator, and make sure to convert the Tensors to NumPy arrays, I should be good to go. This code, however, gives me an error:</p>

<pre><code>import numpy as np
import os
import keras.backend as K
from keras.layers import Dense, Input
from keras.models import Model
import tensorflow as tf
from tensorflow.contrib.data import Dataset

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

with tf.Session() as sess:
    def create_data_generator():
        dat1 = np.arange(4).reshape(-1, 1)
        ds1 = Dataset.from_tensor_slices(dat1).repeat()

        dat2 = np.arange(5, 9).reshape(-1, 1)
        ds2 = Dataset.from_tensor_slices(dat2).repeat()

        ds = Dataset.zip((ds1, ds2)).batch(4)
        iterator = ds.make_one_shot_iterator()
        while True:
            next_val = iterator.get_next()
            yield sess.run(next_val)

datagen = create_data_generator()

input_vals = Input(shape=(1,))
output = Dense(1, activation='relu')(input_vals)
model = Model(inputs=input_vals, outputs=output)
model.compile('rmsprop', 'mean_squared_error')
model.fit_generator(datagen, steps_per_epoch=1, epochs=5,
                    verbose=2, max_queue_size=2)
</code></pre>

<p>Here's the error I get:</p>

<pre><code>Using TensorFlow backend.
Epoch 1/5
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 270, in __init__
    fetch, allow_tensor=True, allow_operation=True))
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2708, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2787, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""IteratorGetNext:0"", shape=(?, 1), dtype=int64) is not an element of this graph.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jsaporta/anaconda3/lib/python3.6/threading.py"", line 916, in _bootstrap_inner
    self.run()
  File ""/home/jsaporta/anaconda3/lib/python3.6/threading.py"", line 864, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/keras/utils/data_utils.py"", line 568, in data_generator_task
    generator_output = next(self._generator)
  File ""./datagen_test.py"", line 25, in create_data_generator
    yield sess.run(next_val)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1109, in _run
    self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 413, in __init__
    self._fetch_mapper = _FetchMapper.for_fetch(fetches)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 233, in for_fetch
    return _ListFetchMapper(fetch)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 340, in __init__
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 340, in &lt;listcomp&gt;
    self._mappers = [_FetchMapper.for_fetch(fetch) for fetch in fetches]
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 241, in for_fetch
    return _ElementFetchMapper(fetches, contraction_fn)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 277, in __init__
    'Tensor. (%s)' % (fetch, str(e)))
ValueError: Fetch argument &lt;tf.Tensor 'IteratorGetNext:0' shape=(?, 1) dtype=int64&gt; cannot be interpreted as a Tensor. (Tensor Tensor(""IteratorGetNext:0"", shape=(?, 1), dtype=int64) is not an element of this graph.)

Traceback (most recent call last):
  File ""./datagen_test.py"", line 34, in &lt;module&gt;
    verbose=2, max_queue_size=2)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/home/jsaporta/anaconda3/lib/python3.6/site-packages/keras/engine/training.py"", line 2011, in fit_generator
    generator_output = next(output_generator)
StopIteration
</code></pre>

<p>Strangely enough, adding a line containing <code>next(datagen)</code> directly after where I initialize <code>datagen</code> causes the code to run just fine, with no errors.</p>

<p>Why does my original code not work? Why does it begin to work when I add that line to my code? Is there a more efficient way to use TensorFlow's Dataset API with Keras that doesn't involve converting Tensors to NumPy arrays and back again?</p>",46140332.0,6,6,,2017/9/9 22:02,39.0,2019/5/20 3:36,,,,,4444582.0,,1,55,tensorflow|keras,32793,188.063,,3,properly combine tensorflow dataset api keras kera model method expect generator produce tuples shape input target element numpy array documentation seem imply simply wrap iterator generator make sure convert tensor numpy array good go code however give error error get strangely enough add line contain directly initialize cause code run fine error original code work begin work add line code efficient way use tensorflow dataset api kera involve convert tensor numpy array back
445,445,48198031,How to add variables to progress bar in Keras?,"<p>I'd like to monitor eg. the learning rate during training in Keras both in the progress bar and in Tensorboard. I figure there must be a way to specify which variables are logged, but there's no immediate clarification on this issue on the Keras <a href=""https://keras.io"" rel=""noreferrer"">website</a>.</p>

<p>I guess it's got something to do with creating a custom <a href=""https://keras.io/callbacks"" rel=""noreferrer"">Callback</a> function, however, it should be possible to modify the already existing progress bar callback, no?</p>",48206009.0,3,0,,2018/1/11 0:06,15.0,2020/10/9 12:46,2020/5/6 14:51,,3924118.0,,4838303.0,,1,16,python|keras|tensorboard|keras-2,5562,65.5809,,5,add variable progress bar kera like monitor eg learning rate training kera progress bar tensorboard figure must way specify variable log immediate clarification issue kera website guess get something create custom callback function however possible modify already exist progress bar callback
280,280,44931347,Keras: Lambda layer function with multiple parameters,"<p>I am trying to write a <code>Lambda</code> layer in Keras which calls a function <code>connection</code>, that runs a loop <code>for i in range(0,k)</code> where <code>k</code> is fed in as an input to the function, <code>connection(x,k)</code>. Now, when I try to call the function in the Functional API, I tried using:</p>

<pre><code>k = 5
y = Lambda(connection)(x)
</code></pre>

<p>Also,</p>

<pre><code>y = Lambda(connection)(x,k)
</code></pre>

<p>But neither of those approaches worked. How can I feed in the value of <code>k</code> without assigning it as a global parameter?</p>",51244109.0,3,2,,2017/7/5 16:16,3.0,2020/3/25 9:30,,,,,8021672.0,,1,14,python|neural-network|keras|keras-layer,12845,59.8349,,3,kera lambda layer function multiple parameter try write layer kera call function run loop feed input function try call function functional api try use also neither approach work feed value without assign global parameter
36,36,54473254,CuDNNLSTM: UnknownError: Fail to find the dnn implementation,"<p>I have run the model with LSTM as the first layer successfully. But out of curiosity, I replace LSTM with CuDNNLSTM.
But after model.fit, it replied the following error message:</p>

<pre><code>UnknownError: Fail to find the dnn implementation.
    [[{{node cu_dnnlstm_5/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, _class=[""loc:@training_2/Adam/gradients/cu_dnnlstm_5/CudnnRNN_grad/CudnnRNNBackprop""], direction=""unidirectional"", dropout=0, input_mode=""linear_input"", is_training=true, rnn_mode=""lstm"", seed=87654321, seed2=0, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](cu_dnnlstm_5/transpose, cu_dnnlstm_5/ExpandDims_1, cu_dnnlstm_5/ExpandDims_1, cu_dnnlstm_5/concat_1)]]
    [[{{node metrics_3/mean_squared_error/Mean_1/_1877}} = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_4852_metrics_3/mean_squared_error/Mean_1"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
</code></pre>

<p>I have tried TestCudnnLSTM() on this <a href=""https://github.com/keras-team/keras/issues/8135"" rel=""noreferrer"">discussion</a> and pass the test successfully:</p>

<pre>
Keras version: 2.2.4
Tensorflow version: 1.12.0
Creating Model
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
cu_dnnlstm_1 (CuDNNLSTM)     (None, 1000, 1)           16        
=================================================================
Total params: 16
Trainable params: 16
Non-trainable params: 0
_________________________________________________________________
None
Model compiled
</pre>

<p>It seems that the problem appears during model fitting. But I don't know exactly what is the problem?</p>",,8,1,,2019/2/1 5:13,3.0,2021/8/7 13:36,,,,,10977076.0,,1,15,lstm|cudnn,13588,81.5326,,4,cudnnlstm unknownerror fail find dnn implementation run model lstm first layer successfully curiosity replace lstm cudnnlstm model fit reply following error message try testcudnnlstm discussion pass test successfully keras version tensorflow version create model layer type output shape param cu dnnlstm cudnnlstm none total params trainable params non trainable params none model compile seem problem appear model fit know exactly problem
695,695,38972380,Keras: How to use fit_generator with multiple outputs of different type,"<p>In a Keras model with the Functional API I need to call fit_generator to train on augmented images data using an ImageDataGenerator.<br />
The problem is my model has two outputs: the mask I'm trying to predict and a binary value.<br />
I obviously only want to augment the input and the mask output and not the binary value.<br />
How can I achieve this?</p>",41872896.0,3,0,,2016/8/16 10:22,10.0,2021/2/19 14:32,2021/2/19 14:32,,1490721.0,,1490721.0,,1,22,python|deep-learning|keras,20225,76.4236,,3,keras use fit generator multiple output different type keras model functional api need call fit generator train augmented image data use imagedatagenerator problem model two output mask try predict binary value obviously want augment input mask output binary value achieve
769,769,51748138,pytorch how to set .requires_grad False,"<p>I want to set some of my model frozen. Following the official docs: </p>

<pre class=""lang-python prettyprint-override""><code>with torch.no_grad():
    linear = nn.Linear(1, 1)
    linear.eval()
    print(linear.weight.requires_grad)
</code></pre>

<p>But it prints <code>True</code> instead of <code>False</code>. If I want to set the model in eval mode, what should I do?</p>",51753038.0,5,1,,2018/8/8 13:36,10.0,2020/12/8 16:46,2018/8/8 16:55,,624547.0,,4503781.0,,1,32,python|pytorch|gradient-descent,68288,119.337,,5,pytorch set require grad false want set model frozen follow official doc print instead want set model eval mode
617,617,50659482,Why can't I get reproducible results in Keras even though I set the random seeds?,"<p>I'm training a MobileNet architecture on dummy data with Keras, on Mac OSX. I set both <code>nump.random</code> and <code>tensorflow.set_random_seed</code>, but for some reasons I can't get reproducible results: each time I rerun the code, I get different results. Why? This is not due to the GPU, because I'm running on a MacBook Pro 2017 which has a Radeon graphics card, thus Tensorflow doesn't take advantage of it. The code is run with</p>

<pre><code>python Keras_test.py
</code></pre>

<p>so it's not a problem of state (I'm not using Jupyter or IPython: the environment should be reset each time I run the code). </p>

<p><strong>EDIT</strong>: I changed my code by moving the setting of all seeds <em>before</em> importing Keras. The results are still not deterministic, however the variance in results is much smaller than it was before. This is very bizarre.</p>

<p>The current model is very small (as far as deep neural network go) without being trivial, it doesn't need a GPU to run and it trains in a few minutes on a modern laptop, so repeating my experiments is within anyone's reach. I invite you to do it: I'd be very interested in learning about the level of variation from a system to another.</p>

<pre><code>import numpy as np
# random seeds must be set before importing keras &amp; tensorflow
my_seed = 512
np.random.seed(my_seed)
import random 
random.seed(my_seed)
import tensorflow as tf
tf.set_random_seed(my_seed)

# now we can import keras
import keras.utils
from keras.applications import MobileNet
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
import os

height = 224
width = 224
channels = 3
epochs = 10
num_classes = 10



# Generate dummy data
batch_size = 32  
n_train = 256
n_test = 64
x_train = np.random.random((n_train, height, width, channels))
y_train = keras.utils.to_categorical(np.random.randint(num_classes, size=(n_train, 1)), num_classes=num_classes)
x_test = np.random.random((n_test, height, width, channels))
y_test = keras.utils.to_categorical(np.random.randint(num_classes, size=(n_test, 1)), num_classes=num_classes)
# Get input shape
input_shape = x_train.shape[1:]

# Instantiate model 
model = MobileNet(weights=None,
                  input_shape=input_shape,
                  classes=num_classes)

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
# Viewing Model Configuration
model.summary()

# Model file name
filepath = 'model_epoch_{epoch:02d}_loss_{loss:0.2f}_val_{val_loss:.2f}.hdf5'

# Define save_best_only checkpointer
checkpointer = ModelCheckpoint(filepath=filepath,
                             monitor='val_acc',
                             verbose=1,
                             save_best_only=True)

# Let's fit!
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_test, y_test),
          callbacks=[checkpointer])
</code></pre>

<p>As always, here are my Python, Keras &amp; Tensorflow versions:</p>

<pre><code>python -c 'import keras; import tensorflow; import sys; print(sys.version, 'keras.__version__', 'tensorflow.__version__')'
/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
('2.7.15 |Anaconda, Inc.| (default, May  1 2018, 18:37:05) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]', '2.1.6', '1.8.0')
</code></pre>

<p>Here are some results obtained running this code multiple times: as you can see, the code saves the best model (best validation accuracy) out of 10 epochs with a descriptive filename, so comparing filenames across different runs allows to judge the variability in results.</p>

<pre><code>model_epoch_01_loss_2.39_val_3.28.hdf5
model_epoch_01_loss_2.39_val_3.54.hdf5
model_epoch_01_loss_2.40_val_3.47.hdf5
model_epoch_01_loss_2.41_val_3.08.hdf5
</code></pre>",52897289.0,2,8,,2018/6/2 17:31,9.0,2020/2/27 18:31,2018/6/3 15:57,,1711271.0,,1711271.0,,1,14,python|tensorflow|keras,18245,60.6446,,4,get reproducible result keras even though set random seed train mobilenet architecture dummy data kera mac osx set reason get reproducible result time rerun code get different result due gpu run macbook pro radeon graphic card thus tensorflow take advantage code run problem state use jupyter ipython environment reset time run code edit change code move setting seed import kera result still deterministic however variance result much small bizarre current model small far deep neural network go without trivial need gpu run train minute modern laptop repeating experiment within anyone reach invite interested learn level variation system another always python kera tensorflow version result obtain run code multiple time see code save best model best validation accuracy epoch descriptive filename comparing filename across different run allows judge variability result
214,214,43979449,"Higher validation accuracy, than training accurracy using Tensorflow and Keras","<p>I'm trying to use deep learning to predict income from 15 self reported attributes from a dating site.</p>

<p>We're getting rather odd results, where our validation data is getting better accuracy and lower loss, than our training data. And this is consistent across different sizes of hidden layers.
This is our model:</p>

<pre class=""lang-py prettyprint-override""><code>for hl1 in [250, 200, 150, 100, 75, 50, 25, 15, 10, 7]:
    def baseline_model():
        model = Sequential()
        model.add(Dense(hl1, input_dim=299, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l1_l2(0.001)))
        model.add(Dropout(0.5, seed=seed))
        model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))

        model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
        return model

    history_logs = LossHistory()
    model = baseline_model()
    history = model.fit(X, Y, validation_split=0.3, shuffle=False, epochs=50, batch_size=10, verbose=2, callbacks=[history_logs])
</code></pre>

<p>And this is an example of the accuracy and losses:
<img src=""https://i.stack.imgur.com/0TfSp.png"" alt=""Accuracy with hidden layer of 250 neurons""> and <img src=""https://i.stack.imgur.com/TH6Nr.png"" alt=""the loss"">.</p>

<p>We've tried to remove regularization and dropout, which, as expected, ended in overfitting (training acc: ~85%). We've even tried to decrease the learning rate drastically, with similiar results.</p>

<p>Has anyone seen similar results?</p>",43982656.0,6,1,,2017/5/15 12:22,27.0,2021/5/26 17:34,2019/5/7 10:54,,11114701.0,,2200569.0,,1,61,tensorflow|machine-learning|neural-network|keras|classification,48444,240.941,,4,high validation accuracy train accurracy use tensorflow kera try use deep learning predict income self report attribute date site get rather odd result validation data get well accuracy low loss training data consistent across different size hidden layer model example accuracy loss try remove regularization dropout expect end overfitting train acc even try decrease learning rate drastically similiar result anyone see similar result
678,678,38189070,How do I create a variable-length input LSTM in Keras?,"<p>I am trying to do some vanilla pattern recognition with an LSTM using Keras to predict the next element in a sequence.</p>

<p>My data look like this:</p>

<p><a href=""https://i.stack.imgur.com/NnEvI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NnEvI.png"" alt=""My data""></a></p>

<p>where the label of the training sequence is the last element in the list: <code>X_train['Sequence'][n][-1]</code>.</p>

<p>Because my <code>Sequence</code> column can have a variable number of elements in the sequence, I believe an RNN to be the best model to use. Below is my attempt to build an LSTM in Keras:</p>

<pre><code># Build the model

# A few arbitrary constants...
max_features = 20000
out_size = 128

# The max length should be the length of the longest sequence (minus one to account for the label)
max_length = X_train['Sequence'].apply(len).max() - 1

# Normal LSTM model construction with sigmoid activation
model = Sequential()
model.add(Embedding(max_features, out_size, input_length=max_length, dropout=0.2))
model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>And here's how I attempt to train my model:</p>

<pre><code># Train the model
for seq in X_train['Sequence']:
    print(""Length of training is {0}"".format(len(seq[:-1])))
    print(""Training set is {0}"".format(seq[:-1]))
    model.fit(np.array([seq[:-1]]), [seq[-1]])
</code></pre>

<p>My output is this:</p>

<pre><code>Length of training is 13
Training set is [1, 3, 13, 87, 1053, 28576, 2141733, 508147108, 402135275365, 1073376057490373, 9700385489355970183, 298434346895322960005291, 31479360095907908092817694945]
</code></pre>

<p>However, I get the following error:</p>

<pre><code>Exception: Error when checking model input: expected embedding_input_1 to have shape (None, 347) but got array with shape (1, 13)
</code></pre>

<p>I believe my training step is correctly setup, so my model construction must be wrong. Note that 347 is <code>max_length</code>.</p>

<p>How can I correctly build a variable-length input LSTM in Keras? I'd prefer not to pad the data. Not sure if it's relevant, but I'm using the Theano backend.</p>",,4,2,,2016/7/4 16:30,26.0,2021/2/1 19:10,2016/7/7 10:30,,2883245.0,,2883245.0,,1,49,python-3.x|keras|lstm|recurrent-neural-network|variable-length,45957,91.8494,,3,create variable length input lstm kera try vanilla pattern recognition lstm use kera predict next element sequence data look like label training sequence last element list column variable number element sequence believe rnn best model use attempt build lstm kera attempt train model output however get following error believe training step correctly setup model construction must wrong note correctly build variable length input lstm kera prefer pad data sure relevant use theano backend
665,665,37659538,Custom padding for convolutions in TensorFlow,"<p>In tensorflow function <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#conv2d"" rel=""noreferrer"">tf.nn.conv2d</a>, the padding option just has 'SAME' and 'VALID'. </p>

<p>But in the conv layer of Caffe, there is <a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"" rel=""noreferrer"">pad option</a> can define the number of pixels to (implicitly) add to each side of the input.</p>

<p>How to achieve that in Tensorflow?</p>

<p>Thank you very much.</p>",37660000.0,1,0,,2016/6/6 14:04,6.0,2017/10/23 3:24,2016/6/6 16:19,,5098368.0,,5651936.0,,1,16,python|tensorflow|caffe,13123,54.6721,,3,custom pad convolution tensorflow tensorflow function tf nn conv padding option valid conv layer caffe pad option define number pixel implicitly add side input achieve tensorflow thank much
736,736,40708169,How to initialize biases in a Keras model?,"<p>I am trying to build a synthetic model in Keras, and I need to assign values for the weights and biases. Assigning the weights is easy, I am using the instructions provided here: <a href=""https://keras.io/initializations/"" rel=""noreferrer"">https://keras.io/initializations/</a>.
However, I could not find any instructions on how to assign the biases. Any ideas?</p>",41688653.0,4,0,,2016/11/20 19:04,2.0,2018/11/29 19:18,,,,,6640916.0,,1,16,python|neural-network|deep-learning|keras,30511,71.7378,,3,initialize bias keras model try build synthetic model kera need assign value weight bias assign weight easy use instruction provide however could find instruction assign bias idea
601,601,49969006,save and load keras.callbacks.History,"<p>I'm training a deep neural net using Keras and looking for a way to save and later load the history object which is of <code>keras.callbacks.History</code> type. Here's the setup:</p>

<pre><code>history_model_1 = model_1.fit_generator(train_generator,
                          steps_per_epoch=100,
                          epochs=20,
                          validation_data=validation_generator,
                          validation_steps=50)
</code></pre>

<p><code>history_model_1</code> is the variable I want to be saved and loaded during another Python session.  </p>",50005651.0,3,3,,2018/4/22 17:55,7.0,2020/4/7 12:14,,,,,8201676.0,,1,15,python|deep-learning|keras|generator,16147,60.8324,,3,save load kera callback history train deep neural net use kera look way save later load history object type setup variable want save load another python session
221,221,44131295,keras - cannot import name Conv2D,"<p>I recently got the deep learning docker from <a href=""https://github.com/floydhub/dl-docker"" rel=""noreferrer"">https://github.com/floydhub/dl-docker</a> running and while trying out the tutorials, received an error when importing the keras layers module. </p>

<pre><code>from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-13-3a12c6f32fcf&gt; in &lt;module&gt;()
      5 from keras.models import Sequential
      6 from keras.layers import Dense, Dropout, Activation, Flatten
----&gt; 7 from keras.layers import Conv2D, MaxPooling2D

ImportError: cannot import name Conv2D
</code></pre>

<p>I am running with ubuntu 14.04, python version 2.7.6 on the ipython notebook and the following versions of the deep learning libraries on docker.</p>

<pre><code>ARG THEANO_VERSION=rel-0.8.2
ARG TENSORFLOW_VERSION=0.12.1 
ARG TENSORFLOW_ARCH=cpu
ARG KERAS_VERSION=1.2.0
ARG LASAGNE_VERSION=v0.1
ARG TORCH_VERSION=latest
ARG CAFFE_VERSION=master
</code></pre>

<p>Im not sure if the problem lies with the version because it seems that there no related issues on the github thread.</p>",44131602.0,3,1,,2017/5/23 9:46,3.0,2020/4/1 5:55,2018/3/13 7:55,,5359882.0,,6467567.0,,1,17,python|tensorflow|neural-network|keras|theano,30281,58.1247,,1,kera import name conv recently get deep learning docker run try tutorial receive error import kera layer module run ubuntu python version ipython notebook following version deep learning library docker im sure problem lie version seem related issue github thread
172,172,43035827,What's the difference between a bidirectional LSTM and an LSTM?,"<p>Can someone please explain this? I know bidirectional LSTMs have a forward and backward pass but what is the advantage of this over a unidirectional LSTM?</p>

<p>What is each of them better suited for?</p>",44082853.0,5,0,,2017/3/26 23:31,41.0,2020/3/22 12:12,,,,,2123328.0,,1,78,machine-learning|neural-network|keras|lstm|recurrent-neural-network,67109,264.307,,3,difference bidirectional lstm lstm someone please explain know bidirectional lstms forward backward pas advantage unidirectional lstm well suit
97,97,41454511,TensorFlow: how is dataset.train.next_batch defined?,"<p>I am trying to learn TensorFlow and studying the example at: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb</a></p>

<p>I then have some questions in the code below:</p>

<pre><code>for epoch in range(training_epochs):
    # Loop over all batches
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop) and cost op (to get loss value)
        _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})
    # Display logs per epoch step
    if epoch % display_step == 0:
        print(""Epoch:"", '%04d' % (epoch+1),
              ""cost="", ""{:.9f}"".format(c))
</code></pre>

<p>Since mnist is just a dataset, what exactly does <code>mnist.train.next_batch</code> mean? How was the <code>dataset.train.next_batch</code> defined?</p>

<p>Thanks!</p>",41454722.0,1,0,,2017/1/4 0:40,7.0,2017/1/4 1:07,,,,,3993270.0,,1,18,python-3.x|tensorflow|neural-network|autoencoder,28675,50.43,,3,tensorflow dataset train next batch define try learn tensorflow study example question code since mnist dataset exactly mean defined thanks
169,169,42986405,How to speed up Gensim Word2vec model load time?,"<p>I'm building a chatbot so I need to vectorize the user's input using Word2Vec. </p>

<p>I'm using a pre-trained model with 3 million words by Google (GoogleNews-vectors-negative300).</p>

<p>So I load the model using Gensim:</p>

<pre><code>import gensim
model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)
</code></pre>

<p>The problem is that it takes about 2 minutes to load the model. I can't let the user wait that long.</p>

<p>So what can I do to speed up the load time?</p>

<p>I thought about putting each of the 3 million words and their corresponding vector into a MongoDB database. That would certainly speed things up but intuition tells me it's not a good idea.</p>",43067907.0,4,0,,2017/3/23 20:30,17.0,2019/10/17 10:36,2017/3/29 18:42,,4333199.0,,4333199.0,,1,24,deep-learning|gensim|word2vec,23740,109.702,,3,speed gensim word vec model load time build chatbot need vectorize user input use word vec use pre train model million word google googlenews vector negative load model use gensim problem take minute load model let user wait long speed load time think put million word corresponding vector mongodb database would certainly speed thing intuition tell good idea
219,219,44068899,What is the difference between Keras and tf.keras in TensorFlow 1.1+?,"<p>Now that TensorFlow 1.1 supports the Keras API under <code>tf.contrib.keras</code>, which one should I use if I intend to use Keras with a TF backend?</p>

<p>Is the <code>tf.contrib.keras</code> version different in any way than a regular Keras distribution? (TF specific optimizations of internal data structures come to mind). Is there any benefit in terms of using Keras and TensorFlow Core together if I use one or the other?</p>

<p>Or is <code>tf.contrib.keras</code> simply a copy of the same codebase as Keras but under a different namespace?</p>",44069109.0,4,0,,2017/5/19 11:29,9.0,2019/11/14 8:47,2019/4/3 8:55,,1735003.0,,72583.0,,1,37,tensorflow|keras,10534,83.6904,,3,difference kera tf kera tensorflow tensorflow support kera api one use intend use kera tf backend version different way regular kera distribution tf specific optimization internal data structure come mind benefit term use kera tensorflow core together use one simply copy codebase kera different namespace
270,270,44796793,Difference between tf.clip_by_value and tf.clip_by_global_norm for RNN's and how to decide max value to clip on?,<p>Want to understand the difference in roles of <code>tf.clip_by_value</code> and <code>tf.clip_by_global_norm</code> during the implementation of Gradient Clipping in TensorFlow. Which one is preferred and how to decide the max value to clip on?</p>,44798131.0,1,0,,2017/6/28 8:00,8.0,2020/2/22 10:05,2020/2/22 10:05,,5634636.0,,7730199.0,,1,22,python|tensorflow|deep-learning,10234,60.4402,,3,difference tf clip value tf clip global norm rnn decide max value clip want understand difference role implementation gradient clipping tensorflow one prefer decide max value clip
378,378,46260775,What is a policy in reinforcement learning?,"<p>I've seen such words as:</p>

<blockquote>
  <p>A policy defines the learning agent's way of behaving at a given time. Roughly
  speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.</p>
</blockquote>

<p>But still didn't fully understand. What exactly is a policy in reinforcement learning?</p>",46265324.0,3,0,,2017/9/17 4:52,19.0,2020/5/25 19:10,2019/9/19 17:41,,712995.0,,2844907.0,,1,38,machine-learning|terminology|reinforcement-learning|markov-decision-process,33142,140.882,2017/9/19 17:27,0,policy reinforcement learning see word policy define learning agent way behave give time roughly speak policy mapping perceive state environment action take state still fully understand exactly policy reinforcement learning
399,399,47137061,"Early stopping with tf.estimator, how?","<p>I'm using <code>tf.estimator</code> in TensorFlow 1.4 and <code>tf.estimator.train_and_evaluate</code> is great but I need early stopping. What's the prefered way of adding that?</p>

<p>I assume there is some <code>tf.train.SessionRunHook</code> somewhere for this. I saw that there was an old contrib package with a <code>ValidationMonitor</code> that seemed to have early stopping, but it doesn't seem to be around anymore in 1.4. Or will the preferred way in the future be to rely on <code>tf.keras</code> (with which early stopping is really easy) instead of <code>tf.estimator/tf.layers/tf.data</code>, perhaps?</p>",51282062.0,4,0,,2017/11/6 12:28,12.0,2019/9/3 16:33,2018/6/16 6:54,,712995.0,,7287271.0,,1,22,python|tensorflow|neural-network|keras|tensorflow-estimator,12399,71.9735,,3,early stop tf estimator use tensorflow great need early stop prefered way add assume somewhere saw old contrib package seem early stopping seem around anymore preferred way future rely early stopping really easy instead perhaps
46,46,54916135,What is the class definition of nn.Linear in PyTorch?,"<p>I have the following code for PyTorch:</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Network(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(784, 256)
        self.output = nn.Linear(256, 10)
    
    def forward(self, x):
        x = F.sigmoid(self.hidden(x))
        x = F.softmax(self.output(x), dim=1)
    
        return x
</code></pre>
<p>My question: What is this <code>self.hidden</code>?</p>
<p>It returns from <code>nn.Linear</code> and it can take <code>x</code> as argument. What exactly is the purpose of <code>self.hidden</code>?</p>",54924812.0,2,0,,2019/2/27 23:32,15.0,2020/11/7 3:45,2020/11/7 3:45,,3250829.0,,5865579.0,,1,20,python|class|deep-learning|neural-network|pytorch,41675,80.4795,,3,class definition nn linear pytorch following code pytorch question return take argument exactly purpose
84,84,12411197,Can evolutionary computation be a method of reinforcement learning?,"<p>What is <em>evolutionary computation</em>? Is it a method of reinforcement learning? Or a separate method of machine learning? Or maybe none?</p>

<p>Please, cite references used to answer this question.</p>",12785302.0,5,0,,2012/9/13 16:56,3.0,2018/1/25 0:44,2018/1/25 0:44,,3924118.0,,906350.0,,1,13,machine-learning|artificial-intelligence|reinforcement-learning|evolutionary-algorithm,6899,54.3551,,0,evolutionary computation method reinforcement learning evolutionary computation method reinforcement learning separate method machine learning maybe none please cite reference use answer question
390,390,46663013,What is y_true and y_pred when creating a custom metric in Keras?,"<p>I want to implement my custom metric in Keras. According to the documentation, my custom metric should be defined as a function that takes as input two tensors, <code>y_pred</code> and <code>y_true</code>, and returns a single tensor value. </p>

<p>However, I'm confused to what exactly will be contained in these tensors <code>y_pred</code> and <code>y_true</code> when the optimization is running. Is it just one data point? Is it the whole batch? The whole epoch (probably not)? Is there a way to obtain these tensors' shapes?</p>

<p>Can someone point to a trustworthy place where I can get this information? Any help would be appreciated. Not sure if relevant, but I'm using TensorFlow backend.</p>

<hr>

<p>Things I tried so far, in order to answer this:</p>

<ul>
<li>Checking the <a href=""https://keras.io/metrics/"" rel=""noreferrer"">Keras metrics documentation</a> (no explanation there about what these tensors are).</li>
<li>Checking the <a href=""https://github.com/fchollet/keras/blob/master/keras/metrics.py"" rel=""noreferrer"">source code for the Keras metrics</a> and trying to understand these tensors by looking at the Keras implementation for other metrics (This seems to suggest that <code>y_true</code> and <code>y_pred</code> have the labels for an entire batch, but I'm not sure).</li>
<li>Reading these stackoverflow questions: <a href=""https://stackoverflow.com/questions/45961428/make-a-custom-loss-function-in-keras"">1</a>, <a href=""https://stackoverflow.com/questions/41458859/keras-custom-metric-for-single-class-accuracy"">2</a>, <a href=""https://stackoverflow.com/questions/43576922/keras-custom-metric-iteration"">3</a>, and others (none answer my question, most are centered on the OP not clearly understanding the difference between a tensor and the values computed using that tensor during the session).</li>
<li>Printing the values of <code>y_true</code> and <code>y_pred</code> during the optimization, by defining a metric like this:</li>
</ul>



<pre class=""lang-py prettyprint-override""><code>    def test_metric(y_true, y_pred):
        y_true = K.print_tensor(y_true)
        y_pred = K.print_tensor(y_pred)
        return y_true - y_pred
</code></pre>

<p>(unfortunately these don't print anything during the optimization).</p>",46667294.0,3,3,,2017/10/10 9:18,11.0,2020/5/18 5:51,2017/10/11 0:17,,463213.0,,2055373.0,,1,37,tensorflow|keras,24061,79.7253,,3,true pred create custom metric kera want implement custom metric kera accord documentation custom metric define function take input two tensor return single tensor value however confuse exactly contain tensor optimization run one data point whole batch whole epoch probably way obtain tensor shape someone point trustworthy place get information help would appreciate sure relevant use tensorflow backend thing try far order answer check kera metric documentation explanation tensor check source code kera metric try understand tensor look kera implementation metric seem suggest label entire batch sure read stackoverflow question others none answer question center op clearly understand difference tensor value compute use tensor session print value optimization define metric like unfortunately print anything optimization
835,835,41260042,Global Weight Decay in Keras,"<p>Is there a way to set a global weight decay in Keras?</p>

<p>I know about the layer wise one using regularizers(<a href=""https://keras.io/regularizers/"" rel=""noreferrer"">https://keras.io/regularizers/</a>), but I could not find any information about a way to set a global weight decay.</p>",42647196.0,3,1,,2016/12/21 10:03,2.0,2019/12/10 11:58,,,,,3994824.0,,1,15,python|neural-network|deep-learning|keras,11609,50.2592,,4,global weight decay kera way set global weight decay kera know layer wise one use regularizers could find information way set global weight decay
120,120,41907598,how to format the image data for training/prediction when images are different in size?,"<p>I am trying to train my model which classifies images. 
The problem I have is, they have different sizes. how should i format my images/or model architecture ?</p>",41916066.0,2,3,,2017/1/28 7:58,61.0,2019/11/6 10:39,2019/11/6 10:39,,6680829.0,,6680829.0,,1,91,deep-learning,52143,215.269,,2,format image data training prediction image different size try train model classify image problem different size format image model architecture
309,309,58028976,TypeError: Unexpected keyword argument passed to optimizer: learning_rate,"<p>I am trying to load a Keras model which was trained on an Azure VM (NC promo). But I am getting the following error.</p>

<blockquote>
  <p>TypeError: Unexpected keyword argument passed to optimizer:learning_rate</p>
</blockquote>

<p>EDIT:</p>

<p>Here is the code snippet that I am using to load my model:</p>

<pre><code>from keras.models import load_model
model = load_model('my_model_name.h5')
</code></pre>",60462166.0,13,4,,2019/9/20 13:05,4.0,2020/9/15 8:41,2019/9/20 13:55,,8810941.0,,8191929.0,,1,16,python|tensorflow|keras|deep-learning|conv-neural-network,31949,123.618,,4,typeerror unexpected keyword argument pass optimizer learn rate try load kera model train azure vm nc promo get following error typeerror unexpected keyword argument pass optimizer learn rate edit code snippet use load model
666,666,37666241,"Importing caffe results in ImportError: ""No module named google.protobuf.internal"" (import enum_type_wrapper)","<p>I installed Anaconda Python on my machine.  When I start the Python Interpreter and type ""import caffe"" in the Python shell, I get the following error:</p>

<pre><code>ImportError: No module named google.protobuf.internal
</code></pre>

<p>I have the following files:</p>

<pre><code>wire_format_lite_inl.h
wire_format_lite.h
wire_format.h
unknown_field_set.h
text_format.h
service.h
repeated_field.h
reflection_ops.h
message_lite.h
message.h
generated_message_util.h
extension_set.h
descriptor.proto
descriptor.h
generated_message_reflection.h
generated_enum_reflection.h
dynamic_message.h
descriptor.pb.h
descriptor_database.h
</code></pre>

<p>What files do I need so the import will work?  Is there an ""internal.h"" file that is required?</p>",,5,3,,2016/6/6 20:23,6.0,2020/1/27 22:33,2016/9/16 17:40,,4789252.0,,4789252.0,,1,19,python|caffe|protocol-buffers,48663,86.7488,,1,import caffe result importerror module name google protobuf internal import enum type wrapper instal anaconda python machine start python interpreter type import caffe python shell get following error following file file need import work internal h file require
461,461,24545725,Deep Belief Networks vs Convolutional Neural Networks,"<p>I am new to the field of neural networks and I would like to know the difference between Deep Belief Networks and Convolutional Networks. 
Also, is there a Deep Convolutional Network which is the combination of Deep Belief and Convolutional Neural Nets?</p>

<p>This is what I have gathered till now. Please correct me if I am wrong.</p>

<p>For an image classification problem, <strong>Deep Belief networks</strong> have many layers, each of which is trained using a greedy layer-wise strategy. 
For example, if my image size is 50 x 50, and I want a Deep Network with 4 layers namely</p>

<ol>
<li>Input Layer</li>
<li>Hidden Layer 1 (HL1)</li>
<li>Hidden Layer 2 (HL2)</li>
<li>Output Layer</li>
</ol>

<p>My input layer will have 50 x 50 = 2500 neurons, HL1 = 1000 neurons (say) , HL2 = 100 neurons (say) and output layer = 10 neurons,
 in order to train the weights (W1) between Input Layer and HL1, I use an AutoEncoder (2500 - 1000 - 2500) and learn W1 of size 2500 x 1000 (This is unsupervised learning). Then I feed forward all images through the first hidden layers to obtain a set of features and then use another autoencoder ( 1000 - 100 - 1000) to get the next set of features and finally use a softmax layer (100 - 10) for classification. (only learning the weights of the last layer (HL2 - Output which is the softmax layer) is supervised learning).</p>

<p>(I could use RBM instead of autoencoder).</p>

<p>If the same problem was solved using <strong>Convolutional Neural Networks</strong>, then for 50x50 input images, I would develop a network using only 7 x 7 patches (say). My layers would be</p>

<ol>
<li>Input Layer (7 x 7 = 49 neurons)</li>
<li>HL1 (25 neurons for 25 different features) - (convolution layer)</li>
<li>Pooling Layer</li>
<li>Output Layer (Softmax)</li>
</ol>

<p>And for learning the weights, I take 7 x 7 patches from images of size 50 x 50, and feed forward through convolutional layer, so I will have 25 different feature maps each of size (50 - 7 + 1) x (50 - 7 + 1) = 44 x 44.</p>

<p>I then use a window of say 11x11 for pooling hand hence get 25 feature maps of size (4 x 4) for as the output of the pooling layer. I use these feature maps for classification.</p>

<p>While learning the weights, I don't use the layer wise strategy as in Deep Belief Networks (Unsupervised Learning), but instead use supervised learning and learn the weights of all the layers simultaneously. Is this correct or is there any other way to learn the weights?</p>

<p>Is what I have understood correct? </p>

<p>So if I want to use DBN's for image classification, I should resize all my images to a particular size (say 200x200) and have that many neurons in the input layer, whereas in case of CNN's, I train only on a smaller patch of the input (say 10 x 10 for an image of size 200x200) and convolve the learnt weights over the entire image?</p>

<p>Do DBNs provide better results than CNNs or is it purely dependent on the dataset?</p>

<p>Thank You.</p>",24590375.0,2,1,,2014/7/3 5:38,27.0,2015/1/8 0:44,,,,,3705926.0,,1,41,machine-learning|computer-vision|neural-network|dbn|autoencoder,22234,78.7881,,0,deep belief network vs convolutional neural network new field neural network would like know difference deep belief network convolutional network also deep convolutional network combination deep belief convolutional neural net gather till please correct wrong image classification problem deep belief network many layer train use greedy layer wise strategy example image size x want deep network layer namely input layer hidden layer hl hide layer hl output layer input layer x neuron hl neuron say hl neuron say output layer neuron order train weight w input layer hl use autoencoder learn w size x unsupervised learn fee forward image first hidden layer obtain set feature use another autoencoder get next set feature finally use softmax layer classification learn weight last layer hl output softmax layer supervise learn could use rbm instead autoencoder problem solve use convolutional neural network x input image would develop network use x patch say layer would input layer x neuron hl neuron different feature convolution layer pool layer output layer softmax learn weight take x patch image size x feed forward convolutional layer different feature map size x x use window say x pool hand hence get feature map size x output pool layer use feature map classification learn weight use layer wise strategy deep belief network unsupervised learn instead use supervise learning learn weight layer simultaneously correct way learn weight understand correct want use dbn image classification resize image particular size say x many neuron input layer whereas case cnn train small patch input say x image size x convolve learnt weight entire image dbns provide good result cnns purely dependent dataset thank
360,360,45806669,How to use predict_generator with ImageDataGenerator?,"<p>I'm very new to Keras. I trained a model and would like to predict some images stored in subfolders (like for training). For testing, I want to predict 2 images from 7 classes (subfolders). The test_generator below sees 14 images, but I get 196 predictions. Where is the mistake? Thanks a lot!</p>

<pre><code>test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(200, 200),
        color_mode=""rgb"",
        shuffle = ""false"",
        class_mode='categorical')

filenames = test_generator.filenames
nb_samples = len(filenames)

predict = model.predict_generator(test_generator,nb_samples)
</code></pre>",45814333.0,5,1,,2017/8/21 23:03,7.0,2021/5/27 5:52,2021/5/8 17:07,,4685471.0,,5794690.0,,1,23,python|machine-learning|keras|deep-learning|generator,65445,95.2635,,2,use predict generator imagedatagenerator new keras train model would like predict image store subfolders like training test want predict image class subfolders test generator see image get prediction mistake thank lot
548,548,48428415,ImportError: libcublas.so.9.0: cannot open shared object file,"<p>currently I have cuda 8.0 and cuda 9.0 installed in Gpu support system. I ran into this error while importing from keras module. It says like failed to load native tensorflow runtime. The error log which i received was:</p>

<pre><code>Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/usr/local/lib/python3.5/dist-
packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
return _load(spec)

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""Try1.py"", line 11, in &lt;module&gt;
from keras.models import Sequential
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/__init__.py"", line 3, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/utils/__init__.py"", line 6, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/utils/conv_utils.py"", line 3, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/backend/__init__.py"", line 83, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/backend/tensorflow_backend.py"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in &lt;module&gt;
from tensorflow.python import *
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in &lt;module&gt;
from tensorflow.python import pywrap_tensorflow
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in &lt;module&gt;
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
</code></pre>

<p>When I run nvcc --version, the cuda version returned is,</p>

<pre><code>Cuda compilation tools, release 8.0, V8.0.61
</code></pre>

<p>I read about some similar post but couldn't solve my issue. Mostly I think this is a clash between two cuda versions. Can anyone tell me how to solve this? Thanks in advance.</p>",48429585.0,12,5,,2018/1/24 17:30,9.0,2021/1/26 22:30,,,,,3316224.0,,1,41,python-3.x|tensorflow|cuda|keras,67811,171.725,,1,importerror libcublas open share object file currently cuda cuda instal gpu support system run error import kera module say like fail load native tensorflow runtime error log receive run nvcc version cuda version return read similar post could solve issue mostly think clash two cuda version anyone tell solve thanks advance
664,664,37657260,how to implement custom metric in keras?,"<p>I get this error : </p>

<blockquote>
  <p>sum() got an unexpected keyword argument 'out'</p>
</blockquote>

<p>when I run this code:</p>

<pre><code>import pandas as pd, numpy as np
import keras
from keras.layers.core import Dense, Activation
from keras.models import Sequential

def AUC(y_true,y_pred):
    not_y_pred=np.logical_not(y_pred)
    y_int1=y_true*y_pred
    y_int0=np.logical_not(y_true)*not_y_pred
    TP=np.sum(y_pred*y_int1)
    FP=np.sum(y_pred)-TP
    TN=np.sum(not_y_pred*y_int0)
    FN=np.sum(not_y_pred)-TN
    TPR=np.float(TP)/(TP+FN)
    FPR=np.float(FP)/(FP+TN)
    return((1+TPR-FPR)/2)

# Input datasets

train_df = pd.DataFrame(np.random.rand(91,1000))
train_df.iloc[:,-2]=(train_df.iloc[:,-2]&gt;0.8)*1


model = Sequential()
model.add(Dense(output_dim=60, input_dim=91, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))
model.add(Dense(output_dim=1, input_dim=60, init=""glorot_uniform""))
model.add(Activation(""sigmoid""))

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=[AUC])


train_df.iloc[:,-1]=np.ones(train_df.shape[0]) #bias
X=train_df.iloc[:,:-1].values
Y=train_df.iloc[:,-1].values
print X.shape,Y.shape

model.fit(X, Y, batch_size=50,show_accuracy = False, verbose = 1)
</code></pre>

<p>Is it possible to implement a custom metric aside from doing a loop on batches and editing the source code?</p>",37663327.0,3,0,,2016/6/6 12:17,11.0,2020/1/6 17:17,2017/7/10 8:59,,5974433.0,,5190014.0,,1,30,python|neural-network|deep-learning|keras|metrics,56695,106.014,,4,implement custom metric kera get error sum get unexpected keyword argument run code possible implement custom metric aside loop batch edit source code
556,556,48686945,reshaping a tensor with padding in pytorch,"<p>I have a tensor with dimensions <code>(30, 35, 49)</code>. I want to reshape it to <code>(30, 35, 512)</code> in order to be able to multiply with another tensor which has also the shape <code>(30, 35, 512)</code>.</p>

<p>I want to do padding on the tensor with <code>(30, 35, 49)</code> dimension in order to make it <code>(30, 35, 512)</code> dimensional.</p>

<p>How can this be done?</p>",53126241.0,6,0,,2018/2/8 13:43,13.0,2021/6/3 5:39,2018/2/8 16:10,,1643939.0,,4393898.0,,1,28,pytorch,45356,150.227,,3,reshape tensor pad pytorch tensor dimension want reshape order able multiply another tensor also shape want pad tensor dimension order make dimensional
192,192,43499199,"Tensorflow: loss decreasing, but accuracy stable","<p>My team is training a CNN in Tensorflow for binary classification of damaged/acceptable parts. We created our code by modifying the cifar10 example code. In my prior experience with Neural Networks, I always trained until the loss was very close to 0 (well below 1). However, we are now evaluating our model with a validation set during training (on a separate GPU), and it seems like the precision stopped increasing after about 6.7k steps, while the loss is still dropping steadily after over 40k steps. Is this due to overfitting? Should we expect to see another spike in accuracy once the loss is very close to zero? The current max accuracy is not acceptable. Should we kill it and keep tuning? What do you recommend? Here is our modified code and graphs of the training process.</p>

<p><a href=""https://gist.github.com/justineyster/6226535a8ee3f567e759c2ff2ae3776b"" rel=""noreferrer"">https://gist.github.com/justineyster/6226535a8ee3f567e759c2ff2ae3776b</a></p>

<p><a href=""https://imgur.com/a/zMzGb"" rel=""noreferrer"">Precision and Loss Images</a></p>",,2,0,,2017/4/19 14:52,4.0,2021/1/6 9:10,,,,,3288329.0,,1,25,tensorflow|neural-network|deep-learning|conv-neural-network|convolution,29198,75.8614,,4,tensorflow loss decrease accuracy stable team train cnn tensorflow binary classification damage acceptable part create code modify cifar example code prior experience neural network always train loss close well however evaluate model validation set training separate gpu seem like precision stop increase k step loss still drop steadily k step due overfitting expect see another spike accuracy loss close zero current max accuracy acceptable kill keep tune recommend modified code graph training process precision loss image
100,100,41494625,Issues using Keras np_utils.to_categorical,"<p>I'm trying to make an array of one-hot vector of integers into an array of one-hot vector that keras will be able to use to fit my model. Here's the relevant part of the code:</p>

<pre><code>Y_train = np.hstack(np.asarray(dataframe.output_vector)).reshape(len(dataframe),len(output_cols))
dummy_y = np_utils.to_categorical(Y_train)
</code></pre>

<p>Below is an image showing what <code>Y_train</code> and <code>dummy_y</code> actually are.</p>

<p><img src=""https://i.stack.imgur.com/j8qFx.png"" alt=""""></p>

<p>I couldn't find any documentation for <code>to_categorical</code> that could help me.</p>

<p>Thanks in advance.</p>",,2,1,,2017/1/5 21:01,7.0,2020/11/10 6:52,2017/1/5 22:08,,1636276.0,,7381295.0,,1,11,python|keras,77136,62.949,,3,issue use kera np utils categorical try make array one hot vector integer array one hot vector kera able use fit model relevant part code image show actually could find documentation could help thanks advance
589,589,49546922,Keras replacing input layer,"<p>The code that I have (that I can't change) uses the Resnet with <code>my_input_tensor</code> as the input_tensor.</p>

<pre><code>model1 = keras.applications.resnet50.ResNet50(input_tensor=my_input_tensor, weights='imagenet')
</code></pre>

<p>Investigating the <a href=""https://github.com/keras-team/keras/blob/master/keras/applications/resnet50.py"" rel=""noreferrer"">source code</a>, ResNet50 function creates a new keras Input Layer with <code>my_input_tensor</code> and then create the rest of the model. This is the behavior that I want to copy with my own model. I load my model from h5 file.</p>

<pre><code>model2 = keras.models.load_model('my_model.h5')
</code></pre>

<p>Since this model already has an Input Layer, I want to replace it with a new Input Layer defined with <code>my_input_tensor</code>.</p>

<p>How can I replace an input layer?</p>",49554465.0,6,3,,2018/3/29 2:08,10.0,2020/11/4 12:25,,,,,6217326.0,,1,29,python|tensorflow|deep-learning|keras,26470,109.491,,3,kera replace input layer code change use resnet input tensor investigate source code resnet function create new kera input layer create rest model behavior want copy model load model h file since model already input layer want replace new input layer define replace input layer
242,242,44476706,What is the difference between Keras model.evaluate() and model.predict()?,"<p>I used Keras biomedical image segmentation to segment brain neurons. I used <code>model.evaluate()</code> it gave me Dice coefficient: 0.916. However, when I used <code>model.predict()</code>, then loop through the predicted images by calculating the Dice coefficient, the Dice coefficient is 0.82. Why are these two values different?</p>",44488571.0,4,0,,2017/6/10 18:28,13.0,2020/4/30 21:50,2019/10/25 17:45,,3924118.0,,6152687.0,,1,41,machine-learning|neural-network|deep-learning|keras|image-segmentation,32982,131.873,,3,difference kera model evaluate model predict use kera biomedical image segmentation segment brain neuron use give dice coefficient however use loop predict image calculate dice coefficient dice coefficient two value different
575,575,49242266,Difference between bidirectional_dynamic_rnn and stack_bidirectional_dynamic_rnn in Tensorflow,"<p>I am building a dynamic RNN network with stacking multiple LSTMs. I see there are 2 options</p>

<pre><code># cells_fw and cells_bw are list of cells eg LSTM cells
stacked_cell_fw = tf.contrib.rnn.MultiRNNCell(cells_fw)
stacked_cell_bw = tf.contrib.rnn.MultiRNNCell(cells_bw)

output = tf.nn.bidirectional_dynamic_rnn(
          stacked_cell_fw, stacked_cell_bw, INPUT,
          sequence_length=LENGTHS, dtype=tf.float32)
</code></pre>

<p>vs </p>

<pre><code>output = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(cells_fw, cells_bw, INPUT,
sequence_length=LENGTHS, dtype=tf.float32)
</code></pre>

<p>What is the difference between the 2 approaches and is one better than the other?</p>",50552539.0,1,1,,2018/3/12 18:35,6.0,2019/4/9 17:02,2019/4/9 17:02,,8279593.0,,4040998.0,,1,13,tensorflow|recurrent-neural-network,5316,51.5023,,3,difference bidirectional dynamic rnn stack bidirectional dynamic rnn tensorflow build dynamic rnn network stack multiple lstms see option v difference approach one good
803,803,53153790,'Tensor' object has no attribute 'lower',"<p>I am fine-tuning a MobileNet with 14 new classes. When I add new layers by:</p>

<pre><code>x=mobile.layers[-6].output
x=Flatten(x)
predictions = Dense(14, activation='softmax')(x)
model = Model(inputs=mobile.input, outputs=predictions)
</code></pre>

<p>I get the error:</p>

<pre><code>'Tensor' object has no attribute 'lower'
</code></pre>

<p>Also using:</p>

<pre><code>model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit_generator(train_batches, steps_per_epoch=18,
                validation_data=valid_batches, validation_steps=3, epochs=60, verbose=2)
</code></pre>

<p>I get the error:</p>

<pre><code>Error when checking target: expected dense_1 to have 4 dimensions, but got array with shape (10, 14)
</code></pre>

<p>What does <code>lower</code> mean? I saw other fine-tuning scripts and there were no other arguments other than the name of the model which is <code>x</code> in this case.</p>",53154110.0,1,0,,2018/11/5 11:46,1.0,2020/4/25 5:16,2020/4/25 5:16,,2099607.0,,10607259.0,,1,24,python|tensorflow|machine-learning|keras|conv-neural-network,14236,71.4136,,4,tensor object attribute low fine tune mobilenet new class add new layer get error also use get error mean saw fine tune script arguments name model case
88,88,41273361,Get the last output of a dynamic_rnn in TensorFlow,"<p>I have a 3-D tensor of shape <code>[batch, None, dim]</code> where the second dimension, i.e. the timesteps, is unknown. I use <code>dynamic_rnn</code> to process such input, like in the following snippet:</p>

<pre><code>import numpy as np
import tensorflow as tf

batch = 2
dim = 3
hidden = 4

lengths = tf.placeholder(dtype=tf.int32, shape=[batch])
inputs = tf.placeholder(dtype=tf.float32, shape=[batch, None, dim])
cell = tf.nn.rnn_cell.GRUCell(hidden)
cell_state = cell.zero_state(batch, tf.float32)
output, _ = tf.nn.dynamic_rnn(cell, inputs, lengths, initial_state=cell_state)
</code></pre>

<p>Actually, running this snipped with some actual numbers, I have some reasonable results:</p>

<pre><code>inputs_ = np.asarray([[[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]],
                    [[6, 6, 6], [7, 7, 7], [8, 8, 8], [9, 9, 9]]],
                    dtype=np.int32)
lengths_ = np.asarray([3, 1], dtype=np.int32)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    output_ = sess.run(output, {inputs: inputs_, lengths: lengths_})
    print(output_)
</code></pre>

<p>And the output is:</p>

<pre><code>[[[ 0.          0.          0.          0.        ]
  [ 0.02188676 -0.01294564  0.05340237 -0.47148666]
  [ 0.0343586  -0.02243731  0.0870839  -0.89869428]
  [ 0.          0.          0.          0.        ]]

 [[ 0.00284752 -0.00315077  0.00108094 -0.99883419]
  [ 0.          0.          0.          0.        ]
  [ 0.          0.          0.          0.        ]
  [ 0.          0.          0.          0.        ]]]
</code></pre>

<p>Is there a way to get a 3-D tensor of shape <code>[batch, 1, hidden]</code> with the <em>last relevant output</em> of the dynamic RNN? Thanks!</p>",43298689.0,4,6,,2016/12/21 23:00,9.0,2017/12/19 20:00,2016/12/22 9:36,,1861627.0,,1861627.0,,1,16,python|tensorflow|neural-network|deep-learning|recurrent-neural-network,7887,58.3876,,5,get last output dynamic rnn tensorflow tensor shape second dimension e timesteps unknown use process input like following snippet actually run snip actual number reasonable result output way get tensor shape last relevant output dynamic rnn thanks
126,126,42064690,Using pre-trained word2vec with LSTM for word generation,"<p>LSTM/RNN can be used for text generation.
<a href=""https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"" rel=""nofollow noreferrer"">This</a> shows way to use pre-trained GloVe word embeddings for Keras model.</p>

<ol>
<li>How to use pre-trained Word2Vec word embeddings with Keras LSTM
model? <a href=""https://codekansas.github.io/gensim"" rel=""nofollow noreferrer"">This</a> post did help.</li>
<li>How to predict / generate next <em>word</em> when the model is provided with the sequence of words as its input?</li>
</ol>

<p>Sample approach tried:</p>

<pre><code># Sample code to prepare word2vec word embeddings    
import gensim
documents = [""Human machine interface for lab abc computer applications"",
             ""A survey of user opinion of computer system response time"",
             ""The EPS user interface management system"",
             ""System and human system engineering testing of EPS"",
             ""Relation of user perceived response time to error measurement"",
             ""The generation of random binary unordered trees"",
             ""The intersection graph of paths in trees"",
             ""Graph minors IV Widths of trees and well quasi ordering"",
             ""Graph minors A survey""]
sentences = [[word for word in document.lower().split()] for document in documents]

word_model = gensim.models.Word2Vec(sentences, size=200, min_count = 1, window = 5)

# Code tried to prepare LSTM model for word generation
from keras.layers.recurrent import LSTM
from keras.layers.embeddings import Embedding
from keras.models import Model, Sequential
from keras.layers import Dense, Activation

embedding_layer = Embedding(input_dim=word_model.syn0.shape[0], output_dim=word_model.syn0.shape[1], weights=[word_model.syn0])

model = Sequential()
model.add(embedding_layer)
model.add(LSTM(word_model.syn0.shape[1]))
model.add(Dense(word_model.syn0.shape[0]))   
model.add(Activation('softmax'))
model.compile(optimizer='sgd', loss='mse')
</code></pre>

<p>Sample code / psuedocode to train LSTM and predict will be appreciated. </p>",,1,1,,2017/2/6 9:47,25.0,2021/7/4 7:48,2020/1/21 19:19,,2449365.0,,9655419.0,,1,25,machine-learning|neural-network|keras|lstm|word2vec,26577,66.69800000000001,,3,use pre train word vec lstm word generation lstm rnn use text generation show way use pre train glove word embeddings kera model use pre trained word vec word embeddings kera lstm model post help predict generate next word model provide sequence word input sample approach try sample code psuedocode train lstm predict appreciate
436,436,47995324,Does model.compile() initialize all the weights and biases in Keras (tensorflow backend)?,"<p>When I start training a model, there is no model saved previously. I can use <code>model.compile()</code> safely. I have now saved the model in a <code>h5</code> file for further training using <code>checkpoint</code>.</p>

<p>Say, I want to train the model further. I am confused at this point: can I use <code>model.compile()</code> here? And should it be placed before or after the <code>model = load_model()</code> statement? If <code>model.compile()</code> reinitializes all the weights and biases, I should place it before <code>model = load_model()</code> statement.</p>

<p>After discovering some discussions, it seems to me that <code>model.compile()</code> is only needed when I have no model saved previously. Once I have saved the model, there is no need to use <code>model.compile()</code>. Is it true or false? And when I want to predict using the trained model, should I use <code>model.compile()</code> before predicting? </p>",47996024.0,2,0,,2017/12/27 16:15,41.0,2019/10/31 16:15,2017/12/27 16:26,,5254777.0,,5254777.0,,1,90,tensorflow|keras,42123,261.498,,3,model compile initialize weight bias kera tensorflow backend start train model model save previously use safely save model file training use say want train model far confuse point use place statement reinitializes weight bias place statement discover discussion seem need model save previously save model need use true false want predict use trained model use predict
358,358,45793856,Binary classification with Softmax,"<p>I am training a binary classifier using Sigmoid activation function with Binary crossentropy which gives good accuracy around 98%.<br>
The same when I train using softmax with categorical_crossentropy gives very low accuracy (&lt; 40%).<br>
I am passing the targets for binary_crossentropy as list of 0s and 1s eg; [0,1,1,1,0]. </p>

<p>Any idea why this is happening?</p>

<p>This is the model I am using for the second classifier:
<a href=""https://i.stack.imgur.com/T4s74.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T4s74.png"" alt=""enter image description here""></a></p>",45794509.0,1,1,,2017/8/21 9:38,9.0,2018/6/28 9:09,2017/8/21 19:17,,7672928.0,,4724057.0,,1,16,binary|classification|keras|softmax|sigmoid,19988,55.4031,,4,binary classification softmax train binary classifier use sigmoid activation function binary crossentropy give good accuracy around train use softmax categorical crossentropy give low accuracy pass target binary crossentropy list eg idea happen model use second classifier
290,290,56741087,"How to fix RuntimeError ""Expected object of scalar type Float but got scalar type Double for argument""?","<p>I'm trying to train a classifier via PyTorch. However, I am experiencing problems with training when I feed the model with training data.
I get this error on <code>y_pred = model(X_trainTensor)</code>:</p>

<blockquote>
  <p>RuntimeError: Expected object of scalar type Float but got scalar type Double for argument #4 'mat1'</p>
</blockquote>

<p>Here are key parts of my code:</p>

<pre class=""lang-py prettyprint-override""><code># Hyper-parameters 
D_in = 47  # there are 47 parameters I investigate
H = 33
D_out = 2  # output should be either 1 or 0
</code></pre>

<pre class=""lang-py prettyprint-override""><code># Format and load the data
y = np.array( df['target'] )
X = np.array( df.drop(columns = ['target'], axis = 1) )
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)  # split training/test data

X_trainTensor = torch.from_numpy(X_train) # convert to tensors
y_trainTensor = torch.from_numpy(y_train)
X_testTensor = torch.from_numpy(X_test)
y_testTensor = torch.from_numpy(y_test)
</code></pre>

<pre class=""lang-py prettyprint-override""><code># Define the model
model = torch.nn.Sequential(
    torch.nn.Linear(D_in, H),
    torch.nn.ReLU(),
    torch.nn.Linear(H, D_out),
    nn.LogSoftmax(dim = 1)
)
</code></pre>

<pre class=""lang-py prettyprint-override""><code># Define the loss function
loss_fn = torch.nn.NLLLoss() 
</code></pre>

<pre class=""lang-py prettyprint-override""><code>for i in range(50):
    y_pred = model(X_trainTensor)
    loss = loss_fn(y_pred, y_trainTensor)
    model.zero_grad()
    loss.backward()
    with torch.no_grad():       
        for param in model.parameters():
            param -= learning_rate * param.grad
</code></pre>",56741419.0,6,8,,2019/6/24 17:05,12.0,2021/7/26 17:49,2019/6/25 9:36,,5884955.0,,6491230.0,,1,82,python|neural-network|deep-learning|classification|pytorch,121280,277.735,,4,fix runtimeerror expect object scalar type float get scalar type double argument try train classifier via pytorch however experience problem training fee model train data get error runtimeerror expect object scalar type float get scalar type double argument mat key part code
49,49,55142951,Tensorflow 2.0 - AttributeError: module 'tensorflow' has no attribute 'Session',"<p>When I am executing the command <code>sess = tf.Session()</code> in Tensorflow 2.0 environment, I am getting an error message as below:</p>
<pre><code>Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
AttributeError: module 'tensorflow' has no attribute 'Session'
</code></pre>
<blockquote>
<p>System Information:</p>
</blockquote>
<ul>
<li>OS Platform and Distribution: Windows 10</li>
<li>Python Version: 3.7.1</li>
<li>Tensorflow Version: 2.0.0-alpha0 (installed with pip)</li>
</ul>
<blockquote>
<p>Steps to reproduce:</p>
<p>Installation:</p>
</blockquote>
<ol>
<li>pip install --upgrade pip</li>
<li><strong>pip install tensorflow==2.0.0-alpha0</strong></li>
<li>pip install keras</li>
<li>pip install numpy==1.16.2</li>
</ol>
<blockquote>
<p>Execution:</p>
</blockquote>
<ol>
<li>Execute command: import tensorflow as tf</li>
<li>Execute command: sess = tf.Session()</li>
</ol>",55143329.0,15,4,,2019/3/13 13:23,44.0,2021/8/19 7:50,2020/6/20 9:12,,-1.0,,8531952.0,,1,168,python|tensorflow|keras|tensorflow2.0,301751,945.919,,1,tensorflow attributeerror module tensorflow attribute session execute command tensorflow environment get error message system information platform distribution window python version tensorflow version alpha instal pip step reproduce installation pip install upgrade pip pip install tensorflow alpha pip install kera pip install numpy execution execute command import tensorflow tf execute command sess tf session
80,80,7098625,How can I apply reinforcement learning to continuous action spaces?,"<p>I'm trying to get an agent to learn the mouse movements necessary to best perform some task in a reinforcement learning setting (i.e. the reward signal is the only feedback for learning).</p>

<p>I'm hoping to use the Q-learning technique, but while I've found <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf"">a way to extend this method to continuous state spaces</a>, I can't seem to figure out how to accommodate a problem with a continuous action space.</p>

<p>I could just force all mouse movement to be of a certain magnitude and in only a certain number of different directions, but any reasonable way of making the actions discrete would yield a huge action space. Since standard Q-learning requires the agent to evaluate <em>all</em> possible actions, such an approximation doesn't solve the problem in any practical sense.</p>",7100856.0,6,0,,2011/8/17 19:54,24.0,2019/7/9 5:39,2019/2/19 8:56,,3924118.0,,821806.0,,1,43,algorithm|machine-learning|reinforcement-learning|q-learning,27892,140.382,,0,apply reinforcement learn continuous action space try get agent learn mouse movement necessary best perform task reinforcement learn set e reward signal feedback learn hop use q learning technique find way extend method continuous state space seem figure accommodate problem continuous action space could force mouse movement certain magnitude certain number different direction reasonable way make action discrete would yield huge action space since standard q learning require agent evaluate possible action approximation solve problem practical sense
196,196,43664444,How can l uninstall PyTorch?,"<p>I can't find any command to uninstall and remove all PyTorch dependencies. Even on the <a href=""http://pytorch.org"" rel=""noreferrer"">pytorch.org</a> website.</p>

<p>I installed PyTorch with</p>

<pre><code>conda install pytorch torchvision cuda80 -c soumith
</code></pre>",,6,0,,2017/4/27 17:33,6.0,2020/5/9 10:41,2019/8/29 9:27,,6900127.0,,7721790.0,,1,22,python|ubuntu|anaconda|pytorch,110912,93.5799,,1,l uninstall pytorch find command uninstall remove pytorch dependency even pytorch org website instal pytorch
352,352,45585104,Save Keras ModelCheckpoints in Google Cloud Bucket,"<p>I'm working on training a LSTM network on Google Cloud Machine Learning Engine using Keras with TensorFlow backend. I managed it to deploy my model and perform a successful training task after some adjustments to the gcloud and my python script. </p>

<p>I then tried to make my model save checkpoints after every epoch using Keras <a href=""https://keras.io/callbacks/#modelcheckpoint"" rel=""noreferrer"">modelCheckpoint callback</a>. Running a local training job with Google Cloud works perfectly as expected. The weights are getting stored in the specified path after each epoch. But when I try to run the same job online on Google Cloud Machine Learning Engine the <code>weights.hdf5</code> does not get written to my Google Cloud Bucket. Instead I get the following error:</p>

<pre><code>...
File ""h5f.pyx"", line 71, in h5py.h5f.open (h5py/h5f.c:1797)
IOError: Unable to open file (Unable to open file: name = 
'gs://.../weights.hdf5', errno = 2, error message = 'no such file or
directory', flags = 0, o_flags = 0)
</code></pre>

<p>I investigated this issue and it turned out, that there is no Problem with the the Bucket itself, as Keras <a href=""https://keras.io/callbacks/#tensorboard"" rel=""noreferrer"">Tensorboard callback</a> does work fine and writes the expected output to the same bucket. I also made sure that <code>h5py</code> gets included by providing it in the <code>setup.py</code> located at:</p>

<pre><code>闂佸疇顫夋竟鏇㈠绩閵忋倕鐓橀柍?setup.py
    闂佸疇顫夐弻锟犲绩閵忋倕鐓橀柍?trainer
    闂佸疇顫夋竟鏇㈠绩閵忋倕鐓橀柍?__init__.py
    闂佸疇顫夋竟鏇㈠绩閵忋倕鐓橀柍?...
</code></pre>

<p>The actual include in <code>setup.py</code> is shown below:</p>

<pre><code># setup.py
from setuptools import setup, find_packages

setup(name='kerasLSTM',
      version='0.1',
      packages=find_packages(),
      author='Kevin Katzke',
      install_requires=['keras','h5py','simplejson'],
      zip_safe=False)
</code></pre>

<p>I guess the problem comes down to the fact that GCS cannot be accessed with Pythons <code>open</code> for I/O since it instead provides a custom implementation:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.lib.io import file_io

with file_io.FileIO(""gs://..."", 'r') as f:
    f.write(""Hi!"")
</code></pre>

<p>After checking how Keras modelCheckpoint callback implements the actual file writing and it turned out, that it is using <a href=""http://docs.h5py.org/en/latest/high/file.html?highlight=open"" rel=""noreferrer"">h5py.File()</a> for I/O:</p>

<pre><code> with h5py.File(filepath, mode='w') as f:
    f.attrs['keras_version'] = str(keras_version).encode('utf8')
    f.attrs['backend'] = K.backend().encode('utf8')
    f.attrs['model_config'] = json.dumps({
        'class_name': model.__class__.__name__,
        'config': model.get_config()
 }, default=get_json_type).encode('utf8')
</code></pre>

<p>And as the <code>h5py package</code> is a Pythonic interface to the <code>HDF5 binary data format</code> the <code>h5py.File()</code> seems to call an underlying <code>HDF5</code> functionality written in Fortran as far as I can tell: <a href=""https://github.com/mokus0/hdf5/blob/master/fortran/examples/h5_rdwt.f90"" rel=""noreferrer"">source</a>, <a href=""https://support.hdfgroup.org/HDF5/doc/RM/RM_H5F.html#File-Open"" rel=""noreferrer"">documentation</a>.</p>

<p>How can I fix this and make the modelCheckpoint callback write to my GCS Bucket? Is there a way for ""monkey patching"" to somehow overwrite how a hdf5 file is opened to make it use GCS's <code>file_io.FileIO()</code>?</p>",46268055.0,8,2,,2017/8/9 8:11,5.0,2020/4/21 7:51,,,,,1280289.0,,1,19,tensorflow|google-cloud-platform|keras|hdf5|h5py,6820,82.7351,,4,save kera modelcheckpoints google cloud bucket work train lstm network google cloud machine learn engine use kera tensorflow backend manage deploy model perform successful training task adjustment gcloud python script try make model save checkpoint every epoch use kera modelcheckpoint callback run local training job google cloud work perfectly expect weight get store specified path epoch try run job online google cloud machine learn engine get write google cloud bucket instead get following error investigate issue turn problem bucket kera tensorboard callback work fine write expected output bucket also make sure get include provide locate actual include show guess problem come fact gc access python since instead provide custom implementation check keras modelcheckpoint callback implement actual file writing turn use h py file pythonic interface seem call underlying functionality write fortran far tell source documentation fix make modelcheckpoint callback write gc bucket way monkey patching somehow overwrite hdf file open make use gcs
327,327,62948332,How to add attention layer to a Bi-LSTM,"<p>I am developing a Bi-LSTM model and want to add a attention layer to it. But I am not getting how to add it.</p>
<p>My current code for the model is</p>
<pre><code>model = Sequential()
model.add(Embedding(max_words, 1152, input_length=max_len, weights=[embeddings]))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(32)))
model.add(BatchNormalization())
model.add(Activation('tanh'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.summary()
</code></pre>
<p>And the model summary is</p>
<pre><code>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 1152, 1152)        278396928 
_________________________________________________________________
batch_normalization_1 (Batch (None, 1152, 1152)        4608      
_________________________________________________________________
activation_1 (Activation)    (None, 1152, 1152)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 1152, 1152)        0         
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                303360    
_________________________________________________________________
batch_normalization_2 (Batch (None, 64)                256       
_________________________________________________________________
activation_2 (Activation)    (None, 64)                0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 278,705,217
Trainable params: 278,702,785
Non-trainable params: 2,432
</code></pre>",62949137.0,2,0,,2020/7/17 6:28,9.0,2021/6/24 18:02,2020/10/6 16:44,,10375049.0,,10097229.0,,1,14,python-3.x|tensorflow|machine-learning|keras|nlp,4521,51.2209,,3,add attention layer bi lstm develop bi lstm model want add attention layer get add current code model model summary
487,487,28937803,What is the difference between Q-learning and Value Iteration?,"<p>How is Q-learning different from value iteration in reinforcement learning? </p>

<p>I know Q-learning is model-free and training samples are transitions <code>(s, a, s', r)</code>. But since we know the transitions and the reward for every transition in Q-learning, is it not the same as model-based learning where we know the reward for a state and action pair, and the transitions for every action from a state (be it stochastic or deterministic)? I do not understand the difference.</p>",28955191.0,3,1,,2015/3/9 8:32,15.0,2019/11/7 15:45,2019/5/13 14:21,,3924118.0,,4609181.0,,1,32,machine-learning|artificial-intelligence|reinforcement-learning|q-learning,19445,102.355,,0,difference q learning value iteration q learn different value iteration reinforcement learning know q learning model free training sample transition since know transition reward every transition q learning model base learn know reward state action pair transition every action state stochastic deterministic understand difference
712,712,39814777,Can Keras deal with input images with different size?,"<p>Can the Keras deal with input images with different size? For example, in the fully convolutional neural network, the input images can have any size. However, we need to specify the input shape when we create a network by Keras. Therefore, how can we use Keras to deal with different input size without resizing the input images to the same size? Thanks for any help.</p>",,2,0,,2016/10/2 7:22,18.0,2016/12/11 23:02,,,,,6234662.0,,1,41,machine-learning|deep-learning|keras,29000,97.2496,,3,keras deal input image different size kera deal input image different size example fully convolutional neural network input image size however need specify input shape create network kera therefore use kera deal different input size without resize input image size thanks help
208,208,43833081,AttributeError: 'module' object has no attribute 'computation',"<p>Im trying to use Keras (Sequential) but I get the following error when I try to import it:</p>

<pre><code>File ""kaggle_titanic_keras.py"", line 3, in &lt;module&gt;
    from keras.models import Sequential
  File ""/anaconda/lib/python2.7/site-packages/keras/__init__.py"", line 4, in &lt;module&gt;
    from . import applications
  File ""/anaconda/lib/python2.7/site-packages/keras/applications/__init__.py"", line 1, in &lt;module&gt;
    from .vgg16 import VGG16
  File ""/anaconda/lib/python2.7/site-packages/keras/applications/vgg16.py"", line 14, in &lt;module&gt;
    from ..models import Model
  File ""/anaconda/lib/python2.7/site-packages/keras/models.py"", line 14, in &lt;module&gt;
    from . import layers as layer_module
  File ""/anaconda/lib/python2.7/site-packages/keras/layers/__init__.py"", line 4, in &lt;module&gt;
    from ..engine import Layer
  File ""/anaconda/lib/python2.7/site-packages/keras/engine/__init__.py"", line 8, in &lt;module&gt;
    from .training import Model
  File ""/anaconda/lib/python2.7/site-packages/keras/engine/training.py"", line 24, in &lt;module&gt;
    from .. import callbacks as cbks
  File ""/anaconda/lib/python2.7/site-packages/keras/callbacks.py"", line 25, in &lt;module&gt;
    from tensorflow.contrib.tensorboard.plugins import projector
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/__init__.py"", line 30, in &lt;module&gt;
    from tensorflow.contrib import factorization
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/factorization/__init__.py"", line 24, in &lt;module&gt;
    from tensorflow.contrib.factorization.python.ops.gmm import *
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py"", line 27, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py"", line 87, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn import *
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py"", line 23, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn import *
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py"", line 25, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn import estimators
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py"", line 297, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py"", line 29, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py"", line 31, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.estimators import estimator
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 49, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py"", line 21, in &lt;module&gt;
    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data
  File ""/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py"", line 26, in &lt;module&gt;
    import dask.dataframe as dd
  File ""/anaconda/lib/python2.7/site-packages/dask/dataframe/__init__.py"", line 3, in &lt;module&gt;
    from .core import (DataFrame, Series, Index, _Frame, map_partitions,
  File ""/anaconda/lib/python2.7/site-packages/dask/dataframe/core.py"", line 38, in &lt;module&gt;
    pd.computation.expressions.set_use_numexpr(False)
AttributeError: 'module' object has no attribute 'computation'
</code></pre>

<p>Im running Python 2.7, TensorFlow 1.1 , Keras 2.0.3 and 'upgraded' to Pandas 0.20.1 yesterday which I suspect is causing the problem but the error message says nothing about it.</p>",44615129.0,5,3,,2017/5/7 15:07,8.0,2017/11/6 12:33,2017/5/7 15:08,,3001761.0,,6094602.0,,1,38,python|python-2.7|import|module|keras,28117,188.796,,4,attributeerror module object attribute computation im try use keras sequential get following error try import im run python tensorflow kera upgrade pandas yesterday suspect cause problem error message say nothing
441,441,48140989,Keras LSTM input dimension setting,"<p>I was trying to train a LSTM model using keras but I think I got something wrong here.</p>

<p>I got an error of </p>

<blockquote>
  <p>ValueError: Error when checking input: expected lstm_17_input to have
  3 dimensions, but got array with shape (10000, 0, 20)</p>
</blockquote>

<p>while my code looks like </p>

<pre><code>model = Sequential()
model.add(LSTM(256, activation=""relu"", dropout=0.25, recurrent_dropout=0.25, input_shape=(None, 20, 64)))
model.add(Dense(1, activation=""sigmoid""))
model.compile(loss='binary_crossentropy',
          optimizer='adam',
          metrics=['accuracy'])
model.fit(X_train, y_train,
      batch_size=batch_size,
      epochs=10)
</code></pre>

<p>where <code>X_train</code> has a shape of <code>(10000, 20)</code> and the first few data points are like </p>

<pre><code>array([[ 0,  0,  0, ..., 40, 40,  9],
   [ 0,  0,  0, ..., 33, 20, 51],
   [ 0,  0,  0, ..., 54, 54, 50],
...
</code></pre>

<p>and <code>y_train</code> has a shape of <code>(10000, )</code>, which is a binary (0/1) label array.</p>

<p>Could someone point out where I was wrong here?</p>",48141688.0,1,4,,2018/1/7 19:56,7.0,2018/1/7 22:53,2018/1/7 22:53,,712995.0,,6659095.0,,1,17,python|tensorflow|deep-learning|keras|lstm,17553,59.3774,,3,kera lstm input dimension set try train lstm model use kera think get something wrong get error valueerror error check input expect lstm input dimension get array shape code look like shape first data point like shape binary label array could someone point wrong
325,325,58692476,What is Adaptive average pooling and How does it work?,"<p>I recently came across a method in Pytorch when I try to implement AlexNet. 
I don't understand how it works. Please explain the idea behind it with some examples. And how it is different from Maxpooling or Average poling in terms of Neural Network functionality</p>

<blockquote>
  <p>nn.AdaptiveAvgPool2d((6, 6))</p>
</blockquote>",,1,0,,2019/11/4 11:26,1.0,2019/11/4 13:13,,,,,7962824.0,,1,26,python|math|neural-network|deep-learning|pytorch,12590,68.6001,,0,adaptive average pooling work recently come across method pytorch try implement alexnet understand work please explain idea behind example different maxpooling average poling term neural network functionality nn adaptiveavgpool
241,241,44472693,How to decode encoded data from deep autoencoder in Keras (unclarity in tutorial),"<p>I have followed the tutorial ""Building Autoencoders in Keras"":
<a href=""https://blog.keras.io/building-autoencoders-in-keras.html"" rel=""noreferrer"">https://blog.keras.io/building-autoencoders-in-keras.html</a></p>

<p>The first, simple, solution works fine. But in the section ""Deep autoencoder"" the code provided in the tutorial does not seem to work fully.</p>

<p>Here is my code (just until where the problem appears), which is just copied from the turorial:</p>

<pre><code>from keras.layers import Input, Dense
from keras.models import Model

encoding_dim = 32

input_img = Input(shape=(784,))
encoded = Dense(128, activation='relu')(input_img)
encoded = Dense(64, activation='relu')(encoded)
encoded = Dense(32, activation='relu')(encoded)  # Multiple encoding
decoded = Dense(64, activation='relu')(encoded)  # and decoding layers.
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)

encoder = Model(input_img, encoded)

encoded_input = Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]
decoder = Model(encoded_input, decoder_layer(encoded_input))  # Crash happens here.
</code></pre>

<p>I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""keras_test.py"", line 20, in &lt;module&gt;
    decoder = Model(encoded_input, decoder_layer(encoded_input))  # Crash happens here
  File ""/Users/paulmagnus/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/site-packages/keras/engine/topology.py"", line 569, in __call__
    self.assert_input_compatibility(inputs)
  File ""/Users/paulmagnus/Library/Enthought/Canopy/edm/envs/User/lib/python2.7/site-packages/keras/engine/topology.py"", line 479, in assert_input_compatibility
    ' but got shape ' + str(x_shape))
ValueError: Input 0 is incompatible with layer dense_6: expected axis -1 of input shape to have value 128 but got shape (None, 32)
</code></pre>

<p>I am guessing that <code>decoder</code>is connected to the wrong decoding layer and/or that the shape of its input or output is wrong. But what should I do about it?</p>

<p>The <code>decoder</code> is not necessary for the <code>autoencoder</code> to work. I can perform the learning and encode the images following the rest of the tutorial. But without <code>decoder</code> I am not able to decode the images back to original format to see if they actually look good. The tutorial doesn't mention anything about this and just shows the decoded images without a word. I guess the author assumed whatever change he did to <code>decoder</code> to achieve this was trivial.</p>

<p>To clarify: the single-layer version works fine, where instead of 3 encoding and 3 decoding layers we just have </p>

<pre><code>encoded = Dense(32, activation='relu')(input_img)
decoded = Dense(784, activation='sigmoid')(encoded)
</code></pre>

<p>and everything else is equal as above. Then there is no error and I can use <code>decoder</code> to recreate the images.</p>",44484654.0,2,3,,2017/6/8 9:31,2.0,2021/3/6 11:04,,,,PaulMag,3956460.0,,1,18,python|deep-learning|keras,4780,50.9177,,4,decode encode data deep autoencoder keras unclarity tutorial follow tutorial building autoencoders kera first simple solution work fine section deep autoencoder code provide tutorial seem work fully code problem appear copy turorial get error guess connect wrong decoding layer shape input output wrong necessary work perform learning encode image follow rest tutorial without able decode image back original format see actually look good tutorial mention anything show decoded image without word guess author assume whatever change achieve trivial clarify single layer version work fine instead encode decode layer everything else equal error use recreate image
738,738,40761185,What is the intuition of using tanh in LSTM?,"<p>In an LSTM network (<a href=""https://i.stack.imgur.com/jbBjY.png"" rel=""noreferrer"">Understanding LSTMs</a>), why does the input gate and output gate use tanh?</p>
<p>What is the intuition behind this?</p>
<p>It is just a nonlinear transformation? If it is, can I change both to another activation function (e.g., ReLU)?</p>",40775144.0,2,1,,2016/11/23 10:00,48.0,2021/3/23 21:32,2021/3/23 21:25,,63550.0,,4251528.0,,1,85,machine-learning|deep-learning|lstm|recurrent-neural-network|activation-function,44034,227.575,2021/1/25 10:07,3,intuition use tanh lstm lstm network understand lstms input gate output gate use tanh intuition behind nonlinear transformation change another activation function e g relu
632,632,51006505,How training and test data is split - Keras on Tensorflow,"<p>I am currently training my data using neural network and using fit function. </p>

<pre><code>history=model.fit(X, encoded_Y, batch_size=50, nb_epoch=500, validation_split = 0.2, verbose=1)
</code></pre>

<p>Now I have used validation_split as 20%. What I understood is that my training data will be 80% and testing data will be 20%. I am confused how this data is dealt on back end. Is it like top 80% samples will be taken for training and below 20% percent for testing or the samples are randomly picked from inbetween? If I want to give separate training and testing data, how will I do that using fit()??</p>

<p>Moreover, my second concern is how to check if data is fitting well on model? I can see from the results that training accuracy is around 90% while the validation accuracy is around 55%. Does this mean it is the case of over-fitting or Under-fitting?</p>

<p>My last question is what does evaluate returns? Document says it returns the loss but I am already getting loss and accuracy during each epoch (as a return of fit() (in history)). What does accuracy and score returned by evaluate shows? If the accuracy returned by evaluate returns 90%, can I say my data is fitting well, regardless of what individual accuracy and loss was for each epoch?</p>

<p>Below is my Code:</p>

<pre><code>import numpy
import pandas
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras.utils import np_utils
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
import itertools

seed = 7
numpy.random.seed(seed)

dataframe = pandas.read_csv(""INPUTFILE.csv"", skiprows=range(0, 0))

dataset = dataframe.values
X = dataset[:,0:50].astype(float) # number of cols-1
Y = dataset[:,50]

encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)

encoded_Y = np_utils.to_categorical(encoded_Y)
print(""encoded_Y="", encoded_Y) 
# baseline model
def create_baseline():
    # create model
    model = Sequential()
    model.add(Dense(5, input_dim=5, kernel_initializer='normal', activation='relu'))
    model.add(Dense(5, kernel_initializer='normal', activation='relu'))
    #model.add(Dense(2, kernel_initializer='normal', activation='sigmoid'))

    model.add(Dense(2, kernel_initializer='normal', activation='softmax'))

    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # for binayr classification
        #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # for multi class
    return model


model=create_baseline();
history=model.fit(X, encoded_Y, batch_size=50, nb_epoch=500, validation_split = 0.2, verbose=1)

print(history.history.keys())
# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()


pre_cls=model.predict_classes(X)    
cm1 = confusion_matrix(encoder.transform(Y),pre_cls)
print('Confusion Matrix : \n')
print(cm1)


score, acc = model.evaluate(X,encoded_Y)
print('Test score:', score)
print('Test accuracy:', acc)
</code></pre>",51006816.0,1,1,,2018/6/24 2:28,9.0,2020/12/31 12:50,2018/6/24 9:20,,9983880.0,,2955608.0,,1,18,validation|tensorflow|machine-learning|neural-network|keras,60337,69.7223,,2,training test data split kera tensorflow currently train data use neural network use fit function use validation split understood training data test data confused data deal back end like top sample take training percent test sample randomly pick inbetween want give separate training test data use fit moreover second concern check data fit well model see result train accuracy around validation accuracy around mean case fitting fit last question evaluate return document say return loss already get loss accuracy epoch return fit history accuracy score return evaluate show accuracy return evaluate return say data fit well regardless individual accuracy loss epoch code
774,774,51807040,TypeError: tensor is not a torch image,"<p>While working through the AI course at Udacity I came across this error during the Transfer Learning section. Here is the code that seems to be causing the trouble:</p>

<pre><code>import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models

data_dir = 'filename'

# TODO: Define transforms for the training data and testing data
train_transforms= transforms.Compose([transforms.Resize((224,224)), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.ToTensor()])
test_transforms= transforms.Compose([transforms.Resize((224,224)), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), transforms.ToTensor()])

# Pass transforms in here, then run the next cell to see how the transforms look
train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)
test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)

trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
testloader = torch.utils.data.DataLoader(test_data, batch_size=32)
</code></pre>",51807745.0,4,0,,2018/8/12 8:21,1.0,2021/8/4 8:54,2018/8/12 13:27,,3697273.0,,8261926.0,,1,16,python|pytorch,16753,62.6964,,4,typeerror tensor torch image work ai course udacity come across error transfer learn section code seem cause trouble
383,383,46464549,Keras custom loss function: Accessing current input pattern,"<p>In Keras (with Tensorflow backend), is the current input pattern available to my custom loss function?</p>

<p>The current input pattern is defined as the input vector used to produce the prediction. For example, consider the following: <code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=False)</code>. Then the current input pattern is the current X_train vector associated with the y_train (which is termed y_true in the loss function).</p>

<p>When designing a custom loss function, I intend to optimize/minimize a value that requires access to the current input pattern, not just the current prediction.</p>

<p>I've taken a look through <a href=""https://github.com/fchollet/keras/blob/master/keras/losses.py"" rel=""noreferrer"">https://github.com/fchollet/keras/blob/master/keras/losses.py</a></p>

<p>I've also looked through ""<a href=""https://github.com/fchollet/keras/issues/7379"" rel=""noreferrer"">Cost function that isn't just y_pred, y_true?</a>""</p>

<p>I am also familiar with previous examples to produce a customized loss function:</p>

<pre><code>import keras.backend as K

def customLoss(y_true,y_pred):
    return K.sum(K.log(y_true) - K.log(y_pred))
</code></pre>

<p>Presumably <code>(y_true,y_pred)</code> are defined elsewhere. I've taken a look through the source code without success and I'm wondering whether I need to define the current input pattern myself or whether this is already accessible to my loss function.</p>",46465774.0,2,0,,2017/9/28 8:34,14.0,2021/4/6 11:59,2021/4/6 11:59,,10375049.0,,216295.0,,1,23,python|tensorflow|machine-learning|keras|deep-learning,8902,58.99800000000001,,4,kera custom loss function access current input pattern kera tensorflow backend current input pattern available custom loss function current input pattern define input vector use produce prediction example consider following current input pattern current x train vector associate train term true loss function design custom loss function intend optimize minimize value require access current input pattern current prediction take look also look cost function pred true also familiar previous example produce customized loss function presumably define elsewhere take look source code without success wonder whether need define current input pattern whether already accessible loss function
397,397,47079111,Create keras callback to save model predictions and targets for each batch during training,"<p>
I am building a simple Sequential model in Keras (tensorflow backend). During training I want to inspect the individual training batches and model predictions. Therefore, I am trying to create a custom <code>Callback</code> that saves the model predictions and targets for each training batch. However, the model is not using the current batch for prediction, but the entire training data.</p>

<p>How can I hand over only the current training batch to the <code>Callback</code>?</p>

<p>And how can I access the batches and targets that the <code>Callback</code> saves in self.predhis and self.targets?</p>

<p>My current version looks as follows:</p>

<pre class=""lang-py prettyprint-override""><code>callback_list = [prediction_history((self.x_train, self.y_train))]

self.model.fit(self.x_train, self.y_train, batch_size=self.batch_size, epochs=self.n_epochs, validation_data=(self.x_val, self.y_val), callbacks=callback_list)

class prediction_history(keras.callbacks.Callback):
    def __init__(self, train_data):
        self.train_data = train_data
        self.predhis = []
        self.targets = []

    def on_batch_end(self, epoch, logs={}):
        x_train, y_train = self.train_data
        self.targets.append(y_train)
        prediction = self.model.predict(x_train)
        self.predhis.append(prediction)
        tf.logging.info(""Prediction shape: {}"".format(prediction.shape))
        tf.logging.info(""Targets shape: {}"".format(y_train.shape))
</code></pre>",47081613.0,4,0,,2017/11/2 15:34,23.0,2020/9/18 10:16,2017/11/9 1:44,,2464597.0,,7384906.0,,1,30,tensorflow|callback|keras,19043,73.1189,,5,create kera callback save model prediction target batch training build simple sequential model kera tensorflow backend training want inspect individual training batch model prediction therefore try create custom save model prediction target training batch however model use current batch prediction entire training data hand current training batch access batch target save self predhis self target current version look follow
807,807,53295570,UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually,"<p>After a training procedure, I wanted to check the accuracy by loading the created <code>model.h5</code> and executing an evaluation procedure. However, I am getting a following warning:</p>

<blockquote>
  <p>/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:269:
  UserWarning: No training configuration found in save file: the model
  was <em>not</em> compiled. Compile it manually.   warnings.warn('No training
  configuration found in save file:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/qSbNC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qSbNC.png"" alt=""enter image description here""></a></p>

<p>This <code>dist-packages/keras/engine/saving.py</code> file</p>

<p>so the problem in loading created model -> this line of code</p>

<pre><code>train_model = load_model('model.h5')
</code></pre>

<p>Problem indicates that the model was not compiled, however, I did it.</p>

<pre><code>optimizer = Adam(lr=lr, clipnorm=0.001)
train_model.compile(loss=dummy_loss, optimizer=optimizer)
</code></pre>

<p>I can't understand what I am doing wrong . . .
Please help me! SOS :-(</p>",,3,2,,2018/11/14 8:09,4.0,2019/7/16 9:26,2018/11/14 8:17,,9744750.0,,8846515.0,,1,38,python|tensorflow|keras,33524,101.901,,4,userwarning training configuration find save file model compile compile manually training procedure want check accuracy load create execute evaluation procedure however get following warning usr local lib python dist package keras engine save py userwarning training configuration find save file model compile compile manually warn warn training configuration find save file file problem load create model line code problem indicate model compile however understand wrong please help sos
24,24,53975717,pytorch - connection between loss.backward() and optimizer.step(),"<p>Where is an explicit connection between the <code>optimizer</code> and the <code>loss</code>? </p>

<p>How does the optimizer know where to get the gradients of the loss without a call liks this <code>optimizer.step(loss)</code>?</p>

<p>-More context-</p>

<p>When I minimize the loss, I didn't have to pass the gradients to the optimizer.</p>

<pre><code>loss.backward() # Back Propagation
optimizer.step() # Gardient Descent
</code></pre>",53975741.0,6,0,,2018/12/30 6:30,28.0,2021/2/14 3:55,2018/12/30 6:39,,1714410.0,,3907250.0,,1,76,machine-learning|neural-network|pytorch|gradient-descent,21242,237.509,,4,pytorch connection loss backward optimizer step explicit connection optimizer know get gradient loss without call liks context minimize loss pass gradient optimizer
353,353,45624116,load weights require h5py,"<p>Im trying to run a keras model,trying to use pre-trained VGGnet-
When i run this Command
<code>base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, img_channel))</code></p>

<p>I get this error:</p>

<pre><code>  ``------------------------------------------------------------------
---------
ImportError                               Traceback (most recent call 
last)
&lt;ipython-input-79-9b18deb3bc0f&gt; in &lt;module&gt;()
  1 
----&gt; 2 base_model = applications.VGG16(weights='imagenet', 
include_top=False, input_shape=(img_rows, img_cols, img_channel))

/usr/local/lib/python3.5/dist-packages/keras/applications/vgg16.py in 
VGG16(include_top, weights, input_tensor, input_shape, pooling, 
classes)
167                                     WEIGHTS_PATH_NO_TOP,
168                                     cache_subdir='models')
--&gt; 169         model.load_weights(weights_path)
170         if K.backend() == 'theano':
171             layer_utils.convert_all_kernels_in_model(model)

/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py in 
load_weights(self, filepath, by_name)
   2563         """"""
   2564         if h5py is None:
-&gt; 2565             raise ImportError('`load_weights` requires h5py.')
   2566         f = h5py.File(filepath, mode='r')
   2567         if 'layer_names' not in f.attrs and 'model_weights' in f:

ImportError: `load_weights` requires h5py.``
</code></pre>

<p>I went through some github issues page where a relevant question was asked,but no solutions were given.
Any suggestions?</p>",45624220.0,4,0,,2017/8/10 22:04,,2018/5/16 11:02,,,,,8176285.0,,1,14,python|tensorflow|keras,17390,58.1612,,4,load weight require h py im try run keras model try use pre trained vggnet run command get error go github issue page relevant question ask solution give suggestion
771,771,51749404,"How to connect LSTM layers in Keras, RepeatVector or return_sequence=True?","<p>I'm trying to develop an Encoder model in keras for timeseries. The shape of data is (5039, 28, 1), meaning that my seq_len is 28 and I have one feature. For the first layer of the encoder, I'm using 112 hunits, second layer will have 56 and to be able to get back to the input shape for decoder, I had to add 3rd layer with 28 hunits (this autoencoder is supposed to reconstruct its input). But I don't know what is the correct approach to connect the LSTM layers together. AFAIK, I can either add <code>RepeatVector</code> or <code>return_seq=True</code>. You can see both of my models in the following code. I wonder what will be the difference and which approach is the correct one? </p>

<p>First model using <code>return_sequence=True</code>:</p>

<pre><code>inputEncoder = Input(shape=(28, 1))
firstEncLayer = LSTM(112, return_sequences=True)(inputEncoder)
snd = LSTM(56, return_sequences=True)(firstEncLayer)
outEncoder = LSTM(28)(snd)

context = RepeatVector(1)(outEncoder)
context_reshaped = Reshape((28,1))(context)

encoder_model = Model(inputEncoder, outEncoder)
firstDecoder = LSTM(112, return_sequences=True)(context_reshaped)
outDecoder = LSTM(1, return_sequences=True)(firstDecoder)

autoencoder = Model(inputEncoder, outDecoder)
</code></pre>

<p>Second model with <code>RepeatVector</code>:</p>

<pre><code>inputEncoder = Input(shape=(28, 1))
firstEncLayer = LSTM(112)(inputEncoder)
firstEncLayer = RepeatVector(1)(firstEncLayer)
snd = LSTM(56)(firstEncLayer)
snd = RepeatVector(1)(snd)
outEncoder = LSTM(28)(snd)
encoder_model = Model(inputEncoder, outEncoder)

context = RepeatVector(1)(outEncoder)
context_reshaped = Reshape((28, 1))(context)

firstDecoder = LSTM(112)(context_reshaped)
firstDecoder = RepeatVector(1)(firstDecoder)
sndDecoder = LSTM(28)(firstDecoder)

outDecoder = RepeatVector(1)(sndDecoder)
outDecoder = Reshape((28, 1))(outDecoder)

autoencoder = Model(inputEncoder, outDecoder)
</code></pre>",51757012.0,1,1,,2018/8/8 14:38,22.0,2019/10/3 14:58,,,,,3711985.0,,1,17,tensorflow|keras|deep-learning|lstm|autoencoder,10730,81.5224,,3,connect lstm layer kera repeatvector return sequence true try develop encoder model kera timeseries shape data mean seq len one feature first layer encoder use hunits second layer able get back input shape decoder add rd layer hunits autoencoder suppose reconstruct input know correct approach connect lstm layer together afaik either add see model following code wonder difference approach correct one first model use second model
167,167,42966393,Is it good learning rate for Adam method?,"<p>I am training my method. I got the result as below. Is it a good learning rate? If not, is it high or low?
This is my result</p>

<p><a href=""https://i.stack.imgur.com/ihfKv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ihfKv.png"" alt=""enter image description here""></a></p>

<pre><code>lr_policy: ""step""
gamma: 0.1
stepsize: 10000
power: 0.75
# lr for unnormalized softmax
base_lr: 0.001
# high momentum
momentum: 0.99
# no gradient accumulation
iter_size: 1
max_iter: 100000
weight_decay: 0.0005
snapshot: 4000
snapshot_prefix: ""snapshot/train""
type:""Adam""
</code></pre>

<p>This is reference</p>

<blockquote>
  <blockquote>
    <p>With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss 
    <a href=""https://i.stack.imgur.com/iMASu.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iMASu.jpg"" alt=""enter image description here""></a></p>
  </blockquote>
</blockquote>",,5,3,,2017/3/23 2:53,8.0,2021/4/6 18:08,,,,,3755376.0,,1,21,machine-learning|neural-network|deep-learning|caffe,33408,69.0954,,4,good learn rate adam method train method get result good learning rate high low result reference low learn rate improvement linear high learn rate start look exponential high learning rate decay loss faster get stick bad value loss
407,407,47305618,What is the role of TimeDistributed layer in Keras?,"<p>I am trying to grasp what TimeDistributed wrapper does in Keras.</p>

<p>I get that TimeDistributed ""applies a layer to every temporal slice of an input.""</p>

<p>But I did some experiment and got the results that I cannot understand.</p>

<p>In short, in connection to LSTM layer, TimeDistributed and just Dense layer bear same results.</p>

<pre><code>model = Sequential()
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add(TimeDistributed(Dense(1)))
print(model.output_shape)

model = Sequential()
model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
model.add((Dense(1)))
print(model.output_shape)
</code></pre>

<p>For both models, I got output shape of <strong>(None, 10, 1)</strong>.</p>

<p>Can anyone explain the difference between TimeDistributed and Dense layer after an RNN layer?</p>",47309453.0,1,3,,2017/11/15 10:57,25.0,2019/1/27 0:33,2019/1/27 0:33,,8708364.0,,7124707.0,,1,95,python|machine-learning|keras|neural-network|deep-learning,43562,135.556,,3,role timedistributed layer kera try grasp timedistributed wrapper keras get timedistributed applies layer every temporal slice input experiment get result understand short connection lstm layer timedistributed dense layer bear result model get output shape none anyone explain difference timedistributed dense layer rnn layer
740,740,40850089,Is Keras thread safe?,"<p>I'm using Python and Keras (currently using Theano backend, but I have no qualms with switching). I have a neural network that I load and process multiple sources of information with in parallel. Currently, I've been running each one in a separate process and it loads its own copy of the network from the file. This seems like a waste of RAM, so I was thinking it would be more efficient to have a single multi-threaded process with one instance of the network that is used by all threads. However, I'm wondering if Keras is thread safe with either backend. If I run <code>.predict(x)</code> on two different inputs at the same time in different threads, will I run into race conditions or other issues?</p>

<p>Thanks</p>",43393252.0,2,0,,2016/11/28 17:25,6.0,2019/12/8 17:04,,,,,1107282.0,,1,23,python|multithreading|keras,11429,56.432,,3,keras thread safe use python kera currently use theano backend qualm switch neural network load process multiple source information parallel currently run one separate process load copy network file seem like waste ram think would efficient single multi thread process one instance network use thread however wonder keras thread safe either backend run two different input time different thread run race condition issue thanks
409,409,47380663,Numpy reverse keras to_categorical,<p>In keras I have used <code>to_categorical</code> to convert by binary nx1 vector y to a nx2 matrix where the first columns is 1 if y=1 and the second column is y=0. How do I reverse this action using numpy?</p>,47381500.0,4,2,,2017/11/19 18:47,1.0,2021/3/18 2:17,,,,,1603548.0,,1,13,numpy|keras,13329,58.8992,,3,numpy reverse kera categorical kera use convert binary nx vector nx matrix first column second column reverse action use numpy
499,499,32419510,How to get reproducible results in keras,"<p>I get different results (test accuracy) every time I run the <code>imdb_lstm.py</code> example from Keras framework (<a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py"">https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py</a>)
The code contains <code>np.random.seed(1337)</code> in the top, before any keras imports. It should prevent it from generating different numbers for every run. What am I missing?  </p>

<p>UPDATE: How to repro:  </p>

<ol>
<li>Install Keras (<a href=""http://keras.io/"">http://keras.io/</a>)   </li>
<li>Execute <a href=""https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py"">https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py</a>  a few times. It will train the model and output test accuracy.<br>
Expected result: Test accuracy is the same on every run.<br>
Actual result: Test accuracy is different on every run.</li>
</ol>

<p>UPDATE2: I'm running it on Windows 8.1 with MinGW/msys, module versions:<br>
theano 0.7.0<br>
numpy 1.8.1<br>
scipy 0.14.0c1</p>

<p>UPDATE3: I narrowed the problem down a bit. If I run the example with GPU (set theano flag device=gpu0) then I get different test accuracy every time, but if I run it on CPU then everything works as expected. My graphics card: NVIDIA GeForce GT 635)</p>",,11,10,,2015/9/6 2:41,30.0,2020/11/18 14:55,2015/9/15 18:04,,1245190.0,,579980.0,,1,86,python|numpy|theano|keras,38323,316.534,,4,get reproducible result kera get different result test accuracy every time run example kera framework code contain top kera import prevent generate different number every run miss update repro install kera execute time train model output test accuracy expect result test accuracy every run actual result test accuracy different every run update run window mingw msys module version theano numpy scipy c update narrow problem bit run example gpu set theano flag device gpu get different test accuracy every time run cpu everything work expect graphic card nvidia geforce gt
752,752,51186330,save model weights at the end of every N epochs,"<p>I'm training a NN and would like to save the model weights every N epochs for a prediction phase. I propose this draft code, it's inspired by @grovina 's response <a href=""https://stackoverflow.com/a/44058144/1545917"">here</a>. Could you, please, make suggestions?
Thanks in advance.</p>

<pre><code>from keras.callbacks import Callback

class WeightsSaver(Callback):
    def __init__(self, model, N):
        self.model = model
        self.N = N
        self.epoch = 0

    def on_batch_end(self, epoch, logs={}):
        if self.epoch % self.N == 0:
            name = 'weights%08d.h5' % self.epoch
            self.model.save_weights(name)
        self.epoch += 1
</code></pre>

<p>Then add it to the fit call: to save weights every 5 epochs:</p>

<pre><code>model.fit(X_train, Y_train, callbacks=[WeightsSaver(model, 5)])
</code></pre>",51186488.0,2,1,,2018/7/5 8:12,2.0,2018/7/5 9:10,2018/7/5 9:10,,1545917.0,,6933148.0,,1,11,python|tensorflow|callback|keras,14013,50.9861,,3,save model weight end every n epoch train nn would like save model weight every n epoch prediction phase propose draft code inspire grovina response could please make suggestion thanks advance add fit call save weight every epoch
384,384,46493419,Use a generator for Keras model.fit_generator,"<p>I originally tried to use <code>generator</code> syntax when writing a custom generator for training a Keras model. So I <code>yield</code>ed from <code>__next__</code>. However, when I would try to train my mode with <code>model.fit_generator</code> I would get an error that my generator was not an iterator. The fix was to change <code>yield</code> to <code>return</code> which also necessitated rejiggering the logic of <code>__next__</code> to track state. It's quite cumbersome compared to letting <code>yield</code> do the work for me.</p>

<p>Is there a way I can make this work with <code>yield</code>? I will need to write several more iterators that will have to have very clunky logic if I have to use a <code>return</code> statement.</p>",,4,1,,2017/9/29 16:43,7.0,2021/5/27 10:20,,,,,8452765.0,,1,16,python-3.x|iterator|keras|generator,40861,77.2452,,3,use generator kera model fit generator originally try use syntax write custom generator train keras model ed however would try train mode would get error generator iterator fix change also necessitate rejiggering logic track state quite cumbersome compare let work way make work need write several iterators clunky logic use statement
651,651,37084155,How to use log_loss as metric in Keras?,"<p>I am using Keras and I want to use logloss as metric for training. How I can pass that into my model?</p>

<p>My code is as follows:</p>

<pre><code>model = Sequential()
model.add(Dense(output_dim=1000, input_dim=390, init='uniform'))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=500, input_dim=1000, init=""lecun_uniform""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=10, input_dim=300, init=""lecun_uniform""))
model.add(Activation(""sigmoid""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=200, input_dim=10, init=""lecun_uniform""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=100, input_dim=200, init =""glorot_normal""))
model.add(Activation(""relu""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=50, input_dim=100, init =""he_normal""))
model.add(Activation(""sigmoid""))
model.add(Dropout(0.5))
model.add(Dense(output_dim=2, input_dim=50, init = ""normal""))
model.add(Activation(""softmax""))
model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])

model.fit(train.values, y1,  nb_epoch=10,
          batch_size=50000, verbose=2,validation_split=0.3, class_weight={1:0.96, 0:0.04})


proba = model.predict_proba(train.values)
log_loss(y, proba[:,1])
</code></pre>

<p>How can I pass log_loss in place of accuracy?</p>",37156047.0,1,0,,2016/5/7 3:21,1.0,2017/9/6 11:51,2017/9/6 11:51,,7117003.0,,1972924.0,,1,13,python-2.7|keras,16118,62.4292,,4,use log loss metric kera use kera want use logloss metric train pass model code follow pas log loss place accuracy
361,361,45943675,Meaning of validation_steps in Keras Sequential fit_generator parameter list,"<p>I am using Keras with a Tensorflow backend in Python. To be more precise tensorflow <strong>1.2.1</strong> and its build-in contrib.keras lib.</p>

<p>I want to use the <code>fit_generator</code> method of a Sequential model object, but I am confused with what I should pass as the method-parameters.</p>

<p>From reading the doc <a href=""https://keras.io/models/sequential/#fit_generator"" rel=""noreferrer"">here</a> I got the following information:  </p>

<ul>
<li><strong>generator</strong> : a python training data batch generator; endlessly looping over its training data</li>
<li><strong>validation_data</strong>:  -<em>in my case</em> - a python validation data batch generator; the doc doesn't mention endless looping over its validation data</li>
<li><strong>steps_per_epoch</strong> : <code>number of training batches = uniqueTrainingData / batchSize</code></li>
<li><strong>validation steps</strong> : <code>???</code> ; = uniqueValidationData / batch size ???</li>
<li><strong>use_multiprocessing</strong> : boolean; don't pass non picklable arguments ???</li>
<li><strong>workers</strong> : max number of used processes </li>
</ul>

<p>As indicated above with ??? I don't really know what validation_steps means.
I know the definition of the above linked doc (<code>Number of steps to yield from validation generator at the end of every epoch</code>) but that only confuses my in the given context. From the doc i know that the validation_data generator has to yield data, label tuples in the form <code>(inputs, targets)</code>. In contrast to that the above statement indicates that there have to be multiple ""steps to yield from validation generator at the end of every epoch"" which in this context would mean, that multiple validation batches would be yielded after each training epoch.  </p>

<p>Questions about <code>validation_steps</code>:</p>

<ul>
<li>Does it really work that way? If so: Why? I thought that after each epoch one validation batch, which ideally wasn't used before, is used for validation to ensure that the training gets validated without risking to ""train"" the model to perform better on already used validation sets.  </li>
<li>In context of the previous question: Why is the recommended amount of validation steps <code>uniqueValidationData / batches</code> and not <code>uniqueValidationData / epochs</code>? Isn't it better to have e.g. 100 validation batches for 100 epochs instead of x validation batches where x could be less or more than the specified number of epochs? Alternatively: If you have much less validation batches than number of epoches, is the model trained without validation for the rest of the epochs or do validation sets get reused / reshuffled+reused? </li>
<li>Is it important that the training and validation batches have the same batch size (shared divisor of the dividends trainingDataCount and validationDataCount)?</li>
</ul>

<p>Additional question about <code>use_multiprocessing</code>:</p>

<ul>
<li>Are numpy arrays picklable or do I have to convert them to multidimensional lists?</li>
</ul>",45944225.0,1,0,,2017/8/29 16:04,19.0,2019/10/8 23:11,2019/10/8 23:11,,3689502.0,,8355555.0,,1,54,parameters|keras|generator|data-fitting,42345,112.307,,3,meaning validation step keras sequential fit generator parameter list use kera tensorflow backend python precise tensorflow build contrib kera lib want use method sequential model object confuse pass method parameter read doc get following information generator python training data batch generator endlessly loop training data validation data case python validation data batch generator doc mention endless loop validation data step per epoch validation step uniquevalidationdata batch size use multiprocessing boolean pass non picklable argument worker max number used process indicate really know validation step mean know definition link doc confuse give context doc know validation data generator yield data label tuples form contrast statement indicate multiple step yield validation generator end every epoch context would mean multiple validation batch would yield training epoch question really work way think epoch one validation batch ideally use use validation ensure training get validate without risk train model perform good already use validation set context previous question recommended amount validation step good e g validation batch epochs instead x validation batch x could less specified number epoch alternatively much less validation batch number epoch model train without validation rest epoch validation set get reuse reshuffle reuse important training validation batch batch size share divisor dividend trainingdatacount validationdatacount additional question numpy array picklable convert multidimensional list
775,775,51821537,AttributeError: 'Node' object has no attribute 'output_masks',"<p>I use Keras pretrained model VGG16. The problem is that after configuring tensorflow to use the GPU I get an error that I didn't have before when using the CPU.</p>

<p>The error is the following one:</p>

<pre><code>    Traceback (most recent call last):
  File ""/home/guillaume/Documents/Allianz/ConstatOrNotConstatv3/train_network.py"",      line 109, in &lt;module&gt;
    model = LeNet.build(width=100, height=100, depth=3, classes=5)
  File ""/home/guillaume/Documents/Allianz/ConstatOrNotConstatv3/lenet.py"", line 39,    in build
    output = model(pretrainedOutput)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 443, in __call__
    previous_mask = _collect_previous_mask(inputs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 1311, in _collect_previous_mask
mask = node.output_masks[tensor_index]
  AttributeError: 'Node' object has no attribute 'output_masks'
</code></pre>

<p>I get it after executing this code : </p>

<pre><code>    pretrained_model = VGG16(
        include_top=False,
        input_shape=(height, width, depth),
        weights='imagenet'
    )
    for layer in pretrained_model.layers:
        layer.trainable = False

    model = Sequential()
    # first (and only) set of FC =&gt; RELU layers
    model.add(Flatten())
    model.add(Dense(200, activation='relu'))
    model.add(Dropout(0.5))
    model.add(BatchNormalization())
    model.add(Dense(400, activation='relu'))
    model.add(Dropout(0.5))
    model.add(BatchNormalization())

    # softmax classifier
    model.add(Dense(classes,activation='softmax'))

    pretrainedInput = pretrained_model.input
    pretrainedOutput = pretrained_model.output
    output = model(pretrainedOutput)
    model = Model(pretrainedInput, output)
</code></pre>

<p>EDIT1 : I've got keras (2.2.2) and tensorflow(1.10.0rc1). I've also tried on keras 2.2.0 and same error. The thing is that the python environment I use works on others non-pretrained NN.</p>

<p>EDIT2 : I'm able to connect two homemade models. It's only whith the pretrained ones there is a problem and not only VGG16.</p>",51831434.0,4,2,,2018/8/13 11:41,3.0,2020/6/14 14:25,2018/8/13 14:43,,4953424.0,,4953424.0,,1,15,python|tensorflow|keras|pre-trained-model,22158,86.3821,,4,attributeerror node object attribute output mask use kera pretrained model vgg problem configure tensorflow use gpu get error use cpu error follow one get execute code edit get kera tensorflow rc also try kera error thing python environment use work others non pretrained nn edit able connect two homemade model whith pretrained one problem vgg
557,557,48709839,StopIteration: generator_output = next(output_generator),"<p>I have the following code which I rewrite to work on a large scale dataset. I am using Python generator to Fit the model on data yielded batch-by-batch.</p>

<pre><code>def subtract_mean_gen(x_source,y_source,avg_image,batch):
    batch_list_x=[]
    batch_list_y=[]
    for line,y in zip(x_source,y_source):
        x=line.astype('float32')
        x=x-avg_image
        batch_list_x.append(x)
        batch_list_y.append(y)
        if len(batch_list_x) == batch:
            yield (np.array(batch_list_x),np.array(batch_list_y))
            batch_list_x=[]
            batch_list_y=[] 

model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

val = subtract_mean_gen(X_test,Y_test,avg_image_test,batch_size)
model.fit_generator(subtract_mean_gen(X_train,Y_train,avg_image_train,batch_size), steps_per_epoch=X_train.shape[0]//batch_size,epochs=nb_epoch,validation_data = val,
                    validation_steps = X_test.shape[0]//batch_size)
</code></pre>

<p>I obtain the following error:</p>

<pre><code>239/249 [===========================&gt;..] - ETA: 60s - loss: 1.3318 - acc: 0.8330Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/utils/data_utils.py"", line 560, in data_generator_task
    generator_output = next(self._generator)
StopIteration

240/249 [===========================&gt;..] - ETA: 54s - loss: 1.3283 - acc: 0.8337Traceback (most recent call last):
  File ""cifa10-copy.py"", line 125, in &lt;module&gt;
    validation_steps = X_test.shape[0]//batch_size)
  File ""/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.py"", line 87, in wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/training.py"", line 1809, in fit_generator
    generator_output = next(output_generator)
StopIteration
</code></pre>

<p>I looked into a similar question posted <a href=""https://stackoverflow.com/questions/46302911/what-raises-stopiteration-in-mine-keras-model-fit-generator"">here</a> however, I am not able to resolve the error why StopIteration is raised. </p>",48709941.0,2,1,,2018/2/9 16:10,1.0,2018/2/9 16:54,,,,,8967121.0,,1,12,python|numpy|keras,8639,50.5459,,4,stopiteration generator output next output generator following code rewrite work large scale dataset use python generator fit model data yield batch batch obtain following error look similar question post however able resolve error stopiteration raise
433,433,47870003,How to multiply a matrix by a vector in PyTorch,"<p>I'm playing around with PyTorch with the aim of learning it, and I have a very dumb question: how can I multiply a matrix by a single vector?</p>

<p>Here's what I've tried:</p>

<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.rand(4,4)
&gt;&gt;&gt; a

 0.3162  0.4434  0.9318  0.8752
 0.0129  0.8609  0.6402  0.2396
 0.5720  0.7262  0.7443  0.0425
 0.4561  0.1725  0.4390  0.8770
[torch.FloatTensor of size 4x4]

&gt;&gt;&gt; b = torch.rand(4)
&gt;&gt;&gt; b

 0.1813
 0.7090
 0.0329
 0.7591
[torch.FloatTensor of size 4]

&gt;&gt;&gt; a.mm(b)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
RuntimeError: invalid argument 2: dimension 1 out of range of 1D tensor at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensor.c:24
&gt;&gt;&gt; a.mm(b.t())
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
RuntimeError: t() expects a 2D tensor, but self is 1D
&gt;&gt;&gt; b.mm(a)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
RuntimeError: matrices expected, got 1D, 2D tensors at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1288
&gt;&gt;&gt; b.t().mm(a)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
RuntimeError: t() expects a 2D tensor, but self is 1D
</code></pre>

<p>On the other hand, if I do</p>

<pre><code>&gt;&gt;&gt; b = torch.rand(4,2)
</code></pre>

<p>then my first attempt, <code>a.mm(b)</code>, works fine. So the problem is just that I'm multiplying a vector rather than a matrix --- but how can I do this?</p>",47871845.0,2,0,,2017/12/18 13:56,,2017/12/19 4:41,,,,,1119340.0,,1,20,pytorch,19451,64.1558,,3,multiply matrix vector pytorch play around pytorch aim learn dumb question multiply matrix single vector try hand first attempt work fine problem multiply vector rather matrix
77,77,4752626,Epoch vs Iteration when training neural networks,<p>What is the difference between <em>epoch</em> and <em>iteration</em> when training a multi-layer perceptron?</p>,,14,1,,2011/1/20 21:11,229.0,2020/12/30 5:43,2018/7/9 21:48,,2956066.0,,522126.0,,1,451,machine-learning|neural-network|deep-learning|artificial-intelligence|terminology,234525,2199.28,2021/2/10 12:31,0,epoch v iteration train neural network difference epoch iteration train multi layer perceptron
830,830,54255431,InvalidArgumentError: cannot compute MatMul as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:MatMul],"<p>Can somebody explain, how does TensorFlow's eager mode work? I am trying to build a simple regression as follows:</p>

<pre><code>import tensorflow as tf

tfe = tf.contrib.eager
tf.enable_eager_execution()

import numpy as np


def make_model():
    net = tf.keras.Sequential()
    net.add(tf.keras.layers.Dense(4, activation='relu'))
    net.add(tf.keras.layers.Dense(1))
    return net

def compute_loss(pred, actual):
    return tf.reduce_mean(tf.square(tf.subtract(pred, actual)))

def compute_gradient(model, pred, actual):
    """"""compute gradients with given noise and input""""""
    with tf.GradientTape() as tape:
        loss = compute_loss(pred, actual)
    grads = tape.gradient(loss, model.variables)
    return grads, loss

def apply_gradients(optimizer, grads, model_vars):
    optimizer.apply_gradients(zip(grads, model_vars))

model = make_model()
optimizer = tf.train.AdamOptimizer(1e-4)

x = np.linspace(0,1,1000)
y = x+np.random.normal(0,0.3,1000)
y = y.astype('float32')
train_dataset = tf.data.Dataset.from_tensor_slices((y.reshape(-1,1)))

epochs = 2# 10
batch_size = 25
itr = y.shape[0] // batch_size
for epoch in range(epochs):
    for data in tf.contrib.eager.Iterator(train_dataset.batch(25)):
        preds = model(data)
        grads, loss = compute_gradient(model, preds, data)
        print(grads)
        apply_gradients(optimizer, grads, model.variables)
#         with tf.GradientTape() as tape:
#             loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))
#         grads = tape.gradient(loss, model.variables)
#         print(grads)
#         optimizer.apply_gradients(zip(grads, model.variables),global_step=None)
</code></pre>

<p><code>Gradient output: [None, None, None, None, None, None]</code>
The error is following:</p>

<pre><code>----------------------------------------------------------------------
ValueError                           Traceback (most recent call last)
&lt;ipython-input-3-a589b9123c80&gt; in &lt;module&gt;
     35         grads, loss = compute_gradient(model, preds, data)
     36         print(grads)
---&gt; 37         apply_gradients(optimizer, grads, model.variables)
     38 #         with tf.GradientTape() as tape:
     39 #             loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))

&lt;ipython-input-3-a589b9123c80&gt; in apply_gradients(optimizer, grads, model_vars)
     17 
     18 def apply_gradients(optimizer, grads, model_vars):
---&gt; 19     optimizer.apply_gradients(zip(grads, model_vars))
     20 
     21 model = make_model()

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
    589     if not var_list:
    590       raise ValueError(""No gradients provided for any variable: %s."" %
--&gt; 591                        ([str(v) for _, v, _ in converted_grads_and_vars],))
    592     with ops.init_scope():
    593       self._create_slots(var_list)

ValueError: No gradients provided for any variable:
</code></pre>

<h3>Edit</h3>

<p>I updated my code. Now, the problem comes in gradients calculation, it is returning zero. I have checked the loss value that is non-zero.</p>",54255819.0,1,0,,2019/1/18 13:52,3.0,2020/1/9 22:15,2020/1/9 22:15,,3924118.0,,9277245.0,,1,25,python|tensorflow|keras|eager-execution,36946,61.2703,,4,invalidargumenterror compute matmul input zero base expect float tensor double tensor op matmul somebody explain tensorflow eager mode work try build simple regression follow error follow edit update code problem come gradient calculation return zero check loss value non zero
559,559,48775305,What function defines accuracy in Keras when the loss is mean squared error (MSE)?,"<p>How is Accuracy defined when the loss function is mean square error? Is it <a href=""https://en.wikipedia.org/wiki/Mean_absolute_percentage_error"" rel=""noreferrer"">mean absolute percentage error</a>?</p>



<p>The model I use has output activation linear and is compiled with <code>loss= mean_squared_error</code></p>

<pre class=""lang-python prettyprint-override""><code>model.add(Dense(1))
model.add(Activation('linear'))  # number

model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>and the output looks like this:</p>

<pre class=""lang-python prettyprint-override""><code>Epoch 99/100
1000/1000 [==============================] - 687s 687ms/step - loss: 0.0463 - acc: 0.9689 - val_loss: 3.7303 - val_acc: 0.3250
Epoch 100/100
1000/1000 [==============================] - 688s 688ms/step - loss: 0.0424 - acc: 0.9740 - val_loss: 3.4221 - val_acc: 0.3701
</code></pre>

<p>So what does e.g. val_acc: 0.3250 mean? Mean_squared_error should be a scalar not a percentage - shouldnt it? So is val_acc - mean squared error, or mean percentage error or another function?</p>

<p>From definition of MSE on wikipedia:<a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Mean_squared_error</a></p>

<blockquote>
  <p>The MSE is a measure of the quality of an estimator闂佺偨鍎查弨鐨?is always
  non-negative, and values closer to zero are better.</p>
</blockquote>

<p>Does that mean a value of <code>val_acc: 0.0</code> is better than <code>val_acc: 0.325</code>?</p>

<p>edit: more examples of the output of accuracy metric when I train - where the accuracy is increase as I train more. While the loss function - mse should decrease. Is Accuracy well defined for mse - and how is it defined in Keras?</p>

<pre class=""lang-python prettyprint-override""><code>lAllocator: After 14014 get requests, put_count=14032 evicted_count=1000 eviction_rate=0.0712657 and unsatisfied allocation rate=0.071714
1000/1000 [==============================] - 453s 453ms/step - loss: 17.4875 - acc: 0.1443 - val_loss: 98.0973 - val_acc: 0.0333
Epoch 2/100
1000/1000 [==============================] - 443s 443ms/step - loss: 6.6793 - acc: 0.1973 - val_loss: 11.9101 - val_acc: 0.1500
Epoch 3/100
1000/1000 [==============================] - 444s 444ms/step - loss: 6.3867 - acc: 0.1980 - val_loss: 6.8647 - val_acc: 0.1667
Epoch 4/100
1000/1000 [==============================] - 445s 445ms/step - loss: 5.4062 - acc: 0.2255 - val_loss: 5.6029 - val_acc: 0.1600
Epoch 5/100
783/1000 [======================&gt;.......] - ETA: 1:36 - loss: 5.0148 - acc: 0.2306
</code></pre>",48788577.0,3,7,,2018/2/13 20:43,11.0,2021/4/27 13:45,2019/1/31 9:34,,4685471.0,,2707144.0,,1,25,machine-learning|keras|regression|loss-function|mean-square-error,19292,86.1415,,4,function define accuracy kera loss mean squared error mse accuracy define loss function mean square error mean absolute percentage error model use output activation linear compile output look like e g val acc mean mean square error scalar percentage shouldnt val acc mean square error mean percentage error another function definition mse wikipedia mse measure quality estimator always non negative value closer zero well mean value good edit example output accuracy metric train accuracy increase train loss function mse decrease accuracy well define mse define kera
288,288,56675943,Meaning of parameters in torch.nn.conv2d,"<p>In the fastai cutting edge deep learning for coders course lecture 7.</p>

<pre><code> self.conv1 = nn.Conv2d(3,10,kernel_size = 5,stride=1,padding=2)
</code></pre>

<p>Does 10 there mean the number of filters or the number activations the filter will give? </p>",,2,1,,2019/6/19 21:15,14.0,2021/7/22 20:40,2020/3/31 13:07,,4197269.0,,8133163.0,,1,23,python|machine-learning|artificial-intelligence|pytorch,25471,70.8242,,3,meaning parameter torch nn conv fastai cut edge deep learning coder course lecture mean number filter number activations filter give
14,14,62359175,Pytorch says that CUDA is not available,"<p>I'm trying to run Pytorch on a laptop that I have. It's an older model but it does have an Nvidia graphics card. I realize it is probably not going to be sufficient for real machine learning but I am trying to do it so I can learn the process of getting CUDA installed.</p>

<p>I have followed the steps on the <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html"" rel=""noreferrer"">installation guide</a> for Ubuntu 18.04 (my specific distribution is Xubuntu).</p>

<p>My graphics card is a GeForce 845M, verified by <code>lspci | grep nvidia</code>:</p>

<pre><code>01:00.0 3D controller: NVIDIA Corporation GM107M [GeForce 845M] (rev a2)
01:00.1 Audio device: NVIDIA Corporation Device 0fbc (rev a1)
</code></pre>

<p>I also have gcc 7.5 installed, verified by <code>gcc --version</code></p>

<pre><code>gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>

<p>And I have the correct headers installed, verified by trying to install them with <code>sudo apt-get install linux-headers-$(uname -r)</code>:</p>

<pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
linux-headers-4.15.0-106-generic is already the newest version (4.15.0-106.107).
</code></pre>

<p>I then followed the installation instructions using a local .deb for version 10.1.</p>

<p>Npw, when I run <code>nvidia-smi</code>, I get:</p>

<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce 845M        On   | 00000000:01:00.0 Off |                  N/A |
| N/A   40C    P0    N/A /  N/A |     88MiB /  2004MiB |      1%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       982      G   /usr/lib/xorg/Xorg                            87MiB |
+-----------------------------------------------------------------------------+
</code></pre>

<p>and I run <code>nvcc -V</code> I get:</p>

<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>

<p>I then performed the post-installation instructions from <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#mandatory-post"" rel=""noreferrer"">section 6.1</a>, and so as a result, <code>echo $PATH</code> looks like this:</p>

<pre><code>/home/isaek/anaconda3/envs/stylegan2_pytorch/bin:/home/isaek/anaconda3/bin:/home/isaek/anaconda3/condabin:/usr/local/cuda-10.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
</code></pre>

<p><code>echo $LD_LIBRARY_PATH</code> looks like this:</p>

<pre><code>/usr/local/cuda-10.1/lib64
</code></pre>

<p>and my <code>/etc/udev/rules.d/40-vm-hotadd.rules</code> file looks like this:</p>

<pre><code># On Hyper-V and Xen Virtual Machines we want to add memory and cpus as soon as they appear
ATTR{[dmi/id]sys_vendor}==""Microsoft Corporation"", ATTR{[dmi/id]product_name}==""Virtual Machine"", GOTO=""vm_hotadd_apply""
ATTR{[dmi/id]sys_vendor}==""Xen"", GOTO=""vm_hotadd_apply""
GOTO=""vm_hotadd_end""

LABEL=""vm_hotadd_apply""

# Memory hotadd request

# CPU hotadd request
SUBSYSTEM==""cpu"", ACTION==""add"", DEVPATH==""/devices/system/cpu/cpu[0-9]*"", TEST==""online"", ATTR{online}=""1""

LABEL=""vm_hotadd_end""
</code></pre>

<p>After all of this, I even compiled and ran the samples. <code>./deviceQuery</code> returns:</p>

<pre><code>./deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: ""GeForce 845M""
  CUDA Driver Version / Runtime Version          10.1 / 10.1
  CUDA Capability Major/Minor version number:    5.0
  Total amount of global memory:                 2004 MBytes (2101870592 bytes)
  ( 4) Multiprocessors, (128) CUDA Cores/MP:     512 CUDA Cores
  GPU Max Clock rate:                            863 MHz (0.86 GHz)
  Memory Clock rate:                             1001 Mhz
  Memory Bus Width:                              64-bit
  L2 Cache Size:                                 1048576 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            No
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 10.1, NumDevs = 1
Result = PASS
</code></pre>

<p>and <code>./bandwidthTest</code> returns:</p>

<pre><code>[CUDA Bandwidth Test] - Starting...
Running on...

 Device 0: GeForce 845M
 Quick Mode

 Host to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         11.7

 Device to Host Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         11.8

 Device to Device Bandwidth, 1 Device(s)
 PINNED Memory Transfers
   Transfer Size (Bytes)    Bandwidth(GB/s)
   32000000         14.5

Result = PASS

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
</code></pre>

<p>But after all of this, this Python snippet (in a conda environment with all dependencies installed):</p>

<pre><code>import torch
torch.cuda.is_available()
</code></pre>

<p>returns <code>False</code></p>

<p>Does anybody have any idea about how to resolve this? I've tried to add <code>/usr/local/cuda-10.1/bin</code> to <code>etc/environment</code> like this:</p>

<pre><code>PATH=""/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games""
PATH=$PATH:/usr/local/cuda-10.1/bin
</code></pre>

<p>And restarting the terminal, but that didn't fix it. I really don't know what else to try.</p>

<h1>EDIT - Results of collect_env for @kHarshit</h1>

<pre><code>Collecting environment information...
PyTorch version: 1.5.0
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.4 LTS
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
CMake version: Could not collect

Python version: 3.6
Is CUDA available: No
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce 845M
Nvidia driver version: 418.87.00
cuDNN version: Could not collect

Versions of relevant libraries:
[pip] numpy==1.18.5
[pip] pytorch-ranger==0.1.1
[pip] stylegan2-pytorch==0.12.0
[pip] torch==1.5.0
[pip] torch-optimizer==0.0.1a12
[pip] torchvision==0.6.0
[pip] vector-quantize-pytorch==0.0.2
[conda] numpy                     1.18.5                   pypi_0    pypi
[conda] pytorch-ranger            0.1.1                    pypi_0    pypi
[conda] stylegan2-pytorch         0.12.0                   pypi_0    pypi
[conda] torch                     1.5.0                    pypi_0    pypi
[conda] torch-optimizer           0.0.1a12                 pypi_0    pypi
[conda] torchvision               0.6.0                    pypi_0    pypi
[conda] vector-quantize-pytorch   0.0.2                    pypi_0    pypi
</code></pre>",62361395.0,1,3,,2020/6/13 11:36,3.0,2021/1/5 16:58,2020/6/13 15:08,,681865.0,,2068931.0,,1,22,linux|pytorch|ubuntu-18.04,43095,52.9377,,1,pytorch say cuda available try run pytorch laptop old model nvidia graphic card realize probably go sufficient real machine learn try learn process get cuda instal follow step installation guide ubuntu specific distribution xubuntu graphic card geforce verify also gcc instal verify correct header instal verify try install follow installation instruction use local deb version npw run get run get perform post installation instruction section result look like look like file look like even compile run sample devicequery return devicequery start cuda device query runtime api version cudart static linking detect cuda capable device device geforce cuda driver version runtime version cuda capability major minor version number total amount global memory mbytes bytes multiprocessor cuda core mp cuda core gpu max clock rate mhz ghz memory clock rate mhz memory bus width bit l cache size byte maximum texture dimension size x z maximum layer texture size num layer layer maximum layered texture size num layer layer total amount constant memory byte total amount shared memory per block byte total number register available per block warp size maximum number thread per multiprocessor maximum number thread per block max dimension size thread block x z max dimension size grid size x z maximum memory pitch bytes texture alignment byte concurrent copy kernel execution yes copy engine run time limit kernel yes integrated gpu share host memory support host page lock memory map yes alignment requirement surface yes device ecc support disable device support unify address uva yes device support compute preemption support cooperative kernel launch supports multidevice co op kernel launch device pci domain id bus id location id compute mode default multiple host thread use cudasetdevice device simultaneously devicequery cuda driver cudart cuda driver version cuda runtime version numdevs result pas bandwidthtest return cuda bandwidth test start run device geforce quick mode host device bandwidth device pin memory transfer transfer size byte bandwidth gb device host bandwidth device pin memory transfer transfer size byte bandwidth gb device device bandwidth device pin memory transfer transfer size byte bandwidth gb result pas note cuda sample mean performance measurement result may vary gpu boost enable python snippet conda environment dependency instal import torch torch cuda available return false anybody idea resolve try add usr local cuda bin etc environment like path usr local sbin usr local bin usr sbin usr bin sbin bin usr game usr local game path path usr local cuda bin restart terminal fix really know else try edit result collect env kharshit collect environment information pytorch version debug build cuda use build pytorch ubuntu lts gcc version ubuntu ubuntu cmake version could collect python version cuda available cuda runtime version gpu model configuration gpu geforce nvidia driver version cudnn version could collect version relevant library pip numpy pip pytorch ranger pip stylegan pytorch pip torch pip torch optimizer pip torchvision pip vector quantize pytorch conda numpy pypi pypi conda pytorch ranger pypi pypi conda stylegan pytorch pypi pypi conda torch pypi pypi conda torch optimizer pypi pypi conda torchvision pypi pypi conda vector quantize pytorch pypi pypi
106,106,41648129,balancing an imbalanced dataset with keras image generator,"<p>The keras </p>

<pre><code>ImageDataGenerator
</code></pre>

<p>can be used to ""<a href=""https://keras.io/preprocessing/image/"" rel=""noreferrer"">Generate batches of tensor image data with real-time data augmentation</a>""</p>

<p>The tutorial <a href=""https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"" rel=""noreferrer"">here</a> demonstrates how a small but balanced dataset can be augmented using the ImageDataGenerator. Is there an easy way to use this generator to augment a heavily unbalanced dataset, such that the resulting, generated dataset is balanced?</p>",41648242.0,2,0,,2017/1/14 8:29,8.0,2021/5/24 20:22,2020/3/2 15:53,,6941400.0,,1934212.0,,1,32,keras,15726,77.5865,,2,balance imbalanced dataset keras image generator kera use generate batch tensor image data real time data augmentation tutorial demonstrate small balanced dataset augment use imagedatagenerator easy way use generator augment heavily unbalanced dataset result generated dataset balance
348,348,45477133,How to change CUDA version,"<p>I met this error when compiling a modified caffe version.</p>

<p><code>OpenCV static library was compiled with CUDA 7.5 support. Please, use the same version or rebuild OpenCV with CUDA 8.0</code></p>

<p>I have some old code may not compatible with CUDA8.0, so I want to change my cuda version for this error.</p>

<p>I modified my ~/.bash_profile like this</p>

<pre><code># export PYTHONPATH=$PYTHONPATH:/usr/local/cuda-8.0/lib64/
# export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64
export PYTHONPATH=$PYTHONPATH:/usr/local/cuda-7.5/targets/x86_64-linux/lib/
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-7.5/targets/x86_64-linux/lib/
</code></pre>

<p>But it did't work. Still the same error. What should I do? Thanks.</p>",46069613.0,3,7,,2017/8/3 7:05,6.0,2021/5/23 9:40,2021/5/23 9:40,,681865.0,,7033937.0,,1,16,linux|opencv|caffe|opencv3.0,45401,55.2283,,1,change cuda version meet error compile modify caffe version old code may compatible cuda want change cuda version error modify bash profile like work still error thanks
744,744,41032551,How to compute Receiving Operating Characteristic (ROC) and AUC in keras?,"<p>I have a multi output(200) binary classification model which I wrote in keras.</p>
<p>In this model I want to add additional metrics such as ROC and AUC but to my knowledge keras dosen't have in-built ROC and AUC metric functions.</p>
<p>I tried to import ROC, AUC functions from scikit-learn</p>
<pre><code>from sklearn.metrics import roc_curve, auc
from keras.models import Sequential
from keras.layers import Dense
.
.
.
model.add(Dense(200, activation='relu'))
model.add(Dense(300, activation='relu'))
model.add(Dense(400, activation='relu'))
model.add(Dense(300, activation='relu'))
model.add(Dense(200,init='normal', activation='softmax')) #outputlayer

model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy','roc_curve','auc'])
</code></pre>
<p>but it's giving this error:</p>
<pre><code>Exception: Invalid metric: roc_curve
</code></pre>
<p>How should I add ROC, AUC to keras?</p>",,8,2,,2016/12/8 5:44,38.0,2021/7/11 21:31,2021/7/11 21:27,,4685471.0,,996366.0,,1,62,python|theano|keras,71514,274.618,,5,compute receive operate characteristic roc auc kera multi output binary classification model write kera model want add additional metric roc auc knowledge kera dose build roc auc metric function try import roc auc function scikit learn give error add roc auc keras
653,653,37179332,Error in keras - name 'Dense' is not defined,"<p>I'm new to Deep Neural Network libraries in python. I've installed Theano &amp; keras in my windows system by following these steps(I already had anaconda):</p>

<p>Install TDM GCC x64.</p>

<p>Run the below code from command prompt</p>

<pre><code>conda update conda
conda update --all
conda install mingw libpython
pip install git+git://github.com/Theano/Theano.git
pip install git+git://github.com/fchollet/keras.git
</code></pre>

<p>When I'm running the following code in Ipython, </p>

<pre><code>import numpy as np
import keras.models
from keras.models import Sequential
model = Sequential()
model.add(Dense(32, input_shape=(784,)))
model.add(Activation('relu'))
</code></pre>

<p>it is showing the following error:</p>

<hr>

<p>NameError</p>

<p>Traceback (most recent call last)</p>

<p>----> 1 model.add(Dense(32, input_shape=(784,)))</p>

<p>NameError: name 'Dense' is not defined</p>

<p><a href=""http://i.stack.imgur.com/q9ZMp.png"" rel=""noreferrer"">Here</a> is the error message screenshot.</p>

<p>How come sequential was imported successfully and 'Dense' was not defined?</p>",37179886.0,2,0,,2016/5/12 6:52,1.0,2020/2/13 0:40,2016/5/12 7:42,,6323923.0,,6323923.0,,1,12,python-2.7|deep-learning|keras,33209,61.885,,1,error kera name dense define new deep neural network library python instal theano kera window system follow step already anaconda install tdm gcc x run code command prompt run following code ipython show following error nameerror traceback recent call last model add dense input shape nameerror name dense define error message screenshot come sequential import successfully dense define
449,449,48243360,How To Determine the 'filter' Parameter in the Keras Conv2D Function,"<p>I'm just beginning my ML journey and have done a few tutorials.  One thing that's not clear (to me) is how the 'filter' parameter is determined for Keras Conv2D.</p>

<p>Most sources I've read simply set the parameter to 32 without explanation.  Is this just a rule of thumb or do the dimensions of the input images play a part?  For example, the images in CIFAR-10 are 32x32</p>

<p>Specifically:</p>



<pre class=""lang-python prettyprint-override""><code>model = Sequential()
filters = 32
model.add(Conv2D(filters, (3, 3), padding='same', input_shape=x_train.shape[1:]))

model.add(Activation('relu'))
model.add(Conv2D(filters, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
</code></pre>

<p>The next layer has a filter parameter of filter*2 or 64.  Again, how is this calculated?</p>

<p>Tx.</p>

<p>Joe</p>",,2,0,,2018/1/13 19:07,17.0,2020/4/20 12:24,2018/4/24 11:36,,5974433.0,,8371191.0,,1,27,machine-learning|neural-network|keras|conv-neural-network|convolution,18453,62.8643,,3,determine filter parameter kera conv function begin ml journey tutorial one thing clear filter parameter determine kera conv source read simply set parameter without explanation rule thumb dimension input image play part example image cifar x specifically next layer filter parameter filter calculated tx joe
720,720,40050397,Deep-Learning Nan loss reasons,"<p>Perhaps too general a question, but can anyone explain what would cause a Convolutional Neural Network to diverge?</p>
<p>Specifics:</p>
<p>I am using Tensorflow's iris_training model with some of my own data and keep getting</p>
<blockquote>
<p>ERROR:tensorflow:Model diverged with loss = NaN.</p>
<p>Traceback...</p>
<p>tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training.</p>
</blockquote>
<p>Traceback originated with line:</p>
<pre><code> tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
                                        hidden_units=[300, 300, 300],
                                        #optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001),                                                          
                                        n_classes=11,
                                        model_dir=&quot;/tmp/iris_model&quot;)
</code></pre>
<p>I've tried adjusting the optimizer, using a zero for learning rate, and using no optimizer. Any insights into network layers, data size, etc is appreciated.</p>",40434284.0,9,4,,2016/10/14 19:07,50.0,2020/9/27 4:02,2020/6/20 9:12,,-1.0,,5031496.0,,1,111,python|tensorflow|machine-learning|keras|theano,142685,408.418,,4,deep learn nan loss reason perhaps general question anyone explain would cause convolutional neural network diverge specific use tensorflow iris train model data keep get error tensorflow model diverge loss nan traceback tensorflow contrib learn python learn monitor nanlossduringtrainingerror nan loss train traceback originate line try adjust optimizer use zero learn rate use optimizer insight network layer data size etc appreciate
432,432,47868265,What is the difference between an Embedding Layer and a Dense Layer?,"<p>The docs for an <a href=""https://keras.io/layers/embeddings/"" rel=""noreferrer"">Embedding Layer</a> in Keras say:</p>

<blockquote>
  <p>Turns positive integers (indexes) into dense vectors of fixed size. eg. <code>[[4], [20]]</code> -> <code>[[0.25, 0.1], [0.6, -0.2]]</code></p>
</blockquote>

<p>I believe this could also be achieved by encoding the inputs as one-hot vectors of length <code>vocabulary_size</code>, and feeding them into a <a href=""https://keras.io/layers/core/#dense"" rel=""noreferrer"">Dense Layer</a>.</p>

<p>Is an Embedding Layer merely a convenience for this two-step process, or is something fancier going on under the hood?</p>",57807971.0,2,1,,2017/12/18 12:12,11.0,2019/9/5 14:56,2017/12/18 16:10,,712995.0,,58866.0,,1,40,machine-learning|neural-network|deep-learning|keras|keras-layer,10010,134.002,,0,difference embed layer dense layer doc embed layer kera say turn positive integer index dense vector fixed size eg believe could also achieve encode input one hot vector length feed dense layer embed layer merely convenience two step process something fancier go hood
382,382,46428604,how to implement early stopping in tensorflow,"<pre><code>def train():
# Model
model = Model()

# Loss, Optimizer
global_step = tf.Variable(1, dtype=tf.int32, trainable=False, name='global_step')
loss_fn = model.loss()
optimizer = tf.train.AdamOptimizer(learning_rate=TrainConfig.LR).minimize(loss_fn, global_step=global_step)

# Summaries
summary_op = summaries(model, loss_fn)

with tf.Session(config=TrainConfig.session_conf) as sess:

    # Initialized, Load state
    sess.run(tf.global_variables_initializer())
    model.load_state(sess, TrainConfig.CKPT_PATH)

    writer = tf.summary.FileWriter(TrainConfig.GRAPH_PATH, sess.graph)

    # Input source
    data = Data(TrainConfig.DATA_PATH)

    loss = Diff()
    for step in xrange(global_step.eval(), TrainConfig.FINAL_STEP):

            mixed_wav, src1_wav, src2_wav, _ = data.next_wavs(TrainConfig.SECONDS, TrainConfig.NUM_WAVFILE, step)

            mixed_spec = to_spectrogram(mixed_wav)
            mixed_mag = get_magnitude(mixed_spec)

            src1_spec, src2_spec = to_spectrogram(src1_wav), to_spectrogram(src2_wav)
            src1_mag, src2_mag = get_magnitude(src1_spec), get_magnitude(src2_spec)

            src1_batch, _ = model.spec_to_batch(src1_mag)
            src2_batch, _ = model.spec_to_batch(src2_mag)
            mixed_batch, _ = model.spec_to_batch(mixed_mag)

            # Initializae our callback.
            #early_stopping_cb = EarlyStoppingCallback(val_acc_thresh=0.5)


            l, _, summary = sess.run([loss_fn, optimizer, summary_op],
                                     feed_dict={model.x_mixed: mixed_batch, model.y_src1: src1_batch,
                                                model.y_src2: src2_batch})

            loss.update(l)
            print('step-{}\td_loss={:2.2f}\tloss={}'.format(step, loss.diff * 100, loss.value))

            writer.add_summary(summary, global_step=step)

            # Save state
            if step % TrainConfig.CKPT_STEP == 0:
                tf.train.Saver().save(sess, TrainConfig.CKPT_PATH + '/checkpoint', global_step=step)

    writer.close()
</code></pre>

<p>I have this neural network code that separates music from a voice in a .wav file.
how can I introduce an early stopping algorithm to stop the train section? I see some project that talks about a ValidationMonitor. Can someone help me?</p>",,4,2,,2017/9/26 14:04,10.0,2021/2/26 16:09,2021/2/26 16:09,,10908375.0,,8634720.0,,1,27,python|tensorflow|keras|neural-network,20725,74.866,,3,implement early stopping tensorflow neural network code separate music voice wav file introduce early stopping algorithm stop train section see project talk validationmonitor someone help
364,364,46009619,Keras: weighted binary crossentropy,"<p>I tried to implement a weighted binary crossentropy with Keras, but I am not sure if the code is correct. The training output seems to be a bit confusing. After a few epochs I just get an accuracy of ~0.15. I think thats much too less (even for a random guess).</p>

<p>There are in general about 11% ones in the output and 89% zeros, therefore the weights are w_zero=0.89 and w_one=0.11.</p>

<p>My code:</p>



<pre class=""lang-python prettyprint-override""><code>def create_weighted_binary_crossentropy(zero_weight, one_weight):

    def weighted_binary_crossentropy(y_true, y_pred):

        # Original binary crossentropy (see losses.py):
        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)

        # Calculate the binary crossentropy
        b_ce = K.binary_crossentropy(y_true, y_pred)

        # Apply the weights
        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight
        weighted_b_ce = weight_vector * b_ce

        # Return the mean error
        return K.mean(weighted_b_ce)

    return weighted_binary_crossentropy
</code></pre>

<p>Maybe someone sees whats wrong?</p>

<p>Thank you</p>",46015144.0,6,3,,2017/9/2 1:12,14.0,2020/11/30 9:33,2017/9/7 14:39,,4685471.0,,916672.0,,1,36,machine-learning|keras|keras-2,30972,109.164,,4,kera weight binary crossentropy try implement weighted binary crossentropy kera sure code correct training output seem bit confusing epoch get accuracy think thats much less even random guess general one output zero therefore weight w zero w one code maybe someone see whats wrong thank
701,701,39289285,How to create a Image Dataset just like MNIST dataset?,"<p>I have 10000 BMP images of some handwritten digits. If i want to feed the datas to a neural network what do i need to do ? For MNIST dataset i just had to write</p>

<pre><code>(X_train, y_train), (X_test, y_test) = mnist.load_data()
</code></pre>

<p>I am using Keras library in python . How can i create such dataset ?</p>",39291033.0,4,0,,2016/9/2 9:42,8.0,2020/12/6 13:06,,,,,6655645.0,,1,22,python|image-processing|dataset|neural-network|keras,28406,52.4136,,2,create image dataset like mnist dataset bmp image handwritten digit want fee data neural network need mnist dataset write use kera library python create dataset
26,26,53994625,How can i process multi loss in pytorch?,"<p><a href=""https://i.stack.imgur.com/yBrXW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yBrXW.png"" alt=""enter image description here""></a></p>

<p>Such as this, I want to using some auxiliary loss to promoting my model performance.<br/>
Which type code can implement it in pytorch?</p>

<pre><code>#one
loss1.backward()
loss2.backward()
loss3.backward()
optimizer.step()
#two
loss1.backward()
optimizer.step() 
loss2.backward()
optimizer.step() 
loss3.backward()
optimizer.step()   
#three
loss = loss1+loss2+loss3
loss.backward()
optimizer.step()
</code></pre>

<p>Thanks for your answer!</p>",53997034.0,4,0,,2019/1/1 10:12,13.0,2020/9/19 13:25,2019/1/2 9:21,,9412325.0,,8005524.0,,1,35,python|pytorch,14927,88.6959,,4,process multi loss pytorch want use auxiliary loss promote model performance type code implement pytorch thanks answer
294,294,56860180,Tensorflow CUDA - CUPTI error: CUPTI could not be loaded or symbol could not be found,"<p>I use the Tensorflow v 1.14.0. I work on Windows 10. And here is how relevant <strong>environment variables</strong> look in the <code>PATH</code>:</p>

<pre><code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\libnvvp
C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common
C:\Users\sinthes\AppData\Local\Programs\Python\Python37
C:\Users\sinthes\AppData\Local\Programs\Python\Python37\Scripts
C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\cuda\bin
</code></pre>

<p>Maybe also worth to mention, just in case it might be relevant.. I use Sublime Text 3 for development and I do not use Anaconda. I find it a bit cumbersome to make updates on tensorflow in the conda environment so I just use Sublime Text right now. (I was using Anaconda (Spyder) previously but I uninstalled it from my computer.) </p>

<p>Things seem to work fine except with some occasional strange warnings. But one consistent warning I get is the following whenever I run the <code>fit</code> function.</p>

<pre><code>E tensorflow/core/platform/default/device_tracer.cc:68] CUPTI error: CUPTI could not be loaded or symbol could not be found.
</code></pre>

<p>And here is how I call the fit function:</p>

<pre><code>history = model.fit(x=train_x,
                    y=train_y,
                    batch_size=BATCH_SIZE,
                    epochs=110,
                    verbose=2,
                    callbacks=[tensorboard, checkpoint, reduce_lr_on_plateau],
                    validation_data=(dev_x, dev_y),
                    shuffle=True,
                    class_weight=class_weight,
                    steps_per_epoch=None,
                    validation_steps=None)
</code></pre>

<p>I just wonder why I see the <strong><code>CUPTI Error</code></strong> message during the run time? It is only printed out once. Is that something that I need to fix or is it something that can be ignored? This message does not tell anything concrete to me to be able to take any action.</p>",56879016.0,8,0,,2019/7/2 21:21,6.0,2021/7/4 17:37,2019/7/2 21:41,,681865.0,,9328846.0,,1,17,python|tensorflow|keras|nvidia,23526,78.6862,,1,tensorflow cuda cupti error cupti could load symbol could find use tensorflow v work window relevant environment variable look maybe also worth mention case might relevant use sublime text development use anaconda find bit cumbersome make update tensorflow conda environment use sublime text right use anaconda spyder previously uninstalled computer thing seem work fine except occasional strange warning one consistent warning get follow whenever run function call fit function wonder see message run time print something need fix something ignore message tell anything concrete able take action
495,495,31978186,Monitor training/validation process in Caffe,"<p>I'm training Caffe Reference Model for classifying images.
My work requires me to monitor the training process by drawing graph of accuracy of the model after every 1000 iterations on entire training set and validation set which has 100K and 50K images respectively. 
Right now, Im taking the naive approach, make snapshots after every 1000 iterations, run the C++ classififcation code which reads raw JPEG image and forward to the net and output the predicted labels. However, this takes too much time on my machine (with a Geforce GTX 560 Ti)</p>

<p>Is there any faster way that I can do to have the graph of accuracy of the snapshot models on both training and validation sets?</p>

<p>I was thinking about using LMDB format instead of raw images. However, I cannot find documentation/code about doing classification in C++ using LMDB format.</p>",32006840.0,3,1,,2015/8/13 1:43,14.0,2017/5/29 5:18,2015/8/13 9:56,,1714410.0,,2281121.0,,1,17,c++|classification|deep-learning|caffe|conv-neural-network,11118,55.3841,,5,monitor train validation process caffe train caffe reference model classify image work require monitor training process draw graph accuracy model every iteration entire training set validation set k k image respectively right im take naive approach make snapshot every iteration run c classififcation code read raw jpeg image forward net output predicted label however take much time machine geforce gtx ti fast way graph accuracy snapshot model training validation set think use lmdb format instead raw image however find documentation code classification c use lmdb format
507,507,34097988,How do I install Keras and Theano in Anaconda Python on Windows?,"<p>I am trying to work on neural networks in Python using the following Keras packages:</p>

<pre><code>from keras.utils import np_utils
from keras.layers.core import Dense, Activation, Dropout
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
</code></pre>

<p>But, I am getting the following error:</p>

<pre><code> 15 import theano
 ---&gt; 16 from theano import gof
 17 from theano.compat.python2x import partial
 18 import theano.compile.mode
 ImportError: cannot import name gof
</code></pre>

<p>Installing installed <code>conda install keras</code>. Later I tried to use <code>pip install Theano</code>, but it did not work. I Tried to install using <code>pip install git</code>, but I am getting this error: <code>cannot find command git.</code> So I installed Git and I set the environment variables.</p>

<p>So, is there any procedure to install these packages?</p>",34975902.0,8,3,,2015/12/4 21:44,55.0,2019/4/19 9:13,2017/2/24 22:22,,400589.0,,5641188.0,,1,64,python-2.7|python-3.x|anaconda|theano|keras,182333,330.4430000000001,,1,install kera theano anaconda python window try work neural network python use following kera package get following error instal instal later try use work try install use get error instal git set environment variable procedure install package
292,292,56858924,Multivariate input LSTM in pytorch,"<p>I would like to <strong>implement</strong> LSTM for multivariate input <strong>in Pytorch</strong>. </p>

<p>Following this article <a href=""https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/"" rel=""noreferrer"">https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/</a> which uses keras, the input data are in shape of (number of samples, number of timesteps, number of parallel features)</p>

<pre><code>in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])
in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])
out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])
. . . 
Input     Output
[[10 15]
 [20 25]
 [30 35]] 65
[[20 25]
 [30 35]
 [40 45]] 85
[[30 35]
 [40 45]
 [50 55]] 105
[[40 45]
 [50 55]
 [60 65]] 125
[[50 55]
 [60 65]
 [70 75]] 145
[[60 65]
 [70 75]
 [80 85]] 165
[[70 75]
 [80 85]
 [90 95]] 185

n_timesteps = 3
n_features = 2
</code></pre>

<p>In keras it seems to be easy:</p>

<p><code>model.add(LSTM(50, activation='relu', input_shape=(n_timesteps, n_features)))</code></p>

<p>Can it be done in other way, than creating <code>n_features</code> of LSTMs as first layer and feed each separately (imagine as multiple streams of sequences) and then flatten their output to linear layer?</p>

<p>I'm not 100% sure but by nature of LSTM the input cannot be flattened and passed as 1D array, because each sequence ""plays by different rules"" which the LSTM is supposed to learn.</p>

<p>So how does such implementation with keras equal to PyTorch
<code>input of shape (seq_len, batch, input_size)</code>(source <a href=""https://pytorch.org/docs/stable/nn.html#lstm"" rel=""noreferrer"">https://pytorch.org/docs/stable/nn.html#lstm</a>)</p>

<hr>

<p><strong>Edit:</strong></p>

<blockquote>
  <p>Can it be done in other way, than creating <code>n_features</code> of LSTMs as first layer and feed each separately (imagine as multiple streams of sequences) and then flatten their output to linear layer? </p>
</blockquote>

<p>According to PyTorch <a href=""https://pytorch.org/docs/stable/nn.html#lstm"" rel=""noreferrer"">docs</a> the <em>input_size</em> parameter actually means number of features (if it means number of parallel sequences)</p>",56893248.0,2,0,,2019/7/2 19:27,6.0,2019/7/4 19:09,2019/7/2 19:49,,5990202.0,,5990202.0,,1,12,python|pytorch|lstm,12221,50.1484,,3,multivariate input lstm pytorch would like implement lstm multivariate input pytorch follow article use keras input data shape number sample number timesteps number parallel feature kera seem easy way create lstms first layer fee separately imagine multiple stream sequence flatten output linear layer sure nature lstm input flatten pass array sequence play different rule lstm suppose learn implementation kera equal pytorch source lstm edit way create lstms first layer fee separately imagine multiple stream sequence flatten output linear layer accord pytorch docs input size parameter actually mean number feature mean number parallel sequence
675,675,38034702,How to put more weight on certain features in machine learning?,"<p>If using a library like scikit-learn, how do I assign more weight on certain features in the input to a classifier like SVM? Is this something people do or is there another solution to my problem?</p>",38042546.0,2,5,,2016/6/26 2:26,12.0,2021/5/28 4:57,,,,,3677562.0,,1,20,machine-learning|nlp|scikit-learn|deep-learning,22072,54.3754,,3,put weight certain feature machine learning use library like scikit learn assign weight certain feature input classifier like svm something people another solution problem
369,369,46127625,Need To Compile Keras Model Before `model.evaluate()`,"<p>I load a <code>Keras</code> model from <strong>.json</strong> and <strong>.hdf5</strong> files. When I call <code>model.evaluate()</code>, it returns an error:</p>

<blockquote>
  <p>You must compile a model before training/testing. Use `model.compile(optimizer, loss)</p>
</blockquote>

<p>Why do I need to compile to run <code>evaluate()</code>? </p>

<p>To add, the model can be passed <code>predict()</code> with no problem. </p>",46132439.0,3,0,,2017/9/9 5:59,2.0,2021/7/18 19:30,2021/7/18 19:30,,9215780.0,,1058511.0,,1,17,tensorflow|keras,20671,52.4614,,0,need compile kera model model evaluate load model json hdf file call return error must compile model train test use model compile optimizer loss need compile run add model pass problem
831,831,53900910,TypeError: can鈥檛 convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first,"<p>I am using a <a href=""https://gist.github.com/promach/186438ad7d2e6c1ec15d9ce5d8435c13/d1914a3f1da97f09ffc3ac095704b0bc9e6b2272#file-predict-py-L34-L53"" rel=""noreferrer"">modified predict.py</a> for testing a <a href=""https://gitlab.com/promach/Pruning-CNN/tree/master/SqueezeNet-Pruning"" rel=""noreferrer"">pruned SqueezeNet Model</a></p>
<pre><code>[phung@archlinux SqueezeNet-Pruning]$ python predict.py --image 3_100.jpg --model model_prunned --num_class 2
prediction in progress
Traceback (most recent call last):
File 鈥減redict.py鈥? line 66, in
prediction = predict_image(imagepath)
File 鈥減redict.py鈥? line 52, in predict_image
index = output.data.numpy().argmax()
TypeError: can鈥檛 convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
[phung@archlinux SqueezeNet-Pruning]$
</code></pre>
<p>I understand that numpy does not support GPU yet.</p>
<p>How shall I modify the code to get away from this error without invoking tensor copy data operation, <code>tensor.cpu()</code> ?</p>",,3,0,,2018/12/23 3:04,4.0,2020/12/23 16:54,2020/12/23 16:54,,4429617.0,,8776167.0,,1,23,numpy|neural-network|pytorch|pruning,57534,76.8397,,5,typeerror convert cuda tensor numpy use tensor cpu copy tensor host memory first use modify predict py test pruned squeezenet model understand numpy support gpu yet shall modify code get away error without invoke tensor copy data operation
321,321,58565394,What is the difference between sparse_categorical_crossentropy and categorical_crossentropy?,"<p>What is the difference between <code>sparse_categorical_crossentropy</code> and <code>categorical_crossentropy</code>? When should one loss be used as opposed to the other? For example, are these losses suitable for linear regression?</p>",58566065.0,3,4,,2019/10/25 20:33,11.0,2021/8/2 7:11,2019/12/5 17:08,,3924118.0,,12264499.0,,1,48,python|tensorflow|machine-learning|keras|deep-learning,31207,117.777,,4,difference sparse categorical crossentropy categorical crossentropy difference one loss use oppose example loss suitable linear regression
561,561,48828478,How do you use Keras LeakyReLU in Python?,"<p>I am trying to produce a CNN using Keras, and wrote the following code:</p>

<pre><code>batch_size = 64
epochs = 20
num_classes = 5

cnn_model = Sequential()
cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='linear',
                     input_shape=(380, 380, 1), padding='same'))
cnn_model.add(Activation('relu'))
cnn_model.add(MaxPooling2D((2, 2), padding='same'))
cnn_model.add(Conv2D(64, (3, 3), activation='linear', padding='same'))
cnn_model.add(Activation('relu'))
cnn_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
cnn_model.add(Conv2D(128, (3, 3), activation='linear', padding='same'))
cnn_model.add(Activation('relu'))
cnn_model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
cnn_model.add(Flatten())
cnn_model.add(Dense(128, activation='linear'))
cnn_model.add(Activation('relu'))
cnn_model.add(Dense(num_classes, activation='softmax'))

cnn_model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer=keras.optimizers.Adam(), metrics=['accuracy'])
</code></pre>

<p>I want to use Keras's <strong>LeakyReLU</strong> activation layer instead of using <code>Activation('relu')</code>. However, I tried using <code>LeakyReLU(alpha=0.1)</code> in place, but this is an activation layer in Keras, and I get an error about using an activation layer and not an activation function.</p>

<p>How can I use <strong>LeakyReLU</strong> in this example?</p>",48828561.0,2,0,,2018/2/16 14:02,4.0,2021/6/19 11:10,2021/2/26 19:54,,4685471.0,,9340209.0,,1,41,python|machine-learning|keras|neural-network,60086,132.515,,3,use kera leakyrelu python try produce cnn use kera write following code want use kera leakyrelu activation layer instead use however try use place activation layer kera get error use activation layer activation function use leakyrelu example
788,788,52265978,How to delete a locally uploaded file on google colab?,"<p>I'm trying to delete a file that I uploaded on Google colab using the following code:</p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>How to delete the file now? e.g If the file's name is 'sample.jpg' .</p>",52267075.0,8,0,,2018/9/10 21:52,5.0,2021/6/30 22:57,,,,,8185479.0,,1,26,python-3.x|keras|jupyter-notebook|google-colaboratory,60336,137.722,,0,delete locally uploaded file google colab try delete file upload google colab use follow code delete file e g file name sample jpg
473,473,59317919,WARNING:tensorflow:sample_weight modes were coerced from ... to ['...'],"<p>Training an image classifier using <code>.fit_generator()</code> or <code>.fit()</code> and passing a dictionary to <code>class_weight=</code> as an argument.</p>

<p>I never got errors in TF1.x but in 2.1 I get the following output when starting training:</p>

<pre class=""lang-none prettyprint-override""><code>WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
</code></pre>

<p>What does it mean to coerce something from <code>...</code> to <code>['...']</code>?</p>

<p>The source for this warning on <code>tensorflow</code>'s repo is <a href=""https://github.com/tensorflow/tensorflow/blob/d1574c209396d38ebe3c20b3ba1cb4c8d82170ae/tensorflow/python/keras/engine/data_adapter.py#L1090"" rel=""noreferrer"">here</a>, comments placed are:</p>

<blockquote>
  <p>Attempt to coerce sample_weight_modes to the target structure. This implicitly depends on the fact that Model flattens outputs for its internal representation.</p>
</blockquote>",60131716.0,4,7,,2019/12/13 7:24,9.0,2020/4/4 19:18,2020/2/19 14:07,,1838257.0,,1838257.0,,1,61,python|tensorflow|keras|tensorflow2.0|tf.keras,18005,116.822,,4,warn tensorflow sample weight mode coerce train image classifier use pass dictionary argument never get error tf x get following output start train mean coerce something source warning repo comment place attempt coerce sample weight mode target structure implicitly depend fact model flattens output internal representation
58,58,55368921,"In Colaboratory, CUDA cannot be used for the torch","<p>The error message is as follows:</p>
<pre><code>RuntimeError   Traceback (most recent call last)
&lt;ipython-input-24-06e96beb03a5&gt; in &lt;module&gt;()
     11
     12 x_test = np.array(test_features)
---&gt; 13 x_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()
     14 test = torch.utils.data.TensorDataset(x_test_cuda)
     15 test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py in _lazy_init()
    160 class CudaError(RuntimeError):
    161     def __init__(self, code):
--&gt; 162         msg = cudart().cudaGetErrorString(code).decode('utf-8')
    163         super(CudaError, self).__init__('{0} ({1})'.format(msg, code))
    164

RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:51
</code></pre>",,1,4,,2019/3/27 2:33,2.0,2020/9/25 22:07,2020/9/25 22:06,,63550.0,,11001676.0,,1,10,pytorch|google-colaboratory|torch,18144,63.0349,,3,colaboratory cuda use torch error message follow
11,11,61742556,"ValueError: Shapes (None, 1) and (None, 2) are incompatible","<p>I am training a facial expression (angry vs happy)  model. Last dense output layer was previously 1 but when i predict an image it's output was always 1 with 64 % accuracy. So i changed it to 2 for 2 outputs. But now i am getting this  error::</p>
<pre><code>Epoch 1/15

---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-54-9c7272c38dcb&gt; in &lt;module&gt;()
     11     epochs=epochs,
     12     validation_data = val_data_gen,
---&gt; 13     validation_steps = validation_steps,
     14 
     15 )

10 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:143 __call__
        losses = self.call(y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:246 call
        return self.fn(y_true, y_pred, **self._fn_kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy
        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy
        target.shape.assert_is_compatible_with(output.shape)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with
        raise ValueError(&quot;Shapes %s and %s are incompatible&quot; % (self, other))

    ValueError: Shapes (None, 1) and (None, 2) are incompatible
</code></pre>
<p>The relevant code is :</p>
<pre><code>    model = Sequential([
    Conv2D(32,3, activation='relu', input_shape=(48,48,1)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(3, 3)),
  
    Flatten(),
    Dense(512, activation='relu'),
    Dense(2,activation='softmax')
])
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])


model.summary()

Model: &quot;sequential_4&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)            (None, 46, 46, 32)        320       
_________________________________________________________________
batch_normalization_4 (Batch (None, 46, 46, 32)        128       
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 15, 15, 32)        0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 7200)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 512)               3686912   
_________________________________________________________________
dense_9 (Dense)              (None, 2)                 1026      
=================================================================
Total params: 3,688,386
Trainable params: 3,688,322
Non-trainable params: 64
_________________________________________________________________


epochs = 15
steps_per_epoch = train_data_gen.n//train_data_gen.batch_size
validation_steps = val_data_gen.n//val_data_gen.batch_size



history = model.fit(
    x=train_data_gen,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data = val_data_gen,
    validation_steps = validation_steps,
    
)
</code></pre>",61742957.0,7,7,,2020/5/12 2:07,7.0,2021/7/6 15:47,2020/9/22 17:16,,860196.0,,13507534.0,,1,24,tensorflow|keras|cnn,66836,132.9,,4,valueerror shape none none incompatible train facial expression angry v happy model last dense output layer previously predict image output always accuracy change output get error relevant code
610,610,50307707,Convert Pandas dataframe to PyTorch tensor?,"<p>I want to train a simple neural network with PyTorch on a pandas dataframe <code>df</code>.</p>
<p>One of the columns is named <code>&quot;Target&quot;</code>, and it is the target variable of the network. How can I use this dataframe as input to the PyTorch network?</p>
<p>I tried this, but it doesn't work:</p>
<pre><code>import pandas as pd
import torch.utils.data as data_utils

target = pd.DataFrame(df['Target'])
train = data_utils.TensorDataset(df, target)
train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)
</code></pre>",50308132.0,6,2,,2018/5/12 15:13,7.0,2021/5/18 21:08,2021/3/30 19:51,,9067615.0,,9781113.0,,1,52,python|pandas|dataframe|pytorch,74913,171.898,,3,convert panda dataframe pytorch tensor want train simple neural network pytorch panda dataframe one column name target variable network use dataframe input pytorch network try work
283,283,44971349,How to turn off dropout for testing in Tensorflow?,"<p>I am fairly new to Tensorflow and ML in general, so I hereby apologize for a (likely) trivial question. </p>

<p>I use the dropout technique to improve learning rates of my network, and it seems to work just fine. Then, I would like to test the network on some data to see if it works like this:</p>

<pre><code>   def Ask(self, image):
        return self.session.run(self.model, feed_dict = {self.inputPh: image})
</code></pre>

<p>Obviously, it yields different results each time as the dropout is still in place. One solution I can think of is to create two separate models - one for a training and the other one for an actual later use of the network, however, such a solution seems impractical to me.</p>

<p>What's the common approach to solving this problem? </p>",44971517.0,6,0,,2017/7/7 12:56,11.0,2020/5/2 1:00,2017/7/7 12:59,,5161074.0,,8256202.0,,1,30,python|machine-learning|tensorflow|neural-network|conv-neural-network,26068,134.664,,5,turn dropout test tensorflow fairly new tensorflow ml general hereby apologize likely trivial question use dropout technique improve learn rate network seem work fine would like test network data see work like obviously yield different result time dropout still place one solution think create two separate model one training one actual later use network however solution seem impractical common approach solve problem
74,74,740389,Good implementations of reinforcement learning?,"<p>For an ai-class project I need to implement a reinforcement learning algorithm which beats a simple game of tetris. The game is written in Java and we have the source code. I know the basics of reinforcement learning theory but was wondering if anyone in the SO community had hands on experience with this type of thing.</p>

<ol>
<li>What would your recommended readings be for an implementation of reinforced learning in a tetris game?</li>
<li>Are there any good open source projects that accomplish similar things that would be worth checking out?</li>
</ol>

<p>Edit: The more specific the better, but general resources about the subject are welcomed.</p>

<p><strong>Follow up:</strong> </p>

<p>Thought it would be nice if I posted a followup.</p>

<p>Here's the solution (code and writeup) I ended up with for any future students :).</p>

<p><strong><a href=""http://dl.getdropbox.com/u/30163/AI.PAPER.DESIMONE.GOCHEV.doc"" rel=""noreferrer"">Paper</a></strong> / <strong><a href=""http://dl.getdropbox.com/u/30163/tetris_done.tar.gz"" rel=""noreferrer"">Code</a></strong></p>",742889.0,9,0,,2009/4/11 16:32,19.0,2019/11/27 16:50,2015/4/7 16:09,,1695962.0,,67445.0,,1,24,language-agnostic|artificial-intelligence|machine-learning|reinforcement-learning,6165,83.3597,,0,good implementation reinforcement learning ai class project need implement reinforcement learn algorithm beat simple game tetri game write java source code know basic reinforcement learn theory wonder anyone community hand experience type thing would recommended reading implementation reinforced learning tetris game good open source project accomplish similar thing would worth check edit specific good general resource subject welcome follow think would nice post followup solution code writeup end future student paper code
746,746,41069903,Why rotation-invariant neural networks are not used in winners of the popular competitions?,"<p>As known, modern most popular CNN (convolutional neural network): VGG/ResNet (FasterRCNN), SSD, Yolo, Yolo v2, DenseBox, DetectNet - are not rotate invariant: <a href=""https://stackoverflow.com/questions/40952163/are-modern-cnn-convolutional-neural-network-as-detectnet-rotate-invariant"">Are modern CNN (convolutional neural network) as DetectNet rotate invariant?</a></p>

<p>Also known, that there are several neural networks with rotate-invariance object detection:</p>

<ol>
<li><p>Rotation-Invariant Neoperceptron 2006 (<a href=""https://www.researchgate.net/profile/Beat_Fasel/publication/224649475_Rotation-Invariant_Neoperceptron/links/02e7e53859a00a588b000000.pdf"" rel=""noreferrer"">PDF</a>): <a href=""https://www.researchgate.net/publication/224649475_Rotation-Invariant_Neoperceptron"" rel=""noreferrer"">https://www.researchgate.net/publication/224649475_Rotation-Invariant_Neoperceptron</a></p></li>
<li><p>Learning rotation invariant convolutional filters for texture classification 2016 (<a href=""https://arxiv.org/pdf/1604.06720v2"" rel=""noreferrer"">PDF</a>): <a href=""https://arxiv.org/abs/1604.06720"" rel=""noreferrer"">https://arxiv.org/abs/1604.06720</a></p></li>
<li><p>RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection 2016 (<a href=""http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.pdf"" rel=""noreferrer"">PDF</a>): <a href=""http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html"" rel=""noreferrer"">http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cheng_RIFD-CNN_Rotation-Invariant_and_CVPR_2016_paper.html</a></p></li>
<li><p>Encoded Invariance in Convolutional Neural Networks 2014 (<a href=""http://theorycenter.cs.uchicago.edu/REU/2014/final-papers/sauder.pdf"" rel=""noreferrer"">PDF</a>)</p></li>
<li><p>Rotation-invariant convolutional neural networks for galaxy morphology prediction (<a href=""https://arxiv.org/pdf/1503.07077v1"" rel=""noreferrer"">PDF</a>): <a href=""https://arxiv.org/abs/1503.07077"" rel=""noreferrer"">https://arxiv.org/abs/1503.07077</a></p></li>
<li><p>Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images 2016: <a href=""http://ieeexplore.ieee.org/document/7560644/"" rel=""noreferrer"">http://ieeexplore.ieee.org/document/7560644/</a></p></li>
</ol>

<p>We know, that in such image-detection competitions as: IMAGE-NET, MSCOCO, PASCAL VOC - used networks ensembles (simultaneously some neural networks). Or networks ensembles in single net such as ResNet (<a href=""https://arxiv.org/abs/1605.06431"" rel=""noreferrer"">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</a>)</p>

<p>But are used rotation invariant network ensembles in winners like as MSRA, and if not, then why? Why in ensemble the additional rotation-invariant network does not add accuracy to detect certain objects such as aircraft objects - which images is done at a different angles of rotation? </p>

<p>It can be:</p>

<ul>
<li><p>aircraft objects which are photographed from the ground 
<a href=""https://i.stack.imgur.com/s9C7w.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/s9C7w.jpg"" alt=""enter image description here""></a></p></li>
<li><p>or ground objects which are photographed from the air
<a href=""https://i.stack.imgur.com/ICqUL.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ICqUL.jpg"" alt=""enter image description here""></a></p></li>
</ul>

<p>Why rotation-invariant neural networks are not used in winners of the popular object-detection competitions?</p>",,3,4,,2016/12/9 22:31,27.0,2019/5/31 3:47,2017/5/23 12:03,,-1.0,,1558037.0,,1,33,machine-learning|computer-vision|neural-network|deep-learning|conv-neural-network,20867,51.0778,,0,rotation invariant neural network use winner popular competition know modern popular cnn convolutional neural network vgg resnet fasterrcnn ssd yolo yolo v densebox detectnet rotate invariant modern cnn convolutional neural network detectnet rotate invariant also know several neural network rotate invariance object detection rotation invariant neoperceptron pdf learn rotation invariant convolutional filter texture classification pdf rifd cnn rotation invariant fisher discriminative convolutional neural network object detection pdf encode invariance convolutional neural network pdf rotation invariant convolutional neural network galaxy morphology prediction pdf learn rotation invariant convolutional neural network object detection vhr optical remote sense image know image detection competition image net mscoco pascal voc use network ensembles simultaneously neural network network ensemble single net resnet residual network behave like ensemble relatively shallow network use rotation invariant network ensemble winner like msra ensemble additional rotation invariant network add accuracy detect certain object aircraft object image different angle rotation aircraft object photograph ground ground object photograph air rotation invariant neural network use winner popular object detection competition
349,349,45492318,Keras retrieve value of node before activation function,"<p>Imagine a fully-connected neural network with its last two layers of the following structure:</p>

<pre><code>[Dense]
    units = 612
    activation = softplus

[Dense]
    units = 1
    activation = sigmoid
</code></pre>

<p>The output value of the net is 1, but I'd like to know what the input x to the sigmoidal function was (must be some high number, since sigm(x) is 1 here).</p>

<p>Folllowing <a href=""https://stackoverflow.com/a/41712013/1922302"">indraforyou's</a> answer I managed to retrieve the output and weights of Keras layers:</p>

<pre><code>outputs = [layer.output for layer in model.layers[-2:]]
functors = [K.function( [model.input]+[K.learning_phase()], [out] ) for out in outputs]

test_input = np.array(...)
layer_outs = [func([test_input, 0.]) for func in functors]

print layer_outs[-1][0]  # -&gt; array([[ 1.]])

dense_0_out = layer_outs[-2][0]                           # shape (612, 1)
dense_1_weights = model.layers[-1].weights[0].get_value() # shape (1, 612)
dense_1_bias = model.layers[-1].weights[1].get_value()

x = np.dot(dense_0_out, dense_1_weights) + dense_1_bias
print x # -&gt; -11.7
</code></pre>

<p>How can x be a negative number? In that case the last layers output should be a number closer to 0.0 than 1.0. Are <code>dense_0_out</code> or <code>dense_1_weights</code> the wrong outputs or weights?</p>",45762340.0,5,8,,2017/8/3 18:50,4.0,2020/12/18 17:31,2017/8/3 19:23,,1922302.0,,1922302.0,,1,24,python|neural-network|keras|keras-layer|sigmoid,5052,57.8139,,3,kera retrieve value node activation function imagine fully connect neural network last two layer following structure output value net like know input x sigmoidal function must high number since sigm x folllowing indraforyou answer manage retrieve output weight kera layer x negative number case last layer output number closer wrong output weight
159,159,42786717,How to calculate the number of parameters for convolutional neural network?,"<p>I'm using Lasagne to create a CNN for the MNIST dataset. I'm following closely to this example: <a href=""http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/"" rel=""noreferrer"">Convolutional Neural Networks and Feature Extraction with Python</a>.</p>

<p>The CNN architecture I have at the moment, which doesn't include any dropout layers, is:</p>

<pre><code>NeuralNet(
    layers=[('input', layers.InputLayer),        # Input Layer
            ('conv2d1', layers.Conv2DLayer),     # Convolutional Layer
            ('maxpool1', layers.MaxPool2DLayer), # 2D Max Pooling Layer
            ('conv2d2', layers.Conv2DLayer),     # Convolutional Layer
            ('maxpool2', layers.MaxPool2DLayer), # 2D Max Pooling Layer
            ('dense', layers.DenseLayer),        # Fully connected layer
            ('output', layers.DenseLayer),       # Output Layer
            ],
    # input layer
    input_shape=(None, 1, 28, 28),

    # layer conv2d1
    conv2d1_num_filters=32,
    conv2d1_filter_size=(5, 5),
    conv2d1_nonlinearity=lasagne.nonlinearities.rectify,

    # layer maxpool1
    maxpool1_pool_size=(2, 2),

    # layer conv2d2
    conv2d2_num_filters=32,
    conv2d2_filter_size=(3, 3),
    conv2d2_nonlinearity=lasagne.nonlinearities.rectify,

    # layer maxpool2
    maxpool2_pool_size=(2, 2),


    # Fully Connected Layer
    dense_num_units=256,
    dense_nonlinearity=lasagne.nonlinearities.rectify,

   # output Layer
    output_nonlinearity=lasagne.nonlinearities.softmax,
    output_num_units=10,

    # optimization method params
    update= momentum,
    update_learning_rate=0.01,
    update_momentum=0.9,
    max_epochs=10,
    verbose=1,
    )
</code></pre>

<p>This outputs the following Layer Information:</p>

<pre><code>  #  name      size
---  --------  --------
  0  input     1x28x28
  1  conv2d1   32x24x24
  2  maxpool1  32x12x12
  3  conv2d2   32x10x10
  4  maxpool2  32x5x5
  5  dense     256
  6  output    10
</code></pre>

<p>and outputs the number of learnable parameters as <strong>217,706</strong></p>

<p>I'm wondering how this number is calculated? I've read a number of resources, including this StackOverflow's <a href=""https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networkscnns"">question</a>, but none clearly generalizes the calculation.</p>

<p>If possible, <em>can the calculation of the learnable parameters per layer be generalised?</em> </p>

<p>For example, convolutional layer: number of filters x filter width x filter height.</p>",42787467.0,3,0,,2017/3/14 12:59,73.0,2021/3/22 6:00,2017/11/5 16:54,,3924118.0,,2151652.0,,1,64,neural-network|deep-learning|conv-neural-network|lasagne|nolearn,62524,204.584,,5,calculate number parameter convolutional neural network use lasagne create cnn mnist dataset follow closely example convolutional neural network feature extraction python cnn architecture moment include dropout layer output following layer information output number learnable parameter wonder number calculate read number resource include stackoverflow question none clearly generalize calculation possible calculation learnable parameter per layer generalise example convolutional layer number filter x filter width x filter height
620,620,50747947,Embedding in pytorch,"<p>I have checked the PyTorch tutorial and questions similar to this one on Stackoverflow.</p>

<p>I get confused; does the embedding in pytorch (<a href=""https://pytorch.org/docs/master/nn.html#torch.nn.Embedding"" rel=""noreferrer"">Embedding</a>) make the similar words closer to each other? And do I just need to give to it all the sentences? Or it is just a lookup table and I need to code the model?</p>",,4,0,,2018/6/7 18:29,19.0,2021/1/19 22:30,2021/1/19 22:30,,249341.0,,1927468.0,,1,50,python|pytorch|word-embedding,70239,182.386,,3,embed pytorch check pytorch tutorial question similar one stackoverflow get confuse embedding pytorch embed make similar word closer need give sentence lookup table need code model
50,50,55178230,What is the difference between keras and tf.keras?,"<p>I'm learning TensorFlow and Keras. I'd like to try <a href=""https://rads.stackoverflow.com/amzn/click/com/1617294438"" rel=""noreferrer"" rel=""nofollow noreferrer"">https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/</a>, and it seems to be written in Keras.</p>

<p>Would it be fairly straightforward to convert code to <code>tf.keras</code>?</p>

<p>I'm not more interested in the portability of the code, rather than the true difference between the two.</p>",55178567.0,3,3,,2019/3/15 8:14,6.0,2020/8/21 6:29,2019/10/15 18:57,,3924118.0,,433570.0,,1,13,tensorflow|keras|difference|tf.keras,10457,65.8776,,3,difference kera tf kera learn tensorflow keras like try seem write kera would fairly straightforward convert code interested portability code rather true difference two
410,410,47414651,Difference between Conv2D and Convolution2D in Keras,"<p>There is already an answer wrt to Tensorflow.
But the problem is that
In my IDE
Conv2D is a class
while Convolution2D is a variable?</p>",47414680.0,1,0,,2017/11/21 13:45,4.0,2020/3/8 12:49,,,,,8913947.0,,1,23,machine-learning|neural-network|keras|convolution|keras-layer,10524,52.6887,,3,difference conv convolution kera already answer wrt tensorflow problem ide conv class convolution variable
426,426,47787011,How to disable dropout while prediction in keras?,"<p>I am using dropout in neural network model in keras. Little bit code is like</p>

<pre><code>model.add(Dropout(0.5))
model.add(Dense(classes))
</code></pre>

<p>For testing, I am using <code>preds = model_1.predict_proba(image)</code>.</p>

<p>But while testing <strong>Dropout</strong> is also participating to predict the score which should not be happen. I search a lot to disable the dropout but didn't get any hint yet.</p>

<p>Do anyone have solution to disable the <strong>Dropout</strong> while testing in keras??</p>",47790168.0,5,0,,2017/12/13 6:41,2.0,2021/4/18 7:59,2021/3/16 19:40,,10375049.0,,7334187.0,,1,26,tensorflow|machine-learning|keras|deep-learning|neural-network,25424,105.621,,5,disable dropout prediction kera use dropout neural network model keras little bit code like test use test dropout also participate predict score happen search lot disable dropout get hint yet anyone solution disable dropout test kera
615,615,50542818,"What's the difference between reinforcement learning, deep learning, and deep reinforcement learning?","<p>What's the difference between reinforcement learning, deep learning, and deep reinforcement learning? Where does Q-learning fit in?</p>",50542822.0,7,0,,2018/5/26 12:34,4.0,2021/3/5 22:13,,,,user9851027,,,1,9,machine-learning|neural-network|deep-learning|reinforcement-learning|q-learning,1482,50.2834,,0,difference reinforcement learn deep learning deep reinforcement learning difference reinforcement learn deep learning deep reinforcement learn q learn fit
758,758,51503851,Calculate the accuracy every epoch in PyTorch,"<p>I am working on a Neural Network problem, to classify data as 1 or 0. I am using Binary cross entropy loss to do this. The loss is fine, however, the accuracy is very low and isn't improving. I am assuming I did a mistake in the accuracy calculation. After every epoch, I am calculating the correct predictions after thresholding the output, and dividing that number by the total number of the dataset. Is there any thing wrong I did in the accuracy calculation? And why isn't it improving, but getting more worse?
This is my code:</p>

<pre class=""lang-py prettyprint-override""><code>net = Model()
criterion = torch.nn.BCELoss(size_average=True)   
optimizer = torch.optim.SGD(net.parameters(), lr=0.1)

num_epochs = 100
for epoch in range(num_epochs):
    for i, (inputs,labels) in enumerate (train_loader):
        inputs = Variable(inputs.float())
        labels = Variable(labels.float())
        output = net(inputs)
        optimizer.zero_grad()
        loss = criterion(output, labels)
        loss.backward()
        optimizer.step()

    #Accuracy
    output = (output&gt;0.5).float()
    correct = (output == labels).float().sum()
    print(""Epoch {}/{}, Loss: {:.3f}, Accuracy: {:.3f}"".format(epoch+1,num_epochs, loss.data[0], correct/x.shape[0]))
</code></pre>

<p>And this is the strange output I get:</p>

<pre><code>Epoch 1/100, Loss: 0.389, Accuracy: 0.035
Epoch 2/100, Loss: 0.370, Accuracy: 0.036
Epoch 3/100, Loss: 0.514, Accuracy: 0.030
Epoch 4/100, Loss: 0.539, Accuracy: 0.030
Epoch 5/100, Loss: 0.583, Accuracy: 0.029
Epoch 6/100, Loss: 0.439, Accuracy: 0.031
Epoch 7/100, Loss: 0.429, Accuracy: 0.034
Epoch 8/100, Loss: 0.408, Accuracy: 0.035
Epoch 9/100, Loss: 0.316, Accuracy: 0.035
Epoch 10/100, Loss: 0.436, Accuracy: 0.035
Epoch 11/100, Loss: 0.365, Accuracy: 0.034
Epoch 12/100, Loss: 0.485, Accuracy: 0.031
Epoch 13/100, Loss: 0.392, Accuracy: 0.033
Epoch 14/100, Loss: 0.494, Accuracy: 0.030
Epoch 15/100, Loss: 0.369, Accuracy: 0.035
Epoch 16/100, Loss: 0.495, Accuracy: 0.029
Epoch 17/100, Loss: 0.415, Accuracy: 0.034
Epoch 18/100, Loss: 0.410, Accuracy: 0.035
Epoch 19/100, Loss: 0.282, Accuracy: 0.038
Epoch 20/100, Loss: 0.499, Accuracy: 0.031
Epoch 21/100, Loss: 0.446, Accuracy: 0.030
Epoch 22/100, Loss: 0.585, Accuracy: 0.026
Epoch 23/100, Loss: 0.419, Accuracy: 0.035
Epoch 24/100, Loss: 0.492, Accuracy: 0.031
Epoch 25/100, Loss: 0.537, Accuracy: 0.031
Epoch 26/100, Loss: 0.439, Accuracy: 0.033
Epoch 27/100, Loss: 0.421, Accuracy: 0.035
Epoch 28/100, Loss: 0.532, Accuracy: 0.034
Epoch 29/100, Loss: 0.234, Accuracy: 0.038
Epoch 30/100, Loss: 0.492, Accuracy: 0.027
Epoch 31/100, Loss: 0.407, Accuracy: 0.035
Epoch 32/100, Loss: 0.305, Accuracy: 0.038
Epoch 33/100, Loss: 0.663, Accuracy: 0.025
Epoch 34/100, Loss: 0.588, Accuracy: 0.031
Epoch 35/100, Loss: 0.329, Accuracy: 0.035
Epoch 36/100, Loss: 0.474, Accuracy: 0.033
Epoch 37/100, Loss: 0.535, Accuracy: 0.031
Epoch 38/100, Loss: 0.406, Accuracy: 0.033
Epoch 39/100, Loss: 0.513, Accuracy: 0.030
Epoch 40/100, Loss: 0.593, Accuracy: 0.030
Epoch 41/100, Loss: 0.265, Accuracy: 0.036
Epoch 42/100, Loss: 0.576, Accuracy: 0.031
Epoch 43/100, Loss: 0.565, Accuracy: 0.027
Epoch 44/100, Loss: 0.576, Accuracy: 0.030
Epoch 45/100, Loss: 0.396, Accuracy: 0.035
Epoch 46/100, Loss: 0.423, Accuracy: 0.034
Epoch 47/100, Loss: 0.489, Accuracy: 0.033
Epoch 48/100, Loss: 0.591, Accuracy: 0.029
Epoch 49/100, Loss: 0.415, Accuracy: 0.034
Epoch 50/100, Loss: 0.291, Accuracy: 0.039
Epoch 51/100, Loss: 0.395, Accuracy: 0.033
Epoch 52/100, Loss: 0.540, Accuracy: 0.026
Epoch 53/100, Loss: 0.436, Accuracy: 0.033
Epoch 54/100, Loss: 0.346, Accuracy: 0.036
Epoch 55/100, Loss: 0.519, Accuracy: 0.029
Epoch 56/100, Loss: 0.456, Accuracy: 0.031
Epoch 57/100, Loss: 0.425, Accuracy: 0.035
Epoch 58/100, Loss: 0.311, Accuracy: 0.039
Epoch 59/100, Loss: 0.406, Accuracy: 0.034
Epoch 60/100, Loss: 0.360, Accuracy: 0.035
Epoch 61/100, Loss: 0.476, Accuracy: 0.030
Epoch 62/100, Loss: 0.404, Accuracy: 0.034
Epoch 63/100, Loss: 0.382, Accuracy: 0.036
Epoch 64/100, Loss: 0.538, Accuracy: 0.031
Epoch 65/100, Loss: 0.392, Accuracy: 0.034
Epoch 66/100, Loss: 0.434, Accuracy: 0.033
Epoch 67/100, Loss: 0.479, Accuracy: 0.031
Epoch 68/100, Loss: 0.494, Accuracy: 0.031
Epoch 69/100, Loss: 0.415, Accuracy: 0.034
Epoch 70/100, Loss: 0.390, Accuracy: 0.036
Epoch 71/100, Loss: 0.330, Accuracy: 0.038
Epoch 72/100, Loss: 0.449, Accuracy: 0.030
Epoch 73/100, Loss: 0.315, Accuracy: 0.039
Epoch 74/100, Loss: 0.450, Accuracy: 0.031
Epoch 75/100, Loss: 0.562, Accuracy: 0.030
Epoch 76/100, Loss: 0.447, Accuracy: 0.031
Epoch 77/100, Loss: 0.408, Accuracy: 0.038
Epoch 78/100, Loss: 0.359, Accuracy: 0.034
Epoch 79/100, Loss: 0.372, Accuracy: 0.035
Epoch 80/100, Loss: 0.452, Accuracy: 0.034
Epoch 81/100, Loss: 0.360, Accuracy: 0.035
Epoch 82/100, Loss: 0.453, Accuracy: 0.031
Epoch 83/100, Loss: 0.578, Accuracy: 0.030
Epoch 84/100, Loss: 0.537, Accuracy: 0.030
Epoch 85/100, Loss: 0.483, Accuracy: 0.035
Epoch 86/100, Loss: 0.343, Accuracy: 0.036
Epoch 87/100, Loss: 0.439, Accuracy: 0.034
Epoch 88/100, Loss: 0.686, Accuracy: 0.023
Epoch 89/100, Loss: 0.265, Accuracy: 0.039
Epoch 90/100, Loss: 0.369, Accuracy: 0.035
Epoch 91/100, Loss: 0.521, Accuracy: 0.027
Epoch 92/100, Loss: 0.662, Accuracy: 0.027
Epoch 93/100, Loss: 0.581, Accuracy: 0.029
Epoch 94/100, Loss: 0.322, Accuracy: 0.034
Epoch 95/100, Loss: 0.375, Accuracy: 0.035
Epoch 96/100, Loss: 0.575, Accuracy: 0.031
Epoch 97/100, Loss: 0.489, Accuracy: 0.030
Epoch 98/100, Loss: 0.435, Accuracy: 0.033
Epoch 99/100, Loss: 0.440, Accuracy: 0.031
Epoch 100/100, Loss: 0.444, Accuracy: 0.033
</code></pre>",51664637.0,8,7,,2018/7/24 16:49,1.0,2021/3/11 22:40,2020/4/9 1:07,,7829174.0,,9109354.0,,1,11,neural-network|pytorch,52066,66.4662,,4,calculate accuracy every epoch pytorch work neural network problem classify data use binary cross entropy loss loss fine however accuracy low improve assume mistake accuracy calculation every epoch calculate correct prediction thresholding output divide number total number dataset thing wrong accuracy calculation improve get worse code strange output get
585,585,49458905,Memory error when using Keras ImageDataGenerator,"<p>I am attempting to predict features in imagery using keras with a TensorFlow backend. Specifically, I am attempting to use a keras <a href=""https://keras.io/preprocessing/image/"" rel=""noreferrer""><code>ImageDataGenerator</code></a>. The model is set to run for 4 epochs and runs fine until the 4th epoch where it fails with a MemoryError.</p>

<p>I am running this model on an AWS <a href=""https://www.ec2instances.info/?selected=g2.2xlarge"" rel=""noreferrer"">g2.2xlarge</a> instance running Ubuntu Server 16.04 LTS (HVM), SSD Volume Type.</p>

<p>The training images are 256x256 RGB pixel tiles (8 bit unsigned) and the training mask is 256x256 single band (8 bit unsigned) tiled data where 255 == a feature of interest and 0 == everything else.</p>

<p>The following 3 functions are the ones pertinent to this error. </p>

<p>How can I resolve this MemoryError?</p>

<hr>

<pre><code>def train_model():
        batch_size = 1
        training_imgs = np.lib.format.open_memmap(filename=os.path.join(data_path, 'data.npy'),mode='r+')
        training_masks = np.lib.format.open_memmap(filename=os.path.join(data_path, 'mask.npy'),mode='r+')
        dl_model = create_model()
        print(dl_model.summary())
        model_checkpoint = ModelCheckpoint(os.path.join(data_path,'mod_weight.hdf5'), monitor='loss',verbose=1, save_best_only=True)
        dl_model.fit_generator(generator(training_imgs, training_masks, batch_size), steps_per_epoch=(len(training_imgs)/batch_size), epochs=4,verbose=1,callbacks=[model_checkpoint])

def generator(train_imgs, train_masks=None, batch_size=None):

# Create empty arrays to contain batch of features and labels#

        if train_masks is not None:
                train_imgs_batch = np.zeros((batch_size,y_to_res,x_to_res,bands))
                train_masks_batch = np.zeros((batch_size,y_to_res,x_to_res,1))

                while True:
                        for i in range(batch_size):
                                # choose random index in features
                                index= random.choice(range(len(train_imgs)))
                                train_imgs_batch[i] = train_imgs[index]
                                train_masks_batch[i] = train_masks[index]
                        yield train_imgs_batch, train_masks_batch
        else:
                rec_imgs_batch = np.zeros((batch_size,y_to_res,x_to_res,bands))
                while True:
                        for i in range(batch_size):
                                # choose random index in features
                                index= random.choice(range(len(train_imgs)))
                                rec_imgs_batch[i] = train_imgs[index]
                        yield rec_imgs_batch

def train_generator(train_images,train_masks,batch_size):
        data_gen_args=dict(rotation_range=90.,horizontal_flip=True,vertical_flip=True,rescale=1./255)
        image_datagen = ImageDataGenerator()
        mask_datagen = ImageDataGenerator()
# # Provide the same seed and keyword arguments to the fit and flow methods
        seed = 1
        image_datagen.fit(train_images, augment=True, seed=seed)
        mask_datagen.fit(train_masks, augment=True, seed=seed)
        image_generator = image_datagen.flow(train_images,batch_size=batch_size)
        mask_generator = mask_datagen.flow(train_masks,batch_size=batch_size)
        return zip(image_generator, mask_generator)
</code></pre>

<hr>

<p>The following os the output from the model detailing the epochs and the error message:</p>

<pre><code>Epoch 00001: loss improved from inf to 0.01683, saving model to /home/ubuntu/deep_learn/client_data/mod_weight.hdf5
Epoch 2/4
7569/7569 [==============================] - 3394s 448ms/step - loss: 0.0049 - binary_crossentropy: 0.0027 - jaccard_coef_int: 0.9983  

Epoch 00002: loss improved from 0.01683 to 0.00492, saving model to /home/ubuntu/deep_learn/client_data/mod_weight.hdf5
Epoch 3/4
7569/7569 [==============================] - 3394s 448ms/step - loss: 0.0049 - binary_crossentropy: 0.0026 - jaccard_coef_int: 0.9982  

Epoch 00003: loss improved from 0.00492 to 0.00488, saving model to /home/ubuntu/deep_learn/client_data/mod_weight.hdf5
Epoch 4/4
7569/7569 [==============================] - 3394s 448ms/step - loss: 0.0074 - binary_crossentropy: 0.0042 - jaccard_coef_int: 0.9975  

Epoch 00004: loss did not improve
Traceback (most recent call last):
  File ""image_rec.py"", line 291, in &lt;module&gt;
    train_model()
  File ""image_rec.py"", line 208, in train_model
    dl_model.fit_generator(train_generator(training_imgs,training_masks,batch_size),steps_per_epoch=1,epochs=1,workers=1)
  File ""image_rec.py"", line 274, in train_generator
    image_datagen.fit(train_images, augment=True, seed=seed)
  File ""/home/ubuntu/pyvirt_test/local/lib/python2.7/site-packages/keras/preprocessing/image.py"", line 753, in fit
    x = np.copy(x)
  File ""/home/ubuntu/pyvirt_test/local/lib/python2.7/site-packages/numpy/lib/function_base.py"", line 1505, in copy
    return array(a, order=order, copy=True)
MemoryError
</code></pre>",,5,4,,2018/3/23 22:07,2.0,2018/7/22 18:34,2018/3/26 1:21,,1446289.0,,1446289.0,,1,15,python|tensorflow|deep-learning|keras,7906,51.5918,,4,memory error use kera imagedatagenerator attempt predict feature imagery use kera tensorflow backend specifically attempt use kera model set run epoch run fine th epoch fail memoryerror run model aws g xlarge instance run ubuntu server lts hvm ssd volume type training image x rgb pixel tile bit unsigned training mask x single band bit unsigned tile data feature interest everything else following function one pertinent error resolve memoryerror following output model detail epoch error message
403,403,47240348,"What is the meaning of the ""None"" in model.summary of KERAS?","<p><a href=""https://i.stack.imgur.com/be1s4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/be1s4.png"" alt=""enter image description here""></a></p>

<p>What is the meaning of the (None, 100) in Output Shape?
Is this(""None"") the Sample number or the hidden dimension?</p>",47241264.0,2,1,,2017/11/11 16:46,10.0,2019/5/9 21:11,2019/4/24 21:42,,5884955.0,,6890808.0,,1,34,tensorflow|machine-learning|keras,15955,82.4116,,5,meaning none model summary kera meaning none output shape none sample number hidden dimension
240,240,44466066,How can I convert a trained Tensorflow model to Keras?,"<p>I have a trained Tensorflow model and weights vector which have been exported to protobuf and weights files respectively.</p>

<p>How can I convert these to JSON or YAML and HDF5 files which can be used by Keras?</p>

<p>I have the code for the Tensorflow model, so it would also be acceptable to convert the <code>tf.Session</code> to a keras model and save that in code.</p>",46210187.0,4,0,,2017/6/9 20:16,14.0,2018/12/10 9:38,2017/6/9 21:18,,5605365.0,,5605365.0,,1,25,tensorflow|keras,30595,72.9426,,5,convert trained tensorflow model keras train tensorflow model weight vector export protobuf weight file respectively convert json yaml hdf file use kera code tensorflow model would also acceptable convert keras model save code
414,414,47485216,How does mask_zero in Keras Embedding layer work?,"<p>I thought <code>mask_zero=True</code> will output 0's when the input value is 0, so the following layers could skip computation or something.</p>

<p>How does <code>mask_zero</code> works? </p>

<p>Example: </p>

<pre><code>data_in = np.array([
  [1, 2, 0, 0]
])
data_in.shape
&gt;&gt;&gt; (1, 4)

# model
x = Input(shape=(4,))
e = Embedding(5, 5, mask_zero=True)(x)

m = Model(inputs=x, outputs=e)
p = m.predict(data_in)
print(p.shape)
print(p)
</code></pre>

<p>The actual output is: (the numbers are random)</p>

<pre><code>(1, 4, 5)
[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]
  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]
  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]
  [ 0.00451764 -0.01433611  0.02606953  0.00328832  0.02650392]]]
</code></pre>

<p>However, I thought the output will be:</p>

<pre><code>[[[ 0.02499047  0.04617121  0.01586803  0.0338897   0.009652  ]
  [ 0.04782704 -0.04035913 -0.0341589   0.03020919 -0.01157228]
  [ 0 0 0 0 0]
  [ 0 0 0 0 0]]]
</code></pre>",53470422.0,2,3,,2017/11/25 11:03,10.0,2020/4/8 13:50,2018/11/25 18:13,,2099607.0,,1461746.0,,1,29,python|machine-learning|keras|word-embedding,11864,67.8969,,3,mask zero kera embed layer work think output input value following layer could skip computation something work example actual output number random however think output
469,469,58926054,How to get the device type of a pytorch module conveniently?,"<p>I have to stack some my own layers on different kinds of pytorch models with different devices.</p>

<p>E.g. <strong>A</strong> is a <strong>cuda</strong> model and <strong>B</strong> is a <strong>cpu</strong> model (but I don't know it before I get the device type). Then the new models are <strong>C</strong> and <strong>D</strong> respectively, where</p>

<pre class=""lang-py prettyprint-override""><code>class NewModule(torch.nn.Module):
    def __init__(self, base):
        super(NewModule, self).__init__()
        self.base = base
        self.extra = my_layer() # e.g. torch.nn.Linear()

    def forward(self,x):
        y = self.base(x)
        z = self.extra(y)
        return z

...

C = NewModule(A) # cuda
D = NewModule(B) # cpu
</code></pre>

<p>However I must move <code>base</code> and <code>extra</code> to the <strong>same</strong> device, i.e. <code>base</code> and <code>extra</code> of C are <strong>cuda</strong> models and D's are <strong>cpu</strong> models. So I tried this <code>__inin__</code>:</p>

<pre class=""lang-py prettyprint-override""><code>def __init__(self, base):
    super(NewModule, self).__init__()
    self.base = base
    self.extra = my_layer().to(base.device)
</code></pre>

<p>Unfortunately, there's no attribute <code>device</code> in <code>torch.nn.Module</code>(raise <code>AttributeError</code>).</p>

<p>What should I do to get the device type of <code>base</code>? Or any other method to make <code>base</code> and <code>extra</code> to be on the same device automaticly even the structure of <code>base</code> is unspecific?</p>",58926343.0,2,0,,2019/11/19 3:09,2.0,2020/8/18 22:20,2019/11/19 4:05,,681865.0,,12128185.0,,1,12,python|gpu|pytorch,25970,58.4579,,3,get device type pytorch module conveniently stack layer different kind pytorch model different device e g cuda model b cpu model know get device type new model c respectively however must move device e c cuda model cpu model try unfortunately attribute raise get device type method make device automaticly even structure unspecific
833,833,38073432,How to suppress verbose Tensorflow logging?,"<p>I'm unittesting my Tensorflow code with nosetests but it produces such amount of verbose output that makes it useless. </p>

<p>The following test</p>

<pre class=""lang-py prettyprint-override""><code>import unittest
import tensorflow as tf

class MyTest(unittest.TestCase):

    def test_creation(self):
        self.assertEquals(True, False)
</code></pre>

<p>when run with <code>nosetests</code> creates a huge amount of useless logging:</p>

<pre><code>FAIL: test_creation (tests.test_tf.MyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/cebrian/GIT/thesis-nilm/code/deepmodels/tests/test_tf.py"", line 10, in test_creation
    self.assertEquals(True, False)
AssertionError: True != False
-------------------- &gt;&gt; begin captured logging &lt;&lt; --------------------
tensorflow: Level 1: Registering Const (&lt;function _ConstantShape at 0x7f4379131c80&gt;) in shape functions.
tensorflow: Level 1: Registering Assert (&lt;function no_outputs at 0x7f43791319b0&gt;) in shape functions.
tensorflow: Level 1: Registering Print (&lt;function _PrintGrad at 0x7f4378effd70&gt;) in gradient.
tensorflow: Level 1: Registering Print (&lt;function unchanged_shape at 0x7f4379131320&gt;) in shape functions.
tensorflow: Level 1: Registering HistogramAccumulatorSummary (None) in gradient.
tensorflow: Level 1: Registering HistogramSummary (None) in gradient.
tensorflow: Level 1: Registering ImageSummary (None) in gradient.
tensorflow: Level 1: Registering AudioSummary (None) in gradient.
tensorflow: Level 1: Registering MergeSummary (None) in gradient.
tensorflow: Level 1: Registering ScalarSummary (None) in gradient.
tensorflow: Level 1: Registering ScalarSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering MergeSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering AudioSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering ImageSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering HistogramSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering HistogramAccumulatorSummary (&lt;function _ScalarShape at 0x7f4378f042a8&gt;) in shape functions.
tensorflow: Level 1: Registering Pack (&lt;function _PackShape at 0x7f4378f047d0&gt;) in shape functions.
tensorflow: Level 1: Registering Unpack (&lt;function _UnpackShape at 0x7f4378f048c0&gt;) in shape functions.
tensorflow: Level 1: Registering Concat (&lt;function _ConcatShape at 0x7f4378f04938&gt;) in shape functions.
tensorflow: Level 1: Registering ConcatOffset (&lt;function _ConcatOffsetShape at 0x7f4378f049b0&gt;) in shape functions.

......
</code></pre>

<p>whereas using tensorflow from the ipython console doesn't seem that verbose:</p>

<pre><code>$ ipython
Python 2.7.11+ (default, Apr 17 2016, 14:00:29) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 4.2.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.

In [1]: import tensorflow as tf
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally

In [2]:
</code></pre>

<p>How could I suppress the former logging when running nosetests?</p>",,3,1,,2016/6/28 10:16,5.0,2019/10/9 0:48,2017/2/3 20:47,,562769.0,,38483.0,,1,27,tensorflow|deep-learning|nose,39314,85.5782,2018/7/28 14:50,5,suppress verbose tensorflow logging unittesting tensorflow code nosetests produce amount verbose output make useless following test run create huge amount useless log whereas use tensorflow ipython console seem verbose could suppress former logging run nosetests
76,76,4230853,Time Series Prediction via Neural Networks,"<p>I have been working on Neural Networks for various purposes lately. I have had great success in digit recognition, XOR, and various other easy/hello world'ish applications.</p>

<p>I would like to tackle the domain of time series estimation. I do not have a University account at the moment to read all the IEEE/ACM papers on the topic (for free), nor can I find many resources detailing using ANN for time series forcasting. </p>

<p>I would like to know if anyone has any suggestions or can recommend any resources concerning using ANN for forcasting via time series data?</p>

<p>I would assume that to train the NN, you would insert a few immediately time steps and the expected output would be the next timestep  (example:  inputs of n-5, n-4, n-3, n-2, n-1 should come out with an output of result at timestep N.  ... and slide down some amount of timesteps and do it all again.</p>

<p>Can anyone confirm this or comment on it? I would appreciate it! </p>",4232764.0,3,0,,2010/11/20 2:16,14.0,2015/12/28 21:54,2015/12/28 21:54,,2838606.0,,514054.0,,1,19,neural-network|time-series|artificial-intelligence|recurrent-neural-network,9319,52.2775,,0,time series prediction via neural network work neural network various purpose lately great success digit recognition xor various easy hello world ish application would like tackle domain time series estimation university account moment read ieee acm paper topic free find many resource detail use ann time series forcasting would like know anyone suggestion recommend resource concern use ann forcasting via time series data would assume train nn would insert immediately time step expect output would next timestep example input n n n n n come output result timestep n slide amount timesteps anyone confirm comment would appreciate
509,509,34220532,How to assign a value to a TensorFlow variable?,"<p>I am trying to assign a new value to a tensorflow variable in python.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.Variable(0)
init = tf.initialize_all_variables()
sess = tf.InteractiveSession()
sess.run(init)

print(x.eval())

x.assign(1)
print(x.eval())
</code></pre>

<p>But the output I get is</p>

<pre><code>0
0
</code></pre>

<p>So the value has not changed. What am I missing?</p>",34220750.0,9,0,,2015/12/11 9:51,20.0,2019/12/13 18:33,2018/1/29 11:24,,5096199.0,,3537687.0,,1,83,python|tensorflow|neural-network|deep-learning|variable-assignment,105236,350.4890000000001,,3,assign value tensorflow variable try assign new value tensorflow variable python output get value change miss
520,520,35049197,How does the unpooling and deconvolution work in DeConvNet,"<p>I have been trying to understand how unpooling and deconvolution works in DeConvNets. </p>

<p>Unpooling</p>

<p>While during the unpooling stage, the activations are restored back to the locations of maximum activation selections, which makes sense, but what about the remaining activations? Do those remaining activations need to be restored as well or interpolated in some way or just filled as zeros in unpooled map.</p>

<p>Deconvolution</p>

<p>After the convolution section (i.e., Convolution layer, Relu, Pooling ), it is common to have more than one feature map output, which would be treated as input channels to successive layers ( Deconv.. ). How could these feature maps be combined together in order to achieve the activation map with same resolution as original input?</p>",36342908.0,2,0,,2016/1/27 22:19,9.0,2019/1/26 1:57,,,,,5724595.0,,1,16,image-processing|machine-learning|neural-network|deep-learning|conv-neural-network,21656,59.7423,,0,unpooling deconvolution work deconvnet try understand unpooling deconvolution work deconvnets unpooling unpooling stage activation restore back location maximum activation selection make sense remain activation remain activation need restore well interpolate way fill zero unpooled map deconvolution convolution section e convolution layer relu pool common one feature map output would treat input channel successive layer deconv could feature map combine together order achieve activation map resolution original input
518,518,34945554,How to set layer-wise learning rate in Tensorflow?,"<p>I am wondering if there is a way that I can use different learning rate for different layers like what is in Caffe. I am trying to modify a pre-trained model and use it for other tasks. What I want is to speed up the training for new added layers and keep the trained layers at low learning rate in order to prevent them from being distorted. for example, I have a 5-conv-layer pre-trained model. Now I add a new conv layer and fine tune it. The first 5 layers would have learning rate of 0.00001 and the last one would have 0.001. Any idea how to achieve this?</p>",34948185.0,7,1,,2016/1/22 11:22,34.0,2020/1/16 21:24,,,,,5825952.0,,1,58,python|deep-learning|tensorflow,29711,218.092,,3,set layer wise learn rate tensorflow wonder way use different learn rate different layer like caffe try modify pre train model use task want speed training new added layer keep trained layer low learn rate order prevent distort example conv layer pre train model add new conv layer fine tune first layer would learn rate last one would idea achieve
816,816,53628622,Loss Function & Its Inputs For Binary Classification PyTorch,"<p>I'm trying to write a neural Network for binary classification in PyTorch and I'm confused about the loss function.</p>

<p>I see that BCELoss is a common function specifically geared for binary classification. I also see that an output layer of N outputs for N possible classes is standard for general classification. However, for binary classification it seems like it could be either 1 or 2 outputs.</p>

<p>So, should I have 2 outputs (1 for each label) and then convert my 0/1 training labels into [1,0] and [0,1] arrays, or use something like a sigmoid for a single-variable output?</p>

<p>Here are the relevant snippets of code so you can see:</p>

<pre><code>self.outputs = nn.Linear(NETWORK_WIDTH, 2) # 1 or 2 dimensions?


def forward(self, x):
  # other layers omitted
  x = self.outputs(x)           
  return F.log_softmax(x)  # &lt;&lt;&lt; softmax over multiple vars, sigmoid over one, or other?

criterion = nn.BCELoss() # &lt;&lt;&lt; Is this the right function?

net_out = net(data)
loss = criterion(net_out, target) # &lt;&lt;&lt; Should target be an integer label or 1-hot vector?
</code></pre>

<p>Thanks in advance.</p>",,2,0,,2018/12/5 9:05,12.0,2020/10/29 8:26,,,,,6186822.0,,1,22,neural-network|pytorch,15270,74.5354,,4,loss function input binary classification pytorch try write neural network binary classification pytorch confuse loss function see bceloss common function specifically gear binary classification also see output layer n output n possible class standard general classification however binary classification seem like could either output output label convert training label array use something like sigmoid single variable output relevant snippet code see thanks advance
502,502,32842308,Random cropping and flipping in convolutional neural networks,"<p>In a lot of research papers I read about Convolutional Neural Networks (CNN), I see that people randomly crop a square region (e.g. 224x224) from the images and then randomly flip it horizontally. Why is this random cropping and flipping done? Also, why do people always crop a square region. Can CNNs not work on rectangular regions?</p>",32844299.0,2,0,,2015/9/29 11:06,10.0,2017/11/12 14:09,,,,,2778860.0,,1,17,image-processing|neural-network|conv-neural-network,10318,55.8544,,0,random cropping flipping convolutional neural network lot research paper read convolutional neural network cnn see people randomly crop square region e g x image randomly flip horizontally random cropping flip also people always crop square region cnns work rectangular region
293,293,56859803,ModuleNotFoundError: No module named 'tools.nnwrap',"<p>I am trying to import a package ""torch"".
For same, I tried to install it using pip command as below, installation even started but after few seconds it got error </p>

<p>below is the command that I executed </p>

<p><code>pip install torch</code></p>

<p>Error that I got: </p>

<pre><code>from tools.nnwrap import generate_wrappers as generate_nn_wrappers
    ModuleNotFoundError: No module named 'tools.nnwrap'
</code></pre>

<p><strong>OS:</strong> Windows.
<strong>IDE</strong> : pyCharm</p>

<p>I got the only link related to this issue but I was not able to interpret it.</p>

<p><a href=""https://www.gitmemory.com/torch"" rel=""noreferrer"">https://www.gitmemory.com/torch</a></p>",56877423.0,17,2,,2019/7/2 20:43,18.0,2021/4/2 7:38,2020/11/17 18:15,,8237992.0,,8237992.0,,1,100,python|pycharm|pytorch,99996,545.0,,1,modulenotfounderror module name tool nnwrap try import package torch try install use pip command installation even start second get error command execute error get os window ide pycharm get link relate issue able interpret
706,706,39561560,Getting gradient of model output w.r.t weights using Keras,"<p>I am interested in building reinforcement learning models with the simplicity of the Keras API. Unfortunately, I am unable to extract the gradient of the output (not error) with respect to the weights. I found the following code that performs a similar function (<a href=""https://stackoverflow.com/questions/36968128/saliency-maps-of-neural-networks-using-keras"">Saliency maps of neural networks (using Keras)</a>)</p>

<pre><code>get_output = theano.function([model.layers[0].input],model.layers[-1].output,allow_input_downcast=True)
fx = theano.function([model.layers[0].input] ,T.jacobian(model.layers[-1].output.flatten(),model.layers[0].input), allow_input_downcast=True)
grad = fx([trainingData])
</code></pre>

<p>Any ideas on how to calculate the gradient of the model output with respect to the weights for each layer would be appreciated.</p>",41398799.0,2,3,,2016/9/18 19:07,27.0,2020/7/11 21:00,2019/1/5 20:45,,5000294.0,,5000294.0,,1,44,python|theano|keras,35455,88.7987,,3,get gradient model output w r weight use kera interested build reinforcement learning model simplicity kera api unfortunately unable extract gradient output error respect weight find following code perform similar function saliency map neural network use kera idea calculate gradient model output respect weight layer would appreciate
467,467,28177298,Import caffe error,"<p>i compiled caffe successfully in my ubuntu machine but cannot import in python.</p>

<p>Caffe is installed /home/pbu/Desktop/caffe</p>

<p>i tried adding the /home/pbu/caffe/python path  to sys.path.append, still not working</p>

<p>i am trying to import caffe</p>

<pre><code>root@pbu-OptiPlex-740-Enhanced:/home/pbu/Desktop# python ./caffe/output.py
Traceback (most recent call last):
  File ""./caffe/output.py"", line 13, in &lt;module&gt;
    import caffe
  File ""/home/pbu/Desktop/caffe/python/caffe/__init__.py"", line 1, in &lt;module&gt;
    from .pycaffe import Net, SGDSolver
  File ""/home/pbu/Desktop/caffe/python/caffe/pycaffe.py"", line 10, in &lt;module&gt;
    from ._caffe import Net, SGDSolver
ImportError: No module named _caffe
</code></pre>",28235061.0,5,2,,2015/1/27 18:17,0.0,2017/9/25 12:28,,,,,1115169.0,,1,14,python|caffe,32874,52.0674,,1,import caffe error compile caffe successfully ubuntu machine import python caffe instal home pbu desktop caffe try add home pbu caffe python path sys path append still work try import caffe
303,303,57751417,What is meant by sequential model in Keras,"<p>I have recently started working Tensorflow for deep learning. I found this statement <code>model = tf.keras.models.Sequential()</code>  bit different. I couldn't understand what is actually meant and is there any other models as well for deep learning?
I worked  a lot on MatconvNet (Matlab library for convolutional neural network). never saw any sequential definition in that.</p>",,4,0,,2019/9/2 4:25,3.0,2021/4/19 6:22,2019/12/20 7:29,,7265871.0,,4339936.0,,1,15,python|tensorflow|keras|tf.keras,18723,65.0895,,3,mean sequential model kera recently start work tensorflow deep learning find statement bit different could understand actually meant model well deep learning work lot matconvnet matlab library convolutional neural network never saw sequential definition
198,198,43702323,How to load only specific weights on Keras,"<p>I have a trained model that I've exported the weights and want to partially load into another model.
My model is built in Keras using TensorFlow as backend.</p>

<p>Right now I'm doing as follows:</p>

<pre><code>model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape, trainable=False))
model.add(Activation('relu', trainable=False))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3), trainable=False))
model.add(Activation('relu', trainable=False))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3), trainable=True))
model.add(Activation('relu', trainable=True))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])


model.load_weights(""image_500.h5"")
model.pop()
model.pop()
model.pop()
model.pop()
model.pop()
model.pop()


model.add(Conv2D(1, (6, 6),strides=(1, 1), trainable=True))
model.add(Activation('relu', trainable=True))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
</code></pre>

<p>I'm sure it's a terrible way to do it, although it works.</p>

<p>How do I load just the first 9 layers?</p>",43703139.0,2,1,,2017/4/30 2:26,12.0,2017/4/30 5:11,,,,,3799743.0,,1,31,machine-learning|tensorflow|keras|conv-neural-network,25615,107.034,,3,load specific weight kera trained model export weight want partially load another model model build kera use tensorflow backend right follow sure terrible way although work load first layer
1,1,60912744,Install PyTorch from requirements.txt,"<p>Torch documentation says use</p>
<pre><code>pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>to install the latest version of PyTorch. This works when I do it manually but when I add it to req.txt and do <code>pip install -r req.txt</code>, it fails and says  <code>ERROR: No matching distribution</code>.</p>
<p>Edit: adding the whole line from req.txt and error here.</p>
<pre><code>torch==1.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html

torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.htmltorch==1.4.0+cpu
</code></pre>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.4.0+cpu (from -r requirements.txt (line 1)) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2, 0.3.1, 0.4.0, 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0)
ERROR: No matching distribution found for torch==1.4.0+cpu (from -r requirements.txt (line 1))
</code></pre>",60913038.0,3,2,,2020/3/29 10:15,3.0,2021/1/17 5:04,2020/10/21 5:55,,6289554.0,,6289554.0,,1,16,python|pip|pytorch|requirements.txt,7117,56.0092,,1,install pytorch requirement txt torch documentation say use install late version pytorch work manually add req txt fail say edit add whole line req txt error
381,381,46422845,What is the way to understand Proximal Policy Optimization Algorithm in RL?,"<p>I know the basics of Reinforcement Learning, but what terms it's necessary to understand to be able read <a href=""https://arxiv.org/abs/1707.06347"" rel=""noreferrer"">arxiv PPO paper</a> ?</p>

<p>What is the roadmap to learn and use <a href=""https://blog.openai.com/openai-baselines-ppo/"" rel=""noreferrer"">PPO</a> ?</p>",50663200.0,4,0,,2017/9/26 9:36,39.0,2021/7/13 15:54,,,,,2844907.0,,1,43,machine-learning|reinforcement-learning,19657,215.574,,0,way understand proximal policy optimization algorithm rl know basic reinforcement learning term necessary understand able read arxiv ppo paper roadmap learn use ppo
513,513,34670112,How to deal with batches with variable-length sequences in TensorFlow?,"<p>I was trying to use an RNN (specifically, LSTM) for sequence prediction. However, I ran into an issue with variable sequence lengths. For example,</p>

<pre><code>sent_1 = ""I am flying to Dubain""
sent_2 = ""I was traveling from US to Dubai""
</code></pre>

<p>I am trying to predicting the next word after the current one with a simple RNN based on this <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""noreferrer"">Benchmark for building a PTB LSTM model</a>.</p>

<p>However, the <code>num_steps</code> parameter (used for unrolling to the previous hidden states), should remain the same in each Tensorflow's epoch. Basically, batching sentences is not possible as the sentences vary in length. </p>

<pre><code> # inputs = [tf.squeeze(input_, [1])
 #           for input_ in tf.split(1, num_steps, inputs)]
 # outputs, states = rnn.rnn(cell, inputs, initial_state=self._initial_state)
</code></pre>

<p>Here, <code>num_steps</code> need to be changed in my case for every sentence. I have tried several hacks, but nothing seems working.</p>",34675264.0,5,1,,2016/1/8 5:29,22.0,2019/2/7 2:57,2019/2/7 2:57,,1419865.0,,4763311.0,,1,25,python|tensorflow|lstm|recurrent-neural-network,45051,93.6148,,3,deal batch variable length sequence tensorflow try use rnn specifically lstm sequence prediction however run issue variable sequence length example try predict next word current one simple rnn base benchmark build ptb lstm model however parameter use unroll previous hidden state remain tensorflow epoch basically batching sentence possible sentence vary length need change case every sentence try several hack nothing seem work
624,624,50793797,"ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None","<pre><code>C:\Users\meela\Anaconda3\python.exe E:/TTIGAN/test.py

You can find the C code in this temporary file: C:\Users\meela\AppData\Local\Temp\theano_compilation_error_zncbj7_k
Traceback (most recent call last):
  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\gof\lazylinker_c.py"", line 81, in &lt;module&gt;
    actual_version, force_compile, _need_reload))

ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\gof\lazylinker_c.py"", line 105, in &lt;module&gt;
    actual_version, force_compile, _need_reload))

ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/TTIGAN/test.py"", line 1, in &lt;module&gt;
    import skipthoughts

  File ""E:\TTIGAN\skipthoughts.py"", line 6, in &lt;module&gt;
    import theano

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\__init__.py"", line 110, in &lt;module&gt;

    from theano.compile import (

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\compile\__init__.py"", line 12, in &lt;module&gt;

    from theano.compile.mode import *

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\compile\mode.py"", line 11, in &lt;module&gt;

    import theano.gof.vm

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\gof\vm.py"", line 674, in &lt;module&gt;

    from . import lazylinker_c

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\gof\lazylinker_c.py"", line 140, in &lt;module&gt;

    preargs=args)

  File ""C:\Users\meela\Anaconda3\lib\site-packages\theano\gof\cmodule.py"", line 2388, in compile_str

    (status, compile_stderr.replace('\n', '. ')))
. 

Process finished with exit code 1
</code></pre>",50986365.0,5,0,,2018/6/11 8:53,2.0,2020/7/23 15:55,2018/11/6 9:36,,32043.0,,8450790.0,,1,10,theano,13198,50.482,,1,importerror version check exist lazylinker compile file look version find none
300,300,57499002,Can't install pytorch with pip on Windows,"<p>I'm trying to install Pytorch with Windows and I'm using the commands of the official site
<a href=""https://pytorch.org/get-started/locally/"" rel=""noreferrer"">https://pytorch.org/get-started/locally/</a></p>

<pre><code>pip3 install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>

<p>This is the command if I choose Windows, Cuda 10.0, and Python 3.7
But if I run this I get the error message:</p>

<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.2.0 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch==1.2.0
</code></pre>

<p>So why does this happen?
My pip is version 19.2 and I am in a newly installed python 3.7 environment</p>",57565285.0,12,1,,2019/8/14 16:54,1.0,2021/7/8 21:19,,,,,7218182.0,,1,23,python|python-3.x|pytorch|python-3.7,30663,134.14600000000002,,1,ca install pytorch pip window try install pytorch window use command official site command choose window cuda python run get error message happen pip version newly instal python environment
38,38,54589669,"confusion matrix error ""Classification metrics can't handle a mix of multilabel-indicator and multiclass targets""","<p>I am getting a</p>

<pre><code>Classification metrics can't handle a mix of multilabel-indicator and multiclass targets
</code></pre>

<p>error when I try to use confusion matrix.</p>

<p>I am doing my first deep learning project. I am new to it. I am using the mnist dataset provided by keras. I have trained and tested my model successfully. </p>

<p>However, when I try to use the scikit learn confusion matrix I get the error stated above. I have searched for an answer and while there are answers on this error, none of them worked for me. From what I found online it probably has something to do with the loss function (I use the <code>categorical_crossentropy</code> in my code). I tried changing it to <code>sparse_categorical_crossentropy</code> but that just gave me the </p>

<pre><code>Error when checking target: expected dense_2 to have shape (1,) but got array with shape (10,)
</code></pre>

<p>when I run the <code>fit()</code> function on the model. </p>

<p>This is the code. (I have left out the imports for the sake of brevity)</p>

<pre><code>model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(28 * 28,)))
model.add(Dense(10, activation='softmax')) 

model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

model.fit(train_images, train_labels, epochs=10, batch_size=128)

rounded_predictions = model.predict_classes(test_images, batch_size=128, verbose=0)

cm = confusion_matrix(test_labels, rounded_predictions)
</code></pre>

<p>How can i fix this? </p>",54595455.0,2,0,,2019/2/8 9:46,5.0,2021/7/12 13:56,2020/4/30 1:29,,4685471.0,,9033081.0,,1,15,python|machine-learning|keras|scikit-learn|confusion-matrix,27988,57.7879,,4,confusion matrix error classification metric handle mix multilabel indicator multiclass target get error try use confusion matrix first deep learning project new use mnist dataset provide kera train test model successfully however try use scikit learn confusion matrix get error state search answer answer error none work find online probably something loss function use code try change give run function model code leave import sake brevity fix
462,462,24752655,Unsupervised pre-training for convolutional neural network in theano,"<p>I would like to design a deep net with one (or more) convolutional layers (CNN) and one or more fully connected hidden layers on top.<br>
For deep network with fully connected layers there are methods in theano for unsupervised pre-training, e.g., using <a href=""http://www.deeplearning.net/tutorial/SdA.html"">denoising auto-encoders</a> or <a href=""http://www.deeplearning.net/tutorial/rbm.html"">RBMs</a>.</p>

<p>My question is: How can I implement (in theano) an unsupervised pre-training stage for convolutional layers?</p>

<p>I do not expect a full implementation as an answer, but I would appreciate a link to a good tutorial or a reliable reference.</p>",26047542.0,1,4,,2014/7/15 7:47,13.0,2016/12/29 13:19,2015/11/22 11:22,,1714410.0,,1714410.0,,1,37,python|neural-network|theano|deep-learning|unsupervised-learning,10316,53.45399999999999,,0,unsupervised pre training convolutional neural network theano would like design deep net one convolutional layer cnn one fully connect hidden layer top deep network fully connect layer method theano unsupervised pre training e g use denoising auto encoders rbms question implement theano unsupervised pre training stage convolutional layer expect full implementation answer would appreciate link good tutorial reliable reference
594,594,49710537,PyTorch / Gensim - How to load pre-trained word embeddings,"<p>I want to load a pre-trained word2vec embedding with gensim into a PyTorch embedding layer.</p>

<p>So my question is, how do I get the embedding weights loaded by gensim into the PyTorch embedding layer.</p>

<p>Thanks in Advance!</p>",49802495.0,6,0,,2018/4/7 18:21,18.0,2020/4/3 8:18,2018/8/10 12:26,,7483494.0,,7483494.0,,1,44,python|neural-network|pytorch|gensim|embedding,36512,146.05,,3,pytorch gensim load pre trained word embeddings want load pre train word vec embed gensim pytorch embed layer question get embed weight load gensim pytorch embed layer thanks advance
605,605,50056356,"""Could not interpret optimizer identifier"" error in Keras","<p>I got this error when I tried to modify the learning rate parameter of SGD optimizer in Keras. Did I miss something in my codes or my Keras was not installed properly? </p>

<p>Here is my code:</p>

<pre><code>from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Activation
import keras
from keras.optimizers import SGD

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('softmax'))
model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics= ['accuracy'])*
</code></pre>

<p>and here is the error message:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""C:\TensorFlow\Keras\ResNet-50\test_sgd.py"", line 10, in 
      model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])   File
  ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\models.py"",
  line 787, in compile
      **kwargs)   File ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\engine\training.py"",
  line 632, in compile
      self.optimizer = optimizers.get(optimizer)   File ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\optimizers.py"",
  line 788, in get
      raise ValueError('Could not interpret optimizer identifier:', identifier) ValueError: ('Could not interpret optimizer identifier:',
  )</p>
</blockquote>",,16,0,,2018/4/27 6:15,8.0,2021/9/3 16:44,,,,,9708599.0,,1,43,python|python-3.x|tensorflow|keras,73741,263.071,,4,could interpret optimizer identifier error kera get error try modify learning rate parameter sgd optimizer kera miss something code kera instal properly code error message traceback recent call last file c tensorflow kera resnet test sgd py line model compile loss mean square error optimizer sgd lr metric accuracy file c user sugiant appdata local program python python lib site package tensorflow python kera impl kera model py line compile kwargs file c user sugiant appdata local program python python lib site package tensorflow python kera impl kera engine training py line compile self optimizer optimizers get optimizer file c user sugiant appdata local program python python lib site package tensorflow python kera impl keras optimizers py line get raise valueerror could interpret optimizer identifier identifier valueerror could interpret optimizer identifier
207,207,43829711,What is the correct way to change image channel ordering between channels first and channels last?,"<p>I can not for the life of me figure out how to switch the image ordering. images are read in (x,x,3) format, theano requires it to be in (3,x,x) format. I tried changing the order with
<code>numpy.array([img[:,:,i] for i in range(3)])</code></p>

<p>which i guess gets the job done, but it is both ugly and i can't figure out how to reverse it to get the original image back.</p>",50751553.0,5,2,,2017/5/7 9:05,8.0,2020/6/16 20:41,2018/5/11 2:30,,7975668.0,,7975668.0,,1,41,python|numpy|machine-learning|keras|theano,53494,127.913,,3,correct way change image channel order channel first channel last life figure switch image ordering image read x x format theano require x x format try change order guess get job ugly figure reverse get original image back
170,170,43017017,Keras: model.predict for a single image,"<p>I'd like to make a prediction for a single image with Keras. I've trained my model so I'm just loading the weights. </p>

<pre><code>from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
import numpy as np
import cv2

# dimensions of our images.
img_width, img_height = 150, 150



def create_model():
  if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
  else:
    input_shape = (img_width, img_height, 3)

  model = Sequential()
  model.add(Conv2D(32, (3, 3), input_shape=input_shape))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(32, (3, 3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Conv2D(64, (3, 3)))
  model.add(Activation('relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))

  model.add(Flatten())
  model.add(Dense(64))
  model.add(Activation('relu'))
  model.add(Dropout(0.5))
  model.add(Dense(1))
  model.add(Activation('sigmoid'))

  return model


img = cv2.imread('./test1/1.jpg')
model = create_model()
model.load_weights('./weight.h5')
model.predict(img)
</code></pre>

<p>I'm loading the image using: </p>

<pre><code>img = cv2.imread('./test1/1.jpg')
</code></pre>

<p>And using the predict function of the model:</p>

<pre><code> model.predict(img)
</code></pre>

<p>But I get the error:</p>

<pre><code>ValueError: Error when checking : expected conv2d_1_input to have 4 dimensions, but got array with shape (499, 381, 3)
</code></pre>

<p>How should I proceed to have predictions on a single image ?</p>",43019294.0,5,0,,2017/3/25 13:35,13.0,2020/11/9 6:05,,,,,1236336.0,,1,32,deep-learning|keras,45321,137.625,,5,kera model predict single image like make prediction single image kera train model load weight load image use use predict function model get error proceed prediction single image
318,318,58479556,NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0) to a numpy array,"<p>I try to pass 2 loss functions to a model as <a href=""https://keras.io/models/model/"" rel=""noreferrer"">Keras allows that.</a></p>

<blockquote>
  <p>loss: String (name of objective function) or objective function or
  Loss instance. See losses. If the model has multiple outputs, you can
  use a different loss on each output by <strong>passing a dictionary or a list
  of losses</strong>. The loss value that will be minimized by the model will
  then be the sum of all individual losses.</p>
</blockquote>

<p>The two loss functions:</p>

<pre><code>def l_2nd(beta):
    def loss_2nd(y_true, y_pred):
        ...
        return K.mean(t)

    return loss_2nd
</code></pre>

<p>and </p>

<pre><code>def l_1st(alpha):
    def loss_1st(y_true, y_pred):
        ...
        return alpha * 2 * tf.linalg.trace(tf.matmul(tf.matmul(Y, L, transpose_a=True), Y)) / batch_size

    return loss_1st
</code></pre>

<p>Then I build the model:</p>

<pre><code>l2 = K.eval(l_2nd(self.beta))
l1 = K.eval(l_1st(self.alpha))
self.model.compile(opt, [l2, l1])
</code></pre>

<p>When I train, it produces the error:</p>

<blockquote>
  <p>1.15.0-rc3 WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630:
  calling BaseResourceVariable.<strong>init</strong> (from
  tensorflow.python.ops.resource_variable_ops) with constraint is
  deprecated and will be removed in a future version. Instructions for</p>
  
  <h2>updating: If using Keras pass *_constraint arguments to layers.</h2>
  
  <p>NotImplementedError                       Traceback (most recent call
  last)  in ()
       47                          create_using=nx.DiGraph(), nodetype=None, data=[('weight', int)])
       48 
  ---> 49     model = SDNE(G, hidden_size=[256, 128],)
       50     model.train(batch_size=100, epochs=40, verbose=2)
       51     embeddings = model.get_embeddings()</p>
  
  <p>10 frames  in <strong>init</strong>(self, graph,
  hidden_size, alpha, beta, nu1, nu2)
       72         self.A, self.L = self._create_A_L(
       73             self.graph, self.node2idx)  # Adj Matrix,L Matrix
  ---> 74         self.reset_model()
       75         self.inputs = [self.A, self.L]
       76         self._embeddings = {}</p>
  
  <p> in reset_model(self, opt)</p>
  
  <p>---> 84         self.model.compile(opt, loss=[l2, l1])
       85         self.get_embeddings()
       86 </p>
  
  <p>/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py
  in _method_wrapper(self, *args, **kwargs)
      455     self._self_setattr_tracking = False  # pylint: disable=protected-access
      456     try:
  --> 457       result = method(self, *args, **kwargs)
      458     finally:
      459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access</p>
  
  <p><strong>NotImplementedError: Cannot convert a symbolic Tensor (2nd_target:0)
  to a numpy array.</strong></p>
</blockquote>

<p>Please help, thanks!</p>",58678385.0,4,2,,2019/10/21 3:48,8.0,2021/8/8 19:47,,,,,1508724.0,,1,43,python|tensorflow|keras|loss-function,82058,181.056,,4,notimplementederror convert symbolic tensor nd target numpy array try pass loss function model kera allow loss string name objective function objective function loss instance see loss model multiple output use different loss output pass dictionary list loss loss value minimize model sum individual loss two loss function build model train produce error rc warn tensorflow usr local lib python dist package tensorflow core python ops resource variable ops py call baseresourcevariable init tensorflow python ops resource variable ops constraint deprecate remove future version instruction update use kera pas constraint argument layer notimplementederror traceback recent call last create use nx digraph nodetype none data weight int model sdne g hidden size model train batch size epoch verbose embeddings model get embeddings frame init self graph hidden size alpha beta nu nu self self l self create l self graph self node idx adj matrix l matrix self reset model self input self self l self embeddings reset model self opt self model compile opt loss l l self get embeddings usr local lib python dist package tensorflow core python train track base py method wrapper self args kwargs self self setattr track false pylint disable protected access try result method self args kwargs finally self self setattr track previous value pylint disable protected access notimplementederror convert symbolic tensor nd target numpy array please help thanks
29,29,54194724,How to use keras layers in custom keras layer,"<p>I am trying to write my own keras layer. In this layer, I want to use some other keras layers. Is there any way to do something like this:</p>

<pre><code>class MyDenseLayer(tf.keras.layers.Layer):
  def __init__(self, num_outputs):
    super(MyDenseLayer, self).__init__()
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.fc = tf.keras.layers.Dense(self.num_outputs)

  def call(self, input):
    return self.fc(input)

layer = MyDenseLayer(10)
</code></pre>

<p>When I do something like</p>

<pre><code>input = tf.keras.layers.Input(shape = (16,))
output = MyDenseLayer(10)(input)
model = tf.keras.Model(inputs = [input], outputs = [output])
model.summary()
</code></pre>

<p>it outputs 
<a href=""https://i.stack.imgur.com/8BTmV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8BTmV.png"" alt=""enter image description here""></a></p>

<p>How do I make weiths in the dense there trainable?</p>",57726937.0,4,0,,2019/1/15 7:58,5.0,2021/1/3 7:26,,,,,8039154.0,,1,15,tensorflow|keras,9437,52.8993,,3,use keras layer custom kera layer try write kera layer layer want use keras layer way something like something like output make weiths dense trainable
428,428,47824598,Why does my training loss have regular spikes?,"<p>I'm training the Keras object detection model linked at the bottom of this question, although I believe my problem has to do neither with Keras nor with the specific model I'm trying to train (SSD), but rather with the way the data is passed to the model during training.</p>

<p>Here is my problem (see image below):
My training loss is decreasing overall, but it shows sharp regular spikes: </p>

<p><a href=""https://i.stack.imgur.com/7zmbx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7zmbx.png"" alt=""Training loss""></a></p>

<p>The unit on the x-axis is not training epochs, but tens of training steps. The spikes occur precisely once every 1390 training steps, which is exactly the number of training steps for one full pass over my training dataset.</p>

<p>The fact that the spikes always occur after each full pass over the training dataset makes me suspect that the problem is not with the model itself, but with the data it is being fed during the training.</p>

<p>I'm using the <a href=""https://github.com/pierluigiferrari/ssd_keras/blob/master/ssd_batch_generator.py"" rel=""noreferrer"">batch generator provided in the repository</a> to generate batches during training. I checked the source code of the generator and it does shuffle the training dataset before each pass using <code>sklearn.utils.shuffle</code>.</p>

<p>I'm confused for two reasons:</p>

<ol>
<li>The training dataset is being shuffled before each pass.</li>
<li>As you can see in <a href=""https://github.com/pierluigiferrari/ssd_keras/blob/master/train_ssd7.ipynb"" rel=""noreferrer"">this Jupyter notebook</a>, I'm using the generator's ad-hoc data augmentation features, so the dataset should theoretically never be same for any pass: All the augmentations are random.</li>
</ol>

<p>I made some test predictions to see if the model is actually learning anything, and it is! The predictions get better over time, but of course the model is learning very slowly since those spikes seem to mess up the gradient every 1390 steps.</p>

<p>Any hints as to what this might be are greatly appreciated! I'm using the exact same Jupyter notebook that is linked above for my training, the only variable I changed is the batch size from 32 to 16. Other than that, the linked notebook contains the exact training process I'm following.</p>

<p>Here is a link to the repository that contains the model:</p>

<p><a href=""https://github.com/pierluigiferrari/ssd_keras"" rel=""noreferrer"">https://github.com/pierluigiferrari/ssd_keras</a></p>",47839270.0,3,2,,2017/12/15 1:33,9.0,2020/9/2 13:04,2019/11/18 13:07,,7517192.0,,7517192.0,,1,20,deep-learning|keras,8931,53.8036,,4,training loss regular spike train kera object detection model link bottom question although believe problem neither kera specific model try train ssd rather way data pass model training problem see image training loss decrease overall show sharp regular spike unit x axis train epoch ten train step spike occur precisely every training step exactly number train step one full pas training dataset fact spike always occur full pas training dataset make suspect problem model data feed training use batch generator provide repository generate batch training check source code generator shuffle training dataset pas use confuse two reason training dataset shuffle pas see jupyter notebook use generator ad hoc data augmentation feature dataset theoretically never pas augmentation random make test prediction see model actually learn anything prediction get well time course model learn slowly since spike seem mess gradient every step hint might greatly appreciate use exact jupyter notebook link train variable change batch size link notebook contain exact training process follow link repository contain model
