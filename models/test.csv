,Unnamed: 0,post_id,title,body,accepted_answer_id,answer_count,comment_count,community_owned_date,creation_date,favorite_count,last_activity_date,last_edit_date,last_editor_display_name,last_editor_user_id,owner_display_name,owner_user_id,parent_id,post_type_id,score,tags,view_count,hot_score,closed_date,pipe_cate,clean_text
273,273,44841654,No N-dimensional tranpose in PyTorch,"<p>PyTorch's <code>torch.transpose</code> function only transposes 2D inputs. Documentation is <a href=""http://pytorch.org/docs/master/torch.html#torch.transpose"" rel=""noreferrer"">here</a>.</p>

<p>On the other hand, Tensorflow's <code>tf.transpose</code> function allows you to transpose a tensor of <code>N</code> arbitrary dimensions.</p>

<p>Can someone please explain why PyTorch does not/cannot have N-dimension transpose functionality? Is this due to the dynamic nature of the computation graph construction in PyTorch versus Tensorflow's Define-then-Run paradigm?</p>",44842308,2,0,,2017/6/30 8:07,5,2019/7/19 5:43,2017/6/30 22:46,,1090562,,6354331,,1,25,torch|pytorch,14863,69.6884,,3,n dimensional tranpose pytorch pytorch function transpose input documentation hand tensorflow function allow transpose tensor arbitrary dimension someone please explain pytorch n dimension transpose functionality due dynamic nature computation graph construction pytorch versus tensorflow define run paradigm
776,776,51856613,keras version to use with tensorflow-gpu 1.4,"<p>I am using ubuntu 16, with python 3, tf-GPU with keras.</p>

<p>I downgraded to tf 1.4 due to cuda errors as explained <a href=""https://github.com/tensorflow/tensorflow/issues/15604"" rel=""noreferrer"">here</a> </p>

<p>But now I am getting this error </p>

<blockquote>
  <p>TypeError: softmax() got an unexpected keyword argument 'axis'</p>
</blockquote>

<p>Seems that this is an <a href=""https://github.com/keras-team/keras/issues/9900"" rel=""noreferrer"">API change</a> in tensorflow and new keras is not suitable for the old tf. </p>

<p>I can't find what is the correct keras version to use with tf 1.4 gpu. What is the correct one? </p>",,5,2,,2018/8/15 9:56,4,2021/4/20 0:50,2019/8/20 14:11,,8563649,,4406595,,1,15,python|tensorflow|keras|gpu|version,33798,58.1156,,1,kera version use tensorflow gpu use ubuntu python tf gpu kera downgrade tf due cuda error explain get error typeerror softmax get unexpected keyword argument axis seem api change tensorflow new kera suitable old tf find correct kera version use tf gpu correct one
542,542,36722975,theano g++ not detected,"<p>I installed <code>theano</code> but when I try to use it I got this error:</p>

<blockquote>
  <p>WARNING (theano.configdefaults): g++ not detected! Theano will be unable to execute 
   optimized C-implementations (for both CPU and GPU) and will default to Python 
   implementations. Performance will be severely degraded.</p>
</blockquote>

<p>I installed <code>g++</code>, and put the correct path in the environment variables, so it is like <code>theano</code> does not detect it.</p>

<p>Does anyone know how to solve the problem or which may be the cause?</p>",,5,0,,2016/4/19 15:31,2,2018/11/9 17:25,2016/4/19 16:07,,1324,,6225687,,1,18,python|g++|theano,29244,55.8641,,1,theano g detect instal try use get error warn theano configdefaults g detect theano unable execute optimized c implementation cpu gpu default python implementation performance severely degraded instal put correct path environment variable like detect anyone know solve problem may cause
252,252,44608552,Keras CNN model parameters calculation,"<p>My cnn model, which is created using Keras 1.1.1, has two convolution-pooling layers followed by two dense layers, and dropout is added following the second convolution-pooling layer and the first dense layer. The codes are as follows:</p>

<pre class=""lang-py prettyprint-override""><code>model = Sequential()
#convolution-pooling layers
model.add(Convolution2D(32, 5, 5, input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Convolution2D(64, 5, 5))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
#dense layers
model.add(Flatten()) 
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add((Dense(2)))
model.add(Activation('softmax'))
#optimizer
sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True) 
model.compile(loss='categorical_crossentropy',
              optimizer = sgd,
              metrics=['accuracy'])
print model.summary()
</code></pre>

<p>The model summary gives the table as follows:</p>

<p><a href=""https://i.stack.imgur.com/2sj2X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2sj2X.png"" alt=""enter image description here""></a></p>

<p>I am not clear of how the number of parameters of the second convolution layer (i.e., 51264 indicated by the red rectangle) is computed. I thought the number would be (5*5 + 1)*64 = 1664, since the convolution kernel is 5*5 in size and 64 feature maps are to be extracted. </p>

<p>Besides, I have already implemented dropout. Why does the parameter table not reflect this point. It seems the parameter number without dropout is given, although the dropout (layer) is listed in the table. Anyone can help me to interpret the parameter summary?</p>",,3,2,,2017/6/17 19:21,7,2021/6/17 3:54,2019/5/9 19:07,,9050921,,6807211,,1,15,keras,9580,68.9255,,3,kera cnn model parameter calculation cnn model create use kera two convolution pool layer follow two dense layer dropout add follow second convolution pool layer first dense layer code follow model summary give table follow clear number parameter second convolution layer e indicate red rectangle compute think number would since convolution kernel size feature map extract besides already implement dropout parameter table reflect point seem parameter number without dropout give although dropout layer list table anyone help interpret parameter summary
823,823,59095824,"What is the difference between .pt, .pth and .pwf extentions in PyTorch?","<p>I have seen in some code examples, that people use .pwf as model file saving format. But in PyTorch documentation .pt and .pth are recommended. I used .pwf and worked fine for small 1->16->16 convolutional network.</p>

<p>My question is what is the difference between these formats? Why is .pwf extension not even recommended in PyTorch documentation and why do people still use it?</p>",59100353,3,0,,2019/11/28 20:33,4,2021/7/3 10:50,2020/7/26 14:46,,4317729,,4317729,,1,17,python|serialization|deep-learning|pytorch|checkpointing,15098,70.9157,,3,difference pt pth pwf extentions pytorch see code examples people use pwf model file save format pytorch documentation pt pth recommend use pwf work fine small convolutional network question difference format pwf extension even recommend pytorch documentation people still use
230,230,44260217,Hyperparameter optimization for Pytorch model,<p>What is the best way to perform hyperparameter optimization for a Pytorch model? Implement e.g. Random Search myself? Use Skicit Learn? Or is there anything else I am not aware of?</p>,,4,0,,2017/5/30 10:51,18,2020/9/6 18:35,2020/9/6 18:35,,249341,,2004627,,1,36,python|machine-learning|deep-learning|pytorch|hyperparameters,23818,104.308,2021/8/10 13:41,3,hyperparameter optimization pytorch model best way perform hyperparameter optimization pytorch model implement e g random search use skicit learn anything else aware
164,164,42883547,"Intuitive understanding of 1D, 2D, and 3D convolutions in convolutional neural networks","<p>Can anyone please clearly explain the difference between 1D, 2D, and 3D convolutions in convolutional neural networks (in deep learning) with the use of examples?</p>",,4,1,,2017/3/19 6:20,160,2021/8/17 14:47,2020/3/17 0:20,,3924118,,7108418,,1,164,machine-learning|deep-learning|signal-processing|conv-neural-network|convolution,100647,681.211,2021/2/11 12:00,0,intuitive understanding convolution convolutional neural network anyone please clearly explain difference convolution convolutional neural network deep learning use example
586,586,49466894,"How to correctly give inputs to Embedding, LSTM and Linear layers in PyTorch?","<p>I need some clarity on how to correctly prepare inputs for batch-training using different components of the <code>torch.nn</code> module. Specifically, I'm looking to create an encoder-decoder network for a seq2seq model.</p>

<p>Suppose I have a module with these three layers, in order:</p>

<ol>
<li><code>nn.Embedding</code></li>
<li><code>nn.LSTM</code></li>
<li><code>nn.Linear</code></li>
</ol>

<h1><code>nn.Embedding</code></h1>

<p><strong>Input:</strong> <code>batch_size * seq_length</code><br>
<strong>Output:</strong> <code>batch_size * seq_length * embedding_dimension</code></p>

<p>I don't have any problems here, I just want to be explicit about the expected shape of the input and output.</p>

<h1><code>nn.LSTM</code></h1>

<p><strong>Input:</strong> <code>seq_length * batch_size * input_size</code> (<code>embedding_dimension</code> in this case)<br>
<strong>Output:</strong> <code>seq_length * batch_size * hidden_size</code><br>
<strong>last_hidden_state:</strong> <code>batch_size * hidden_size</code><br>
<strong>last_cell_state:</strong> <code>batch_size * hidden_size</code></p>

<p>To use the output of the <code>Embedding</code> layer as input for the <code>LSTM</code> layer, I need to transpose axis 1 and 2.</p>

<p>Many examples I've found online do something like <code>x = embeds.view(len(sentence), self.batch_size , -1)</code>, but that confuses me. How does this view ensure that elements of the same batch remain in the same batch? What happens when <code>len(sentence)</code> and <code>self.batch</code> size are of same size?</p>

<h1><code>nn.Linear</code></h1>

<p><strong>Input:</strong> <code>batch_size</code> x <code>input_size</code> (hidden_size of LSTM in this case or ??)<br>
<strong>Output:</strong> <code>batch_size</code> x <code>output_size</code></p>

<p>If I only need the <code>last_hidden_state</code> of <code>LSTM</code>, then I can give it as input to <code>nn.Linear</code>.</p>

<p>But if I want to make use of Output (which contains all intermediate hidden states as well) then I need to change <code>nn.Linear</code>'s input size to <code>seq_length * hidden_size</code> and to use Output as input to <code>Linear</code> module I need to transpose axis 1 and 2 of output and then I can view with <code>Output_transposed(batch_size, -1)</code>.</p>

<p>Is my understanding here correct? How do I carry out these transpose operations in tensors <code>(tensor.transpose(0, 1))</code>?</p>",49473068,1,0,,2018/3/24 16:07,26,2018/7/25 6:07,2018/7/25 6:07,,1089957,,1280726,,1,37,lstm|pytorch,16949,78.3166,,3,correctly give input embed lstm linear layer pytorch need clarity correctly prepare input batch training use different component module specifically look create encoder decoder network seq seq model suppose module three layer order input output problem want explicit expected shape input output input case output last hidden state last cell state use output layer input layer need transpose axis many example find online something like confuse view ensure element batch remain batch happen len sentence self batch size size nn linear input batch size x input size hidden size lstm case output batch size x output size need last hidden state lstm give input nn linear want make use output contain intermediate hidden state well need change nn linear input size seq length hidden size use output input linear module need transpose axis output view output transpose batch size understanding correct carry transpose operation tensor tensor transpose
395,395,47066635,Checkpointing keras model: TypeError: can't pickle _thread.lock objects,"<p>It seems like the error has occurred in the past in different contexts <a href=""https://github.com/tensorflow/tensorflow/issues/11157"" rel=""noreferrer"">here</a>, but I'm not dumping the model directly -- I'm using the ModelCheckpoint callback. Any idea what could be going wrong?</p>

<p>Information:</p>

<ul>
<li>Keras version 2.0.8</li>
<li>Tensorflow version 1.3.0</li>
<li>Python 3.6</li>
</ul>

<p>Minimal example to reproduce the error:</p>

<pre><code>from keras.layers import Input, Lambda, Dense
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from keras.optimizers import Adam
import tensorflow as tf
import numpy as np

x = Input(shape=(30,3))
low = tf.constant(np.random.rand(30, 3).astype('float32'))
high = tf.constant(1 + np.random.rand(30, 3).astype('float32'))
clipped_out_position = Lambda(lambda x, low, high: tf.clip_by_value(x, low, high),
                                      arguments={'low': low, 'high': high})(x)

model = Model(inputs=x, outputs=[clipped_out_position])
optimizer = Adam(lr=.1)
model.compile(optimizer=optimizer, loss=""mean_squared_error"")
checkpoint = ModelCheckpoint(""debug.hdf"", monitor=""val_loss"", verbose=1, save_best_only=True, mode=""min"")
training_callbacks = [checkpoint]
model.fit(np.random.rand(100, 30, 3), [np.random.rand(100, 30, 3)], callbacks=training_callbacks, epochs=50, batch_size=10, validation_split=0.33)
</code></pre>

<p>Error output:</p>

<pre><code>Train on 67 samples, validate on 33 samples
Epoch 1/50
10/67 [===&gt;..........................] - ETA: 0s - loss: 0.1627Epoch 00001: val_loss improved from inf to 0.17002, saving model to debug.hdf
Traceback (most recent call last):
  File ""debug_multitask_inverter.py"", line 19, in &lt;module&gt;
    model.fit(np.random.rand(100, 30, 3), [np.random.rand(100, 30, 3)], callbacks=training_callbacks, epochs=50, batch_size=10, validation_split=0.33)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/training.py"", line 1631, in fit

闂?    validation_steps=validation_steps)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/training.py"", line 1233, in _fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/callbacks.py"", line 73, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/callbacks.py"", line 414, in on_epoch_end
    self.model.save(filepath, overwrite=True)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/topology.py"", line 2556, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/models.py"", line 107, in save_model
    'config': model.get_config()
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/site-packages/keras/engine/topology.py"", line 2397, in get_config
    return copy.deepcopy(config)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""/om/user/lnj/openmind_env/tensorflow-gpu/lib/python3.6/copy.py"", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle _thread.lock objects
</code></pre>",47068604,3,0,,2017/11/2 2:30,8,2019/4/2 9:56,,,,,1810854,,1,15,tensorflow|keras|pickle,19189,52.1322,,4,checkpointing kera model typeerror pickle thread lock object seem like error occur past different context dump model directly use modelcheckpoint callback idea could go wrong information kera version tensorflow version python minimal example reproduce error error output
343,343,45328843,Keras confusion about number of layers,"<p>I'm a bit confused about the number of layers that are used in Keras models. The documentation is rather opaque on the matter.</p>

<p>According to Jason Brownlee the first layer technically consists of two layers, the input layer, specified by <code>input_dim</code> and a hidden layer. See the first questions on <a href=""http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"" rel=""noreferrer"">his blog</a>.</p>

<p>In all of the Keras documentation the first layer is generally specified as
<code>model.add(Dense(number_of_neurons, input_dim=number_of_cols_in_input, activtion=some_activation_function))</code>.</p>

<p>The most basic model we could make would therefore be:</p>

<pre><code> model = Sequential()
 model.add(Dense(1, input_dim = 100, activation = None))
</code></pre>

<p>Does this model consist of a single layer, where 100 dimensional input is passed through a single input neuron, or does it consist of two layers, first a 100 dimensional input layer and second a 1 dimensional hidden layer?</p>

<p>Further, if I were to specify a model like this, how many layers does it have?</p>

<pre><code>model = Sequential()
model.add(Dense(32, input_dim = 100, activation = 'sigmoid'))
model.add(Dense(1)))
</code></pre>

<p>Is this a model with 1 input layer, 1 hidden layer, and 1 output layer or is this a model with 1 input layer and 1 output layer?</p>",45329100,2,0,,2017/7/26 13:46,5,2017/7/27 7:52,,,,,4577697,,1,30,python|tensorflow|neural-network|deep-learning|keras,13259,64.49,,3,kera confusion number layer bit confused number layer use keras model documentation rather opaque matter accord jason brownlee first layer technically consist two layer input layer specify hidden layer see first question blog kera documentation first layer generally specify basic model could make would therefore model consist single layer dimensional input pass single input neuron consist two layer first dimensional input layer second dimensional hidden layer far specify model like many layer model input layer hide layer output layer model input layer output layer
683,683,38292760,TensorFlow - introducing both L2 regularization and dropout into the network. Does it makes any sense?,"<p>I am currently playing with ANN which is part of Udactity DeepLearning course.</p>

<p>I successful built and train network and introduced the L2 regularization on all weights and biases. Right now I am trying out the dropout for hidden layer in order to improve generalization. I wonder, does it makes sense to both introduce the L2 regularization into the hidden layer and dropout on that same layer? If so, how to do this properly?</p>

<p>During dropout we literally switch off half of the activations of hidden layer and double the amount outputted by rest of the neurons. While using the L2 we compute the L2 norm on all hidden weights. But I am not sure how  to compute L2 in case we use dropout. We switch off some activations, shouldn't we remove the weights which are 'not used' now from the L2 calculation? Any references on that matter will be useful, I haven't found any info.</p>

<p>Just in case you are interested, my code for ANN with L2 regularization is below:</p>

<pre class=""lang-py prettyprint-override""><code>#for NeuralNetwork model code is below
#We will use SGD for training to save our time. Code is from Assignment 2
#beta is the new parameter - controls level of regularization. Default is 0.01
#but feel free to play with it
#notice, we introduce L2 for both biases and weights of all layers

beta = 0.01

#building tensorflow graph
graph = tf.Graph()
with graph.as_default():
      # Input data. For the training data, we use a placeholder that will be fed
  # at run time with a training minibatch.
  tf_train_dataset = tf.placeholder(tf.float32,
                                    shape=(batch_size, image_size * image_size))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_valid_dataset = tf.constant(valid_dataset)
  tf_test_dataset = tf.constant(test_dataset)

  #now let's build our new hidden layer
  #that's how many hidden neurons we want
  num_hidden_neurons = 1024
  #its weights
  hidden_weights = tf.Variable(
    tf.truncated_normal([image_size * image_size, num_hidden_neurons]))
  hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))

  #now the layer itself. It multiplies data by weights, adds biases
  #and takes ReLU over result
  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)

  #time to go for output linear layer
  #out weights connect hidden neurons to output labels
  #biases are added to output labels  
  out_weights = tf.Variable(
    tf.truncated_normal([num_hidden_neurons, num_labels]))  

  out_biases = tf.Variable(tf.zeros([num_labels]))  

  #compute output  
  out_layer = tf.matmul(hidden_layer,out_weights) + out_biases
  #our real output is a softmax of prior result
  #and we also compute its cross-entropy to get our loss
  #Notice - we introduce our L2 here
  loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
    out_layer, tf_train_labels) +
    beta*tf.nn.l2_loss(hidden_weights) +
    beta*tf.nn.l2_loss(hidden_biases) +
    beta*tf.nn.l2_loss(out_weights) +
    beta*tf.nn.l2_loss(out_biases)))

  #now we just minimize this loss to actually train the network
  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

  #nice, now let's calculate the predictions on each dataset for evaluating the
  #performance so far
  # Predictions for the training, validation, and test data.
  train_prediction = tf.nn.softmax(out_layer)
  valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)
  valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases) 

  test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)
  test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)



#now is the actual training on the ANN we built
#we will run it for some number of steps and evaluate the progress after 
#every 500 steps

#number of steps we will train our ANN
num_steps = 3001

#actual training
with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print(""Initialized"")
  for step in range(num_steps):
    # Pick an offset within the training data, which has been randomized.
    # Note: we could use better randomization across epochs.
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    # Generate a minibatch.
    batch_data = train_dataset[offset:(offset + batch_size), :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    # Prepare a dictionary telling the session where to feed the minibatch.
    # The key of the dictionary is the placeholder node of the graph to be fed,
    # and the value is the numpy array to feed to it.
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 500 == 0):
      print(""Minibatch loss at step %d: %f"" % (step, l))
      print(""Minibatch accuracy: %.1f%%"" % accuracy(predictions, batch_labels))
      print(""Validation accuracy: %.1f%%"" % accuracy(
        valid_prediction.eval(), valid_labels))
      print(""Test accuracy: %.1f%%"" % accuracy(test_prediction.eval(), test_labels))
</code></pre>",38297231,3,1,,2016/7/10 14:06,12,2017/5/28 5:09,2016/8/8 10:00,,429322,,3633250,,1,25,machine-learning|neural-network|tensorflow|deep-learning|regularized,22906,63.4398,,4,tensorflow introduce l regularization dropout network make sense currently play ann part udactity deeplearning course successful built train network introduce l regularization weight bias right try dropout hidden layer order improve generalization wonder make sense introduce l regularization hidden layer dropout layer properly dropout literally switch half activation hidden layer double amount output rest neuron use l compute l norm hidden weight sure compute l case use dropout switch activation remove weight use l calculation reference matter useful find info case interested code ann l regularization
652,652,37107223,How to add regularizations in TensorFlow?,"<p>I found in many available neural network code implemented using TensorFlow that regularization terms are often implemented by manually adding an additional term to loss value.</p>

<p>My questions are:</p>

<ol>
<li><p>Is there a more elegant or recommended way of regularization than doing it manually?</p></li>
<li><p>I also find that <code>get_variable</code> has an argument <code>regularizer</code>. How should it be used? According to my observation, if we pass a regularizer to it (such as <code>tf.contrib.layers.l2_regularizer</code>, a tensor representing regularized term will be computed and added to a graph collection named <code>tf.GraphKeys.REGULARIZATOIN_LOSSES</code>. Will that collection be automatically used by TensorFlow (e.g. used by optimizers when training)? Or is it expected that I should use that collection by myself?</p></li>
</ol>",37143333,10,3,,2016/5/9 3:04,63,2019/3/25 11:36,2016/10/31 15:07,,4304503,,4794308,,1,95,python|neural-network|tensorflow|deep-learning,76319,384.531,,4,add regularization tensorflow find many available neural network code implement use tensorflow regularization term often implement manually add additional term loss value question elegant recommended way regularization manually also find argument use accord observation pass regularizer tensor represent regularized term compute add graph collection name collection automatically use tensorflow e g use optimizers training expect use collection
155,155,42711144,How can I install torchtext?,"<p>I have PyTorch installed in my machine but whenever I try to do the following-</p>

<pre><code>from torchtext import data
from torchtext import datasets
</code></pre>

<p>I get the following error.</p>

<pre><code>ImportError: No module named 'torchtext'
</code></pre>

<p>How can I install torchtext?</p>",42735403,8,0,,2017/3/10 5:47,3,2021/2/3 19:51,2018/8/22 12:01,,2956066,,5352399,,1,16,python|nlp|deep-learning|pytorch|natural-language-processing,25972,81.258,,1,install torchtext pytorch instal machine whenever try following get following error install torchtext
56,56,55322434,How to clear Cuda memory in PyTorch,"<p>I am trying to get the output of a neural network which I have already trained. The input is an image of the size 300x300. I am using a batch size of 1, but I still get a <code>CUDA error: out of memory</code> error after I have successfully got the output for 25 images. </p>

<p>I searched for some solutions online and came across <code>torch.cuda.empty_cache()</code>. But this still doesn't seem to solve the problem.</p>

<p>This is the code I am using.</p>

<pre><code>device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

train_x = torch.tensor(train_x, dtype=torch.float32).view(-1, 1, 300, 300)
train_x = train_x.to(device)
dataloader = torch.utils.data.DataLoader(train_x, batch_size=1, shuffle=False)

right = []
for i, left in enumerate(dataloader):
    print(i)
    temp = model(left).view(-1, 1, 300, 300)
    right.append(temp.to('cpu'))
    del temp
    torch.cuda.empty_cache()
</code></pre>

<p>This <code>for loop</code> runs for 25 times every time before giving the memory error.</p>

<p>Every time, I am sending a new image in the network for computation. So, I don't really need to store the previous computation results in the GPU after every iteration in the loop. Is there any way to achieve this?</p>

<p>Any help will be appreciated. Thanks.</p>",55340037,1,0,,2019/3/24 9:38,8,2019/3/25 14:24,2019/3/24 9:55,,681865,,9536387,,1,36,python|pytorch,54634,84.1498,,4,clear cuda memory pytorch try get output neural network already train input image size x use batch size still get error successfully get output image search solution online come across still seem solve problem code use run time every time give memory error every time send new image network computation really need store previous computation result gpu every iteration loop way achieve help appreciate thanks
607,607,50125844,How to standard scale a 3D matrix?,"<p>I am working on a signal classification problem and would like to scale the dataset matrix first, but my data is in a 3D format (batch, length, channels).<br>
I tried to use Scikit-learn Standard Scaler:</p>

<pre><code>from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
</code></pre>

<p>But I've got this error message:</p>

<blockquote>
  <p>Found array with dim 3. StandardScaler expected &lt;= 2</p>
</blockquote>

<p>I think one solution would be to split the matrix by each channel in multiples 2D matrices, scale them separately and then put back in 3D format, but I wonder if there is a better solution.<br>
Thank you very much.</p>",50134698,7,0,,2018/5/2 1:06,2,2021/7/19 6:09,2020/10/7 9:43,,10375049,,9028779,,1,19,python|machine-learning|keras|scikit-learn|deep-learning,20078,91.8109,,3,standard scale matrix work signal classification problem would like scale dataset matrix first data format batch length channel try use scikit learn standard scaler get error message find array dim standardscaler expect think one solution would split matrix channel multiple matrix scale separately put back format wonder good solution thank much
282,282,44947842,Can someone explain to me the difference between activation and recurrent activation arguments passed in initialising keras lstm layer?,"<p>Can someone explain to me the difference between activation and recurrent activation arguments passed in initialising keras lstm layer?</p>

<p>According to my understanding LSTM has 4 layers. Please explain what are th e default activation functions of each layer if I do not pass any activation argument to the LSTM constructor?</p>",,5,0,,2017/7/6 11:39,1,2019/2/18 19:51,,,,,8264386,,1,19,keras|lstm|keras-layer,7936,68.5984,,3,someone explain difference activation recurrent activation argument pass initialise kera lstm layer someone explain difference activation recurrent activation argument pass initialise kera lstm layer accord understanding lstm layer please explain th e default activation function layer pass activation argument lstm constructor
757,757,51463383,Buffered data was truncated after reaching the output size limit,"<p>When I use Colaboratory to run my NIN model, it occurs an error in the output of training process which tells ""Buffered data was truncated after reaching the output size limit."" in the 61th epoch. I have no idea about this crash.Is my code still running ? How can I solve this problem?</p>

<p>Here is some information about the output of my training process:</p>

<pre><code>Epoch 57/200
391/391 [==============================] - 53s 135ms/step - loss: 0.8365 - acc: 0.7784 - val_loss: 0.9250 - val_acc: 0.7625
Epoch 58/200
 28/391 [=&gt;............................] - ETA: 46s - loss: 0.8356 - acc: 0.7835391/391 [==============================] - 53s 136ms/step - loss: 0.8288 - acc: 0.7811 - val_loss: 0.8977 - val_acc: 0.7608
Epoch 59/200
326/391 [========================&gt;.....] - ETA: 8s - loss: 0.8309 - acc: 0.7789391/391 [==============================] - 53s 136ms/step - loss: 0.8297 - acc: 0.7798 - val_loss: 0.9030 - val_acc: 0.7628
Epoch 60/200
391/391 [==============================] - 53s 134ms/step - loss: 0.8245 - acc: 0.7825 - val_loss: 0.8378 - val_acc: 0.7767
Epoch 61/200
 28/391 [=&gt;............................] - ETA: 46s - loss: 0.8281 - acc: 0.7879390/391 [============================&gt;.] - ETA: 0s - loss: 0.8177 - acc: 0.7851Buffered data was truncated after reaching the output size limit.
</code></pre>",,3,0,,2018/7/22 8:39,7,2020/5/18 17:04,2018/7/22 10:28,,9871094,,8612257,,1,24,python|keras|deep-learning|google-colaboratory,20768,59.6696,,4,buffer data truncate reach output size limit use colaboratory run nin model occur error output training process tell buffer data truncate reach output size limit th epoch idea crash code still run solve problem information output training process
229,229,44238154,What is the difference between Luong attention and Bahdanau attention?,"<p>These two attentions are used in <strong>seq2seq</strong> modules. The two different attentions are introduced as multiplicative and additive attentions in <a href=""https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq"" rel=""noreferrer"">this</a> TensorFlow documentation. What is the difference?</p>",44239754,5,0,,2017/5/29 8:43,13,2020/12/3 4:58,2020/10/26 12:21,,63550,,5915270,,1,28,tensorflow|deep-learning|nlp|attention-model,27243,107.741,,3,difference luong attention bahdanau attention two attention use seq seq modules two different attention introduce multiplicative additive attention tensorflow documentation difference
728,728,40472499,Issue NaN with Adam solver,"<p>I'm training networks with the Adam solver and ran into the problem, that optimization hits 'nan' at some point, but the loss seems to decrease nicely up to that point. It happens only for some specific configurations and after a couple of thousand iterations. For example the network with batch size 5 will have the problem, while with a batch size of one it works. So I started debugging my code:</p>

<p>1) First thing that came to my mind is to check the inputs when the net hits 'nan', but they look reasonable (correctly labled ground truth and input with an okayish value range)</p>

<p>2) While searching I discovered <code>tf.verify_tensor_all_finite(..)</code> and I put that all over my code to see, which tensor first becomes 'nan'.
I could narrow down the problem to the following lines:</p>

<pre><code>kernel = tf.verify_tensor_all_finite(kernel, 'kernel')
in_tensor = tf.verify_tensor_all_finite(in_tensor, 'in_tensor')
tmp_result = tf.nn.conv2d_transpose(value=in_tensor, filter=kernel, output_shape=output_shape,
                strides=strides, padding='SAME')
tmp_result = tf.verify_tensor_all_finite(tmp_result, 'convres')
</code></pre>

<p>Which throw an error, which reads:</p>

<pre><code>InvalidArgumentError (see above for traceback): convres : Tensor had NaN values
     [[Node: upconv_logits5_fs/VerifyFinite_2/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=[""loc:@upconv_logits5_fs/conv2d_transpose""], message=""convres"", _device=""/job:localhost/replica:0/task:0/gpu:0""](upconv_logits5_fs/conv2d_transpose)]]
     [[Node: Adam/update/_2794 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_154_Adam/update"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>Now I'm not sure what happened here. </p>

<p>I guess, that during the forward pass everything went well, because the scalar loss didn't trigger an error and also kernel &amp; input were still valid numbers. It seems as some Adam update node modifies the value of my <code>upconv_logits5_fs</code> towards nan. This transposed convolution op is the very last of my network and therefore the first one to be updated. </p>

<p>I'm working with a <code>tf.nn.softmax_cross_entropy_with_logits()</code> loss and put <code>tf.verify_tensor_all_finite()</code> on all it's in- and outputs, but they don't trigger errors. The only conclusion I can draw is that there might be a numerical issue with the Adam solver. </p>

<ul>
<li>What do you think about that conclusion?</li>
<li>Does anybody have any idea how to proceed or what I could try? </li>
</ul>

<p>Your help is very much appreciated.</p>

<p><strong>EDIT:</strong>
I was able to solve my problem by increasing the solvers <em>epsilon</em> parameter from 1e-8 to 1e-4. It seemed like some of my parameters tend to have very little to zero variance and that resulted in <code>tf.sqrt(0.0 + epsilon)</code>, which caused numerical problems.</p>",,4,0,,2016/11/7 18:55,9,2021/1/29 0:34,2016/11/10 10:40,,6375313,,6375313,,1,14,python|tensorflow|deep-learning,9365,54.086,,4,issue nan adam solver train network adam solver run problem optimization hit nan point loss seem decrease nicely point happen specific configuration couple thousand iteration example network batch size problem batch size one work start debug code first thing come mind check input net hit nan look reasonable correctly labled ground truth input okayish value range search discover put code see tensor first become nan could narrow problem follow line throw error read sure happen guess forward pas everything go well scalar loss trigger error also kernel input still valid number seem adam update node modify value towards nan transposed convolution op last network therefore first one update work loss put output trigger error conclusion draw might numerical issue adam solver think conclusion anybody idea proceed could try help much appreciate edit able solve problem increase solver epsilon parameter e e seem like parameter tend little zero variance result cause numerical problem
99,99,41488279,Neural network always predicts the same class,"<p>I'm trying to implement a neural network that classifies images into one of the two discrete categories. The problem is, however, that it currently always predicts 0 for any input and I'm not really sure why.</p>

<p>Here's my feature extraction method:</p>

<pre><code>def extract(file):
    # Resize and subtract mean pixel
    img = cv2.resize(cv2.imread(file), (224, 224)).astype(np.float32)
    img[:, :, 0] -= 103.939
    img[:, :, 1] -= 116.779
    img[:, :, 2] -= 123.68
    # Normalize features
    img = (img.flatten() - np.mean(img)) / np.std(img)

    return np.array([img])
</code></pre>

<p>Here's my gradient descent routine:</p>

<pre><code>def fit(x, y, t1, t2):
    """"""Training routine""""""
    ils = x.shape[1] if len(x.shape) &gt; 1 else 1
    labels = len(set(y))

    if t1 is None or t2 is None:
        t1 = randweights(ils, 10)
        t2 = randweights(10, labels)

    params = np.concatenate([t1.reshape(-1), t2.reshape(-1)])
    res = grad(params, ils, 10, labels, x, y)
    params -= 0.1 * res

    return unpack(params, ils, 10, labels)
</code></pre>

<p>Here are my forward and back(gradient) propagations:</p>

<pre><code>def forward(x, theta1, theta2):
    """"""Forward propagation""""""

    m = x.shape[0]

    # Forward prop
    a1 = np.vstack((np.ones([1, m]), x.T))
    z2 = np.dot(theta1, a1)

    a2 = np.vstack((np.ones([1, m]), sigmoid(z2)))
    a3 = sigmoid(np.dot(theta2, a2))

    return (a1, a2, a3, z2, m)

def grad(params, ils, hls, labels, x, Y, lmbda=0.01):
    """"""Compute gradient for hypothesis Theta""""""

    theta1, theta2 = unpack(params, ils, hls, labels)

    a1, a2, a3, z2, m = forward(x, theta1, theta2)
    d3 = a3 - Y.T
    print('Current error: {}'.format(np.mean(np.abs(d3))))

    d2 = np.dot(theta2.T, d3) * (np.vstack([np.ones([1, m]), sigmoid_prime(z2)]))
    d3 = d3.T
    d2 = d2[1:, :].T

    t1_grad = np.dot(d2.T, a1.T)
    t2_grad = np.dot(d3.T, a2.T)

    theta1[0] = np.zeros([1, theta1.shape[1]])
    theta2[0] = np.zeros([1, theta2.shape[1]])

    t1_grad = t1_grad + (lmbda / m) * theta1
    t2_grad = t2_grad + (lmbda / m) * theta2

    return np.concatenate([t1_grad.reshape(-1), t2_grad.reshape(-1)])
</code></pre>

<p>And here's my prediction function:</p>

<pre><code>def predict(theta1, theta2, x):
    """"""Predict output using learned weights""""""
    m = x.shape[0]

    h1 = sigmoid(np.hstack((np.ones([m, 1]), x)).dot(theta1.T))
    h2 = sigmoid(np.hstack((np.ones([m, 1]), h1)).dot(theta2.T))

    return h2.argmax(axis=1)
</code></pre>

<p>I can see that the error rate is gradually decreasing with each iteration, generally converging somewhere around 1.26e-05.</p>

<p>What I've tried so far:</p>

<ol>
<li>PCA</li>
<li>Different datasets (Iris from sklearn and handwritten numbers from Coursera ML course, achieving about 95% accuracy on both). However, both of those were processed in a batch, so I can assume that my general implementation is correct, but there is something wrong with either how I extract features, or how I train the classifier.</li>
<li>Tried sklearn's SGDClassifier and it didn't perform much better, giving me a ~50% accuracy. So something wrong with the features, then?</li>
</ol>

<p><strong>Edit</strong>:
An average output of h2 looks like the following:</p>

<pre><code>[0.5004899   0.45264441]
[0.50048522  0.47439413]
[0.50049019  0.46557124]
[0.50049261  0.45297816]
</code></pre>

<p>So, very similar sigmoid outputs for all validation examples.</p>",41716648,10,7,,2017/1/5 15:06,45,2021/2/12 5:23,2017/1/6 14:23,,7022561,,7022561,,1,60,python-3.x|numpy|neural-network|deep-learning|gradient-descent,75272,299.507,,4,neural network always predict class try implement neural network classify image one two discrete category problem however currently always predicts input really sure feature extraction method gradient descent routine forward back gradient propagation prediction function see error rate gradually decrease iteration generally converge somewhere around e try far pca different datasets iris sklearn handwritten number coursera ml course achieve accuracy however process batch assume general implementation correct something wrong either extract feature train classifier try sklearn sgdclassifier perform much well give accuracy something wrong feature edit average output h look like follow similar sigmoid output validation example
595,595,49768306,Pytorch tensor to numpy array,"<p>I have a <code>pytorch</code> Tensor of size <code>torch.Size([4, 3, 966, 1296])</code></p>

<p>I want to convert it to <code>numpy</code> array using the following code:</p>

<p><code>imgs = imgs.numpy()[:, ::-1, :, :]</code></p>

<p>Can anyone please explain what this code is doing ?</p>",49768500,6,4,,2018/4/11 6:49,14,2021/5/10 23:43,,,,,1249184,,1,74,python|numpy|pytorch,208096,230.073,,3,pytorch tensor numpy array tensor size want convert array use following code anyone please explain code
190,190,43469281,How to predict input image using trained model in Keras?,"<p>I'm only beginning with keras and machine learning in general.</p>

<p>I trained a model to classify images from 2 classes and saved it using <code>model.save()</code>. Here is the code I used:</p>

<pre><code>from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K


# dimensions of our images.
img_width, img_height = 320, 240

train_data_dir = 'data/train'
validation_data_dir = 'data/validation'
nb_train_samples = 200  #total
nb_validation_samples = 10  # total
epochs = 6
batch_size = 10

if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(
    train_generator,
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=5)

model.save('model.h5')
</code></pre>

<p>It successfully trained with 0.98 accuracy which is pretty good. To load and test this model on new images, I used the below code:</p>

<pre><code>from keras.models import load_model
import cv2
import numpy as np

model = load_model('model.h5')

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

img = cv2.imread('test.jpg')
img = cv2.resize(img,(320,240))
img = np.reshape(img,[1,320,240,3])

classes = model.predict_classes(img)

print classes
</code></pre>

<p>It outputs:</p>

<blockquote>
  <p>[[0]]</p>
</blockquote>

<p>Why wouldn't it give out the actual name of the class and why <code>[[0]]</code>?</p>

<p>Thanks in advance.</p>",43470074,5,1,,2017/4/18 10:09,38,2019/3/10 14:18,2018/5/24 10:51,,2786884,,6554943,,1,50,python|machine-learning|keras|computer-vision,111167,160.184,,5,predict input image use trained model kera begin kera machine learning general train model classify image class save use code use successfully train accuracy pretty good load test model new image use code output would give actual name class thanks advance
199,199,43702481,Why does Keras LSTM batch size used for prediction have to be the same as fitting batch size?,"<p>When using a Keras LSTM to predict on time series data I've been getting errors when I'm trying to train the model using a batch size of 50, while then trying to predict on the same model using a batch size of 1 (ie just predicting the next value).  </p>

<p>Why am I not able to train and fit the model with multiple batches at once, and then use that model to predict for anything other than the same batch size.  It doesn't seem to make sense, but then I could easily be missing something about this.</p>

<p>Edit:  this is the model.  <code>batch_size</code> is 50, <code>sl</code> is sequence length, which is set at 20 currently.</p>

<pre><code>    model = Sequential()
    model.add(LSTM(1, batch_input_shape=(batch_size, 1, sl), stateful=True))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size, verbose=2)
</code></pre>

<p>here is the line for predicting on the training set for RMSE</p>

<pre><code>    # make predictions
    trainPredict = model.predict(trainX, batch_size=batch_size)
</code></pre>

<p>here is the actual prediction of unseen time steps</p>

<pre><code>for i in range(test_len):
    print('Prediction %s: ' % str(pred_count))

    next_pred_res = np.reshape(next_pred, (next_pred.shape[1], 1, next_pred.shape[0]))
    # make predictions
    forecastPredict = model.predict(next_pred_res, batch_size=1)
    forecastPredictInv = scaler.inverse_transform(forecastPredict)
    forecasts.append(forecastPredictInv)
    next_pred = next_pred[1:]
    next_pred = np.concatenate([next_pred, forecastPredict])

    pred_count += 1
</code></pre>

<p>This issue is with the line:</p>

<p><code>forecastPredict = model.predict(next_pred_res, batch_size=batch_size)</code></p>

<p>The error when batch_size here is set to 1 is:</p>

<p><code>ValueError: Cannot feed value of shape (1, 1, 2) for Tensor 'lstm_1_input:0', which has shape '(10, 1, 2)'</code> which is the same error that throws when <code>batch_size</code> here is set to 50 like the other batch sizes as well.</p>

<p>The total error is:</p>

<pre><code>    forecastPredict = model.predict(next_pred_res, batch_size=1)
  File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/keras/models.py"", line 899, in predict
    return self.model.predict(x, batch_size=batch_size, verbose=verbose)
  File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/keras/engine/training.py"", line 1573, in predict
    batch_size=batch_size, verbose=verbose)
   File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/keras/engine/training.py"", line 1203, in _predict_loop
    batch_outs = f(ins_batch)
  File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py"", line 2103, in __call__
    feed_dict=feed_dict)
  File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/home/entelechy/tf_keras/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 944, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (1, 1, 2) for Tensor 'lstm_1_input:0', which has shape '(10, 1, 2)'
</code></pre>

<p>Edit:  Once I set the model to <code>stateful=False</code> then I am able to use different batch sizes for fitting/training and prediction.  What is the reason for this?</p>",43770736,6,7,,2017/4/30 2:56,19,2018/7/4 8:13,2017/5/3 2:54,,5355831,,5355831,,1,26,deep-learning|keras|lstm,26538,101.895,,3,keras lstm batch size use prediction fit batch size use keras lstm predict time series data get error try train model use batch size try predict model use batch size ie predict next value able train fit model multiple batch use model predict anything batch size seem make sense could easily miss something edit model sequence length set currently line predict training set rmse actual prediction unseen time step issue line error batch size set error throw set like batch size well total error edit set model able use different batch size fit training prediction reason
760,760,51591550,logits and labels must be broadcastable error in Tensorflow RNN,"<p>I am new to Tensorflow and deep leaning. I am trying to see how the loss decreases over 10 epochs in my RNN model that I created to read a dataset from kaggle which contains <a href=""https://www.kaggle.com/mlg-ulb/creditcardfraud"" rel=""noreferrer"">credit card fraud data</a>. I am trying to classify the transactions as fraud(1) and not fraud(0). When I try to run the below code I keep getting the below error:</p>
<pre class=""lang-py prettyprint-override""><code>&gt; 2018-07-30 14:59:33.237749: W
&gt; tensorflow/core/kernels/queue_base.cc:277]
&gt; _1_shuffle_batch/random_shuffle_queue: Skipping cancelled enqueue attempt with queue not closed Traceback (most recent call last):  
&gt; File
&gt; &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1322, in _do_call
&gt;     return fn(*args)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1307, in _run_fn
&gt;     options, feed_dict, fetch_list, target_list, run_metadata)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1409, in _call_tf_sessionrun
&gt;     run_metadata) tensorflow.python.framework.errors_impl.InvalidArgumentError: logits
&gt; and labels must be broadcastable: logits_size=[1,2] labels_size=[1,24]
&gt;    [[Node: softmax_cross_entropy_with_logits_sg =
&gt; SoftmaxCrossEntropyWithLogits[T=DT_FLOAT,
&gt; _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](add, softmax_cross_entropy_with_logits_sg/Reshape_1)]]
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; Traceback (most recent call last):   File
&gt; &quot;/home/suleka/Documents/untitled1/RNN_CrediCard.py&quot;, line 96, in
&gt; &lt;module&gt;
&gt;     train_neural_network(x)   File &quot;/home/suleka/Documents/untitled1/RNN_CrediCard.py&quot;, line 79, in
&gt; train_neural_network
&gt;     _, c = sess.run([optimizer, cost], feed_dict={x: feature_batch, y: label_batch})   File
&gt; &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 900, in run
&gt;     run_metadata_ptr)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1135, in _run
&gt;     feed_dict_tensor, options, run_metadata)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1316, in _do_run
&gt;     run_metadata)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py&quot;,
&gt; line 1335, in _do_call
&gt;     raise type(e)(node_def, op, message) tensorflow.python.framework.errors_impl.InvalidArgumentError: logits
&gt; and labels must be broadcastable: logits_size=[1,2] labels_size=[1,24]
&gt;    [[Node: softmax_cross_entropy_with_logits_sg =
&gt; SoftmaxCrossEntropyWithLogits[T=DT_FLOAT,
&gt; _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](add, softmax_cross_entropy_with_logits_sg/Reshape_1)]]
&gt; 
&gt; Caused by op 'softmax_cross_entropy_with_logits_sg', defined at:  
&gt; File &quot;/home/suleka/Documents/untitled1/RNN_CrediCard.py&quot;, line 96, in
&gt; &lt;module&gt;
&gt;     train_neural_network(x)   File &quot;/home/suleka/Documents/untitled1/RNN_CrediCard.py&quot;, line 63, in
&gt; train_neural_network
&gt;     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,
&gt; labels=y))   File
&gt; &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py&quot;,
&gt; line 250, in new_func
&gt;     return func(*args, **kwargs)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py&quot;,
&gt; line 1968, in softmax_cross_entropy_with_logits
&gt;     labels=labels, logits=logits, dim=dim, name=name)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py&quot;,
&gt; line 1879, in softmax_cross_entropy_with_logits_v2
&gt;     precise_logits, labels, name=name)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py&quot;,
&gt; line 7205, in softmax_cross_entropy_with_logits
&gt;     name=name)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py&quot;,
&gt; line 787, in _apply_op_helper
&gt;     op_def=op_def)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py&quot;,
&gt; line 3414, in create_op
&gt;     op_def=op_def)   File &quot;/home/suleka/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py&quot;,
&gt; line 1740, in __init__
&gt;     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
&gt; 
&gt; InvalidArgumentError (see above for traceback): logits and labels must
&gt; be broadcastable: logits_size=[1,2] labels_size=[1,24]     [[Node:
&gt; softmax_cross_entropy_with_logits_sg =
&gt; SoftmaxCrossEntropyWithLogits[T=DT_FLOAT,
&gt; _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](add, softmax_cross_entropy_with_logits_sg/Reshape_1)]]

</code></pre>
<br>
<p>Can anyone point out what I am doing wrong in my code and also any problem in my code if possible. Thank you in advance.</p>
<p>Shown below is my code:</p>
<br>
<pre><code>import tensorflow as tf
from tensorflow.contrib import rnn



# cycles of feed forward and backprop
hm_epochs = 10
n_classes = 2
rnn_size = 128
col_size = 30
batch_size = 24
try_epochs = 1
fileName = &quot;creditcard.csv&quot;

def create_file_reader_ops(filename_queue):
    reader = tf.TextLineReader(skip_header_lines=1)
    _, csv_row = reader.read(filename_queue)
    record_defaults = [[1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1]]
    col1, col2, col3, col4, col5, col6, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17, col18, col19, col20, col21, col22, col23, col24, col25, col26, col27, col28, col29, col30, col31 = tf.decode_csv(csv_row, record_defaults=record_defaults)
    features = tf.stack([col1, col2, col3, col4, col5, col6, col7, col8, col9, col10, col11, col12, col13, col14, col15, col16, col17, col18, col19, col20, col21, col22, col23, col24, col25, col26, col27, col28, col29, col30])
    return features, col31


def input_pipeline(fName, batch_size, num_epochs=None):
    # this refers to multiple files, not line items within files
    filename_queue = tf.train.string_input_producer([fName], shuffle=True, num_epochs=num_epochs)
    features, label = create_file_reader_ops(filename_queue)
    min_after_dequeue = 10000 # min of where to start loading into memory
    capacity = min_after_dequeue + 3 * batch_size # max of how much to load into memory
    # this packs the above lines into a batch of size you specify:
    feature_batch, label_batch = tf.train.shuffle_batch(
        [features, label],
        batch_size=batch_size,
        capacity=capacity,
        min_after_dequeue=min_after_dequeue)
    return feature_batch, label_batch


creditCard_data, creditCard_label = input_pipeline(fileName, batch_size, try_epochs)


x = tf.placeholder('float',[None,col_size])
y = tf.placeholder('float')


def recurrent_neural_network_model(x):
    #giving the weights and biases random values
    layer ={ 'weights': tf.Variable(tf.random_normal([rnn_size, n_classes])),
            'bias': tf.Variable(tf.random_normal([n_classes]))}

    x = tf.split(x, 24, 0)
    print(x)

    lstm_cell = rnn.BasicLSTMCell(rnn_size)
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32 )
    output = tf.matmul(outputs[-1], layer['weights']) + layer['bias']

    return output

def train_neural_network(x):
    prediction = recurrent_neural_network_model(x)
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
    optimizer = tf.train.AdamOptimizer().minimize(cost)


    with tf.Session() as sess:

        gInit = tf.global_variables_initializer().run()
        lInit = tf.local_variables_initializer().run()
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        for epoch in range(hm_epochs):
            epoch_loss = 0

            for counter in range(101):
                    feature_batch, label_batch = sess.run([creditCard_data, creditCard_label])
                    print(label_batch.shape)
                    _, c = sess.run([optimizer, cost], feed_dict={x: feature_batch, y: label_batch})
                    epoch_loss += c
            print('Epoch', epoch, 'compleated out of', hm_epochs, 'loss:', epoch_loss)



train_neural_network(x)
</code></pre>",,6,0,,2018/7/30 10:04,1,2021/6/5 5:16,2020/7/1 4:58,,9817556,,7866526,,1,18,python|tensorflow|machine-learning|deep-learning|recurrent-neural-network,32603,52.653,,4,logits label must broadcastable error tensorflow rnn new tensorflow deep leaning try see loss decrease epoch rnn model create read dataset kaggle contain credit card fraud data try classify transaction fraud fraud try run code keep get error anyone point wrong code also problem code possible thank advance show code
258,258,44674847,What are the differences between all these cross-entropy losses in Keras and TensorFlow?,"<p>What are the differences between all these cross-entropy losses?</p>

<p>Keras is talking about</p>

<ul>
<li><strong>Binary cross-entropy</strong></li>
<li><strong>Categorical cross-entropy</strong></li>
<li><strong>Sparse categorical cross-entropy</strong></li>
</ul>

<p>While TensorFlow has</p>

<ul>
<li><strong>Softmax cross-entropy with logits</strong></li>
<li><strong>Sparse softmax cross-entropy with logits</strong></li>
<li><strong>Sigmoid cross-entropy with logits</strong></li>
</ul>

<p>What are the differences and relationships between them? What are the typical applications for them? What's the mathematical background? Are there other cross-entropy types that one should know? Are there any cross-entropy types without logits?</p>",44684178,2,1,,2017/6/21 11:29,23,2020/4/17 13:26,2020/1/6 13:30,,3924118,,2612484,,1,37,tensorflow|machine-learning|keras|loss-function|cross-entropy,4200,68.293,,4,difference cross entropy loss kera tensorflow difference cross entropy loss kera talk binary cross entropy categorical cross entropy sparse categorical cross entropy tensorflow softmax cross entropy logits sparse softmax cross entropy logits sigmoid cross entropy logits difference relationship typical application mathematical background cross entropy type one know cross entropy type without logits
596,596,49785536,Get learning rate of keras model,"<p>I cannot seem to get the value of learning rate. What I get is below. </p>

<p>I've tried the model for 200 epochs and want to see/change the learning rate. Is this not the correct way?</p>

<pre><code>&gt;&gt;&gt; print(ig_cnn_model.optimizer.lr)
&lt;tf.Variable 'lr_6:0' shape=() dtype=float32_ref&gt;
</code></pre>",49785582,6,2,,2018/4/11 22:56,3,2020/11/26 14:08,2018/4/12 10:20,,7117003,,3657151,,1,28,python|machine-learning|neural-network|keras,33507,134.701,,4,get learning rate kera model seem get value learn rate get try model epoch want see change learning rate correct way
128,128,42112260,How do I use the Tensorboard callback of Keras?,"<p>I have built a neural network with Keras. I would visualize its data by Tensorboard, therefore I have utilized:</p>

<pre class=""lang-py prettyprint-override""><code>keras.callbacks.TensorBoard(log_dir='/Graph', histogram_freq=0,
                            write_graph=True, write_images=True)
</code></pre>

<p>as explained in <a href=""https://keras.io/callbacks/#tensorboard"" rel=""noreferrer"">keras.io</a>. When I run the callback I get <code>&lt;keras.callbacks.TensorBoard at 0x7f9abb3898&gt;</code>, but I don't get any file in my folder ""Graph"". Is there something wrong in how I have used this callback?</p>",42112935,10,4,,2017/2/8 11:47,64,2020/2/25 11:53,2018/6/19 11:33,,2099607,,7387749,,1,148,keras|tensorboard,119183,655.305,,5,use tensorboard callback kera build neural network kera would visualize data tensorboard therefore utilize explain kera io run callback get get file folder graph something wrong use callback
16,16,62494412,"TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'auto' of type 'str' instead","<p>I am making a neural network to predict audio data (to learn more about how neural networks function and how to use tensorflow), and everything is going pretty smoothly so far, with one exception. I've looked around quite a bit to solve this issue and haven't been able to find anything specific enough to help me. I set up the dataset and model and those work fine, but for some reason when I try to train the model, it gives me a type error, even though all of the values in the dataset are 32 bit floats. It'd be much appreciated if someone could answer this for me, or at least push in the right direction to figuring this out. Code and console outputs are below. (BTW all values in dataset are between 0 and 1, I don't know if that's relevant but I thought I'd add that in)</p>
<p>EDIT: I've included the AudioHandler class as well, which you can use to reproduce the error. <code>get_audio_array</code> or <code>get_audio_arrays</code> can be used to convert a single mp3 or a directory of mp3s into array(s) of the audio data. You can also use <code>dataset_from_arrays</code> to generate a dataset from the audio arrays created with <code>dataset_from_arrays</code>.</p>
<pre><code>from AudioHandler import AudioHandler
import os

seq_length = 22050
BATCH_SIZE = 64
BUFFER_SIZE = 10000

audio_arrays = AudioHandler.get_audio_arrays(&quot;AudioDataset&quot;, normalized=True)

dataset = AudioHandler.dataset_from_arrays(audio_arrays, seq_length, BATCH_SIZE, buffer_size=BUFFER_SIZE)

print(dataset)

rnn_units = 256

def build_model(rnn_units, batch_size):
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(batch_input_shape=(batch_size, None, 2)),

        tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True),

        tf.keras.layers.Dense(2)
    ])
    return model


model = build_model(rnn_units, BATCH_SIZE)

model.summary()

model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError)

EPOCHS = 10

history = model.fit(dataset, epochs=EPOCHS)
</code></pre>
<pre><code>from pydub import AudioSegment
import numpy as np
from pathlib import Path
from tensorflow import data
import os


class AudioHandler:

    @staticmethod
    def print_audio_info(file_name):
        audio_segment = AudioSegment.from_file(file_name)
        print(&quot;Information of '&quot; + file_name + &quot;':&quot;)
        print(&quot;Sample rate: &quot; + str(audio_segment.frame_rate) + &quot;kHz&quot;)
        # Multiply frame_width by 8 to get bits, since it is given in bytes
        print(&quot;Sample width: &quot; + str(audio_segment.frame_width * 8) + &quot; bits per sample (&quot; + str(
            int(audio_segment.frame_width * 8 / audio_segment.channels)) + &quot; bits per channel)&quot;)
        print(&quot;Channels: &quot; + str(audio_segment.channels))

    @staticmethod
    def get_audio_array(file_name, normalized=True):
        audio_segment = AudioSegment.from_file(file_name)
        # Get bytestring of raw audio data
        raw_audio_bytestring = audio_segment.raw_data
        # Adjust sample width to accommodate multiple channels in each sample
        sample_width = audio_segment.frame_width / audio_segment.channels
        # Convert bytestring to numpy array
        if sample_width == 1:
            raw_audio = np.array(np.frombuffer(raw_audio_bytestring, dtype=np.int8))
        elif sample_width == 2:
            raw_audio = np.array(np.frombuffer(raw_audio_bytestring, dtype=np.int16))
        elif sample_width == 4:
            raw_audio = np.array(np.frombuffer(raw_audio_bytestring, dtype=np.int32))
        else:
            raw_audio = np.array(np.frombuffer(raw_audio_bytestring, dtype=np.int16))
        # Normalize the audio data
        if normalized:
            # Cast the audio data as 32 bit floats
            raw_audio = raw_audio.astype(dtype=np.float32)
            # Make all values positive
            raw_audio += np.power(2, 8*sample_width)/2
            # Normalize all values between 0 and 1
            raw_audio *= 1/np.power(2, 8*sample_width)
        # Reshape the array to accommodate multiple channels
        if audio_segment.channels &gt; 1:
            raw_audio = raw_audio.reshape((-1, audio_segment.channels))

        return raw_audio

    @staticmethod
    # Return an array of all audio files in directory, as arrays of audio data
    def get_audio_arrays(directory, filetype='mp3', normalized=True):

        file_count_total = len([name for name in os.listdir(directory) if os.path.isfile(os.path.join(directory, name))]) - 1

        audio_arrays = []
        # Iterate through all audio files
        pathlist = Path(directory).glob('**/*.' + filetype)
        # Keep track of progress
        file_count = 0
        print(&quot;Loading audio files... 0%&quot;)
        for path in pathlist:
            path_string = str(path)
            audio_array = AudioHandler.get_audio_array(path_string, normalized=normalized)
            audio_arrays.append(audio_array)
            # Update Progress
            file_count += 1
            print('Loading audio files... ' + str(int(file_count/file_count_total*100)) + '%')

        return audio_arrays

    @staticmethod
    def export_to_file(audio_data_array, file_name, normalized=True, file_type=&quot;mp3&quot;, bitrate=&quot;256k&quot;):
        if normalized:
            audio_data_array *= np.power(2, 16)
            audio_data_array -= np.power(2, 16)/2
        audio_data_array = audio_data_array.astype(np.int16)
        audio_data_array = audio_data_array.reshape((1, -1))[0]
        raw_audio = audio_data_array.tostring()
        audio_segment = AudioSegment(data=raw_audio, sample_width=2, frame_rate=44100, channels=2)
        audio_segment.export(file_name, format=file_type, bitrate=bitrate)

    # Splits a sequence into input values and target values
    @staticmethod
    def __split_input_target(chunk):
        input_audio = chunk[:-1]
        target_audio = chunk[1:]
        return input_audio, target_audio

    @staticmethod
    def dataset_from_arrays(audio_arrays, sequence_length, batch_size, buffer_size=10000):
        # Create main data set, starting with first audio array
        dataset = data.Dataset.from_tensor_slices(audio_arrays[0])
        dataset = dataset.batch(sequence_length + 1, drop_remainder=True)
        # Split each audio array into sequences individually,
        # then concatenate each individual data set with the main data set
        for i in range(1, len(audio_arrays)):
            audio_data = audio_arrays[i]
            tensor_slices = data.Dataset.from_tensor_slices(audio_data)
            audio_dataset = tensor_slices.batch(sequence_length + 1, drop_remainder=True)
            dataset.concatenate(audio_dataset)

        dataset = dataset.map(AudioHandler.__split_input_target)

        dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

        return dataset
</code></pre>
<pre><code>Loading audio files... 0%
Loading audio files... 25%
Loading audio files... 50%
Loading audio files... 75%
Loading audio files... 100%
2020-06-21 00:20:10.796993: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-06-21 00:20:10.811357: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fddb7b23fd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-21 00:20:10.811368: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
&lt;BatchDataset shapes: ((64, 22050, 2), (64, 22050, 2)), types: (tf.float32, tf.float32)&gt;
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru (GRU)                    (64, None, 256)           199680    
_________________________________________________________________
dense (Dense)                (64, None, 2)             514       
=================================================================
Total params: 200,194
Trainable params: 200,194
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
Traceback (most recent call last):
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/RNN.py&quot;, line 57, in &lt;module&gt;
    history = model.fit(dataset, epochs=EPOCHS)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 848, in fit
    tmp_logs = train_function(iterator)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 580, in __call__
    result = self._call(*args, **kwds)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 506, in _initialize
    *args, **kwds))
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__
        losses = self.call(y_true, y_pred)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call
        return self.fn(y_true, y_pred, **self._fn_kwargs)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:313 __init__
        mean_squared_error, name=name, reduction=reduction)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:229 __init__
        super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:94 __init__
        losses_utils.ReductionV2.validate(reduction)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/ops/losses/loss_reduction.py:67 validate
        if key not in cls.all():
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:1491 tensor_equals
        return gen_math_ops.equal(self, other, incompatible_shape_error=False)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py:3224 equal
        name=name)
    /Users/anonteau/Desktop/Development/Python/Lo-FiGenerator/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:479 _apply_op_helper
        repr(values), type(values).__name__, err))

    TypeError: Expected float32 passed to parameter 'y' of op 'Equal', got 'auto' of type 'str' instead. Error: Expected float32, got 'auto' of type 'str' instead.```
</code></pre>",,3,2,,2020/6/21 4:32,2,2021/8/17 1:17,2020/6/26 0:19,,13784216,,13784216,,1,15,python|tensorflow|machine-learning|keras|recurrent-neural-network,6649,69.291,,4,typeerror expect float pass parameter op equal get auto type str instead make neural network predict audio data learn neural network function use tensorflow everything go pretty smoothly far one exception look around quite bit solve issue able find anything specific enough help set dataset model work fine reason try train model give type error even though value dataset bit floats much appreciate someone could answer least push right direction figure code console output btw value dataset know relevant think add edit include audiohandler class well use reproduce error use convert single mp directory mp array audio data also use generate dataset audio array create
158,158,42763094,How to save final model using keras?,"<p>I use KerasClassifier to train the classifier.</p>

<p>The code is below:</p>

<pre><code>import numpy
from pandas import read_csv
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataframe = read_csv(""iris.csv"", header=None)
dataset = dataframe.values
X = dataset[:,0:4].astype(float)
Y = dataset[:,4]
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
#print(""encoded_Y"")
#print(encoded_Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)
#print(""dummy_y"")
#print(dummy_y)
# define baseline model
def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(4, input_dim=4, init='normal', activation='relu'))
    #model.add(Dense(4, init='normal', activation='relu'))
    model.add(Dense(3, init='normal', activation='softmax'))
    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0)
#global_model = baseline_model()
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))
</code></pre>

<p>But How to save the final model for future prediction?</p>

<p>I usually use below code to save model:</p>

<pre><code># serialize model to JSON
model_json = model.to_json()
with open(""model.json"", ""w"") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights(""model.h5"")
print(""Saved model to disk"")
</code></pre>

<p>But I don't know how to insert the saving model's code into KerasClassifier's code.</p>

<p>Thank you.</p>",,7,0,,2017/3/13 11:57,14,2021/1/3 18:42,,,,,5559232,,1,77,python|machine-learning|keras,116709,315.068,,5,save final model use kera use kerasclassifier train classifier code save final model future prediction usually use code save model know insert save model code kerasclassifier code thank
511,511,34597316,Why input is scaled in tf.nn.dropout in tensorflow?,"<p>I can't understand why dropout works like this in tensorflow. The blog of <a href=""http://cs231n.github.io/neural-networks-2/"">CS231n</a> says that, <code>""dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise.""</code> Also you can see this from picture(Taken from the same site)
<a href=""https://i.stack.imgur.com/SbXq1.jpg""><img src=""https://i.stack.imgur.com/SbXq1.jpg"" alt=""enter image description here""></a></p>

<p>From tensorflow site, <code>With probability keep_prob, outputs the input element scaled up by 1 / keep_prob, otherwise outputs 0.</code></p>

<p>Now, why the input element is scaled up by <code>1/keep_prob</code>? Why not keep the input element as it is with probability and not scale it with <code>1/keep_prob</code>? </p>",34597667,4,1,,2016/1/4 18:17,15,2019/10/15 8:05,2016/1/5 1:40,,2838606,,4341948,,1,41,machine-learning|neural-network|deep-learning|tensorflow,14077,106.394,,3,input scale tf nn dropout tensorflow understand dropout work like tensorflow blog c n say also see picture take site tensorflow site input element scale keep input element probability scale
773,773,51804692,What exactly is the definition of a 'Module' in PyTorch?,"<p>Please excuse the novice question, but is <code>Module</code> just the same as saying <code>model</code>?</p>

<p>That's what it sounds like, when the documentation says:</p>

<blockquote>
  <p>Whenever you want a model more complex than a simple sequence of existing Modules you will need to define your model (as a custom <code>Module</code> subclass).</p>
</blockquote>

<p>Or... when they mention <code>Module</code>, are they referring to something more formal and computer-sciency, like a protocol / interface type thing?</p>",51808347,4,1,,2018/8/12 0:05,5,2019/12/26 18:53,2018/8/13 9:14,,624547,,5225453,,1,25,python|class|pytorch,10114,64.0197,,3,exactly definition module pytorch please excuse novice question say sound like documentation say whenever want model complex simple sequence exist module need define model custom subclass mention refer something formal computer sciency like protocol interface type thing
745,745,41061457,keras: how to save the training history attribute of the history object,"<p>In Keras, we can return the output of <code>model.fit</code> to a history as follows:</p>

<pre><code> history = model.fit(X_train, y_train, 
                     batch_size=batch_size, 
                     nb_epoch=nb_epoch,
                     validation_data=(X_test, y_test))
</code></pre>

<p>Now, how to save the history attribute of the history object to a file for further uses (e.g. draw plots of acc or loss against epochs)?</p>",44674337,9,2,,2016/12/9 13:20,22,2021/7/23 12:24,2019/10/18 23:53,,3731622,,6807211,,1,84,python|machine-learning|neural-network|deep-learning|keras,72398,353.639,,5,keras save training history attribute history object kera return output history follow save history attribute history object file us e g draw plot acc loss epoch
312,312,58151507,"Why Pytorch officially use mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] to normalize images?","<p>In this page (<a href=""https://pytorch.org/vision/stable/models.html"" rel=""nofollow noreferrer"">https://pytorch.org/vision/stable/models.html</a>), it says that &quot;All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using <code>mean = [0.485, 0.456, 0.406]</code> and <code>std = [0.229, 0.224, 0.225]</code>&quot;.</p>
<p>Shouldn't the usual <code>mean</code> and <code>std</code> of normalization be <code>[0.5, 0.5, 0.5]</code> and <code>[0.5, 0.5, 0.5]</code>? Why is it setting such strange values?</p>",58151903,3,1,,2019/9/29 1:31,12,2021/6/22 13:35,2021/6/22 7:42,,5524761,,9066532,,1,29,python|pytorch|normalize,18120,77.4326,,3,pytorch officially use mean std normalize image page say pre train model expect input image normalize way e mini batch channel rgb image shape x h x w h w expect least image load range normalize use usual normalization set strange value
543,543,36740533,What are forward and backward passes in neural networks?,"<p>What is the meaning of <em>forward pass</em> and <em>backward pass</em> in neural networks?</p>

<p>Everybody is mentioning these expressions when talking about backpropagation and epochs. </p>

<p>I understood that forward pass and backward pass together form an epoch.</p>",48319902,1,1,,2016/4/20 10:10,7,2021/3/29 10:28,2017/10/17 19:33,,3924118,,5343790,,1,30,neural-network|backpropagation|conv-neural-network,34935,67.173,,0,forward backward pass neural network meaning forward pas backward pas neural network everybody mention expression talk backpropagation epoch understand forward pas backward pas together form epoch
245,245,44503603,Tensorflow on windows - ImportError: DLL load failed: The specified module could not be found,"<p>I'm using Anaconda 3.1.0 on Windows 7 64 bit. I have installed tensorflow(GPU). I am getting errors while running following command.</p>

<p><code>&gt;&gt;&gt; import tensorflow as tf</code></p>

<p><strong><em>The complete traceback</em></strong></p>

<pre><code>    (tensorflow) C:\windows\system32&gt;python
Python 3.5.3 |Continuum Analytics, Inc.| (default, May 15 2017, 10:43:23) [MSC v
.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in s
wig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 919, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;
module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in s
wig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\__init__.py"", line 24, in &lt;module&gt;
    from tensorflow.python import *
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\__init__.py"", line 51, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in s
wig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 919, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;
module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in s
wig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\mbharsakale\AppData\Local\Continuum\Anaconda2\envs\tensorflow\l
ib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</code></pre>",45215096,8,6,,2017/6/12 15:46,4,2021/5/9 5:21,,,,,5713713,,1,14,python|tensorflow|deep-learning|anaconda|windows-7-x64,26828,67.1144,,1,tensorflow window importerror dll load fail specify module could find use anaconda window bit instal tensorflow gpu get error run follow command complete traceback
129,129,42177658,How to switch Backend with Keras (from TensorFlow to Theano),"<p>I tried to switch Backend with Keras (from TensorFlow to Theano) but did not manage.
I followed the temps described <a href=""https://keras.io/backend/"" rel=""noreferrer"">here</a> but it doesn't work. I created a keras.json in the keras' directory (as it did not exist) but it doesn't change anything when I import it from Python.</p>",42177775,8,0,,2017/2/11 15:32,18,2018/2/4 10:26,2017/8/23 10:31,,5954249,,1426385,,1,55,backend|theano|keras,79814,237.608,,1,switch backend kera tensorflow theano try switch backend kera tensorflow theano manage follow temp describe work create keras json keras directory exist change anything import python
476,476,59461811,Random Choice with Pytorch?,"<p>I have a tensor of pictures, and would like to randomly select from it. I'm looking for the equivalent of <code>np.random.choice()</code>. </p>

<pre><code>import torch

pictures = torch.randint(0, 256, (1000, 28, 28, 3))
</code></pre>

<p>Let's say I want 10 of these pictures.</p>",59461812,5,0,,2019/12/23 22:14,2,2021/6/10 12:15,2020/7/10 22:10,,10908375,,10908375,,1,15,python|python-3.x|numpy|machine-learning|pytorch,14236,67.6135,,3,random choice pytorch tensor picture would like randomly select look equivalent let say want picture
484,484,59868132,What does backbone mean in a neural network?,"<p>I am getting confused with the meaning of &quot;backbone&quot; in neural networks, especially in <a href=""https://arxiv.org/abs/1802.02611"" rel=""noreferrer"">the DeepLabv3+ paper</a>. I did some research and found out that backbone could mean</p>
<blockquote>
<p>the feature extraction part of a network</p>
</blockquote>
<p>DeepLabv3+ took <a href=""https://arxiv.org/abs/1610.02357"" rel=""noreferrer"">Xception</a> and ResNet-101 as its backbone. However, I am not familiar with the entire structure of DeepLabv3+, which part the backbone refers to, and which parts remain the same?</p>
<p>A generalized description or definition of backbone would also be appreciated.</p>",60400388,4,1,,2020/1/22 20:57,3,2021/6/23 10:06,2021/2/9 14:24,,123695,,11558143,,1,18,deep-learning|neural-network|deeplab,14970,77.1009,,0,backbone mean neural network get confuse meaning backbone neural network especially deeplabv paper research find backbone could mean feature extraction part network deeplabv take xception resnet backbone however familiar entire structure deeplabv part backbone refers part remain generalized description definition backbone would also appreciate
18,18,53841509,How does adaptive pooling in pytorch work?,"<p>Adaptive pooling is a great function, but how does it work?  It seems to be inserting pads or shrinking/expanding kernel sizes in what seems like a pattered but fairly arbitrary way.  The pytorch documentation I can find is not more descriptive than ""put desired output size here.""  Does anyone know how this works or can point to where it's explained?</p>

<p>Some test code on a 1x1x6 tensor, (1,2,3,4,5,6), with an adaptive output of size 8:</p>

<pre><code>import torch
import torch.nn as nn

class TestNet(nn.Module):
    def __init__(self):
        super(TestNet, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool1d(8)

    def forward(self,x):
        print(x)
        x = self.avgpool(x)
        print(x)
        return x

def test():
    x = torch.Tensor([[[1,2,3,4,5,6]]])
    net = TestNet()
    y = net(x)
    return y

test()
</code></pre>

<p>Output:</p>

<pre><code>tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.]]])
tensor([[[ 1.0000,  1.5000,  2.5000,  3.0000,  4.0000,  4.5000,  5.5000,
       6.0000]]])
</code></pre>

<p>If it mirror pads by on the left and right (operating on (1,1,2,3,4,5,6,6)), and has a kernel of 2, then the outputs for all positions except for 4 and 5 make sense, except of course the output isn't the right size.  Is it also padding the 3 and 4 internally?  If so, it's operating on (1,1,2,3,3,4,4,5,6,6), which, if using a size 2 kernel, produces the wrong output size and would also miss a 3.5 output.  Is it changing the size of the kernel?</p>

<p>Am I missing something obvious about the way this works?</p>",,2,0,,2018/12/18 21:42,10,2021/3/15 7:19,2021/3/15 7:19,,7579547,,8189391,,1,36,python|pytorch,20898,78.6804,,3,adaptive pooling pytorch work adaptive pooling great function work seem insert pad shrink expand kernel size seem like pattered fairly arbitrary way pytorch documentation find descriptive put desired output size anyone know work point explain test code x x tensor adaptive output size output mirror pad left right operating kernel output position except make sense except course output right size also pad internally operate use size kernel produce wrong output size would also miss output change size kernel miss something obvious way work
286,286,45022734,Understanding a simple LSTM pytorch,"<pre><code>import torch,ipdb
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)
input = Variable(torch.randn(5, 3, 10))
h0 = Variable(torch.randn(2, 3, 20))
c0 = Variable(torch.randn(2, 3, 20))
output, hn = rnn(input, (h0, c0))
</code></pre>

<p>This is the LSTM example from the docs. I don't know understand the following things:</p>

<ol>
<li>What is output-size and why is it not specified anywhere?</li>
<li>Why does the input have 3 dimensions. What does 5 and 3 represent?</li>
<li>What are 2 and 3 in h0 and c0, what do those represent?</li>
</ol>

<p>Edit:</p>

<pre><code>import torch,ipdb
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
import torch.nn.functional as F

num_layers=3
num_hyperparams=4
batch = 1
hidden_size = 20
rnn = nn.LSTM(input_size=num_hyperparams, hidden_size=hidden_size, num_layers=num_layers)

input = Variable(torch.randn(1, batch, num_hyperparams)) # (seq_len, batch, input_size)
h0 = Variable(torch.randn(num_layers, batch, hidden_size)) # (num_layers, batch, hidden_size)
c0 = Variable(torch.randn(num_layers, batch, hidden_size))
output, hn = rnn(input, (h0, c0))
affine1 = nn.Linear(hidden_size, num_hyperparams)

ipdb.set_trace()
print output.size()
print h0.size()
</code></pre>

<blockquote>
  <p>*** RuntimeError: matrices expected, got 3D, 2D tensors at</p>
</blockquote>",45023288,3,0,,2017/7/10 22:41,21,2018/12/20 14:54,2018/7/7 19:50,,3646408,,3646408,,1,35,neural-network|lstm|pytorch|rnn,30595,105.943,,3,understand simple lstm pytorch lstm example doc know understand following thing output size specify anywhere input dimension represent h c represent edit runtimeerror matrix expect get tensor
98,98,41455101,What is the meaning of the word logits in TensorFlow?,"<p>In the following TensorFlow function, we must feed the activation of artificial neurons in the final layer. That I understand. But I don't understand why it is called logits? Isn't that a mathematical function? </p>

<pre><code>loss_function = tf.nn.softmax_cross_entropy_with_logits(
     logits = last_layer,
     labels = target_output
)
</code></pre>",,11,1,,2017/1/4 2:02,140,2020/8/19 14:04,2019/4/30 1:52,,2956066,,6773030,,1,324,tensorflow|machine-learning|neural-network|deep-learning|cross-entropy,157613,1447.59,2020/8/19 18:45,3,meaning word logits tensorflow following tensorflow function must fee activation artificial neuron final layer understand understand call logits mathematical function
466,466,26497564,Theano HiddenLayer Activation Function,"<p>Is there anyway to use Rectified Linear Unit (ReLU) as the activation function of the hidden layer instead of <code>tanh()</code> or <code>sigmoid()</code> in Theano? The implementation of the hidden layer is as follows and as far as I have searched on the internet ReLU is not implemented inside the Theano.</p>

<pre><code>class HiddenLayer(object):
  def __init__(self, rng, input, n_in, n_out, W=None, b=None, activation=T.tanh):
    pass
</code></pre>",26498509,5,0,,2014/10/21 22:38,1,2017/5/3 21:20,2016/1/9 22:22,,2838606,,3681744,,1,11,python|machine-learning|neural-network|theano,8985,53.8141,,3,theano hiddenlayer activation function anyway use rectified linear unit relu activation function hidden layer instead theano implementation hidden layer follow far search internet relu implement inside theano
425,425,47765595,Tensorflow: Attempting to use uninitialized value beta1_power,"<p>I got the following error when I try to run the code at the end of the post. But it is not clear to me what is wrong with my code. Could anybody let me know the tricks in debugging a tensorflow program?</p>

<pre><code>$ ./main.py 
Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
2017-12-11 22:53:16.061163: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Traceback (most recent call last):
  File ""./main.py"", line 55, in &lt;module&gt;
    sess.run(opt, feed_dict={x: batch_x, y: batch_y})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value beta1_power
     [[Node: beta1_power/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](beta1_power)]]

Caused by op u'beta1_power/read', defined at:
  File ""./main.py"", line 46, in &lt;module&gt;
    opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 353, in minimize
    name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py"", line 474, in apply_gradients
    self._create_slots([_get_variable_for(v) for v in var_list])
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/training/adam.py"", line 130, in _create_slots
    trainable=False)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1927, in variable
    caching_device=caching_device, name=name, dtype=dtype)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 213, in __init__
    constraint=constraint)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 356, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 125, in identity
    return gen_array_ops.identity(input, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 2071, in identity
    ""Identity"", input=input, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta1_power
     [[Node: beta1_power/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](beta1_power)]]
</code></pre>

<p>The code is here. It uses LSTM.</p>

<pre><code>#!/usr/bin/env python
# vim: set noexpandtab tabstop=2 shiftwidth=2 softtabstop=-1 fileencoding=utf-8:

import tensorflow as tf
from tensorflow.contrib import rnn

#import mnist dataset
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets(""/tmp/data/"", one_hot=True)

learning_rate=0.001

#defining placeholders
#input image placeholder
time_steps=28
n_input=28
x=tf.placeholder(""float"", [None, time_steps, n_input])

#processing the input tensor from [batch_size,n_steps,n_input] to ""time_steps"" number of [batch_size,n_input] tensors
input=tf.unstack(x, time_steps, 1)

#defining the network
num_units=128
lstm_layer = rnn.BasicLSTMCell(num_units, forget_bias=1)
outputs,_ = rnn.static_rnn(lstm_layer, input, dtype=""float32"")

#weights and biases of appropriate shape to accomplish above task
n_classes=10
out_weights=tf.Variable(tf.random_normal([num_units, n_classes]))
out_bias=tf.Variable(tf.random_normal([n_classes]))

#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication
prediction=tf.matmul(outputs[-1], out_weights) + out_bias

y=tf.placeholder(""float"", [None, n_classes])
loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))
#optimization

#model evaluation
correct_prediction=tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))
accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#initialize variables
init=tf.global_variables_initializer()
batch_size=128
opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
with tf.Session() as sess:
    sess.run(init)
    iter=1
    while iter&lt;800:
        batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)

        batch_x=batch_x.reshape((batch_size, time_steps, n_input))

        sess.run(opt, feed_dict={x: batch_x, y: batch_y})

        if iter %10==0:
            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})
            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})
            print(""For iter "",iter)
            print(""Accuracy "",acc)
            print(""Loss "",los)
            print(""__________________"")

        iter=iter+1


#calculating test accuracy
test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))
test_label = mnist.test.labels[:128]
print(""Testing Accuracy:"", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))
</code></pre>",47780342,1,1,,2017/12/12 4:58,6,2019/6/1 20:18,2017/12/12 20:10,,712995,,1424739,,1,19,python|machine-learning|tensorflow|lstm|recurrent-neural-network,18433,70.8624,,4,tensorflow attempt use uninitialized value beta power get following error try run code end post clear wrong code could anybody let know trick debug tensorflow program code use lstm
330,330,63390725,Should the custom loss function in Keras return a single loss value for the batch or an arrary of losses for every sample in the training batch?,"<p>I'm learning keras API in tensorflow(2.3). In this <a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses"" rel=""noreferrer"">guide</a> on tensorflow website, I found an example of custom loss funciton:</p>
<pre><code>    def custom_mean_squared_error(y_true, y_pred):
        return tf.math.reduce_mean(tf.square(y_true - y_pred))
</code></pre>
<p>The <code>reduce_mean</code> function in this custom loss function will return an scalar.</p>
<p>Is it right to define loss function like this? As far as I know, the first dimension of the shapes of <code>y_true</code> and <code>y_pred</code> is the batch size. I think the loss function should return loss values for every sample in the batch. So the loss function shoud give an array of shape <code>(batch_size,)</code>. But the above function gives a single value for the whole batch.</p>
<p>Maybe the above example is wrong? Could anyone give me some help on this problem?</p>
<hr />
<p>p.s. <strong>Why do I think the loss function should return an array rather than a single value?</strong></p>
<p>I read the source code of <a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L159-L2634"" rel=""noreferrer"">Model</a> class. When you provide a loss function (please note it's a <strong>function</strong>, not a loss <strong>class</strong>) to <code>Model.compile()</code> method, ths loss function is used to construct a <code>LossesContainer</code> object, which is stored in <code>Model.compiled_loss</code>. This loss function passed to the constructor of <code>LossesContainer</code> class is used once again to construct a <code>LossFunctionWrapper</code> object, which is stored in <code>LossesContainer._losses</code>.</p>
<p><strong>According to the source code of <a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/losses.py"" rel=""noreferrer"">LossFunctionWrapper</a> class, the overall loss value for a training batch is calculated by the <code>LossFunctionWrapper.__call__()</code> method (inherited from <code>Loss</code> class), i.e. it returns a single loss value for the whole batch.</strong> But the <code>LossFunctionWrapper.__call__()</code> first calls the <code>LossFunctionWrapper.call()</code> method to obtain an array of losses for every sample in the training batch. Then these losses are fianlly averaged to get the single loss value for the whole batch. It's in the <code>LossFunctionWrapper.call()</code> method that the loss function provided to the <code>Model.compile()</code> method is called.</p>
<p>That's why I think the custom loss funciton should return an array of losses, insead of a single scalar value. Besides, if we write a custom <code>Loss</code> class for the <code>Model.compile()</code> method, the <code>call()</code> method of our custom <code>Loss</code> class should also return an array, rather than a signal value.</p>
<hr />
<p>I opened an <a href=""https://github.com/tensorflow/tensorflow/issues/42446"" rel=""noreferrer"">issue</a> on github. It's confirmed that custom loss function is required to return one loss value per sample. The example will need to be updated to reflect this.</p>",63481422,6,0,,2020/8/13 8:04,1,2020/12/3 14:47,2020/8/19 7:04,,2099607,,4151926,,1,14,tensorflow|machine-learning|keras|tensorflow2.0|loss-function,3444,50.9483,,4,custom loss function kera return single loss value batch arrary loss every sample training batch learn kera api tensorflow guide tensorflow website find example custom loss funciton function custom loss function return scalar right define loss function like far know first dimension shape batch size think loss function return loss value every sample batch loss function shoud give array shape function give single value whole batch maybe example wrong could anyone give help problem p think loss function return array rather single value read source code model class provide loss function please note function loss class method ths loss function use construct object store loss function pass constructor class use construct object store accord source code lossfunctionwrapper class overall loss value training batch calculate method inherit class e return single loss value whole batch first call method obtain array loss every sample training batch loss fianlly average get single loss value whole batch method loss function provide model compile method call think custom loss funciton return array loss insead single scalar value besides write custom loss class model compile method call method custom loss class also return array rather signal value open issue github confirm custom loss function require return one loss value per sample example need update reflect
782,782,52074153,Cannot convert list to array: ValueError: only one element tensors can be converted to Python scalars,"<p>I'm currently working with the PyTorch framework and trying to understand foreign code. I got an indices issue and wanted to print the shape of a list.<br>
The only way of doing so (as far as Google tells me) is to convert the list into a numpy array and then getting the shape with numpy.ndarray.shape().  </p>

<p>But trying to convert my list into an array, I got a <code>ValueError: only one element tensors can be converted to Python scalars</code>.  </p>

<p>My List is a converted PyTorch Tensor (<code>list(pytorchTensor)</code>) and looks somewhat like this:  </p>

<blockquote>
  <p>[tensor([[-0.2781, -0.2567, -0.2353,  ..., -0.9640, -0.9855, -1.0069],<br>
          [-0.2781, -0.2567, -0.2353,  ..., -1.0069, -1.0283, -1.0927],<br>
          [-0.2567, -0.2567, -0.2138,  ..., -1.0712, -1.1141, -1.1784],<br>
          ...,<br>
          [-0.6640, -0.6425, -0.6211,  ..., -1.0712, -1.1141, -1.0927],<br>
          [-0.6640, -0.6425, -0.5997,  ..., -0.9426, -0.9640, -0.9640],<br>
          [-0.6640, -0.6425, -0.5997,  ..., -0.9640, -0.9426, -0.9426]]),   tensor([[-0.0769, -0.0980, -0.076                          9,  ..., -0.9388, -0.9598, -0.9808],<br>
          [-0.0559, -0.0769, -0.0980,  ..., -0.9598, -1.0018, -1.0228],<br>
          [-0.0559, -0.0769, -0.0769,  ..., -1.0228, -1.0439, -1.0859],<br>
          ...,<br>
          [-0.4973, -0.4973, -0.4973,  ..., -1.0018, -1.0439, -1.0228],<br>
          [-0.4973, -0.4973, -0.4973,  ..., -0.8757, -0.9177, -0.9177],<br>
          [-0.4973, -0.4973, -0.4973,  ..., -0.9177, -0.8967, -0.8967]]),   tensor([[-0.1313, -0.1313, -0.110                          0,  ..., -0.8115, -0.8328, -0.8753],<br>
          [-0.1313, -0.1525, -0.1313,  ..., -0.8541, -0.8966, -0.9391],<br>
          [-0.1100, -0.1313, -0.1100,  ..., -0.9391, -0.9816, -1.0666],<br>
          ...,<br>
          [-0.4502, -0.4714, -0.4502,  ..., -0.8966, -0.8966, -0.8966],<br>
          [-0.4502, -0.4714, -0.4502,  ..., -0.8115, -0.8115, -0.7903],<br>
          [-0.4502, -0.4714, -0.4502,  ..., -0.8115, -0.7690, -0.7690]])] </p>
</blockquote>

<p>Is there a way of getting the shape of that list without converting it into a numpy array?</p>",52074876,3,2,,2018/8/29 9:33,2,2021/6/11 17:50,2019/7/10 10:05,,10284295,,10284295,,1,20,python|numpy|pytorch|numpy-ndarray,62521,52.1841,,3,convert list array valueerror one element tensor convert python scalar currently work pytorch framework try understand foreign code get index issue want print shape list way far google tell convert list numpy array get shape numpy ndarray shape try convert list array get list converted pytorch tensor look somewhat like tensor tensor tensor way get shape list without convert numpy array
429,429,47826730,how to save resized images using ImageDataGenerator and flow_from_directory in keras,"<p>I am resizing my RGB images stored in a folder(two classes) using following code:</p>

<pre><code>from keras.preprocessing.image import ImageDataGenerator
dataset=ImageDataGenerator()
dataset.flow_from_directory('/home/1',target_size=(50,50),save_to_dir='/home/resized',class_mode='binary',save_prefix='N',save_format='jpeg',batch_size=10)
</code></pre>

<p>My data tree is like following:</p>

<pre><code>1/
 1_1/
     img1.jpg
     img2.jpg
     ........
 1_2/
     IMG1.jpg
     IMG2.jpg
     ........
resized/
        1_1/ (here i want to save resized images of 1_1)
        2_2/ (here i want to save resized images of 1_2)
</code></pre>

<p>After running the code i am getting following output but not images:</p>

<pre><code>Found 271 images belonging to 2 classes.
Out[12]: &lt;keras.preprocessing.image.DirectoryIterator at 0x7f22a3569400&gt;
</code></pre>

<p>How to save images?</p>",53124455,6,0,,2017/12/15 6:09,3,2021/6/12 18:25,2017/12/15 6:51,,8293726,,8293726,,1,11,keras|keras-2,16384,58.0577,,2,save resize image use imagedatagenerator flow directory kera resize rgb image store folder two class use follow code data tree like follow run code get follow output image save image
725,725,40393629,How to pass a parameter to Scikit-Learn Keras model function,"<p>I have the following code, using <a href=""https://github.com/fchollet/keras/blob/master/keras/wrappers/scikit_learn.py"" rel=""noreferrer"">Keras Scikit-Learn Wrapper</a>, which work fine:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
from sklearn import datasets
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
import numpy as np


def create_model():
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=4, init='uniform', activation='relu'))
    model.add(Dense(6, init='uniform', activation='relu'))
    model.add(Dense(1, init='uniform', activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


def main():
    """"""
    Description of main
    """"""


    iris = datasets.load_iris()
    X, y = iris.data, iris.target

    NOF_ROW, NOF_COL =  X.shape

    # evaluate using 10-fold cross validation
    seed = 7
    np.random.seed(seed)
    model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10, verbose=0)
    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)
    results = cross_val_score(model, X, y, cv=kfold)

    print(results.mean())
    # 0.666666666667


if __name__ == '__main__':
    main()
</code></pre>

<p>The <code>pima-indians-diabetes.data</code>  can be downloaded <strong><a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"" rel=""noreferrer"">here</a></strong>.</p>

<p>Now what I want to do is to pass a value <code>NOF_COL</code> into a parameter of <code>create_model()</code> function the following way</p>

<pre><code>model = KerasClassifier(build_fn=create_model(input_dim=NOF_COL), nb_epoch=150, batch_size=10, verbose=0)
</code></pre>

<p>With the <code>create_model()</code> function that looks like this:</p>

<pre><code>def create_model(input_dim=None):
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=input_dim, init='uniform', activation='relu'))
    model.add(Dense(6, init='uniform', activation='relu'))
    model.add(Dense(1, init='uniform', activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
</code></pre>

<p>But it fails giving this error:</p>

<pre><code>TypeError: __call__() takes at least 2 arguments (1 given)
</code></pre>

<p>What's the right way to do it?</p>",40455745,3,0,,2016/11/3 3:57,3,2021/5/3 14:28,2016/11/3 7:47,,67405,,67405,,1,21,python|function|scikit-learn|keras|callable,9834,58.5709,,3,pass parameter scikit learn kera model function following code use kera scikit learn wrapper work fine download want pass value parameter function following way function look like fail give error right way
387,387,46560313,Is there an easy way to get something like Keras model.summary in Tensorflow?,"<p>I have been working with Keras and really liked the <code>model.summary()</code>
It gives a good overview of the size of the different layers and especially an overview of the number of parameters the model has.</p>

<p>Is there a similar function in Tensorflow? I could find nothing on Stackoverflow or the Tensorflow API documentation.</p>",46580957,3,0,,2017/10/4 8:27,11,2020/1/4 10:48,,,,,8258911,,1,34,tensorflow|keras,31404,92.3879,,5,easy way get something like kera model summary tensorflow work kera really like give good overview size different layer especially overview number parameter model similar function tensorflow could find nothing stackoverflow tensorflow api documentation
320,320,58498651,What is FLOPS in field of deep learning?,"<p>What is FLOPS in field of deep learning? Why we don't use the term just FLO?</p>

<p>We use the term FLOPS to measure the number of operations of a frozen deep learning network.</p>

<p>Following Wikipedia, FLOPS = floating point operations per second. When we test computing units, we should consider of the time. But in case of measuring deep learning network, how can I understand this concept of time? Shouldn't we use the term just FLO(floating point operations)?</p>

<p>Why do people use the term FLOPS? If there is anything I don't know, what is it?</p>

<p>==== attachment ===</p>

<p>Frozen deep learning networks that I mentioned is just a kind of software. It's not about hardware. In the field of deep learning, people use the term FLOPS to measure how many operations are needed to run the network model. In this case, in my opinion, we should use the term FLO. I thought people confused about the term FLOPS and I want to know if others think the same or if I'm wrong.</p>

<p>Please look at these cases:</p>

<p><a href=""https://stackoverflow.com/questions/43490555/how-to-calculate-a-nets-flops-in-cnn"">how to calculate a net&#39;s FLOPs in CNN</a></p>

<p><a href=""https://iq.opengenus.org/floating-point-operations-per-second-flops-of-machine-learning-models/"" rel=""noreferrer"">https://iq.opengenus.org/floating-point-operations-per-second-flops-of-machine-learning-models/</a></p>",60275432,3,0,,2019/10/22 6:57,3,2020/5/26 18:29,2019/10/24 2:17,,10258755,,10258755,,1,17,performance|deep-learning|flops,13982,68.7823,,0,flop field deep learning flop field deep learn use term flo use term flop measure number operation frozen deep learning network follow wikipedia flop float point operation per second test compute unit consider time case measure deep learning network understand concept time use term flo float point operation people use term flop anything know attachment frozen deep learning network mention kind software hardware field deep learning people use term flop measure many operation need run network model case opinion use term flo think people confuse term flop want know others think wrong please look case calculate net flop cnn
717,717,39930952,Cannot import keras after installation,"<p>I'm trying to setup <code>keras</code> deep learning library for <code>Python3.5</code> on Ubuntu 16.04 LTS and use <code>Tensorflow</code> as a backend. I have <code>Python2.7</code> and <code>Python3.5</code> installed. I have installed <code>Anaconda</code> and with help of it  <code>Tensorflow</code>, <code>numpy</code>, <code>scipy</code>, <code>pyyaml</code>. Afterwards I have installed <code>keras</code> with command</p>

<blockquote>
  <p>sudo python setup.py install</p>
</blockquote>

<p>Although I can see <code>/usr/local/lib/python3.5/dist-packages/Keras-1.1.0-py3.5.egg</code> directory, I cannot use <code>keras</code> library. When I try to import it in python it says</p>

<blockquote>
  <p>ImportError: No module named 'keras'</p>
</blockquote>

<p>I have tried to install <code>keras</code> using<code>pip3</code>, but got the same result. </p>

<p>What am I doing wrong? Any Ideas?</p>",39931939,4,0,,2016/10/8 9:46,7,2019/4/29 9:22,,,,,1928515,,1,28,python|ubuntu|tensorflow|anaconda|keras,127504,84.8221,,1,import kera installation try setup deep learning library ubuntu lts use backend instal instal help afterwards instal command sudo python setup py install although see directory use library try import python say importerror module name kera try install use get result wrong idea
123,123,41948406,Why is my GPU slower than CPU when training LSTM/RNN models?,"<p>My machine has the following spec: </p>

<p>CPU: Xeon E5-1620 v4</p>

<p>GPU: Titan X (Pascal) </p>

<p>Ubuntu 16.04</p>

<p>Nvidia driver 375.26</p>

<p>CUDA tookit 8.0</p>

<p>cuDNN 5.1</p>

<p>I've benchmarked on the following Keras examples with Tensorflow as the backed <a href=""https://github.com/fchollet/keras/tree/master/examples"" rel=""noreferrer"">reference</a>: </p>

<pre><code>SCRIPT NAME                  GPU       CPU
stated_lstm.py               5sec      5sec 
babi_rnn.py                  10sec     12sec
imdb_bidirectional_lstm.py   240sec    116sec
imbd_lstm.py                 113sec    106sec
</code></pre>

<p>My gpu is clearly out performing my cpu in non-lstm models. </p>

<pre><code>SCRIPT NAME                  GPU       CPU
cifar10_cnn.py               12sec     123sec
imdb_cnn.py                  5sec      119sec
mnist_cnn.py                 3sec      47sec 
</code></pre>

<p>Has anyone else experienced this? </p>",,4,0,,2017/1/31 1:37,8,2020/8/18 5:39,2017/1/31 17:35,,4926741,,4926741,,1,34,machine-learning|tensorflow|nvidia|keras,29395,118.073,,4,gpu slow cpu training lstm rnn model machine following spec cpu xeon e v gpu titan x pascal ubuntu nvidia driver cuda tookit cudnn benchmarked following kera example tensorflow backed reference gpu clearly perform cpu non lstm model anyone else experience
107,107,41651628,Negative dimension size caused by subtracting 3 from 1 for 'Conv2D',"<p>I'm using <a href=""https://keras.io/"" rel=""noreferrer"">Keras</a> with <a href=""https://www.tensorflow.org/"" rel=""noreferrer"">Tensorflow</a> as backend , here is my code:</p>

<pre><code>import numpy as np
np.random.seed(1373) 
import tensorflow as tf
tf.python.control_flow_ops = tf

import os
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.utils import np_utils

batch_size = 128
nb_classes = 10
nb_epoch = 12


img_rows, img_cols = 28, 28

nb_filters = 32

nb_pool = 2

nb_conv = 3


(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape[0])

X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)
X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)


X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255


print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')


Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

model = Sequential()

model.add(Convolution2D(nb_filters, nb_conv, nb_conv,
border_mode='valid',
input_shape=(1, img_rows, img_cols)))
model.add(Activation('relu'))
model.add(Convolution2D(nb_filters, nb_conv, nb_conv))
model.add(Activation('relu'))

model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes)) 
model.add(Activation('softmax')) 

model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=[""accuracy""])


model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
verbose=1, validation_data=(X_test, Y_test))

score = model.evaluate(X_test, Y_test, verbose=0)

print('Test score:', score[0])
print('Test accuracy:', score[1])
</code></pre>

<p>and Trackback error:</p>

<pre><code>Using TensorFlow backend.
60000
('X_train shape:', (60000, 1, 28, 28))
(60000, 'train samples')
(10000, 'test samples')
Traceback (most recent call last):
  File ""mnist.py"", line 154, in &lt;module&gt;
    input_shape=(1, img_rows, img_cols)))
  File ""/usr/local/lib/python2.7/dist-packages/keras/models.py"", line 276, in add
    layer.create_input_layer(batch_input_shape, input_dtype)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 370, in create_input_layer
    self(x)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 514, in __call__
    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 572, in add_inbound_node
    Node.create_node(self, inbound_layers, node_indices, tensor_indices)
  File ""/usr/local/lib/python2.7/dist-packages/keras/engine/topology.py"", line 149, in create_node
    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
  File ""/usr/local/lib/python2.7/dist-packages/keras/layers/convolutional.py"", line 466, in call
    filter_shape=self.W_shape)
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 1579, in conv2d
    x = tf.nn.conv2d(x, kernel, strides, padding=padding)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 396, in conv2d
    data_format=data_format, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2242, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1568, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 610, in call_cpp_shape_fn
    debug_python_shape_fn, require_shape_fn)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 675, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Negative dimension size caused by subtracting 3 from 1 for 'Conv2D' (op: 'Conv2D') with input shapes: [?,1,28,28], [3,3,28,32].
</code></pre>

<p>First I saw some answers that problem is with <code>Tensorflow</code> version so I upgrade <code>Tensorflow</code> to <code>0.12.0</code>, but still exist , is that problem with network or I missing something, what should <code>input_shape</code> looks like?</p>

<p><strong>Update</strong>
Here is <code>./keras/keras.json</code>:</p>

<pre><code>{
    ""image_dim_ordering"": ""tf"", 
    ""epsilon"": 1e-07, 
    ""floatx"": ""float32"", 
    ""backend"": ""tensorflow""
}
</code></pre>",41742406,8,4,,2017/1/14 15:27,18,2021/5/24 7:48,2017/2/21 13:04,,2326911,,2326911,,1,66,python|tensorflow|keras,69085,289.958,,4,negative dimension size cause subtract conv use kera tensorflow backend code trackback error first saw answer problem version upgrade still exist problem network miss something look like update
718,718,40028175,How do you get the name of the tensorflow output nodes in a Keras Model?,"<p>I'm trying to create a pb file from my Keras (tensorflow backend) model so I can build it on iOS. I'm using freeze.py and I need to pass the output nodes. How do i get the names of the output nodes of my Keras model?</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py</a></p>",49154874,4,1,,2016/10/13 18:16,10,2019/7/1 12:55,,,,,4926741,,1,34,python|tensorflow|keras,38602,90.5464,,5,get name tensorflow output node keras model try create pb file kera tensorflow backend model build io use freeze py need pass output node get name output node kera model
367,367,46086030,How to check which version of Keras is installed?,"<p>Question is the same as the title says. </p>

<p>I prefer not to open Python and I use either MacOS or Ubuntu.</p>",46086039,5,0,,2017/9/7 0:14,9,2021/4/4 1:09,,,,,3907250,,1,85,keras,196763,324.176,,1,check version kera installed question title say prefer open python use either macos ubuntu
223,223,44151760,"Received a label value of 1 which is outside the valid range of [0, 1) - Python, Keras","<p>I am working on a simple cnn classifier using keras with tensorflow background.</p>

<pre><code>def cnnKeras(training_data, training_labels, test_data, test_labels, n_dim):
  print(""Initiating CNN"")
  seed = 8
  numpy.random.seed(seed)
  model = Sequential()
  model.add(Convolution2D(64, 1, 1, init='glorot_uniform', 
   border_mode='valid',input_shape=(16, 1, 1), activation='relu'))
  model.add(MaxPooling2D(pool_size=(1, 1)))
  model.add(Convolution2D(32, 1, 1, init='glorot_uniform', 
   activation='relu'))
  model.add(MaxPooling2D(pool_size=(1, 1)))
  model.add(Dropout(0.25))
  model.add(Flatten())
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(1, activation='softmax'))
  # Compile model
  model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam', metrics=['accuracy'])
  model.fit(training_data, training_labels, validation_data=(
    test_data, test_labels), nb_epoch=30, batch_size=8, verbose=2)

  scores = model.evaluate(test_data, test_labels, verbose=1)
  print(""Baseline Error: %.2f%%"" % (100 - scores[1] * 100))
  # model.save('trained_CNN.h5')
  return None
</code></pre>

<p>It is a binary classification problem, but I keep getting the message <code>Received a label value of 1 which is outside the valid range of [0, 1)</code> which does not make any sense to me. Any suggesstions?</p>",44152923,8,0,,2017/5/24 7:29,9,2021/8/25 5:05,2019/12/3 0:12,,7432133,,7432133,,1,43,python|machine-learning|keras,48964,186.56,,4,receive label value outside valid range python kera work simple cnn classifier use kera tensorflow background binary classification problem keep get message make sense suggesstions
734,734,40690203,How can I import the MNIST dataset that has been manually downloaded?,"<p>I have been experimenting with a Keras example, which needs to import MNIST data</p>

<pre><code>from keras.datasets import mnist
import numpy as np
(x_train, _), (x_test, _) = mnist.load_data()
</code></pre>

<p>It generates error messages such as <code>Exception: URL fetch failure on https://s3.amazonaws.com/img-datasets/mnist.pkl.gz: None -- [Errno 110] Connection timed out
</code></p>

<p>It should be related to the network environment I am using. <em>Is there any function or code that can let me directly import the MNIST data set that has been manually downloaded?</em></p>

<p>I tried the following approach </p>

<pre><code>import sys
import pickle
import gzip
f = gzip.open('/data/mnist.pkl.gz', 'rb')
  if sys.version_info &lt; (3,):
    data = pickle.load(f)
else:
    data = pickle.load(f, encoding='bytes')
f.close()
import numpy as np
(x_train, _), (x_test, _) = data
</code></pre>

<p>Then I get the following error message</p>

<pre><code>Traceback (most recent call last):
File ""test.py"", line 45, in &lt;module&gt;
(x_train, _), (x_test, _) = data
ValueError: too many values to unpack (expected 2)
</code></pre>",40693405,5,0,,2016/11/19 7:10,6,2021/5/21 18:49,2019/10/31 16:00,,3924118,,785099,,1,14,keras,29629,59.8869,,2,import mnist dataset manually download experiment kera example need import mnist data generate error message relate network environment use function code let directly import mnist data set manually download try following approach get following error message
737,737,40747679,keras: what is the difference between model.predict and model.predict_proba,"<p>I found <code>model.predict</code> and <code>model.predict_proba</code> both give an identical 2D matrix representing probabilities at each categories for each row. </p>

<p>What is the difference of the two functions?</p>",,4,0,,2016/11/22 17:06,6,2021/8/18 15:47,2020/6/18 17:40,,4685471,,6807211,,1,26,python|machine-learning|deep-learning|keras,42288,71.3049,,3,keras difference model predict model predict proba find give identical matrix represent probability category row difference two function
68,68,55681502,Label Smoothing in PyTorch,"<p>I'm building a <code>ResNet-18</code> classification model for the <strong>Stanford Cars</strong> dataset using transfer learning. I would like to implement <a href=""https://arxiv.org/pdf/1701.06548.pdf"" rel=""noreferrer"">label smoothing</a> to penalize overconfident predictions and improve generalization.</p>
<p><code>TensorFlow</code> has a simple keyword argument in <a href=""https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy"" rel=""noreferrer""><code>CrossEntropyLoss</code></a>. Has anyone built a similar function for <code>PyTorch</code> that I could plug-and-play with?</p>",66773267,5,0,,2019/4/15 1:14,7,2021/5/8 8:10,2021/4/2 21:29,,9215780,,2096883,,1,24,python|machine-learning|pytorch|transfer-learning,20370,74.236,,4,label smooth pytorch build classification model stanford car dataset use transfer learning would like implement label smooth penalize overconfident prediction improve generalization simple keyword argument anyone build similar function could plug play
579,579,49390842,Cross Entropy in PyTorch,"<p>I'm a bit confused by the cross entropy loss in PyTorch.</p>

<p>Considering this example:</p>



<pre class=""lang-python prettyprint-override""><code>import torch
import torch.nn as nn
from torch.autograd import Variable

output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)
target = Variable(torch.LongTensor([3]))

criterion = nn.CrossEntropyLoss()
loss = criterion(output, target)
print(loss)
</code></pre>

<p>I would expect the loss to be 0. But I get:</p>

<pre class=""lang-python prettyprint-override""><code>Variable containing:
 0.7437
[torch.FloatTensor of size 1]
</code></pre>

<p>As far as I know cross entropy can be calculated like this:</p>

<p><a href=""https://i.stack.imgur.com/W3xm0.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/W3xm0.gif"" alt=""enter image description here""></a></p>

<p>But shouldn't be the result then 1*log(1) = 0 ?</p>

<p>I tried different inputs like one-hot encodings, but this doesn't work at all, so it seems the input shape of the loss function is okay.</p>

<p>I would be really grateful if someone could help me out and tell me where my mistake is.</p>

<p>Thanks in advance!</p>",49839941,5,0,,2018/3/20 17:39,17,2021/5/9 13:09,2021/5/9 13:09,,9067615,,7483494,,1,45,python|machine-learning|pytorch|loss,52070,179.866,,4,cross entropy pytorch bit confuse cross entropy loss pytorch consider example would expect loss get far know cross entropy calculate like result log try different input like one hot encoding work seem input shape loss function okay would really grateful someone could help tell mistake thanks advance
693,693,38947948,How can I hot one encode in Matlab?,"<p>Often you are given a vector of integer values representing your labels (aka classes), for example</p>

<pre><code>[2; 1; 3; 3; 2]
</code></pre>

<p>and you would like to hot one encode this vector, such that each value is represented by a 1 in the column indicated by the value in each row of the labels vector, for example</p>

<pre><code>[0 1 0;
 1 0 0;
 0 0 1;
 0 0 1;
 0 1 0]
</code></pre>",,5,2,,2016/8/15 0:45,6,2017/3/28 20:50,,,,,3809616,,1,11,matlab|neural-network|octave|deep-learning,11239,58.2029,2017/4/2 17:46,3,hot one encode matlab often give vector integer value represent label aka class example would like hot one encode vector value represent column indicate value row label vector example
779,779,51995977,How can I use a pre-trained neural network with grayscale images?,"<p>I have a dataset containing grayscale images and I want to train a state-of-the-art CNN on them. I'd very much like to fine-tune a pre-trained model (like the ones <a href=""https://github.com/tensorflow/models/tree/master/research/slim#Pretrained"" rel=""noreferrer"">here</a>).</p>

<p>The problem is that almost all models I can find the weights for have been trained on the ImageNet dataset, which contains RGB images.</p>

<p>I can't use one of those models because their input layer expects a batch of shape <code>(batch_size, height, width, 3)</code> or <code>(64, 224, 224, 3)</code> in my case, but my images batches are <code>(64, 224, 224)</code>.</p>

<p>Is there any way that I can use one of those models? I've thought of dropping the input layer after I've loaded the weights and adding my own (like we do for the top layers). Is this approach correct?</p>",51996037,11,7,,2018/8/24 0:33,33,2020/11/22 0:41,,,,,10267239,,1,51,python|tensorflow|machine-learning|keras|deep-learning,49806,240.989,,3,use pre train neural network grayscale image dataset contain grayscale image want train state art cnn much like fine tune pre trained model like one problem almost model find weight train imagenet dataset contain rgb image use one model input layer expect batch shape case image batch way use one model think drop input layer load weight add like top layer approach correct
256,256,44647258,LSTM Autoencoder,"<p>I'm trying to build a LSTM autoencoder with the goal of getting a fixed sized vector from a sequence, which represents the sequence as good as possible. This autoencoder consists of two parts:</p>

<ul>
<li><code>LSTM</code> Encoder: Takes a sequence and returns an output vector (<code>return_sequences = False</code>)</li>
<li><code>LSTM</code> Decoder: Takes an output vector and returns a sequence (<code>return_sequences = True</code>)</li>
</ul>

<p>So, in the end, the encoder is a <strong>many to one</strong> LSTM and the decoder is a <strong>one to many</strong> LSTM.</p>

<p><a href=""https://i.stack.imgur.com/kwhAP.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kwhAP.jpg"" alt=""enter image description here""></a>
Image source: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">Andrej Karpathy</a></p>

<p>On a high level the coding looks like this (similar as described <a href=""https://github.com/fchollet/keras/issues/5138"" rel=""noreferrer"">here</a>):</p>

<pre><code>encoder = Model(...)
decoder = Model(...)

autoencoder = Model(encoder.inputs, decoder(encoder(encoder.inputs)))

autoencoder.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

autoencoder.fit(data, data,
          batch_size=100,
          epochs=1500)
</code></pre>

<p>The shape (number of training examples, sequence length, input dimension) of the <code>data</code> array is <code>(1200, 10, 5)</code> and looks like this:</p>

<pre><code>array([[[1, 0, 0, 0, 0],
        [0, 1, 0, 0, 0],
        [0, 0, 1, 0, 0],
        ..., 
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0]],
        ... ]
</code></pre>

<p><strong>Problem:</strong> I am not sure how to proceed, especially how to integrate <code>LSTM</code> to <code>Model</code> and how to get the decoder to generate a sequence from a vector.</p>

<p>I am using <code>keras</code> with <code>tensorflow</code> backend.</p>

<p><strong>EDIT:</strong> If someone wants to try out, here is my procedure to generate random sequences with moving ones (including padding):</p>

<pre><code>import random
import math

def getNotSoRandomList(x):
    rlen = 8
    rlist = [0 for x in range(rlen)]
    if x &lt;= 7:
        rlist[x] = 1
    return rlist


sequence = [[getNotSoRandomList(x) for x in range(round(random.uniform(0, 10)))] for y in range(5000)]

### Padding afterwards

from keras.preprocessing import sequence as seq

data = seq.pad_sequences(
    sequences = sequence,
    padding='post',
    maxlen=None,
    truncating='post',
    value=0.
)
</code></pre>",44658650,3,7,,2017/6/20 8:03,27,2020/1/3 9:58,2018/2/28 0:58,,951894,,2612484,,1,42,python|machine-learning|tensorflow|deep-learning|keras,24840,81.7806,,3,lstm autoencoder try build lstm autoencoder goal get fix size vector sequence represent sequence good possible autoencoder consist two part encoder take sequence return output vector decoder take output vector return sequence end encoder many one lstm decoder one many lstm image source andrej karpathy high level cod look like similar described shape number train example sequence length input dimension array look like problem sure proceed especially integrate get decoder generate sequence vector use backend edit someone want try procedure generate random sequence move one include pad
630,630,50999977,What does the gather function do in pytorch in layman terms?,"<p>I have been through the <a href=""https://pytorch.org/docs/stable/torch.html#torch.gather"" rel=""noreferrer"">official doc</a> and <a href=""https://stackoverflow.com/a/46802039/3564468"">this</a> but it is hard to understand what is going on.</p>

<p>I am trying to understand a <a href=""https://github.com/transedward/pytorch-dqn/blob/master/dqn_learn.py"" rel=""noreferrer"">DQN</a> source code and it uses the gather function on line 197.</p>

<p>Could someone explain in simple terms what the gather function does? What is the purpose of that function?</p>",51032153,4,2,,2018/6/23 9:54,40,2021/5/20 15:54,,,,,3564468,,1,85,pytorch,45141,423.618,,3,gather function pytorch layman term official doc hard understand go try understand dqn source code use gather function line could someone explain simple term gather function purpose function
566,566,48951136,Plot multiple graphs in one plot using Tensorboard,"<p>I am using Keras with Tensorflow backend. My work involves comparing the performances of several models such as Inception, VGG, Resnet etc on my dataset.
I would like to plot the training accuracies of several models in one graph. I am trying to do this in Tensorboard, but it is not working.</p>

<p>Is there a way of plotting multiple graphs in one plot using Tensorboard or is there some other way I can do this?</p>

<p>Thank you</p>",,3,1,,2018/2/23 15:26,5,2021/2/20 1:03,,,,,9402273,,1,26,tensorflow|keras|tensorboard,26507,62.2934,,5,plot multiple graph one plot use tensorboard use kera tensorflow backend work involve compare performance several model inception vgg resnet etc dataset would like plot training accuracy several model one graph try tensorboard work way plot multiple graph one plot use tensorboard way thank
648,648,36992855,Keras : How should I prepare input data for RNN?,"<p>I'm having trouble with preparing input data for RNN on Keras.</p>

<p>Currently, my training data dimension is: <code>(6752, 600, 13)</code></p>

<ul>
<li>6752: number of training data </li>
<li>600: number of time steps </li>
<li>13: size of feature vectors (the vector is in float)</li>
</ul>

<p><code>X_train</code> and <code>Y_train</code> are both in this dimension.</p>

<p>I want to prepare this data to be fed into <code>SimpleRNN</code> on Keras.
Suppose that we're going through time steps, from step #0 to step #599.
Let's say I want to use <code>input_length = 5</code>, which means that I want to use recent 5 inputs. (e.g. step #10, #11,#12,#13,#14 @ step #14).</p>

<p>How should I reshape <code>X_train</code>?</p>

<p>should it be <code>(6752, 5, 600, 13)</code> or should it be <code>(6752, 600, 5, 13)</code>?</p>

<p>And what shape should <code>Y_train</code> be in?</p>

<p>Should it be <code>(6752, 600, 13)</code> or <code>(6752, 1, 600, 13)</code> or <code>(6752, 600, 1, 13)</code>?</p>",37009670,2,0,,2016/5/2 22:51,15,2021/3/1 14:51,2020/8/28 11:34,,10375049,,5513231,,1,20,tensorflow|keras|deep-learning|lstm|recurrent-neural-network,10807,50.1348,,2,keras prepare input data rnn trouble prepare input data rnn kera currently training data dimension number train data number time step size feature vector vector float dimension want prepare data feed kera suppose go time step step step let say want use mean want use recent input e g step step reshape shape
102,102,41538692,Using sparse matrices with Keras and Tensorflow,"<p>My data can be viewed as a matrix of 10B entries (100M x 100), which is very sparse (&lt; 1/100 * 1/100 of entries are non-zero).  I would like to feed the data into into a Keras Neural Network model which I have made, using a Tensorflow backend.</p>

<p>My first thought was to expand the data to be dense, that is, write out all 10B entries into a series of CSVs, with most entries zero.  However, this is quickly overwhelming my resources (even doing the ETL overwhelmed pandas and is causing postgres to struggle).  So I need to use true sparse matrices.</p>

<p>How can I do that with Keras (and Tensorflow)? While numpy doesn't support sparse matrices, scipy and tensorflow both do.  There's lots of discussion (e.g.  <a href=""https://github.com/fchollet/keras/pull/1886"" rel=""noreferrer"">https://github.com/fchollet/keras/pull/1886</a> <a href=""https://github.com/fchollet/keras/pull/3695/files"" rel=""noreferrer"">https://github.com/fchollet/keras/pull/3695/files</a> <a href=""https://github.com/pplonski/keras-sparse-check"" rel=""noreferrer"">https://github.com/pplonski/keras-sparse-check</a> <a href=""https://groups.google.com/forum/#!topic/keras-users/odsQBcNCdZg"" rel=""noreferrer"">https://groups.google.com/forum/#!topic/keras-users/odsQBcNCdZg</a> ) about this idea - either using scipy's sparse matrixcs or going directly to Tensorflow's sparse matrices.  But I can't find a clear conclusion, and I haven't been able to get anything to work (or even know clearly which way to go!).</p>

<p>How can I do this?</p>

<p>I believe there are two possible approaches:</p>

<ol>
<li>Keep it as a scipy sparse matrix, then, when giving Keras a minibatch, make it dense</li>
<li>Keep it sparse all the way through, and use Tensorflow Sparse Tensors</li>
</ol>

<p>I also think #2 is preferred, because you'll get much better performance all the way through (I believe), but #1 is probably easier and will be adequate.  I'll be happy with either.</p>

<p>How can either be implemented?</p>",,2,0,,2017/1/8 22:42,15,2019/6/26 10:13,,,,,785494,,1,41,tensorflow|sparse-matrix|keras,29019,58.2507,,3,use sparse matrix kera tensorflow data view matrix b entry x sparse entry non zero would like fee data keras neural network model make use tensorflow backend first thought expand data dense write b entry series csvs entry zero however quickly overwhelming resource even etl overwhelm panda cause postgres struggle need use true sparse matrix kera tensorflow numpy support sparse matrix scipy tensorflow lot discussion e g topic kera user odsqbcncdzg idea either use scipy sparse matrixcs go directly tensorflow sparse matrix find clear conclusion able get anything work even know clearly way go believe two possible approach keep scipy sparse matrix give keras minibatch make dense keep sparse way use tensorflow sparse tensor also think prefer get much good performance way believe probably easy adequate happy either either implement
538,538,36498127,How to apply gradient clipping in TensorFlow?,"<p>Considering the <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/recurrent_network.py"">example code</a>.</p>

<p>I would like to know How to apply gradient clipping on this network on the RNN where there is a possibility of exploding gradients.</p>

<pre><code>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)
</code></pre>

<p>This is an example that could be used but where do I introduce this ?
In the def of RNN </p>

<pre><code>    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)
    # Split data because rnn cell needs a list of inputs for the RNN inner loop
    _X = tf.split(0, n_steps, _X) # n_steps
tf.clip_by_value(_X, -1, 1, name=None)
</code></pre>

<p>But this doesn't make sense as the tensor _X is the input and not the grad what is to be clipped? </p>

<p>Do I have to define my own Optimizer for this or is there a simpler option?</p>",36501922,7,0,,2016/4/8 11:09,50,2020/12/1 14:33,2020/12/1 14:33,,10908375,,2527680,,1,106,python|tensorflow|machine-learning|keras|deep-learning,82978,478.076,,3,apply gradient clipping tensorflow consider example code would like know apply gradient clip network rnn possibility explode gradient example could use introduce def rnn make sense tensor x input grad clip define optimizer simpler option
259,259,44679439,ModuleNotFoundError: No module named 'tensorflow.tensorboard.tensorboard',"<p>There seems to be a problem with recent TensorFlow build. The TensorBoard visualization tool would not run when it is compiled from sources to use with GPU. The error is as follows:</p>

<pre><code>$ tensorboard
Traceback (most recent call last):
  File ""/home/gpu/anaconda3/envs/tensorflow/bin/tensorboard"", line 7, in &lt;module&gt;
    from tensorflow.tensorboard.tensorboard import main
ModuleNotFoundError: No module named 'tensorflow.tensorboard.tensorboard'
</code></pre>

<p>Specs of system: Ubuntu 16.04, NVIDIA GTX 1070, cuda-8.0, cudnn 6.0.
Installed using Bazel from sources as described here:
<a href=""https://www.tensorflow.org/install/install_sources"" rel=""noreferrer"">https://www.tensorflow.org/install/install_sources</a></p>

<p>Installed into fresh anaconda3 environment 'tensorflow', environment is activated when performing command.</p>

<p>Would appreciate any help!</p>",49838782,5,0,,2017/6/21 14:49,10,2018/12/16 13:23,2017/6/21 15:03,,681865,,6096869,,1,16,python|tensorflow|deep-learning|tensorboard,18555,60.0738,,1,modulenotfounderror module name tensorflow tensorboard tensorboard seem problem recent tensorflow build tensorboard visualization tool would run compile source use gpu error follow spec system ubuntu nvidia gtx cuda cudnn instal use bazel source described instal fresh anaconda environment tensorflow environment activate perform command would appreciate help
713,713,39815518,Keras Maxpooling2d layer gives ValueError,"<p>I am trying to replicate VGG16 model in keras, the following is my code:</p>

<pre><code>model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2))) ###This line gives error
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1000, activation='softmax'))
</code></pre>

<p>The maxpooling2d layer gives an error at the line which is commented</p>

<p>The error says:</p>

<pre><code>ValueError: Negative dimension size caused by subtracting 2 from 1 for 'MaxPool_7' (op: 'MaxPool') with input shapes: [?,1,112,128].
</code></pre>

<p>What might be the reason behind this? How to solve this?</p>

<p>Edit:
A more detailed error log:</p>

<blockquote>
  <hr>
  
  <p>ValueError                                Traceback (most recent call
  last)  in ()
       12 model.add(Convolution2D(128, 3, 3, activation='relu'))
       13 
  ---> 14 model.add(MaxPooling2D((2,2), strides=(2,2)))
       15 
       16 model.add(ZeroPadding2D((1,1)))</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/models.pyc in add(self,
  layer)
      306                  output_shapes=[self.outputs[0]._keras_shape])
      307         else:
  --> 308             output_tensor = layer(self.outputs[0])
      309             if type(output_tensor) is list:
      310                 raise Exception('All layers in a Sequential model '</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  <strong>call</strong>(self, x, mask)
      512         if inbound_layers:
      513             # this will call layer.build() if necessary
  --> 514             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)
      515             input_added = True
      516 </p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  add_inbound_node(self, inbound_layers, node_indices, tensor_indices)
      570         # creating the node automatically updates self.inbound_nodes
      571         # as well as outbound_nodes on inbound layers.
  --> 572         Node.create_node(self, inbound_layers, node_indices, tensor_indices)
      573 
      574     def get_output_shape_for(self, input_shape):</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc in
  create_node(cls, outbound_layer, inbound_layers, node_indices,
  tensor_indices)
      147 
      148         if len(input_tensors) == 1:
  --> 149             output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))
      150             output_masks = to_list(outbound_layer.compute_mask(input_tensors[0], input_masks[0]))
      151             # TODO: try to auto-infer shape if exception is raised by get_output_shape_for</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/layers/pooling.pyc in
  call(self, x, mask)
      160                                         strides=self.strides,
      161                                         border_mode=self.border_mode,
  --> 162                                         dim_ordering=self.dim_ordering)
      163         return output
      164 </p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/layers/pooling.pyc in
  _pooling_function(self, inputs, pool_size, strides, border_mode, dim_ordering)
      210                           border_mode, dim_ordering):
      211         output = K.pool2d(inputs, pool_size, strides,
  --> 212                           border_mode, dim_ordering, pool_mode='max')
      213         return output
      214 </p>
  
  <p>/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc
  in pool2d(x, pool_size, strides, border_mode, dim_ordering, pool_mode)
  1699     1700     if pool_mode == 'max':
  -> 1701         x = tf.nn.max_pool(x, pool_size, strides, padding=padding)    1702     elif pool_mode == 'avg':    1703<br>
  x = tf.nn.avg_pool(x, pool_size, strides, padding=padding)</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.pyc
  in max_pool(value, ksize, strides, padding, data_format, name)    1391
  padding=padding,    1392<br>
  data_format=data_format,
  -> 1393                                 name=name)    1394     1395 </p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc
  in _max_pool(input, ksize, strides, padding, data_format, name)<br>
  1593   result = _op_def_lib.apply_op(""MaxPool"", input=input,
  ksize=ksize,    1594                                 strides=strides,
  padding=padding,
  -> 1595                                 data_format=data_format, name=name)    1596   return result    1597 </p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc
  in apply_op(self, op_type_name, name, **keywords)
      747           op = g.create_op(op_type_name, inputs, output_types, name=scope,
      748                            input_types=input_types, attrs=attr_protos,
  --> 749                            op_def=op_def)
      750           outputs = op.outputs
      751           return _Restructure(ops.convert_n_to_tensor(outputs),</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc
  in create_op(self, op_type, inputs, dtypes, input_types, name, attrs,
  op_def, compute_shapes, compute_device)    2388<br>
  original_op=self._default_original_op, op_def=op_def)    2389     if
  compute_shapes:
  -> 2390       set_shapes_for_outputs(ret)    2391     self._add_op(ret)    2392<br>
  self._record_op_seen_by_control_dependencies(ret)</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc
  in set_shapes_for_outputs(op)    1783       raise RuntimeError(""No
  shape function registered for standard op: %s""    1784<br>
  % op.type)
  -> 1785   shapes = shape_func(op)    1786   if shapes is None:    1787     raise RuntimeError(</p>
  
  <p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc
  in call_cpp_shape_fn(op, input_tensors_needed, debug_python_shape_fn)
      594                                                              status)
      595   except errors.InvalidArgumentError as err:
  --> 596     raise ValueError(err.message)
      597 
      598   # Convert TensorShapeProto values in output_shapes.</p>
  
  <p>ValueError: Negative dimension size caused by subtracting 2 from 1 for
  'MaxPool_7' (op: 'MaxPool') with input shapes: [?,1,112,128].</p>
</blockquote>",39851572,7,0,,2016/10/2 9:11,4,2020/8/26 5:43,2016/11/3 19:56,,147019,,4516640,,1,28,python|neural-network|tensorflow|deep-learning|keras,23762,87.7035,,4,kera maxpooling layer give valueerror try replicate vgg model kera following code maxpooling layer give error line comment error say might reason behind solve edit detailed error log valueerror traceback recent call last model add convolution activation arelu model add maxpooling stride model add zeropadding usr local lib python dist package keras model pyc add self layer output shape self output keras shape else output tensor layer self output type output tensor list raise exception layer sequential model usr local lib python dist package keras engine topology pyc call self x mask inbound layer call layer build necessary self add inbound node inbound layer node index tensor index input add true usr local lib python dist package keras engine topology pyc add inbound node self inbound layer node index tensor index create node automatically update self inbound node well outbound node inbound layer node create node self inbound layer node index tensor index def get output shape self input shape usr local lib python dist package keras engine topology pyc create node cl outbound layer inbound layer node index tensor index len input tensor output tensor list outbound layer call input tensor mask input mask output mask list outbound layer compute mask input tensor input mask todo try auto infer shape exception raise get output shape usr local lib python dist package keras layer pool pyc call self x mask stride self stride border mode self border mode dim order self dim order return output usr local lib python dist package keras layer pool pyc pool function self input pool size stride border mode dim order border mode dim order output k pool input pool size stride border mode dim order pool mode max return output usr local lib python dist package keras backend tensorflow backend pyc pool x pool size stride border mode dim order pool mode pool mode max x tf nn max pool x pool size stride pad pad elif pool mode avg x tf nn avg pool x pool size stride pad pad usr local lib python dist package tensorflow python ops nn ops pyc max pool value ksize stride pad data format name pad pad data format data format name name usr local lib python dist package tensorflow python ops gen nn ops pyc max pool input ksize stride pad data format name result op def lib apply op maxpool input input ksize ksize stride stride pad pad data format data format name name return result usr local lib python dist package tensorflow python framework op def library pyc apply op self op type name name keywords op g create op op type name input output type name scope input type input type attrs attr protos op def op def output op output return restructure ops convert n tensor output usr local lib python dist package tensorflow python framework ops pyc create op self op type input dtypes input type name attrs op def compute shape compute device original op self default original op op def op def compute shape set shape output ret self add op ret self record op see control dependency ret usr local lib python dist package tensorflow python framework ops pyc set shape output op raise runtimeerror shape function register standard op op type shape shape func op shape none raise runtimeerror usr local lib python dist package tensorflow python framework common shape pyc call cpp shape fn op input tensor need debug python shape fn status except error invalidargumenterror err raise valueerror err message convert tensorshapeproto value output shape valueerror negative dimension size cause subtract maxpool op maxpool input shape
705,705,39415263,How-to run TensorFlow on multiple core and threads,"<p>I should start saying that I am completely new to any kind of parallelism/multithreading/multiprocessing programming.</p>

<p>Now, I have the chance to run my TensorFlow CNN on 32 cores (each with 2 hyperthreads). I've spent a lot of time trying to understand how should I modify (if I have to) my code in order to exploit all of that computational power. Unfortuantely, I didn't come to anything. I hoped that TF could do that automatically but when I launch my model and check with <code>top</code> the CPU usage, I see most of the time a 100% CPU usage and a few 200% peaks.</p>

<p>If all the cores were used, I would expect to see a 100*64=6400% usage (correct?). How can I accomplish this? </p>

<p>Should I do something similar to what is explained <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/threading_and_queues/index.html"" rel=""noreferrer"">here</a>?</p>

<p>If that is the case, do I understand correctly that all the multithreading is only applied to calculations which involve Queue?</p>

<p>Is this really all that can be done to use all the computational power available (since it appears to me that queue are only used when reading and batching training samples)? </p>

<p>This is what my code looks like, if needed:
(main.py)</p>

<pre><code># pylint: disable=missing-docstring
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time

from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf
from pylab import *

import argparse
import cnn
import freader_2

training_feats_file = [""file_name""]
training_lbls_file = [""file_name""]
test_feats_file = 'file_name'
test_lbls_file = 'file_name'
learning_rate = 0.1
testset_size = 1000
batch_size = 1000
testset_size = 793
tot_samples = 810901
max_steps = 3300

def placeholder_inputs(batch_size):

    images_placeholder = tf.placeholder(tf.float32, shape=(testset_size, cnn.IMAGE_HEIGHT, cnn.IMAGE_WIDTH, 1))
    labels_placeholder = tf.placeholder(tf.float32, shape=(testset_size, 15))
    return images_placeholder, labels_placeholder

def reader(images_file, lbls_file, images_pl, labels_pl, im_height, im_width):

    images = loadtxt(images_file)
    labels_feed = loadtxt(lbls_file)
    images_feed = reshape(images, [images.shape[0], im_height, im_width, 1])

    feed_dict = {
        images_pl: images_feed,
        labels_pl: labels_feed,
    }

    return feed_dict

tot_training_loss = []
tot_test_loss = []
tot_grad = []

print('Starting TensorFlow session...')
with tf.Graph().as_default():

    DS = freader_2.XICSDataSet()
    images, labels = DS.trainingset_files_reader(training_feats_file, training_lbls_file)
    keep_prob = tf.placeholder(tf.float32) 
    logits = cnn.inference(images, batch_size, keep_prob)
    loss = cnn.loss(logits, labels)
    global_step = tf.Variable(0, trainable=False)
    train_op, grad_norm = cnn.training(loss, learning_rate, global_step)
    summary_op = tf.merge_all_summaries()   

    test_images_pl, test_labels_pl = placeholder_inputs(testset_size)
    test_pred = cnn.inference(test_images_pl, testset_size, keep_prob, True)
    test_loss = cnn.loss(test_pred, test_labels_pl)

    saver = tf.train.Saver()
    sess = tf.Session()
    summary_writer = tf.train.SummaryWriter(""CNN"", sess.graph)

    init = tf.initialize_all_variables()
    sess.run(init)
    tf.train.start_queue_runners(sess=sess)
    test_feed = reader(test_feats_file, test_lbls_file, test_images_pl, test_labels_pl, DS.height, DS.width)
    test_feed[keep_prob] = 1.    

    # Start the training loop.
    print('Starting training loop...')
    start_time = time.time()
    for step in xrange(max_steps):

        _, grad, loss_value= sess.run([train_op, grad_norm, loss], feed_dict = {keep_prob:0.5})  
        tot_training_loss.append(loss_value)
        tot_grad.append(grad)

        _, test_loss_val = sess.run([test_pred, test_loss], feed_dict=test_feed)
        tot_test_loss.append(test_loss_val)

        if step % 1 == 0:        
            duration = time.time() - start_time
            print('Step %d (%.3f sec):\n training loss = %f\n test loss = %f ' % (step, duration, loss_value, test_loss_val))
            print(' gradient = %f'%grad)
#            summary_str = sess.run(summary_op)#, feed_dict=feed_dict)
#            summary_writer.add_summary(summary_str, step)
#            summary_writer.flush()

        if (step+1) % 100 == 0:
            print('Saving checkpoint...')
            saver.save(sess, ""chkpts/medias-res"", global_step = global_step)

        if test_loss_val &lt; 0.01:# or grad &lt; 0.01:
            print(""Stopping condition reached."")
            break

    print('Saving final network...')
    saver.save(sess, ""chkpts/final.chkpt"")
    print('Total training time: ' + str((time.time() - start_time)/3600) + ' h')
</code></pre>

<p>cnn.py:</p>

<pre><code>from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import math

import tensorflow as tf

NUM_OUTPUT = 15

IMAGE_WIDTH = 195
IMAGE_HEIGHT = 20
IMAGE_PIXELS = IMAGE_WIDTH * IMAGE_HEIGHT

def inference(images, num_samples, keep_prob, reuse=None):

    with tf.variable_scope('conv1', reuse=reuse):
        kernel = tf.get_variable(name='weights', shape=[3, 30, 1, 5], initializer=tf.contrib.layers.xavier_initializer(uniform=False))        
        weight_decay = tf.mul(tf.nn.l2_loss(kernel), 0.001, name='weight_loss')
        tf.add_to_collection('losses', weight_decay)
        conv = tf.nn.conv2d(images, kernel, [1, 1, 5, 1], padding='VALID')
        # output dim: 18x34
        biases = tf.Variable(tf.constant(0.0, name='biases', shape=[5]))
        bias = tf.nn.bias_add(conv, biases)
        conv1 = tf.nn.relu(bias, name='conv1')

    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool1')    
    #output dim: 9x17

    with tf.variable_scope('conv2', reuse=reuse):
        kernel = tf.get_variable(name='weights', shape=[2, 2, 5, 5], initializer=tf.contrib.layers.xavier_initializer(uniform=False))
        weight_decay = tf.mul(tf.nn.l2_loss(kernel), 0.001, name='weight_loss')
        tf.add_to_collection('losses', weight_decay)
        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='VALID')
        #output dim: 8x16
        biases = tf.Variable(tf.constant(0.1, name='biases', shape=[5]))
        bias = tf.nn.bias_add(conv, biases)
        conv2 = tf.nn.relu(bias, name='conv2')


    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool2')
    #output dim: 4x8

    h_fc1_drop = tf.nn.dropout(pool2, keep_prob)

    with tf.variable_scope('fully_connected', reuse=reuse):
        reshape = tf.reshape(h_fc1_drop, [num_samples, -1])
        dim = reshape.get_shape()[1].value
        weights = tf.get_variable(name='weights', shape=[dim, 20], initializer=tf.contrib.layers.xavier_initializer(uniform=False))
        weight_decay = tf.mul(tf.nn.l2_loss(weights), 0.004, name='weight_loss')
        tf.add_to_collection('losses', weight_decay)
        biases = tf.Variable(tf.zeros([20], name='biases'))
        fully_connected = tf.nn.relu(tf.matmul(reshape, weights) + biases, name='fully_connected')

    with tf.variable_scope('identity', reuse=reuse):
        weights = tf.get_variable(name='weights', shape=[20,NUM_OUTPUT], initializer=tf.contrib.layers.xavier_initializer(uniform=False))
        weight_decay = tf.mul(tf.nn.l2_loss(weights), 0.004, name='weight_loss')
        tf.add_to_collection('losses', weight_decay)
        biases = tf.Variable(tf.zeros([NUM_OUTPUT], name='biases'))
        output = tf.matmul(fully_connected, weights) + biases

    return output


def loss(outputs, labels):

    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.sub(labels, outputs))), name=""rmse"")
    loss_list = tf.get_collection('losses')
    loss_list.append(rmse)
    rmse_tot = tf.add_n(loss_list, name='total_loss')  
    return rmse_tot


def training(loss, starter_learning_rate, global_step):

      tf.scalar_summary(loss.op.name, loss)
#      optimizer = tf.train.AdamOptimizer()
      learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 200, 0.8, staircase=True)
      optimizer = tf.train.MomentumOptimizer(learning_rate, 0.8)
      grads_and_vars = optimizer.compute_gradients(loss)
      grad_norms = [tf.nn.l2_loss(g[0]) for g in grads_and_vars]      
      grad_norm = tf.add_n(grad_norms)
      train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)
#      train_op = optimizer.minimize(loss, global_step=global_step)
      return train_op, grad_norm
</code></pre>

<p>freader_2.py:</p>

<pre><code># -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os 
import collections
import numpy as np

from six.moves import xrange  
import tensorflow as tf

class XICSDataSet:    
    def __init__(self, height=20, width=195, batch_size=1000, noutput=15):
        self.depth = 1
        self.height = height
        self.width = width
        self.batch_size = batch_size
        self.noutput = noutput

    def trainingset_files_reader(self, im_file_name, lb_file_name, nfiles=1):

        im_filename_queue = tf.train.string_input_producer(im_file_name, shuffle=False)
        lb_filename_queue = tf.train.string_input_producer(lb_file_name, shuffle=False)

        imreader = tf.TextLineReader()
        lbreader = tf.TextLineReader()
        imkey, imvalue = imreader.read(im_filename_queue)
        lbkey, lbvalue = lbreader.read(lb_filename_queue)
        im_record_defaults = [[.0]]*self.height*self.width
        lb_record_defaults = [[.0]]*self.noutput
        im_data_tuple = tf.decode_csv(imvalue, record_defaults=im_record_defaults, field_delim = ' ')
        lb_data_tuple = tf.decode_csv(lbvalue, record_defaults=lb_record_defaults, field_delim = ' ')
        features = tf.pack(im_data_tuple)
        label = tf.pack(lb_data_tuple)

        depth_major = tf.reshape(features, [self.height, self.width, self.depth])

        min_after_dequeue = 10
        capacity = min_after_dequeue + 3 * self.batch_size
        example_batch, label_batch = tf.train.shuffle_batch([depth_major, label], batch_size=self.batch_size, capacity=capacity,
                                                            min_after_dequeue=min_after_dequeue)

        return example_batch, label_batch
</code></pre>",,3,5,,2016/9/9 15:42,5,2020/2/27 16:28,2020/2/27 16:28,,1466970,,6479782,,1,23,python|multithreading|neural-network|tensorflow|conv-neural-network,31628,51.8003,,3,run tensorflow multiple core thread start say completely new kind parallelism multithreading multiprocessing program chance run tensorflow cnn core hyperthreads spend lot time try understand modify code order exploit computational power unfortuantely come anything hop tf could automatically launch model check cpu usage see time cpu usage peak core use would expect see usage correct accomplish something similar explain case understand correctly multithreading apply calculation involve queue really use computational power available since appear queue use reading batch training sample code look like need main py cnn py freader py
431,431,47863001,How Pytorch Tensor get the index of specific value,"<p>In python list, we can use <code>list.index(somevalue)</code>. How can pytorch do this?<br>
For example: </p>

<pre><code>    a=[1,2,3]
    print(a.index(2))
</code></pre>

<p>Then, <code>1</code> will be output. How can a pytorch tensor do this without converting it to a python list?</p>",,8,1,,2017/12/18 6:13,7,2021/7/16 16:05,2017/12/18 8:18,,5411817,,8987314,,1,47,python|pytorch,80573,160.825,,3,pytorch tensor get index specific value python list use pytorch example output pytorch tensor without convert python list
453,453,48302810,"What's the difference between ""hidden"" and ""output"" in PyTorch LSTM?","<p>I'm having trouble understanding the documentation for PyTorch's LSTM module (and also RNN and GRU, which are similar). Regarding the outputs, it says:</p>

<blockquote>
  <p>Outputs: output, (h_n, c_n)</p>
  
  <ul>
  <li>output (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.</li>
  <li>h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len</li>
  <li>c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len</li>
  </ul>
</blockquote>

<p>It seems that the variables <code>output</code> and <code>h_n</code> both give the values of the hidden state. Does <code>h_n</code> just redundantly provide the last time step that's already included in <code>output</code>, or is there something more to it than that?</p>",48305882,5,0,,2018/1/17 13:54,54,2021/6/13 9:30,2020/4/14 13:45,,2956066,,1119340,,1,77,deep-learning|pytorch|lstm|recurrent-neural-network|tensor,30065,268.912,,3,difference hidden output pytorch lstm trouble understand documentation pytorch lstm module also rnn gru similar regard output say outputs output h n c n output seq len batch hidden size num direction tensor contain output feature h last layer rnn torch nn utils rnn packedsequence give input output also packed sequence h n num layer num direction batch hidden size tensor contain hidden state seq len c n num layer num direction batch hidden size tensor contain cell state seq len seem variable give value hidden state redundantly provide last time step already include something
173,173,43080583,AttributeError: cannot assign module before Module.__init__() call,"<p>I am getting the following error.</p>

<pre><code>Traceback (most recent call last):
  File ""main.py"", line 63, in &lt;module&gt;
    question_classifier = QuestionClassifier(corpus.dictionary, embeddings_index, corpus.max_sent_length, args)
  File ""/net/if5/wua4nw/wasi/academic/research_with_prof_chang/projects/question_answering/duplicate_question_detection/source/question_classifier.py"", line 26, in __init__
    self.embedding = EmbeddingLayer(len(dictionary), args.emsize, args.dropout)
  File ""/if5/wua4nw/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py"", line 255, in __setattr__
    ""cannot assign module before Module.__init__() call"")
AttributeError: cannot assign module before Module.__init__() call
</code></pre>

<p>I have a class as follows.</p>

<pre><code>class QuestionClassifier(nn.Module):

     def __init__(self, dictionary, embeddings_index, max_seq_length, args):
         self.embedding = EmbeddingLayer(len(dictionary), args.emsize, args.dropout)
         self.encoder = EncoderRNN(args.emsize, args.nhid, args.model, args.bidirection, args.nlayers, args.dropout)
         self.drop = nn.Dropout(args.dropout)
</code></pre>

<p>So, when I run the following line:</p>

<pre><code>question_classifier = QuestionClassifier(corpus.dictionary, embeddings_index, corpus.max_sent_length, args)
</code></pre>

<p>I get the above mentioned error. Here, <code>EmbeddingLayer</code> and <code>EncoderRNN</code> is a class written by me which inherits <code>nn.module</code> like the <code>QuestionClassifier</code> class.</p>

<p>What I am doing wrong here?</p>",43080779,3,5,,2017/3/28 21:50,1,2020/4/19 12:18,2017/3/28 21:57,,5352399,,5352399,,1,18,python|pytorch,21442,67.1251,,4,attributeerror assign module module init call get following error class follow run following line get mention error class write inherit like class wrong
729,729,40475246,Why does one not use IOU for training?,"<p>When people try to solve the task of semantic segmentation with CNN's they usually use a softmax-crossentropy loss during training (see <a href=""http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html"" rel=""noreferrer"" title=""FCN Longquot;"">Fully conv. - Long</a>). But when it comes to comparing the performance of different approaches measures like intersection-over-union are reported.</p>

<p>My question is why don't people train directly on the measure they want to optimize? Seems odd to me to train on some measure during training, but evaluate on another measure for benchmarks.</p>

<p>I can see that the IOU has problems for training samples, where the class is not present (union=0 and intersection=0 => division zero by zero). But when I can ensure that every sample of my ground truth contains all classes, is there another reason for not using this measure?</p>",,4,0,,2016/11/7 21:48,7,2020/7/1 18:24,,,,,6375313,,1,13,machine-learning|computer-vision|deep-learning|image-segmentation,13840,54.9645,,0,one use iou training people try solve task semantic segmentation cnn usually use softmax crossentropy loss train see fully conv long come compare performance different approach measure like intersection union report question people train directly measure want optimize seem odd train measure training evaluate another measure benchmark see iou problem training sample class present union intersection division zero zero ensure every sample ground truth contain class another reason use measure
460,460,24509921,How do you decide the parameters of a Convolutional Neural Network for image classification?,"<p>I am using <strong>Convolutional Neural Networks</strong> (Unsupervised Feature learning to detect features + Softmax Regression Classifier) for image classification. I have gone through all the tutorials by Andrew NG in this area. (<a href=""http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial"" rel=""noreferrer"">http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial</a>).</p>

<p>The network that I have developed has an :</p>

<ul>
<li>Input layer - size 8x8 (64 neurons)</li>
<li>Hidden layer - size 400 neurons</li>
<li>Output layer - size 3</li>
</ul>

<p>I have learnt the weights connecting the input layer to the hidden layer using a sparse autoencoder and hence have 400 different features.</p>

<p>By taking continuous 8x8 patches from any input image (64x64) and feeding it to the input layer, I get 400 feature maps of size (57x57). </p>

<p>I then use max pooling  with a window of size 19 x 19 to obtain 400 feature maps of size 3x3.</p>

<p>I feed this feature map to a softmax layer to classify it into 3 different categories. </p>

<p>These parameters such as the number of hidden layers (depth of the network), and number of neurons per layer, were suggested in the tutorials as they had successfully been used on one particular data-set where all images were of size 64x64.</p>

<p>I would like to extend this to my own data set where the images are much larger (say 400x400).
How do I decide on </p>

<ol>
<li><p>The number of layers.</p></li>
<li><p>The number of neurons per layer.</p></li>
<li><p>The size of the pooling window (max pooling).</p></li>
</ol>",25421946,2,0,,2014/7/1 12:10,21,2017/1/17 21:54,2014/7/1 12:14,,2893131,,3705926,,1,13,computer-vision|neural-network|unsupervised-learning|autoencoder,24196,53.735,,0,decide parameter convolutional neural network image classification use convolutional neural network unsupervised feature learn detect feature softmax regression classifier image classification go tutorial andrew ng area network develop input layer size x neuron hide layer size neuron output layer size learn weight connect input layer hidden layer use sparse autoencoder hence different feature take continuous x patch input image x feed input layer get feature map size x use max pool window size x obtain feature map size x fee feature map softmax layer classify different category parameter number hidden layer depth network number neuron per layer suggest tutorial successfully use one particular data set image size x would like extend data set image much large say x decide number layer number neuron per layer size pooling window max pool
415,415,47538391,keras BatchNormalization axis clarification,"<p>The keras <a href=""https://keras.io/layers/normalization/"" rel=""noreferrer""><code>BatchNormalization</code> layer</a> uses <code>axis=-1</code> as a default value and states that the feature axis is typically normalized. Why is this the case?</p>

<p>I suppose this is surprising because I'm more familiar with using something like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""noreferrer""><code>StandardScaler</code></a>, which would be equivalent to using <code>axis=0</code>. This would normalize the features individually.</p>

<p>Is there a reason why samples are individually normalized by default (i.e. <code>axis=-1</code>) in keras as opposed to features?</p>

<p><strong>Edit: example for concreteness</strong></p>

<p>It's common to transform data such that each feature has zero mean and unit variance. Let's just consider the ""zero mean"" part with this mock dataset, where each row is a sample:</p>

<pre><code>&gt;&gt;&gt; data = np.array([[   1,   10,  100, 1000],
                     [   2,   20,  200, 2000],
                     [   3,   30,  300, 3000]])

&gt;&gt;&gt; data.mean(axis=0)
array([    2.,    20.,   200.,  2000.])

&gt;&gt;&gt; data.mean(axis=1)
array([ 277.75,  555.5 ,  833.25])
</code></pre>

<p>Wouldn't it make more sense to subtract the <code>axis=0</code> mean, as opposed to the <code>axis=1</code> mean? Using <code>axis=1</code>, the units and scales can be completely different.</p>

<p>Edit 2:</p>

<p>The first equation of section 3 in <a href=""https://arxiv.org/pdf/1502.03167.pdf"" rel=""noreferrer"">this paper</a> seems to imply that <code>axis=0</code> should be used for calculating expectations and variances for each feature individually, assuming you have an (m, n) shaped dataset where m is the number of samples and n is the number of features.</p>

<p>Edit 3: another example</p>

<p>I wanted to see the dimensions of the means and variances <code>BatchNormalization</code> was calculating on a toy dataset:</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.datasets import load_iris

from keras.optimizers import Adam
from keras.models import Model
from keras.layers import BatchNormalization, Dense, Input


iris = load_iris()
X = iris.data
y = pd.get_dummies(iris.target).values

input_ = Input(shape=(4, ))
norm = BatchNormalization()(input_)
l1 = Dense(4, activation='relu')(norm)
output = Dense(3, activation='sigmoid')(l1)

model = Model(input_, output)
model.compile(Adam(0.01), 'categorical_crossentropy')
model.fit(X, y, epochs=100, batch_size=32)

bn = model.layers[1]
bn.moving_mean  # &lt;tf.Variable 'batch_normalization_1/moving_mean:0' shape=(4,) dtype=float32_ref&gt;
</code></pre>

<p>The input X has shape (150, 4), and the <code>BatchNormalization</code> layer calculated 4 means, which means it operated over <code>axis=0</code>.</p>

<p>If <code>BatchNormalization</code> has a default of <code>axis=-1</code> then shouldn't there be 150 means?</p>",47549590,3,5,,2017/11/28 18:19,10,2019/8/16 23:15,2017/11/29 15:17,,1319915,,1319915,,1,27,python|machine-learning|deep-learning|keras,14057,94.7916,,3,kera batchnormalization axis clarification kera layer use default value state feature axis typically normalized case suppose surprising familiar use something like would equivalent use would normalize feature individually reason sample individually normalize default e kera oppose feature edit example concreteness common transform data feature zero mean unit variance let consider zero mean part mock dataset row sample would make sense subtract mean oppose mean use unit scale completely different edit first equation section paper seem imply use calculate expectation variance feature individually assume n shape dataset number sample n number feature edit another example want see dimension mean variance calculate toy dataset input x shape layer calculate mean mean operate default mean
122,122,41924453,PyTorch: How to use DataLoaders for custom Datasets,"<p>How to make use of the <code>torch.utils.data.Dataset</code> and <code>torch.utils.data.DataLoader</code> on your own data (not just the <code>torchvision.datasets</code>)?</p>

<p>Is there a way to use the inbuilt <code>DataLoaders</code> which they use on <code>TorchVisionDatasets</code> to be used on any dataset?</p>",42054194,5,0,,2017/1/29 18:31,36,2021/3/9 13:58,2017/2/3 2:17,,1611389,,3398648,,1,60,python|torch|pytorch,43082,156.537,,2,pytorch use dataloaders custom datasets make use data way use inbuilt use use dataset
424,424,47754749,How to install pytorch in windows?,"<p>I am trying to install pytorch on windows and there is one which is available for it but shows an error.</p>

<pre><code>conda install -c peterjc123 pytorch=0.1.12
</code></pre>",,16,3,,2017/12/11 14:10,5,2020/9/23 15:31,2020/3/31 22:16,,11301900,,8505504,,1,14,python|windows|anaconda|pytorch|conda,99053,137.783,,1,install pytorch window try install pytorch window one available show error
278,278,44907377,"What is ""epoch"" in keras.models.Model.fit?","<p>What is ""epoch"" in <code>keras.models.Model.fit</code>? Is it one gradient update? If it is more than one gradient update, then what is defining an epoch?</p>

<p>Suppose I am feeding my own batches to <code>fit</code>. I would regard ""epoch"" as finishing to process entire training set (is this correct)? Then how to control keras for this way? Can I set <code>batch_size</code> equal to <code>x</code> and <code>y</code> size and <code>epochs</code> to 1?</p>",44907684,1,0,,2017/7/4 13:30,10,2017/7/4 13:45,,,,,258483,,1,41,machine-learning|keras|batch-processing,29703,70.0912,,3,epoch keras model model fit epoch one gradient update one gradient update define epoch suppose feed batch would regard epoch finish process entire training set correct control keras way set equal size
578,578,49311929,Install issues with 'lr_utils' in python,"<p>I am trying to complete some homework in a DeepLearning.ai course assignment.</p>
<p>When I try the assignment in Coursera platform everything works fine, however, when I try to do the same <code>imports</code> on my local machine it gives me an error,</p>
<pre><code>ModuleNotFoundError: No module named 'lr_utils'
</code></pre>
<p>I have tried resolving the issue by installing <code>lr_utils</code> but to no avail.</p>
<p>There is no mention of this module online, and now I started to wonder if that's a proprietary to <code>deeplearning.ai</code>?</p>
<p>Or can we can resolve this issue in any other way?</p>",52980903,13,0,,2018/3/16 1:39,6,2021/4/27 15:10,2021/2/13 13:35,,4685471,,4118039,,1,16,python|deep-learning,26104,110.267,,1,install issue lr utils python try complete homework deeplearning ai course assignment try assignment coursera platform everything work fine however try local machine give error try resolve issue instal avail mention module online start wonder proprietary resolve issue way
465,465,26182980,Can anyone give a real life example of supervised learning and unsupervised learning?,"<p>I recently studied about supervised learning and unsupervised learning. From theory, I know that supervised means getting the information from labeled datasets and unsupervised means clustering the data without any labels given.</p>

<p>But, the problem is I always get confused to identify whether the given example is supervised learning or unsupervised learning during my studies.</p>

<p>Can anyone please give a real life example?</p>",26197400,6,0,,2014/10/3 16:29,12,2019/2/1 17:28,2018/5/1 3:01,,4014075,,3309627,,1,38,machine-learning|deep-learning|data-mining|supervised-learning|unsupervised-learning,90734,168.431,2019/2/3 1:15,0,anyone give real life example supervise learning unsupervised learning recently study supervise learning unsupervised learning theory know supervise mean get information label datasets unsupervised mean cluster data without label give problem always get confuse identify whether give example supervise learn unsupervised learning study anyone please give real life example
35,35,54374935,"How to fix this strange error: ""RuntimeError: CUDA error: out of memory""","<p>I ran a code about the deep learning network,first I trained the network,and it works well,but this error occurs when running to the validate network.</p>

<p>I have five epoch,every epoch has a process of training and validation. I met the error when validate in the first epoch. So I don not run the validate code, I found that code can run to the second epoch and have no error.</p>

<p>My code:</p>

<pre><code>for epoch in range(10,15): # epoch: 10~15
    if(options[""training""][""train""]):
        trainer.epoch(model, epoch)

    if(options[""validation""][""validate""]):
    #if(epoch == 14):
        validator.epoch(model)
</code></pre>

<p><img src=""https://i.stack.imgur.com/J3nES.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/CJkyi.png"" alt=""enter image description here""></p>

<p>I feel the code of validation may have some bugs. But I can not find that.</p>",,10,4,,2019/1/26 1:39,6,2021/6/9 14:55,2019/1/26 2:07,,8961316,,10912236,,1,32,python|pycharm|pytorch,123810,167.371,,4,fix strange error runtimeerror cuda error memory run code deep learning network first train network work well error occur run validate network five epoch every epoch process training validation meet error validate first epoch run validate code find code run second epoch error code feel code validation may bug find
669,669,37892784,Using Keras & Tensorflow with AMD GPU,"<p>I'm starting to learn Keras, which I believe is a layer on top of Tensorflow and Theano.  However, I only have access to AMD GPUs such as the AMD R9 280X.</p>

<p>How can I setup my Python environment such that I can make use of my AMD GPUs through Keras/Tensorflow support for OpenCL?</p>

<p>I'm running on OSX.</p>",,8,6,,2016/6/18 2:51,43,2020/11/20 13:04,2016/6/18 4:09,,3222797,,741099,,1,82,python|python-2.7|opencl|tensorflow|keras,113532,297.42,,1,use kera tensorflow amd gpu start learn kera believe layer top tensorflow theano however access amd gpus amd r x setup python environment make use amd gpus kera tensorflow support opencl run osx
193,193,43511819,ImportError: No module named 'keras.utils.visualize_util',"<p>Hi when I am trying to run a code in keras it is showing me the following error:</p>

<pre><code>from keras.utils.visualize_util import plot
ImportError: No module named 'keras.utils.visualize_util'
</code></pre>

<p>How can I solve this? thanks</p>",43513161,4,0,,2017/4/20 6:28,4,2021/6/2 6:10,,,,,6494707,,1,13,python-3.x|deep-learning|keras|keras-layer,24142,75.9311,,1,importerror module name kera utils visualize util hi try run code kera show follow error solve thanks
226,226,44184834,Value error: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples,"<p>I'm trying to do a 8-class classification. Here is the code:</p>

<pre><code>import keras
import numpy as np
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense
from keras import applications
from keras.optimizers import SGD
from keras import backend as K
K.set_image_dim_ordering('tf')
img_width, img_height = 48,48
top_model_weights_path = 'modelom.h5'
train_data_dir = 'chCdata1/train'
validation_data_dir = 'chCdata1/validation'
nb_train_samples = 6400
nb_validation_samples = 1600
epochs = 50
batch_size = 10
def save_bottlebeck_features():
   datagen = ImageDataGenerator(rescale=1. / 255)
   model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(48,48,3))
   generator = datagen.flow_from_directory(
               train_data_dir,
               target_size=(img_width, img_height),
               batch_size=batch_size,
               class_mode='categorical',
               shuffle=False)
   bottleneck_features_train = model.predict_generator(
               generator, nb_train_samples // batch_size)
   np.save(open('bottleneck_features_train', 'wb'),bottleneck_features_train)

   generator = datagen.flow_from_directory(
               validation_data_dir,
               target_size=(img_width, img_height),
               batch_size=batch_size,
               class_mode='categorical',
               shuffle=False)
   bottleneck_features_validation = model.predict_generator(
               generator, nb_validation_samples // batch_size)
   np.save(open('bottleneck_features_validation', 'wb'),bottleneck_features_validation)

def train_top_model():
   train_data = np.load(open('bottleneck_features_train', 'rb'))
   train_labels = np.array([0] * (nb_train_samples // 8) + [1] * (nb_train_samples // 8) + [2] * (nb_train_samples // 8) + [3] * (nb_train_samples // 8) + [4] * (nb_train_samples // 8) + [5] * (nb_train_samples // 8) + [6] * (nb_train_samples // 8) + [7] * (nb_train_samples // 8))
   validation_data = np.load(open('bottleneck_features_validation', 'rb'))
   validation_labels = np.array([0] * (nb_train_samples // 8) + [1] * (nb_train_samples // 8) + [2] * (nb_train_samples // 8) + [3] * (nb_train_samples // 8) + [4] * (nb_train_samples // 8) + [5] * (nb_train_samples // 8) + [6] * (nb_train_samples // 8) + [7] * (nb_train_samples // 8))
   train_labels = keras.utils.to_categorical(train_labels, num_classes = 8)
   validation_labels = keras.utils.to_categorical(validation_labels, num_classes = 8)
   model = Sequential()
   model.add(Flatten(input_shape=train_data.shape[1:]))
   model.add(Dense(512, activation='relu'))
   model.add(Dropout(0.5))
   model.add(Dense(8, activation='softmax'))
   sgd = SGD(lr=1e-2, decay=0.00371, momentum=0.9, nesterov=False)
   model.compile(optimizer=sgd,
         loss='categorical_crossentropy', metrics=['accuracy'])
   model.fit(train_data, train_labels,
          epochs=epochs,
          batch_size=batch_size,
   validation_data=(validation_data, validation_labels))
   model.save_weights(top_model_weights_path)

save_bottlebeck_features()
train_top_model()
</code></pre>

<p>I've added the full list of error here:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-14-1d34826b5dd5&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/rajaramans2/codes/untitled15.py', wdir='C:/Users/rajaramans2/codes')

  File ""C:\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
    execfile(filename, namespace)

  File ""C:\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/rajaramans2/codes/untitled15.py"", line 71, in &lt;module&gt;
    train_top_model()

  File ""C:/Users/rajaramans2/codes/untitled15.py"", line 67, in train_top_model
    validation_data=(validation_data, validation_labels))

  File ""C:\Anaconda3\lib\site-packages\keras\models.py"", line 856, in fit
    initial_epoch=initial_epoch)

  File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1449, in fit
    batch_size=batch_size)

  File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 1317, in _standardize_user_data
    _check_array_lengths(x, y, sample_weights)

  File ""C:\Anaconda3\lib\site-packages\keras\engine\training.py"", line 235, in _check_array_lengths
    'and ' + str(list(set_y)[0]) + ' target samples.')

ValueError: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples.
</code></pre>

<p>The ""ValueError: Input arrays should have the same number of samples as target arrays. Found 1600 input samples and 6400 target samples"" pops up. Kindly help with the solution and the necessary modifications to the code. Thanks in advance.</p>",44184954,5,0,,2017/5/25 15:57,2,2020/3/18 5:10,2017/5/25 16:36,,7575552,,7575552,,1,18,python|arrays|numpy|keras,42575,68.5166,,4,value error input array number sample target array find input sample target sample try class classification code add full list error valueerror input array number sample target array find input sample target sample pop kindly help solution necessary modification code thanks advance
377,377,46218407,How to interpret Keras model.fit output?,"<p>I've just started using Keras.  The sample I'm working on has a model and the following snippet is used to run the model</p>

<pre><code>from sklearn.preprocessing import LabelBinarizer
label_binarizer = LabelBinarizer()
y_one_hot = label_binarizer.fit_transform(y_train)

model.compile('adam', 'categorical_crossentropy', ['accuracy'])
history = model.fit(X_normalized, y_one_hot, nb_epoch=3, validation_split=0.2)
</code></pre>

<p>I get the following response:</p>

<pre><code>Using TensorFlow backend. Train on 80 samples, validate on 20 samples Epoch 1/3

32/80 [===========&gt;..................] - ETA: 0s - loss: 1.5831 - acc:
0.4062 80/80 [==============================] - 0s - loss: 1.3927 - acc:
0.4500 - val_loss: 0.7802 - val_acc: 0.8500 Epoch 2/3

32/80 [===========&gt;..................] - ETA: 0s - loss: 0.9300 - acc:
0.7500 80/80 [==============================] - 0s - loss: 0.8490 - acc:
0.8000 - val_loss: 0.5772 - val_acc: 0.8500 Epoch 3/3

32/80 [===========&gt;..................] - ETA: 0s - loss: 0.6397 - acc:
0.8750 64/80 [=======================&gt;......] - ETA: 0s - loss: 0.6867 - acc:
0.7969 80/80 [==============================] - 0s - loss: 0.6638 - acc:
0.8000 - val_loss: 0.4294 - val_acc: 0.8500
</code></pre>

<p>The <a href=""https://keras.io/models/model/"" rel=""noreferrer"">documentation</a> says that fit returns </p>

<blockquote>
  <p>A History instance. Its history attribute contains all information
  collected during training.</p>
</blockquote>

<p>Does anyone know how to interpret the history instance?  </p>

<p>For example, what does 32/80 mean?  I assume 80 is the number of samples but what is 32?  ETA: 0s ??</p>",,2,0,,2017/9/14 11:52,5,2019/10/4 16:15,2017/10/18 20:48,,2569186,,2569186,,1,22,keras,18563,54.8746,,3,interpret kera model fit output start use keras sample work model following snippet use run model get following response documentation say fit return history instance history attribute contain information collect training anyone know interpret history instance example mean assume number sample eta
731,731,40560795,Tensorflow AttributeError: 'NoneType' object has no attribute 'TF_DeleteStatus',"<p>Tensorflow is giving me this unresolved error:</p>

<pre><code>Exception ignored in: &lt;bound method BaseSession.__del__ of &lt;tensorflow.python.client.session.Session object at 0x7f68d14b6668&gt;&gt;
Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 532, in __del__
AttributeError: 'NoneType' object has no attribute 'TF_DeleteStatus'
</code></pre>

<p>The error has been discussed <a href=""https://github.com/tensorflow/tensorflow/issues/3388"">here</a>. The problem is it is not showing up consistently. However, it is showing up in my terminal frequently. Has anybody managed to get around it.Thanks.</p>",,4,1,,2016/11/12 7:55,5,2019/1/9 19:13,2016/11/12 8:17,,1508156,,1508156,,1,22,python|tensorflow|keras,17378,63.56,,4,tensorflow attributeerror nonetype object attribute tf deletestatus tensorflow give unresolved error error discuss problem show consistently however show terminal frequently anybody manage get around thanks
279,279,44924690,Keras: the difference between LSTM dropout and LSTM recurrent dropout,"<p>From the Keras documentation:</p>

<p>dropout: Float between 0 and 1. Fraction of the units to drop for the
 linear transformation of the inputs.</p>

<p>recurrent_dropout: Float between 0 and 1. Fraction of the units to
 drop for the linear transformation of the recurrent state.</p>

<p>Can anyone point to where on the image below each dropout happens?</p>

<p><a href=""https://i.stack.imgur.com/DS97N.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DS97N.png"" alt=""enter image description here""></a></p>",44929759,2,1,,2017/7/5 11:13,29,2020/7/17 21:06,2018/4/3 13:18,,2858407,,1998149,,1,84,keras|lstm|dropout,31512,146.594,,3,keras difference lstm dropout lstm recurrent dropout kera documentation dropout float fraction unit drop linear transformation inputs recurrent dropout float fraction unit drop linear transformation recurrent state anyone point image dropout happen
521,521,35049379,Training on imbalanced data using TensorFlow,"<p><strong>The Situation:</strong></p>

<p>I am wondering how to use TensorFlow optimally when my training data is imbalanced in label distribution between 2 labels. For instance, suppose the <a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html"" rel=""noreferrer"">MNIST tutorial</a> is simplified to only distinguish between 1's and 0's, where all images available to us are either 1's or 0's. This is straightforward to train using the provided TensorFlow tutorials when we have roughly 50% of each type of image to train and test on. But what about the case where 90% of the images available in our data are 0's and only 10% are 1's? I observe that in this case, TensorFlow routinely predicts my entire test set to be 0's, achieving an accuracy of a meaningless 90%.</p>

<p>One strategy I have used to some success is to pick random batches for training that do have an even distribution of 0's and 1's. This approach ensures that I can still use all of my training data and produced decent results, with less than 90% accuracy, but a much more useful classifier. Since accuracy is somewhat useless to me in this case, my metric of choice is typically area under the ROC curve (AUROC), and this produces a result respectably higher than .50.</p>

<p><strong>Questions:</strong></p>

<p>(1) Is the strategy I have described an accepted or optimal way of training on imbalanced data, or is there one that might work better?</p>

<p>(2) Since the accuracy metric is not as useful in the case of imbalanced data, is there another metric that can be maximized by altering the cost function? I can certainly calculate AUROC post-training, but can I train in such a way as to maximize AUROC?</p>

<p>(3) Is there some other alteration I can make to my cost function to improve my results for imbalanced data? Currently, I am using a default suggestion given in TensorFlow tutorials:</p>

<pre><code>cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
</code></pre>

<p>I have heard this may be possible by up-weighting the cost of miscategorizing the smaller label class, but I am unsure of how to do this.</p>",38742267,4,3,,2016/1/27 22:30,11,2020/5/4 7:46,,,,,5849136,,1,29,machine-learning|neural-network|deep-learning|tensorflow|perceptron,17471,59.1693,,4,train imbalanced data use tensorflow situation wonder use tensorflow optimally training data imbalanced label distribution label instance suppose mnist tutorial simplify distinguish image available u either straightforward train use provide tensorflow tutorial roughly type image train test case image available data observe case tensorflow routinely predicts entire test set achieve accuracy meaningless one strategy use success pick random batch training even distribution approach ensure still use training data produce decent result less accuracy much useful classifier since accuracy somewhat useless case metric choice typically area roc curve auroc produce result respectably high question strategy describe accepted optimal way training imbalanced data one might work well since accuracy metric useful case imbalanced data another metric maximize alter cost function certainly calculate auroc post training train way maximize auroc alteration make cost function improve result imbalanced data currently use default suggestion give tensorflow tutorial hear may possible weight cost miscategorizing small label class unsure
317,317,58446290,"""UserWarning: An input could not be retrieved. It could be because a worker has died. We do not have any information on the lost sample.""","<p>While training model I got this warning ""UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.)"", after showing this warning, model starts training. What does this warning means? Is it something that will affect my training and I need to worry about?</p>",,8,0,,2019/10/18 7:51,1,2021/8/14 18:39,2019/10/18 9:42,,9678047,,9678047,,1,13,machine-learning|deep-learning|tf.keras,12183,51.143,,4,userwarning input could retrieve could worker die information lose sample train model get warning userwarning input could retrieve could worker die information lose sample show warning model start train warning mean something affect training need worry
562,562,48831131,"CMake on Linux CentOS 7, how to force the system to use cmake3?","<p>I tried to install <a href=""http://pytorch.org"" rel=""noreferrer"">PyTorch</a> on my Linux CentOS 7.3. I downloaded its package, ran this command and got this error:</p>

<pre><code>sudo python setup.py install

running install
running build_deps
CMake Error at CMakeLists.txt:1 (cmake_minimum_required):
  CMake 3.0 or higher is required.  You are running version 2.8.12.2


-- Configuring incomplete, errors occurred!
</code></pre>

<p>So I tried to install CMake 3 by using the command </p>

<pre><code>sudo yum -y install cmake3
</code></pre>

<p>The installation went alright, but the system still uses cmake2.8 as default.
If I type the yum info comnmand, I get this:</p>

<pre><code>sudo yum info cmake

Installed Packages
Name        : cmake
Arch        : x86_64
Version     : 2.8.12.2
Release     : 2.el7
Size        : 27 M
Repo        : installed
From repo   : base
Summary     : Cross-platform make system
URL         : http://www.cmake.org
License     : BSD and MIT and zlib
Description : CMake is used to control the software compilation process using simple
            : platform and compiler independent configuration files. CMake generates
            : native makefiles and workspaces that can be used in the compiler
            : environment of your choice. CMake is quite sophisticated: it is possible
            : to support complex environments requiring system configuration, preprocessor
            : generation, code generation, and template instantiation.
</code></pre>

<p>So, the problem is clear: <strong>the system still sees cmake2.8 as default, and therefore Python does not use cmake3 for its PyTorch installation.</strong>
How can I solve this problem?</p>

<p>Thanks</p>",,5,0,,2018/2/16 16:35,15,2020/12/1 14:31,,,,,1082019,,1,26,python-2.7|cmake|centos|centos7|pytorch,57349,106.034,,1,cmake linux centos force system use cmake try install pytorch linux centos download package run command get error try install cmake use command installation go alright system still use cmake default type yum info comnmand get problem clear system still see cmake default therefore python use cmake pytorch installation solve problem thanks
284,284,44972565,What is the difference between the predict and predict_on_batch methods of a Keras model?,"<p>According to the keras <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">documentation</a>:</p>

<pre><code>predict_on_batch(self, x)
Returns predictions for a single batch of samples.
</code></pre>

<p>However, there does not seem to be any difference with the standard <code>predict</code> method when called on a batch, whether it being with one or multiple elements.</p>

<pre><code>model.predict_on_batch(np.zeros((n, d_in)))
</code></pre>

<p>is the same as </p>

<pre><code>model.predict(np.zeros((n, d_in)))
</code></pre>

<p>(a <code>numpy.ndarray</code> of shape <code>(n, d_out</code>)</p>",44972728,3,1,,2017/7/7 13:52,3,2020/4/18 11:16,,,,,3860928,,1,25,python|deep-learning|keras,26174,64.6715,,3,difference predict predict batch method kera model accord kera documentation however seem difference standard method call batch whether one multiple element shape
357,357,45755022,Cannot add layers to saved Keras Model. 'Model' object has no attribute 'add',"<p>I have a saved a model using <code>model.save()</code>. I'm trying to reload the model and add a few layers and tune some hyper-parameters, however, it throws the AttributeError.</p>

<p>Model is loaded using <code>load_model()</code>.</p>

<p>I guess I'm missing understanding how to add layers to saved layers. If someone can guide me here, it will be great. I'm a novice to deep learning and using keras, so probably my request would be silly.</p>

<p>Snippet:</p>

<pre><code>prev_model = load_model('final_model.h5') # loading the previously saved model.

prev_model.add(Dense(256,activation='relu'))
prev_model.add(Dropout(0.5))
prev_model.add(Dense(1,activation='sigmoid'))

model = Model(inputs=prev_model.input, outputs=prev_model(prev_model.output))
</code></pre>

<p>And the error it throws:</p>

<pre><code>Traceback (most recent call last):
  File ""image_classifier_3.py"", line 39, in &lt;module&gt;
    prev_model.add(Dense(256,activation='relu'))
AttributeError: 'Model' object has no attribute 'add'
</code></pre>

<p>I know adding layers works on new Sequential() model, but how do we add to existing saved models?</p>",45755797,2,0,,2017/8/18 10:57,5,2017/8/18 11:39,,,,,6157456,,1,11,python|deep-learning|keras|keras-layer,15888,50.2043,,3,add layer save kera model model object attribute add save model use try reload model add layer tune hyper parameter however throw attributeerror model load use guess miss understand add layer save layer someone guide great novice deep learning use kera probably request would silly snippet error throw know add layer work new sequential model add exist save model
811,811,53503389,How to set parameters in keras to be non-trainable?,"<p>I am new to Keras and I am building a model. I want to freeze the weights of the last few layers of the model while training the previous layers. I tried to set the trainable property of the lateral model to be False, but it dosen't seem to work. Here is the code and the model summary:</p>

<pre><code>opt = optimizers.Adam(1e-3)
domain_layers = self._build_domain_regressor()
domain_layers.trainble = False
feature_extrator = self._build_common()
img_inputs = Input(shape=(160, 160, 3))
conv_out = feature_extrator(img_inputs)
domain_label = domain_layers(conv_out)
self.domain_regressor = Model(img_inputs, domain_label)
self.domain_regressor.compile(optimizer = opt, loss='binary_crossentropy', metrics=['accuracy'])
self.domain_regressor.summary()
</code></pre>

<p>The model summary: <a href=""https://i.stack.imgur.com/nWR8q.png"" rel=""noreferrer"">model summary</a>.</p>

<p>As you can see, <code>model_1</code> is trainable. But according to the code, it is set to be non-trainable.</p>",53512269,4,5,,2018/11/27 15:52,2,2019/11/8 18:13,2019/10/23 15:46,,3924118,,10712335,,1,15,python|keras|deep-learning,31038,70.9676,,3,set parameter kera non trainable new keras build model want freeze weight last layer model train previous layer try set trainable property lateral model false dose seem work code model summary model summary model summary see trainable accord code set non trainable
806,806,53290306,Confusion matrix and test accuracy for PyTorch Transfer Learning tutorial,"<p>Following the Pytorch Transfer learning tutorial, I am interested in reporting only train and test accuracy as well as confusion matrix (say using sklearn confusionmatrix). How can I do that? The current tutorial only reports train/val accuracy and I am having hard time figuring how to incorporate the sklearn confusionmatrix code there. Link to original tutorial here: <a href=""https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</a></p>

<pre><code>%matplotlib inline
from graphviz import Digraph
import torch
from torch.autograd import Variable
# Author: Sasank Chilamkurthy

from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}


data_dir = ""images""
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")

def imshow(inp, title=None):
    """"""Imshow for Tensor.""""""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                scheduler.step()
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc &gt; best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['val']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title('predicted: {}'.format(class_names[preds[j]]))
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 9)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                       num_epochs=25)

visualize_model(model_ft)
</code></pre>",53291323,5,0,,2018/11/13 22:13,12,2021/1/16 21:33,,,,,2414957,,1,10,python|scikit-learn|pytorch|confusion-matrix|transfer-learning,22760,54.4287,,4,confusion matrix test accuracy pytorch transfer learn tutorial follow pytorch transfer learn tutorial interested report train test accuracy well confusion matrix say use sklearn confusionmatrix current tutorial report train val accuracy hard time figure incorporate sklearn confusionmatrix code link original tutorial
338,338,45175469,TypeError: concat() got multiple values for argument 'axis',"<p>This is my convolution neural net:</p>

<pre><code>def convolutional_neural_network(frame):
    wts = {'conv1': tf.random_normal([5, 5, 3, 32]),
            'conv2': tf.random_normal([5, 5, 32, 64]),
            'fc': tf.random_normal([158*117*64 + 4, 128]),
            'out': tf.random_normal([128, n_classes])
            }
    biases = {'fc': tf.random_normal([128]),
                'out': tf.random_normal([n_classes])
            }

    conv1 = conv2d(frame, wts['conv1'])
    # print(conv1)
    conv1 = maxpool2d(conv1)
    # print(conv1)
    conv2 = conv2d(conv1, wts['conv2'])
    conv2 = maxpool2d(conv2)
    # print(conv2)
    conv2 = tf.reshape(conv2, shape=[-1,158*117*64])
    print(conv2)
    print(controls_at_each_frame)
    conv2 = tf.concat(conv2, controls_at_each_frame, axis=1)
    fc = tf.add(tf.matmul(conv2, wts['fc']), biases['fc'])

    output = tf.nn.relu(tf.add(tf.matmul(fc, wts['out']), biases['out']))

    return output
</code></pre>

<p>where </p>

<pre><code>frame = tf.placeholder('float', [None, 640-10, 465, 3])
controls_at_each_frame = tf.placeholder('float', [None, 4]) # [w, a, s, d] (1/0)
</code></pre>

<p>are the used placeholder.</p>

<p>I am making a self driving car in GTA San Andreas. What I want to do is concatenate <code>frame</code> and <code>controls_at_each_frame</code> into a single layer which will be then sent to an fully connected layer. When I run I get an error <code>TypeError: concat() got multiple values for argument 'axis'</code> at </p>

<pre><code>conv2 = tf.concat(conv2, controls_at_each_frame, axis=1)
</code></pre>

<p>Could you explain why this happening?</p>",45175485,1,0,,2017/7/18 19:27,3,2017/7/18 19:29,,,,,6288172,,1,12,python|tensorflow|neural-network|conv-neural-network,36054,55.6278,,4,typeerror concat get multiple value argument axis convolution neural net used placeholder make self drive car gta san andreas want concatenate single layer send fully connect layer run get error could explain happening
281,281,44931689,How to disable printing reports after each epoch in Keras?,"<p>After each epoch I have printout like below:</p>

<pre><code>Train on 102 samples, validate on 26 samples
Epoch 1/1
Epoch 00000: val_acc did not improve
102/102 [==============================] - 3s - loss: 0.4934 - acc: 0.8997 - val_loss: 0.4984 - val_acc: 0.9231
</code></pre>

<p>I am not using built-in epochs, so I would like to disable these printouts and print something myself.</p>

<p>How to do that?</p>

<p>I am using tensorflow backend if it matters.</p>",44931989,1,0,,2017/7/5 16:36,4,2021/7/15 12:29,,,,,258483,,1,27,python|tensorflow|keras,13415,59.9104,,5,disable printing report epoch kera epoch printout like use build epoch would like disable printout print something use tensorflow backend matter
799,799,52866333,PyTorch: new_ones vs ones,"<p>In PyTorch what is the difference between <code>new_ones()</code> vs <code>ones()</code>. For example,</p>

<pre><code>x2.new_ones(3,2, dtype=torch.double)
</code></pre>

<p>vs </p>

<pre><code>torch.ones(3,2, dtype=torch.double)
</code></pre>",,2,1,,2018/10/18 2:58,5,2020/3/23 8:28,2018/10/20 5:27,,5352399,,4497662,,1,22,pytorch,6562,53.0681,,3,pytorch new one v one pytorch difference v example v
31,31,54268029,How to convert a pytorch tensor into a numpy array?,"<p>I have a torch tensor</p>

<pre><code>a = torch.randn(1, 2, 3, 4, 5)
</code></pre>

<p><strong>How can I get it in numpy?</strong></p>

<p>Something like</p>

<pre><code>b = a.tonumpy()
</code></pre>

<p>output should be the same as if I did</p>

<pre><code>b = np.random.randn(1, 2, 3, 4, 5)
</code></pre>",54268111,6,0,,2019/1/19 14:21,3,2021/3/29 13:41,2019/1/22 9:00,,1403604,,913098,,1,20,python|numpy|pytorch,36601,91.254,,3,convert pytorch tensor numpy array torch tensor get numpy something like output
43,43,54754153,Autograd.grad() for Tensor in pytorch,"<p>I want to compute the gradient between two tensors in a net. The input X tensor (batch size x m) is sent through a set of convolutional layers which give me back and output Y tensor(batch size x n).</p>
<p>I闂備胶鍋ㄩ崕鏌ユ偘?creating a new loss and I would like to know the gradient of Y w.r.t. X. Something that in tensorflow would be like:</p>
<p><code>tf.gradients(ys=Y, xs=X)</code></p>
<p>Unfortunately, I闂備胶鍋ㄩ崕鏌ユ偘閻?been making tests with <code>torch.autograd.grad()</code>, but I could not figure out how to do it. I get errors like: <code>闂備胶鍋ㄩ崕鏌ユ偟閹櫞Timeerror: grad can be implicitly created only for scalar outputs闂?/code>.</p>
<p>What should be the inputs in <code>torch.autograd.grad()</code> if I want to know the gradient of Y w.r.t. X?</p>",54757383,1,1,,2019/2/18 19:32,10,2021/1/24 17:24,2021/1/24 17:24,,4050510,,6879130,,1,13,pytorch|autograd,13609,50.1353,,4,autograd grad tensor pytorch want compute gradient two tensor net input x tensor batch size x send set convolutional layer give back output tensor batch size x n create new loss would like know gradient w r x something tensorflow would like unfortunately make test could figure get error like input want know gradient w r x
231,231,44273249,"In Keras, what exactly am I configuring when I create a stateful `LSTM` layer with N `units`?","<p>The first arguments in a normal <code>Dense</code> layer is also <code>units</code>, and is the number of neurons/nodes in that layer. A standard LSTM unit however looks like the following:</p>

<p><a href=""https://i.stack.imgur.com/aTDpS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aTDpS.png"" alt=""enter image description here""></a></p>

<p>(This is a reworked version of ""<a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">Understanding LSTM Networks</a>"")</p>

<p>In Keras, when I create an LSTM object like this <code>LSTM(units=N, ...)</code>, am I actually creating <code>N</code> of these LSTM units? Or is it the size of the ""Neural Network"" layers inside the LSTM unit, i.e., the <code>W</code>'s in the formulas? Or is it something else?</p>

<p>For context, I'm working based on <a href=""https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py"" rel=""noreferrer"">this example code</a>.</p>

<p>The following is the documentation: <a href=""https://keras.io/layers/recurrent/"" rel=""noreferrer"">https://keras.io/layers/recurrent/</a></p>

<p>It says:</p>

<blockquote>
  <p>units: Positive integer, dimensionality of the output space.</p>
</blockquote>

<p>It makes me think it is the number of outputs from the Keras LSTM ""layer"" object. Meaning the next layer will have <code>N</code> inputs. Does that mean there actually exists <code>N</code> of these LSTM units in the LSTM layer, or maybe that that exactly <em>one</em> LSTM unit is run for <code>N</code> iterations outputting <code>N</code> of these <code>h[t]</code> values, from, say, <code>h[t-N]</code> up to <code>h[t]</code>?</p>

<p>If it only defines the number of outputs, does that mean the input still can be, say, just <em>one</em>, or do we have to manually create lagging input variables <code>x[t-N]</code> to <code>x[t]</code>, one for each LSTM unit defined by the <code>units=N</code> argument?</p>

<p>As I'm writing this it occurs to me what the argument <code>return_sequences</code> does. If set to <code>True</code> all the <code>N</code> outputs are passed forward to the next layer, while if it is set to <code>False</code> it only passes the last <code>h[t]</code> output to the next layer. Am I right?</p>",44277785,4,3,,2017/5/30 23:05,35,2020/9/21 18:39,2017/5/30 23:11,,604048,,604048,,1,65,tensorflow|neural-network|keras|lstm,29613,137.886,,3,kera exactly configuring create stateful lstm layer n unit first argument normal layer also number neuron node layer standard lstm unit however look like follow reworked version understand lstm network kera create lstm object like actually create lstm unit size neural network layer inside lstm unit e formula something else context work base example code following documentation say unit positive integer dimensionality output space make think number output kera lstm layer object mean next layer input mean actually exist lstm unit lstm layer maybe exactly one lstm unit run iteration output value say define number output mean input still say one manually create lag input variable one lstm unit define argument write occur argument set true n output pass forward next layer set false pass last h output next layer right
140,140,42479902,"How does the ""view"" method work in PyTorch?","<p>I am confused about the method <code>view()</code> in the following code snippet.</p>

<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool  = nn.MaxPool2d(2,2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1   = nn.Linear(16*5*5, 120)
        self.fc2   = nn.Linear(120, 84)
        self.fc3   = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16*5*5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
</code></pre>

<p>My confusion is regarding the following line.</p>

<pre><code>x = x.view(-1, 16*5*5)
</code></pre>

<p>What does <code>tensor.view()</code> function do? I have seen its usage in many places, but I can't understand how it interprets its parameters. </p>

<p>What happens if I give negative values as parameters to the <code>view()</code> function? For example, what happens if I call, <code>tensor_variable.view(1, 1, -1)</code>?</p>

<p>Can anyone explain the main principle of <code>view()</code> function with some examples?</p>",42482819,9,0,,2017/2/27 7:21,84,2021/9/4 9:34,2019/6/12 5:33,,1569237,,5352399,,1,268,python|memory|pytorch|torch|tensor,192437,969.537,,3,view method work pytorch confuse method following code snippet confusion regard following line function see usage many place understand interpret parameter happen give negative value parameter function example happen call anyone explain main principle function example
139,139,42475381,Add dropout layers between pretrained dense layers in keras,"<p>In <code>keras.applications</code>, there is a VGG16 model pre-trained on imagenet.</p>

<pre><code>from keras.applications import VGG16
model = VGG16(weights='imagenet')
</code></pre>

<p>This model has the following structure.</p>

<pre><code>
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 3, 224, 224)   0                                            
____________________________________________________________________________________________________
block1_conv1 (Convolution2D)     (None, 64, 224, 224)  1792        input_1[0][0]                    
____________________________________________________________________________________________________
block1_conv2 (Convolution2D)     (None, 64, 224, 224)  36928       block1_conv1[0][0]               
____________________________________________________________________________________________________
block1_pool (MaxPooling2D)       (None, 64, 112, 112)  0           block1_conv2[0][0]               
____________________________________________________________________________________________________
block2_conv1 (Convolution2D)     (None, 128, 112, 112) 73856       block1_pool[0][0]                
____________________________________________________________________________________________________
block2_conv2 (Convolution2D)     (None, 128, 112, 112) 147584      block2_conv1[0][0]               
____________________________________________________________________________________________________
block2_pool (MaxPooling2D)       (None, 128, 56, 56)   0           block2_conv2[0][0]               
____________________________________________________________________________________________________
block3_conv1 (Convolution2D)     (None, 256, 56, 56)   295168      block2_pool[0][0]                
____________________________________________________________________________________________________
block3_conv2 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv1[0][0]               
____________________________________________________________________________________________________
block3_conv3 (Convolution2D)     (None, 256, 56, 56)   590080      block3_conv2[0][0]               
____________________________________________________________________________________________________
block3_pool (MaxPooling2D)       (None, 256, 28, 28)   0           block3_conv3[0][0]               
____________________________________________________________________________________________________
block4_conv1 (Convolution2D)     (None, 512, 28, 28)   1180160     block3_pool[0][0]                
____________________________________________________________________________________________________
block4_conv2 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv1[0][0]               
____________________________________________________________________________________________________
block4_conv3 (Convolution2D)     (None, 512, 28, 28)   2359808     block4_conv2[0][0]               
____________________________________________________________________________________________________
block4_pool (MaxPooling2D)       (None, 512, 14, 14)   0           block4_conv3[0][0]               
____________________________________________________________________________________________________
block5_conv1 (Convolution2D)     (None, 512, 14, 14)   2359808     block4_pool[0][0]                
____________________________________________________________________________________________________
block5_conv2 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv1[0][0]               
____________________________________________________________________________________________________
block5_conv3 (Convolution2D)     (None, 512, 14, 14)   2359808     block5_conv2[0][0]               
____________________________________________________________________________________________________
block5_pool (MaxPooling2D)       (None, 512, 7, 7)     0           block5_conv3[0][0]               
____________________________________________________________________________________________________
flatten (Flatten)                (None, 25088)         0           block5_pool[0][0]                
____________________________________________________________________________________________________
fc1 (Dense)                      (None, 4096)          102764544   flatten[0][0]                    
____________________________________________________________________________________________________
fc2 (Dense)                      (None, 4096)          16781312    fc1[0][0]                        
____________________________________________________________________________________________________
predictions (Dense)              (None, 1000)          4097000     fc2[0][0]                        
====================================================================================================
Total params: 138,357,544
Trainable params: 138,357,544
Non-trainable params: 0
____________________________________________________________________________________________________

</code></pre>

<p>I would like to fine-tune this model with dropout layers between the dense layers (fc1, fc2 and predictions), while keeping all the pre-trained weights of the model intact. I know it's possible to access each layer individually with <code>model.layers</code>, but I haven't found anywhere how to add new layers between the existing layers.</p>

<p>What's the best practice of doing this?</p>",42476116,2,0,,2017/2/26 23:08,15,2020/3/6 21:45,2019/9/24 13:47,,1647057,,1647057,,1,35,python|keras,13962,83.5798,,3,add dropout layer pretrained dense layer kera vgg model pre train imagenet model following structure would like fine tune model dropout layer dense layer fc fc prediction keep pre train weight model intact know possible access layer individually find anywhere add new layer exist layer best practice
440,440,48127550,Early stopping with Keras and sklearn GridSearchCV cross-validation,"<p>I wish to implement early stopping with Keras and sklean's <code>GridSearchCV</code>.</p>

<p>The working code example below is modified from <a href=""https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"" rel=""noreferrer"">How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras</a>. The data set may be <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data"" rel=""noreferrer"">downloaded from here</a>.</p>

<p>The modification adds the Keras <code>EarlyStopping</code> callback class to prevent over-fitting. For this to be effective it requires the <code>monitor='val_acc'</code> argument for monitoring validation accuracy. For <code>val_acc</code> to be available <code>KerasClassifier</code> requires the <code>validation_split=0.1</code> to generate validation accuracy, else <code>EarlyStopping</code> raises <code>RuntimeWarning: Early stopping requires val_acc available!</code>. Note the <code>FIXME:</code> code comment!</p>

<p>Note we could replace <code>val_acc</code> by <code>val_loss</code>! </p>

<p><strong>Question:</strong> How can I use the cross-validation data set generated by the <code>GridSearchCV</code> k-fold algorithm instead of wasting 10% of the training data for an early stopping validation set? </p>

<pre class=""lang-python prettyprint-override""><code># Use scikit-learn to grid search the learning rate and momentum
import numpy
from sklearn.model_selection import GridSearchCV
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.optimizers import SGD

# Function to create model, required for KerasClassifier
def create_model(learn_rate=0.01, momentum=0):
    # create model
    model = Sequential()
    model.add(Dense(12, input_dim=8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    optimizer = SGD(lr=learn_rate, momentum=momentum)
    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Early stopping
from keras.callbacks import EarlyStopping
stopper = EarlyStopping(monitor='val_acc', patience=3, verbose=1)

# fix random seed for reproducibility
seed = 7
numpy.random.seed(seed)
# load dataset
dataset = numpy.loadtxt(""pima-indians-diabetes.csv"", delimiter="","")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
# create model
model = KerasClassifier(
    build_fn=create_model,
    epochs=100, batch_size=10,
    validation_split=0.1, # FIXME: Instead use GridSearchCV k-fold validation data.
    verbose=2)
# define the grid search parameters
learn_rate = [0.01, 0.1]
momentum = [0.2, 0.4]
param_grid = dict(learn_rate=learn_rate, momentum=momentum)
grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2, n_jobs=1)

# Fitting parameters
fit_params = dict(callbacks=[stopper])
# Grid search.
grid_result = grid.fit(X, Y, **fit_params)

# summarize results
print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(""%f (%f) with: %r"" % (mean, stdev, param))
</code></pre>",48139341,3,1,,2018/1/6 12:54,15,2021/4/7 21:13,2018/1/7 21:07,,4685471,,3744784,,1,31,machine-learning|scikit-learn|keras|cross-validation|grid-search,9624,69.5334,,4,early stop kera sklearn gridsearchcv cross validation wish implement early stopping kera sklean work code example modify grid search hyperparameters deep learning model python kera data set may download modification add kera callback class prevent fit effective require argument monitor validation accuracy available require generate validation accuracy else raise note code comment note could replace question use cross validation data set generate k fold algorithm instead wasting training data early stopping validation set
662,662,37571514,Regularization for LSTM in tensorflow,"<p>Tensorflow offers a nice LSTM wrapper.</p>

<pre><code>rnn_cell.BasicLSTM(num_units, forget_bias=1.0, input_size=None,
           state_is_tuple=False, activation=tanh)
</code></pre>

<p>I would like to use regularization, say L2 regularization. However, I don't have direct access to the different weight matrices used in the LSTM cell, so I cannot explicitly do something like</p>

<pre><code>loss = something + beta * tf.reduce_sum(tf.nn.l2_loss(weights))
</code></pre>

<p>Is there a way to access the matrices or use regularization somehow with LSTM?</p>",37572186,3,1,,2016/6/1 14:26,5,2017/3/31 18:34,,,,,6246880,,1,14,neural-network|tensorflow|lstm|recurrent-neural-network,12318,50.7622,,3,regularization lstm tensorflow tensorflow offer nice lstm wrapper would like use regularization say l regularization however direct access different weight matrix use lstm cell explicitly something like way access matrix use regularization somehow lstm
490,490,31253870,Caffe: opencv error,"<p>I've built opencv 3.0 from source and can run a few sample apps, build against the headers ok so I presume it's installed successfully. </p>

<p>I'm also using python3 and I now go to install and build caffe. I set a few variables in Makefile.config as I'm using the CPU due to having an AMD GPU and also Anaconda.</p>

<p>When I run make all I get this error:</p>

<pre><code>$ make all
CXX/LD -o .build_release/examples/cpp_classification/classification.bin
/usr/bin/ld: .build_release/examples/cpp_classification/classification.o: undefined reference to symbol '_ZN2cv6imreadERKNS_6StringEi'
//usr/local/lib/libopencv_imgcodecs.so.3.0: error adding symbols: DSO missing from command line
collect2: error: ld returned 1 exit status
Makefile:565: recipe for target '.build_release/examples/cpp_classification/classification.bin' failed
make: *** [.build_release/examples/cpp_classification/classification.bin] Error 1
</code></pre>

<p>from searching I think this is something to do with using openCV 3 but I'm not sure where to start looking for a solution. Any help?</p>

<p>And yes I'm one of the horde of inexperienced users looking to fiddle with the Google Inception learning technique.</p>",,6,0,,2015/7/6 19:17,3,2019/11/10 20:59,,,,,1561108,,1,7,c++|opencv|python-3.x|opencv3.0|caffe,15770,51.1913,,1,caffe opencv error build opencv source run sample apps build header ok presume instal successfully also use python go install build caffe set variable makefile config use cpu due amd gpu also anaconda run make get error search think something use opencv sure start look solution help yes one horde inexperienced user look fiddle google inception learning technique
450,450,48265926,Keras: find out the number of layers,"<p>Is there a way to get the number of layers (not parameters) in a Keras model?</p>

<p><code>model.summary()</code> is very informative, but it is not straightforward to get the number of layers from it.</p>",48267194,2,0,,2018/1/15 15:25,,2021/8/19 6:41,2018/12/15 23:49,,712995,,673592,,1,23,python|machine-learning|keras|deep-learning|keras-layer,14188,65.8077,,3,kera find number layer way get number layer parameter keras model informative straightforward get number layer
81,81,9782071,Why must a nonlinear activation function be used in a backpropagation neural network?,"<p>I've been reading some things on neural networks and I understand the general principle of a single layer neural network. I understand the need for aditional layers, but why are nonlinear activation functions used?</p>

<p>This question is followed by this one: <a href=""https://stackoverflow.com/questions/9785754/what-is-a-derivative-of-the-activation-function-used-for-in-backpropagation"">What is a derivative of the activation function used for in backpropagation?</a></p>",9783865,13,0,,2012/3/20 6:06,85,2020/8/15 17:59,2017/8/4 19:18,,66549,,924313,,1,160,math|machine-learning|neural-network|deep-learning,92483,745.864,2020/8/16 0:28,0,must nonlinear activation function use backpropagation neural network read thing neural network understand general principle single layer neural network understand need aditional layer nonlinear activation function use question follow one derivative activation function use backpropagation
276,276,44856691,How do you alter the size of a Pytorch Dataset?,"<p>Say I am loading MNIST from torchvision.datasets.MNIST, but I only want to load in 10000 images total, how would I slice the data to limit it to only some number of data points?  I understand that the DataLoader is a generator yielding data in the size of the specified batch size, but how do you slice datasets?</p>

<pre><code>tr = datasets.MNIST('../data', train=True, download=True, transform=transform)
te = datasets.MNIST('../data', train=False, transform=transform)
train_loader = DataLoader(tr, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)
test_loader = DataLoader(te, batch_size=args.batch_size, shuffle=True, num_workers=4, **kwargs)
</code></pre>",44982812,3,0,,2017/7/1 1:52,2,2021/3/15 13:59,,,,,3993741,,1,14,python|machine-learning|dataset|torch|pytorch,10594,54.5002,,2,alter size pytorch dataset say load mnist torchvision datasets mnist want load image total would slice data limit number data point understand dataloader generator yield data size specified batch size slice datasets
137,137,42415909,Initializing LSTM hidden state Tensorflow/Keras,"<p>Can someone explain how can I initialize hidden state of LSTM in tensorflow? I am trying to build LSTM recurrent auto-encoder, so after i have that model trained i want to transfer learned hidden state of unsupervised model to hidden state of supervised model.
Is that even possible with current API?
This is paper I am trying to recreate:</p>

<p><a href=""http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf"" rel=""noreferrer"">http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning.pdf</a></p>",42420389,4,0,,2017/2/23 12:34,10,2020/3/11 10:00,2017/7/10 9:11,,5974433,,3022072,,1,14,tensorflow|neural-network|deep-learning|keras|lstm,15792,52.9937,,3,initialize lstm hidden state tensorflow kera someone explain initialize hidden state lstm tensorflow try build lstm recurrent auto encoder model train want transfer learned hidden state unsupervised model hide state supervised model even possible current api paper try recreate
451,451,48274929,"Pytorch - RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed","<p>I keep running into this error:</p>

<blockquote>
  <p>RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.</p>
</blockquote>

<p>I had searched in Pytorch forum, but still can闂備胶鍋ㄩ崕鏌ユ偘?find out what I have done wrong in my custom loss function. My model is nn.GRU, and here is my custom loss function:</p>

<pre><code>def _loss(outputs, session, items):  # `items` is a dict() contains embedding of all items
    def f(output, target):
        pos = torch.from_numpy(np.array([items[target[""click""]]])).float()
        neg = torch.from_numpy(np.array([items[idx] for idx in target[""suggest_list""] if idx != target[""click""]])).float()
        if USE_CUDA:
            pos, neg = pos.cuda(), neg.cuda()
        pos, neg = Variable(pos), Variable(neg)

        pos = F.cosine_similarity(output, pos)
        if neg.size()[0] == 0:
            return torch.mean(F.logsigmoid(pos))
        neg = F.cosine_similarity(output.expand_as(neg), neg)

        return torch.mean(F.logsigmoid(pos - neg))

    loss = map(f, outputs, session)
return -torch.mean(torch.cat(loss))
</code></pre>

<p>Training code:</p>

<pre><code>    # zero the parameter gradients
    model.zero_grad()

    # forward + backward + optimize
    outputs, hidden = model(inputs, hidden)
    loss = _loss(outputs, session, items)
    acc_loss += loss.data[0]

    loss.backward()
    # Add parameters' gradients to their values, multiplied by learning rate
    for p in model.parameters():
        p.data.add_(-learning_rate, p.grad.data)
</code></pre>",48278089,1,0,,2018/1/16 5:57,9,2020/7/18 13:16,2020/7/18 13:16,,4228275,,4728650,,1,38,python|deep-learning|pytorch|recurrent-neural-network|autograd,31724,69.6056,,4,pytorch runtimeerror try backward graph second time buffer already free keep run error runtimeerror try backward graph second time buffer already free specify retain graph true call backward first time search pytorch forum still find wrong custom loss function model nn gru custom loss function training code
398,398,47118678,Difference between Fasttext .vec and .bin file,"<p>I recently downloaded fasttext pretrained model for english. I got two files:</p>

<ol>
<li>wiki.en.vec</li>
<li>wiki.en.bin</li>
</ol>

<p>I am not sure what is the difference between the two files?</p>",49439794,2,0,,2017/11/5 6:03,4,2018/3/22 23:01,,,,,2600357,,1,22,python|nlp|deep-learning|word2vec|fasttext,10657,62.9105,,3,difference fasttext vec bin file recently download fasttext pretrained model english get two file wiki en vec wiki en bin sure difference two file
494,494,31774953,"Test labels for regression caffe, float not allowed?","<p>I am doing regression using caffe, and my <code>test.txt</code> and <code>train.txt</code> files are like this:</p>

<pre><code>/home/foo/caffe/data/finetune/flickr/3860781056.jpg 2.0  
/home/foo/caffe/data/finetune/flickr/4559004485.jpg 3.6  
/home/foo/caffe/data/finetune/flickr/3208038920.jpg 3.2  
/home/foo/caffe/data/finetune/flickr/6170430622.jpg 4.0  
/home/foo/caffe/data/finetune/flickr/7508671542.jpg 2.7272
</code></pre>

<p>My problem is it seems caffe does not allow float labels like 2.0, when I use float labels while reading, for example the <code>'test.txt'</code> file caffe only
recognizes </p>

<blockquote>
  <p>a total of 1 images</p>
</blockquote>

<p>which is wrong.</p>

<p>But when I for example change the 2.0 to 2 in the file and the following lines same, caffe now gives </p>

<blockquote>
  <p>a total of 2 images</p>
</blockquote>

<p>implying that the float labels are responsible for the problem.</p>

<p>Can anyone help me here, to solve this problem, I definitely need to use float labels for regression, so does anyone know about a work around or solution for this? Thanks in advance.</p>

<p><b>EDIT</b>
For anyone facing a similar issue <a href=""https://stackoverflow.com/questions/31617486/use-caffe-to-train-lenet-with-csv-data"">use caffe to train Lenet with CSV data</a> might be of help. Thanks to @Shai.</p>",31808324,4,1,,2015/8/2 18:05,9,2018/3/2 12:17,2017/5/23 12:10,,-1,,3698878,,1,12,neural-network|regression|deep-learning|caffe,10861,55.7435,,4,test label regression caffe float allow regression use caffe file like problem seem caffe allow float label like use float label read example file caffe recognize total image wrong example change file following line caffe give total image imply float label responsible problem anyone help solve problem definitely need use float label regression anyone know work around solution thanks advance edit anyone face similar issue use caffe train lenet csv data might help thanks shai
468,468,58878421,Unexpected keyword argument 'ragged' in Keras,"<p>Trying to run a trained keras model with the following python code:</p>

<pre class=""lang-py prettyprint-override""><code>from keras.preprocessing.image import img_to_array
from keras.models import load_model

from imutils.video import VideoStream
from threading import Thread
import numpy as np
import imutils
import time
import cv2
import os

MODEL_PATH = ""/home/pi/Documents/converted_keras/keras_model.h5""

print(""[info] loading model.."")
model = load_model(MODEL_PATH)


print(""[info] starting vid stream.."")
vs = VideoStream(usePiCamera=True).start()
time.sleep(2.0)

while True:
    frame = vs.Read()
    frame = imutils.resize(frame, width=400)

    image = cv2.resize(frame, (28, 28))
    image = image.astype(""float"") / 255.0
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    (fuel, redBall, whiteBall, none) = model.predict(image)[0]
    label = ""none""
    proba = none

    if fuel &gt; none and fuel &gt; redBall and fuel &gt; whiteBall:
        label = ""Fuel""
        proba = fuel
    elif redBall &gt; none and redBall &gt; fuel and redBall &gt; whiteBall:
        label = ""Red Ball""
        proba = redBall
    elif whiteBall &gt; none and whiteBall &gt; redBall and whiteBall &gt; fuel:
        label = ""white ball""
        proba = whiteBall
    else:
        label = ""none""
        proba = none

    label = ""{}:{:.2f%}"".format(label, proba * 100)
    frame = cv2.putText(frame, label, (10, 25),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
    cv2.imshow(""Frame"", frame)
    key = cv2.waitKey(1) &amp; 0xFF

    if key == ord(""q""):
        break

print(""[info] cleaning up.."")
cv2.destroyAllWindows()
vs.stop()
</code></pre>

<p>When I run it with python3, I get the following error:
<code>TypeError: __init__() got an unexpected keyword argument 'ragged'</code></p>

<p>What's causing the error, and how do I get around it? </p>

<p>Versions:
Keras v2.3.1
tensorflow v1.13.1</p>

<p>Edit to add:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/pi/Documents/converted_keras/keras-script.py"", line 18, in &lt;module&gt;
    model = load_model(MODEL_PATH)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py"", line 492, in load_wrapper
    return load_function(*args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py"", line 584, in load_model
    model = _deserialize_model(h5dict, custom_objects, compile)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py"", line 274, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py"", line 627, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py"", line 147, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py"", line 301, in from_config
    custom_objects=custom_objects)
  File ""/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py"", line 147, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py"", line 301, in from_config
    custom_objects=custom_objects)
  File ""/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py"", line 147, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/network.py"", line 1056, in from_config
    process_layer(layer_data)
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/network.py"", line 1042, in process_layer
    custom_objects=custom_objects)
  File ""/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py"", line 168, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py"", line 149, in deserialize_keras_object
    return cls.from_config(config['config'])
  File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 1179, in from_config
    return cls(**config)
  File ""/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'ragged'
</code></pre>

<p><a href=""https://drive.google.com/file/d/1-8ADI40ujjmcLpv-Shn9b5qhvIZNqYcn/view?usp=sharing"" rel=""noreferrer"">h5 file link (google drive)</a></p>",58881339,2,11,,2019/11/15 13:50,6,2020/11/4 21:53,2019/11/15 14:26,,12378777,,12378777,,1,30,python|tensorflow|keras,23654,97.4956,,4,unexpected keyword argument rag kera try run trained kera model follow python code run python get following error cause error get around versions kera v tensorflow v edit add h file link google drive
583,583,49407303,RuntimeError: Expected object of type torch.DoubleTensor but found type torch.FloatTensor for argument #2 'weight',"<p>My input tensor is torch.DoubleTensor type. But I got the RuntimeError below:</p>

<pre><code>RuntimeError: Expected object of type torch.DoubleTensor but found type torch.FloatTensor for argument #2 'weight'
</code></pre>

<p>I didn't specify the type of the weight explicitly(i.e. I did not init my weight by myself. The weight is created by pytorch). What will influence the type of weight in the forward process?</p>

<p>Thanks a lot!! </p>",49432639,2,1,,2018/3/21 13:13,3,2018/12/8 7:21,,,,,8464665,,1,32,pytorch,33376,71.8937,,4,runtimeerror expect object type torch doubletensor find type torch floattensor argument weight input tensor torch doubletensor type get runtimeerror specify type weight explicitly e init weight weight create pytorch influence type weight forward process thank lot
749,749,41147734,looking for source code of from gen_nn_ops in tensorflow,"<p>I am new to tensorflow for deep learning and interested in deconvolution (convolution transpose) operation in tensorflow. I need to take a look at the source code for operating deconvolution. The function is I guess <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1007"" rel=""noreferrer"">conv2d_transpose() in nn_ops.py</a>.</p>

<p>However, in the function it calls another function called <code>gen_nn_ops.conv2d_backprop_input()</code>. I need to take a look at what is inside this function, but I am unable to find it in the repository. Any help would be appreciated.</p>",,4,0,,2016/12/14 16:40,9,2021/1/31 16:02,2016/12/14 20:44,,1951176,,7297756,,1,37,python|tensorflow|deep-learning|deconvolution,12595,106.001,,3,look source code gen nn ops tensorflow new tensorflow deep learning interested deconvolution convolution transpose operation tensorflow need take look source code operate deconvolution function guess conv transpose nn ops py however function call another function call need take look inside function unable find repository help would appreciate
455,455,48343857,What's the reason of the error ValueError: Expected more than 1 value per channel?,"<p><a href=""http://forums.fast.ai/t/how-do-we-use-our-model-against-a-specific-image/7661/50"" rel=""noreferrer"">reference fast.ai</a></p>

<p><a href=""https://github.com/fastai/fastai/tree/master/fastai"" rel=""noreferrer"">github repository of fast.ai</a>
(as the code elevates the library which is built on top of <strong>PyTorch</strong>)</p>

<blockquote>
  <p>Please scroll the discussion a bit</p>
</blockquote>

<p>I am running the following code, and get an error while trying to pass the data to the predict_array function</p>

<p>The code is failing when i am trying to use it to predict directly on a single image but it run's perfectly when that same image is in a <code>test</code> folder</p>

<pre><code>from fastai.conv_learner import *
from planet import f2

PATH = 'data/shopstyle/'

metrics=[f2]
f_model = resnet34

def get_data(sz):
    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, max_zoom=1.05)
    return ImageClassifierData.from_csv(PATH, 'train', label_csv, tfms=tfms, suffix='.jpg', val_idxs=val_idxs, test_name='test')

def print_list(list_or_iterator):
        return ""["" + "", "".join( str(x) for x in list_or_iterator) + ""]""

label_csv = f'{PATH}prod_train.csv'
n = len(list(open(label_csv)))-1
val_idxs = get_cv_idxs(n)

sz = 64
data = get_data(sz)

print(""Loading model..."")
learn = ConvLearner.pretrained(f_model, data, metrics=metrics)
learn.load(f'{sz}')
#learn.load(""tmp"")

print(""Predicting..."")
learn.precompute=False
trn_tfms, val_tfrms = tfms_from_model(f_model, sz)
#im = val_tfrms(open_image(f'{PATH}valid/4500132.jpg'))
im = val_tfrms(np.array(PIL.Image.open(f'{PATH}valid/4500132.jpg')))
preds = learn.predict_array(im[None])
p=list(zip(data.classes, preds))
print(""predictions = "" + print_list(p))
</code></pre>

<p><strong>Here's the Traceback I am Getting</strong></p>

<pre><code>  Traceback (most recent call last):
  File ""predict.py"", line 34, in &lt;module&gt;
    preds = learn.predict_array(im[None])
  File ""/home/ubuntu/fastai/courses/dl1/fastai/learner.py"", line 266, in predict_array
    def predict_array(self, arr): return to_np(self.model(V(T(arr).cuda())))
  File ""/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 67, in forward
    input = module(input)
  File ""/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 325, in __call__
    result = self.forward(*input, **kwargs)
  File ""/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py"", line 37, in forward
    self.training, self.momentum, self.eps)
  File ""/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py"", line 1011, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size [1, 1024]
</code></pre>

<p><strong>Things I have Tried</strong></p>

<ul>
<li><p><code>np.expand_dims(IMG,axis=0) or image = image[..., np.newaxis]</code></p></li>
<li><p>Tried a different way of reading the image</p>

<pre><code>img = cv2.imread(img_path)
img = cv2.resize(img, dsize = (200,200))
img = np.einsum('ijk-&gt;kij', img)
img = np.expand_dims(img, axis =0) 
img = torch.from_numpy(img) 
learn.model(Variable(img.float()).cuda())
</code></pre></li>
</ul>

<p>BTW the error still remains </p>

<pre><code>ValueError: Expected more than 1 value per channel when training, got input size [1, 1024]
</code></pre>

<p>Can't find any <strong>reference</strong> in The <em>Google search</em> also..</p>",48344268,3,0,,2018/1/19 14:58,1,2021/6/23 6:11,2019/4/28 14:59,,6296561,,6524169,,1,9,python|torch|pytorch,8759,65.1698,,4,reason error valueerror expect value per channel reference fast ai github repository fast ai code elevate library build top pytorch please scroll discussion bit run following code get error try pass data predict array function code fail try use predict directly single image run perfectly image folder traceback get thing try try different way read image btw error still remain ca find reference google search also
614,614,50411191,How to compute the cosine_similarity in pytorch for all rows in a matrix with respect to all rows in another matrix,"<p>In pytorch, given that I have 2 matrixes how would I compute cosine similarity of all rows in each with all rows in the other.</p>

<p>For example </p>

<p>Given the input =</p>

<pre><code>matrix_1 = [a b] 
           [c d] 
matrix_2 = [e f] 
           [g h]
</code></pre>

<p>I would like the output to be </p>

<p>output =</p>

<pre><code> [cosine_sim([a b] [e f])  cosine_sim([a b] [g h])]
 [cosine_sim([c d] [e f])  cosine_sim([c d] [g h])] 
</code></pre>

<p>At the moment I am using torch.nn.functional.cosine_similarity(matrix_1, matrix_2) which returns the cosine of the row with only that corresponding row in the other matrix. </p>

<p>In my example I have only 2 rows, but I would like a solution which works for many rows. I would even like to handle the case where the number of rows in the each matrix is different. </p>

<p>I realize that I could use the expand, however I want to do it without using such a large memory footprint.</p>",50426321,3,3,,2018/5/18 12:18,4,2021/5/18 14:31,2018/5/18 12:30,,6401281,,6401281,,1,16,machine-learning|neural-network|pytorch,14681,57.267,,3,compute cosine similarity pytorch row matrix respect row another matrix pytorch give matrixes would compute cosine similarity row row example give input would like output output moment use torch nn functional cosine similarity matrix matrix return cosine row correspond row matrix example row would like solution work many row would even like handle case number row matrix different realize could use expand however want without use large memory footprint
250,250,44563418,Low GPU usage by Keras / Tensorflow?,"<p>I'm using keras with tensorflow backend on a computer with a nvidia Tesla K20c GPU. (CUDA 8)</p>

<p>I'm tranining a relatively simple Convolutional Neural Network, during training I run the terminal program <code>nvidia-smi</code> to check the GPU use. As you can see in the following output, the GPU utilization commonly shows around 7%-13%</p>

<p>My question is: during the CNN training shouldn't the GPU usage be higher? is this a sign of a bad GPU configuration or usage by keras/tensorflow?</p>

<p><a href=""https://i.stack.imgur.com/jcEDO.png"" rel=""noreferrer"">nvidia-smi output</a></p>

<p><a href=""https://i.stack.imgur.com/jcEDO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jcEDO.png"" alt=""enter image description here""></a></p>",,5,0,,2017/6/15 9:09,3,2021/1/30 13:21,2017/6/15 9:21,,681865,,4457749,,1,18,tensorflow|gpu|keras,18984,51.1136,,4,low gpu usage kera tensorflow use kera tensorflow backend computer nvidia tesla k c gpu cuda tranining relatively simple convolutional neural network training run terminal program check gpu use see following output gpu utilization commonly show around question cnn training gpu usage high sign bad gpu configuration usage kera tensorflow nvidia smi output
808,808,53419474,Using Dropout in Pytorch: nn.Dropout vs. F.dropout,"<p>By using pyTorch there is two ways to dropout
 <code>torch.nn.Dropout</code> and <code>torch.nn.functional.Dropout</code>.</p>

<p>I struggle to see the difference between the use of them: <br></p>

<ul>
<li>When to use what?<br></li>
<li>Does it make a difference?<br></li>
</ul>

<p>I don't see any performance difference when I switched them around.</p>",53452827,3,0,,2018/11/21 19:44,16,2019/11/23 21:16,2019/11/23 21:16,,7483494,,6224975,,1,54,neural-network|deep-learning|pytorch|dropout,71526,132.818,,3,use dropout pytorch nn dropout v f dropout use pytorch two way dropout struggle see difference use use make difference see performance difference switch around
514,514,34702041,How to tell which Keras model is better?,"<p>I don't understand which accuracy in the output to use to compare my 2 Keras models to see which one is better. </p>

<p>Do I use the ""acc"" (from the training data?) one or the ""val acc"" (from the validation data?) one?</p>

<p>There are different accs and val accs for each epoch. How do I know the acc or val acc for my model as a whole? Do I average all of the epochs accs or val accs to find the acc or val acc of the model as a whole?</p>

<p><strong>Model 1 Output</strong></p>

<pre><code>Train on 970 samples, validate on 243 samples
Epoch 1/20
0s - loss: 0.1708 - acc: 0.7990 - val_loss: 0.2143 - val_acc: 0.7325
Epoch 2/20
0s - loss: 0.1633 - acc: 0.8021 - val_loss: 0.2295 - val_acc: 0.7325
Epoch 3/20
0s - loss: 0.1657 - acc: 0.7938 - val_loss: 0.2243 - val_acc: 0.7737
Epoch 4/20
0s - loss: 0.1847 - acc: 0.7969 - val_loss: 0.2253 - val_acc: 0.7490
Epoch 5/20
0s - loss: 0.1771 - acc: 0.8062 - val_loss: 0.2402 - val_acc: 0.7407
Epoch 6/20
0s - loss: 0.1789 - acc: 0.8021 - val_loss: 0.2431 - val_acc: 0.7407
Epoch 7/20
0s - loss: 0.1789 - acc: 0.8031 - val_loss: 0.2227 - val_acc: 0.7778
Epoch 8/20
0s - loss: 0.1810 - acc: 0.8010 - val_loss: 0.2438 - val_acc: 0.7449
Epoch 9/20
0s - loss: 0.1711 - acc: 0.8134 - val_loss: 0.2365 - val_acc: 0.7490
Epoch 10/20
0s - loss: 0.1852 - acc: 0.7959 - val_loss: 0.2423 - val_acc: 0.7449
Epoch 11/20
0s - loss: 0.1889 - acc: 0.7866 - val_loss: 0.2523 - val_acc: 0.7366
Epoch 12/20
0s - loss: 0.1838 - acc: 0.8021 - val_loss: 0.2563 - val_acc: 0.7407
Epoch 13/20
0s - loss: 0.1835 - acc: 0.8041 - val_loss: 0.2560 - val_acc: 0.7325
Epoch 14/20
0s - loss: 0.1868 - acc: 0.8031 - val_loss: 0.2573 - val_acc: 0.7407
Epoch 15/20
0s - loss: 0.1829 - acc: 0.8072 - val_loss: 0.2581 - val_acc: 0.7407
Epoch 16/20
0s - loss: 0.1878 - acc: 0.8062 - val_loss: 0.2589 - val_acc: 0.7407
Epoch 17/20
0s - loss: 0.1833 - acc: 0.8072 - val_loss: 0.2613 - val_acc: 0.7366
Epoch 18/20
0s - loss: 0.1837 - acc: 0.8113 - val_loss: 0.2605 - val_acc: 0.7325
Epoch 19/20
0s - loss: 0.1906 - acc: 0.8010 - val_loss: 0.2555 - val_acc: 0.7407
Epoch 20/20
0s - loss: 0.1884 - acc: 0.8062 - val_loss: 0.2542 - val_acc: 0.7449
</code></pre>

<p><strong>Model 2 Output</strong></p>

<pre><code>Train on 970 samples, validate on 243 samples
Epoch 1/20
0s - loss: 0.1735 - acc: 0.7876 - val_loss: 0.2386 - val_acc: 0.6667
Epoch 2/20
0s - loss: 0.1733 - acc: 0.7825 - val_loss: 0.1894 - val_acc: 0.7449
Epoch 3/20
0s - loss: 0.1781 - acc: 0.7856 - val_loss: 0.2028 - val_acc: 0.7407
Epoch 4/20
0s - loss: 0.1717 - acc: 0.8021 - val_loss: 0.2545 - val_acc: 0.7119
Epoch 5/20
0s - loss: 0.1757 - acc: 0.8052 - val_loss: 0.2252 - val_acc: 0.7202
Epoch 6/20
0s - loss: 0.1776 - acc: 0.8093 - val_loss: 0.2449 - val_acc: 0.7490
Epoch 7/20
0s - loss: 0.1833 - acc: 0.7897 - val_loss: 0.2272 - val_acc: 0.7572
Epoch 8/20
0s - loss: 0.1827 - acc: 0.7928 - val_loss: 0.2376 - val_acc: 0.7531
Epoch 9/20
0s - loss: 0.1795 - acc: 0.8062 - val_loss: 0.2445 - val_acc: 0.7490
Epoch 10/20
0s - loss: 0.1746 - acc: 0.8103 - val_loss: 0.2491 - val_acc: 0.7449
Epoch 11/20
0s - loss: 0.1831 - acc: 0.8082 - val_loss: 0.2477 - val_acc: 0.7449
Epoch 12/20
0s - loss: 0.1831 - acc: 0.8113 - val_loss: 0.2496 - val_acc: 0.7490
Epoch 13/20
0s - loss: 0.1920 - acc: 0.8000 - val_loss: 0.2459 - val_acc: 0.7449
Epoch 14/20
0s - loss: 0.1945 - acc: 0.7928 - val_loss: 0.2446 - val_acc: 0.7490
Epoch 15/20
0s - loss: 0.1852 - acc: 0.7990 - val_loss: 0.2459 - val_acc: 0.7449
Epoch 16/20
0s - loss: 0.1800 - acc: 0.8062 - val_loss: 0.2495 - val_acc: 0.7449
Epoch 17/20
0s - loss: 0.1891 - acc: 0.8000 - val_loss: 0.2469 - val_acc: 0.7449
Epoch 18/20
0s - loss: 0.1891 - acc: 0.8041 - val_loss: 0.2467 - val_acc: 0.7531
Epoch 19/20
0s - loss: 0.1853 - acc: 0.8072 - val_loss: 0.2511 - val_acc: 0.7449
Epoch 20/20
0s - loss: 0.1905 - acc: 0.8062 - val_loss: 0.2460 - val_acc: 0.7531
</code></pre>",34705152,2,0,,2016/1/10 4:23,22,2016/4/1 15:00,,,,,4984897,,1,43,python|machine-learning|keras|data-science,22435,103.604,,5,tell kera model well understand accuracy output use compare kera model see one well use acc training data one val acc validation data one different acc val acc epoch know acc val acc model whole average epochs acc val acc find acc val acc model whole model output model output
537,537,36412098,Convert Keras model to TensorFlow protobuf,"<p>We're currently training various neural networks using Keras, which is ideal because it has a nice interface and is relatively easy to use, but we'd like to be able to apply them in our production environment. </p>

<p>Unfortunately the production environment is C++, so our plan is to:</p>

<ul>
<li>Use the TensorFlow backend to save the model to a protobuf</li>
<li>Link our production code to TensorFlow, and then load in the protobuf</li>
</ul>

<p>Unfortunately I don't know how to access the TensorFlow saving utilities from Keras, which normally saves to HDF5 and JSON. How do I save to protobuf?</p>",,5,1,,2016/4/4 20:16,7,2019/4/1 10:32,2016/4/4 23:33,,3574081,,915501,,1,21,c++|protocol-buffers|tensorflow|keras,9716,53.95,,3,convert kera model tensorflow protobuf currently train various neural network use kera ideal nice interface relatively easy use like able apply production environment unfortunately production environment c plan use tensorflow backend save model protobuf link production code tensorflow load protobuf unfortunately know access tensorflow save utility kera normally save hdf json save protobuf
724,724,40331510,How to stack multiple lstm in keras?,"<p>I am using deep learning library keras and trying to stack multiple LSTM with no luck.
Below is my code</p>

<pre><code>model = Sequential()
model.add(LSTM(100,input_shape =(time_steps,vector_size)))
model.add(LSTM(100))
</code></pre>

<p>The above code returns error in the third line <code>Exception: Input 0 is incompatible with layer lstm_28: expected ndim=3, found ndim=2
</code></p>

<p>The input X is a tensor of shape (100,250,50). I am running keras on tensorflow backend</p>",40336582,3,0,,2016/10/30 17:07,29,2020/9/7 11:11,,,,,2270136,,1,78,tensorflow|deep-learning|keras|lstm|keras-layer,52101,218.667,,3,stack multiple lstm kera use deep learning library kera try stack multiple lstm luck code code return error third line input x tensor shape run kera tensorflow backend
400,400,47157526,"ResNet: 100% accuracy during training, but 33% prediction accuracy with the same data","<p>I am new to machine learning and deep learning, and for learning purposes I tried to play with Resnet. I tried to overfit over small data (3 <em>different</em> images) and see if I can get almost 0 loss and 1.0 accuracy - and I did.</p>

<p>The problem is that predictions on the <strong>training</strong> images (i.e. the same 3 images used for training) are not correct..</p>

<p><strong>Training Images</strong></p>

<p><a href=""https://i.stack.imgur.com/e7rjH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/e7rjH.jpg"" alt=""image 1""></a>  <a href=""https://i.stack.imgur.com/P1uJd.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/P1uJd.jpg"" alt=""image 2""></a>
  <a href=""https://i.stack.imgur.com/JGtlL.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JGtlL.jpg"" alt=""image 3""></a></p>

<p><strong>Image labels</strong></p>

<p><code>[1,0,0]</code>, <code>[0,1,0]</code>, <code>[0,0,1]</code></p>

<p><strong>My python code</strong></p>



<pre class=""lang-python prettyprint-override""><code>#loading 3 images and resizing them
imgs = np.array([np.array(Image.open(""./Images/train/"" + fname)
                          .resize((197, 197), Image.ANTIALIAS)) for fname in
                 os.listdir(""./Images/train/"")]).reshape(-1,197,197,1)
# creating labels
y = np.array([[1,0,0],[0,1,0],[0,0,1]])
# create resnet model
model = ResNet50(input_shape=(197, 197,1),classes=3,weights=None)

# compile &amp; fit model
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])

model.fit(imgs,y,epochs=5,shuffle=True)

# predict on training data
print(model.predict(imgs))
</code></pre>

<p>The model does overfit the data:</p>

<pre class=""lang-python prettyprint-override""><code>3/3 [==============================] - 22s - loss: 1.3229 - acc: 0.0000e+00
Epoch 2/5
3/3 [==============================] - 0s - loss: 0.1474 - acc: 1.0000
Epoch 3/5
3/3 [==============================] - 0s - loss: 0.0057 - acc: 1.0000
Epoch 4/5
3/3 [==============================] - 0s - loss: 0.0107 - acc: 1.0000
Epoch 5/5
3/3 [==============================] - 0s - loss: 1.3815e-04 - acc: 1.0000
</code></pre>

<p>but predictions are:</p>

<pre class=""lang-python prettyprint-override""><code> [[  1.05677405e-08   9.99999642e-01   3.95520459e-07]
 [  1.11955103e-08   9.99999642e-01   4.14905685e-07]
 [  1.02637095e-07   9.99997497e-01   2.43751242e-06]]
</code></pre>

<p>which means that all images got <code>label=[0,1,0]</code></p>

<p>why? and how can that happen?</p>",47210349,4,13,,2017/11/7 12:03,22,2020/10/12 13:41,2017/11/7 17:08,,3169750,,3169750,,1,38,machine-learning|deep-learning|keras,17138,73.3358,,4,resnet accuracy training prediction accuracy data new machine learning deep learning learn purpose try play resnet try overfit small data different image see get almost loss accuracy problem prediction training image e image use train correct training image image label python code model overfit data prediction mean image get happen
154,154,42704283,Adding L1/L2 regularization in PyTorch?,"<p>Is there any way, I can add simple L1/L2 regularization in PyTorch? We can probably compute the regularized loss by simply adding the <code>data_loss</code> with the <code>reg_loss</code> but is there any explicit way, any support from PyTorch library to do it more easily without doing it manually?</p>",42723573,7,0,,2017/3/9 19:54,17,2021/5/31 12:30,2020/12/12 23:54,,7924573,,5352399,,1,63,python|machine-learning|pytorch|loss-function,115564,273.451,,4,add l l regularization pytorch way add simple l l regularization pytorch probably compute regularized loss simply add explicit way support pytorch library easily without manually
691,691,38714959,Understanding Keras LSTMs,"<p>I am trying to reconcile my understand of LSTMs and pointed out here in <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">this post by Christopher Olah</a> implemented in Keras. I am following the <a href=""http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"" rel=""noreferrer"">blog written by Jason Brownlee</a> for the Keras tutorial. What I am mainly confused about is,</p>
<ol>
<li>The reshaping of the data series into <code>[samples, time steps, features]</code> and,</li>
<li>The stateful LSTMs</li>
</ol>
<p>Lets concentrate on the above two questions with reference to the code pasted below:</p>
<pre><code># reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)

# reshape input to be [samples, time steps, features]
trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, 1))
testX = numpy.reshape(testX, (testX.shape[0], look_back, 1))
########################
# The IMPORTANT BIT
##########################
# create and fit the LSTM network
batch_size = 1
model = Sequential()
model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
for i in range(100):
    model.fit(trainX, trainY, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)
    model.reset_states()
</code></pre>
<p>Note: create_dataset takes a sequence of length N and returns a <code>N-look_back</code> array of which each element is a <code>look_back</code> length sequence.</p>
<h1>What is Time Steps and Features?</h1>
<p>As can be seen TrainX is a 3-D array with Time_steps and Feature being the last two dimensions respectively (3 and 1 in this particular code). With respect to the image below, does this mean that we are considering the <code>many to one</code> case, where the number of pink boxes are 3? Or does it literally mean the chain length is 3 (i.e. only 3 green boxes considered). <a href=""https://i.stack.imgur.com/kwhAP.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kwhAP.jpg"" alt=""enter image description here"" /></a></p>
<p>Does the features argument become relevant when we consider multivariate series? e.g. modelling two financial stocks simultaneously?</p>
<h1>Stateful LSTMs</h1>
<p>Does stateful LSTMs mean that we save the cell memory values between runs of batches? If this is the case, <code>batch_size</code> is one, and the memory is reset between the training runs so what was the point of saying that it was stateful. I'm guessing this is related to the fact that training data is not shuffled, but I'm not sure how.</p>
<p>Any thoughts?
Image reference: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>
<h2>Edit 1:</h2>
<p>A bit confused about @van's comment about the red and green boxes being equal. So just to confirm, does the following API calls correspond to the unrolled diagrams? Especially noting the second diagram (<code>batch_size</code> was arbitrarily chosen.):
<a href=""https://i.stack.imgur.com/sW207.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sW207.jpg"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/15V2C.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/15V2C.jpg"" alt=""enter image description here"" /></a></p>
<h2>Edit 2:</h2>
<p>For people who have done Udacity's deep learning course and still confused about the time_step argument, look at the following discussion: <a href=""https://discussions.udacity.com/t/rnn-lstm-use-implementation/163169"" rel=""noreferrer"">https://discussions.udacity.com/t/rnn-lstm-use-implementation/163169</a></p>
<h2>Update:</h2>
<p>It turns out <code>model.add(TimeDistributed(Dense(vocab_len)))</code> was what I was looking for. Here is an example: <a href=""https://github.com/sachinruk/ShakespeareBot"" rel=""noreferrer"">https://github.com/sachinruk/ShakespeareBot</a></p>
<h2>Update2:</h2>
<p>I have summarised most of my understanding of LSTMs here: <a href=""https://www.youtube.com/watch?v=ywinX5wgdEU"" rel=""noreferrer"">https://www.youtube.com/watch?v=ywinX5wgdEU</a></p>",38737941,4,8,,2016/8/2 8:04,337,2021/8/20 15:28,2020/6/20 9:12,,-1,,2530674,,1,378,python|deep-learning|keras|lstm,64333,783.634,,3,understand kera lstms try reconcile understand lstms point post christopher olah implement kera follow blog write jason brownlee kera tutorial mainly confused reshaping data series stateful lstms let concentrate two question reference code paste note create dataset take sequence length n return array element length sequence time step feature see trainx array time step feature last two dimension respectively particular code respect image mean consider case number pink box literally mean chain length e green box consider feature argument become relevant consider multivariate series e g model two financial stock simultaneously stateful lstms stateful lstms mean save cell memory value run batch case one memory reset training run point say stateful guess relate fact train data shuffle sure thought image reference edit bit confused van comment red green box equal confirm following api call correspond unrolled diagram especially note second diagram arbitrarily choose edit people udacity deep learning course still confuse time step argument look following discussion update turn look example update summarise understanding lstms
571,571,49100556,What is the use of train_on_batch() in keras?,<p>How <code>train_on_batch()</code> is different from <code>fit()</code>? What are the cases when we should use <code>train_on_batch()</code>?</p>,49100617,4,1,,2018/3/4 21:13,9,2020/2/7 9:18,2019/10/8 22:32,,3924118,,6633920,,1,42,machine-learning|deep-learning|keras,45629,122.237,,3,use train batch kera different case use
593,593,49643225,What's the difference between reshape and view in pytorch?,"<p>In numpy, we use <code>ndarray.reshape()</code> for reshaping an array.</p>

<p>I noticed that in pytorch, people use <code>torch.view(...)</code> for the same purpose, but at the same time, there is also a <code>torch.reshape(...)</code> existing.</p>

<p>So I am wondering what the differences are between them and when I should use either of them?</p>",49644300,3,0,,2018/4/4 5:09,43,2021/7/22 20:50,,,,,4794308,,1,136,pytorch,66562,309.893,,3,difference reshape view pytorch numpy use reshape array notice pytorch people use purpose time also exist wonder difference use either
687,687,38446771,Importing theano: AttributeError: 'module' object has no attribute 'find_graphviz',"<p>When I run <code>import theano</code> in Python, I get the following error message:</p>

<pre><code>Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import theano
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/theano/__init__.py"", line 74, in &lt;module&gt;
    from theano.printing import pprint, pp
  File ""/usr/local/lib/python2.7/dist-packages/theano/printing.py"", line 35, in &lt;module&gt;
    if pd.find_graphviz():
AttributeError: 'module' object has no attribute 'find_graphviz'
</code></pre>

<p>What could be the issue, and how to fix it?</p>

<p>I use Theano 0.8.2 on Ubuntu 14.04.4 LTS x64 with Python 2.7.6 x64.</p>

<hr>

<p>I unsuccessfully tried:</p>

<ul>
<li><code>sudo apt-get install -y graphviz libgraphviz-dev</code></li>
</ul>",38455189,4,0,,2016/7/18 22:12,8,2016/11/27 6:33,2016/7/18 22:17,,395857,,395857,,1,13,python|theano,16150,65.2327,,1,import theano attributeerror module object attribute find graphviz run python get following error message could issue fix use theano ubuntu lts x python x unsuccessfully try
703,703,39373230,What does TensorFlow's `conv2d_transpose()` operation do?,"<p>The documentation for the <code>conv2d_transpose()</code> operation does not clearly explain what it does:</p>

<blockquote>
  <p>The transpose of conv2d.</p>
  
  <p>This operation is sometimes called ""deconvolution"" after
  <a href=""http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf"" rel=""noreferrer"">Deconvolutional Networks</a>, but is actually the transpose (gradient) of
  conv2d rather than an actual deconvolution.</p>
</blockquote>

<p>I went through the paper that the doc points to, but it did not help.</p>

<p>What does this operation do and what are examples of why you would want to use it?</p>",39373603,6,0,,2016/9/7 14:55,22,2020/11/7 20:48,,,,,38626,,1,45,tensorflow|conv-neural-network,33019,171.075,,3,tensorflow conv transpose operation documentation operation clearly explain transpose conv operation sometimes call deconvolution deconvolutional network actually transpose gradient conv rather actual deconvolution go paper doc point help operation example would want use
789,789,52270177,How to use .predict_generator() on new Images - Keras,"<p>I've used <code>ImageDataGenerator</code> and <code>flow_from_directory</code> for training and validation.</p>

<p>These are my directories:</p>

<pre><code>train_dir = Path('D:/Datasets/Trell/images/new_images/training')
test_dir = Path('D:/Datasets/Trell/images/new_images/validation')
pred_dir = Path('D:/Datasets/Trell/images/new_images/testing')
</code></pre>

<p>ImageGenerator Code:</p>

<pre><code>img_width, img_height = 28, 28
batch_size=32
train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')
</code></pre>

<blockquote>
  <p>Found 1852 images belonging to 4 classes</p>
  
  <p>Found 115 images belonging to 4 classes</p>
</blockquote>

<p>This is my model training code:</p>

<pre><code>history = cnn.fit_generator(
        train_generator,
        steps_per_epoch=1852 // batch_size,
        epochs=20,
        validation_data=validation_generator,
        validation_steps=115 // batch_size)
</code></pre>

<p>Now I have some new images in a test folder (all images are inside the same folder only), on which I want to predict. But when I use <code>.predict_generator</code> I get:</p>

<blockquote>
  <p>Found 0 images belonging to 0 class</p>
</blockquote>

<p>So I tried these solutions:</p>

<p>1) <a href=""https://stackoverflow.com/questions/45806669/keras-how-to-use-predict-generator-with-imagedatagenerator"">Keras: How to use predict_generator with ImageDataGenerator?</a> This didn't work out, because its trying on validation set only.</p>

<p>2) <a href=""https://github.com/keras-team/keras/issues/6993"" rel=""noreferrer"">How to predict the new image by using model.predict?</a> <code>module image not found</code></p>

<p>3) <a href=""https://datascience.stackexchange.com/questions/13894/how-to-get-predictions-with-predict-generator-on-streaming-test-data-in-keras"">How to get predictions with predict_generator on streaming test data in Keras?</a> This also didn't work out.</p>

<p>My train data is basically stored in 4 separate folders, i.e. 4 specific classes, validation also stored  in same way and works out pretty well.</p>

<p>So in my test folder I have around 300 images, on which I want to predict and make a dataframe, like this:</p>

<pre><code>image_name    class
gghh.jpg       1
rrtq.png       2
1113.jpg       1
44rf.jpg       4
tyug.png       1
ssgh.jpg       3
</code></pre>

<p>I have also used this following code:</p>

<pre><code>img = image.load_img(pred_dir, target_size=(28, 28))
img_tensor = image.img_to_array(img)
img_tensor = np.expand_dims(img_tensor, axis=0)
img_tensor /= 255.

cnn.predict(img_tensor)
</code></pre>

<p>But I get this error: <code>[Errno 13] Permission denied: 'D:\\Datasets\\Trell\\images\\new_images\\testing'</code></p>

<p>But I haven't been able to <code>predict_generator</code> on my test images. So how can I predict on my new images using Keras. I have googled a lot, searched on Kaggle Kernels also but haven't been able to get a solution.</p>",52273258,5,0,,2018/9/11 6:47,6,2021/4/26 14:44,2018/9/11 7:45,,9997666,,9997666,,1,15,python|pandas|image-processing|keras,23789,76.5055,,2,use predict generator new image keras use training validation directory imagegenerator code find image belong class find image belong class model train code new image test folder image inside folder want predict use get find image belong class try solution keras use predict generator imagedatagenerator work try validation set predict new image use model predict get prediction predict generator stream test data kera also work train data basically store separate folder e specific class validation also store way work pretty well test folder around image want predict make dataframe like also use following code get error able test image predict new image use kera google lot search kaggle kernel also able get solution
524,524,35169491,How to implement a deep bidirectional LSTM with Keras?,"<p>I am trying to implement a LSTM based speech recognizer. So far I could set up bidirectional LSTM (i think it is working as a bidirectional LSTM) by following the example in Merge layer. Now I want to try it with another bidirectional LSTM layer, which make it a deep bidirectional LSTM. But I am unable to figure out how to connect the output of the previously merged two layers into a second set of LSTM layers. I don't know whether it is possible with Keras. Hope someone can help me with this.</p>

<p>Code for my single layer bidirectional LSTM is as follows</p>

<pre><code>left = Sequential()
left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',
               forget_bias_init='one', return_sequences=True, activation='tanh',
               inner_activation='sigmoid', input_shape=(99, 13)))
right = Sequential()
right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',
               forget_bias_init='one', return_sequences=True, activation='tanh',
               inner_activation='sigmoid', input_shape=(99, 13), go_backwards=True))

model = Sequential()
model.add(Merge([left, right], mode='sum'))

model.add(TimeDistributedDense(nb_classes))
model.add(Activation('softmax'))

sgd = SGD(lr=0.1, decay=1e-5, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd)
print(""Train..."")
model.fit([X_train, X_train], Y_train, batch_size=1, nb_epoch=nb_epoches, validation_data=([X_test, X_test], Y_test), verbose=1, show_accuracy=True)
</code></pre>

<p>Dimensions of my x and y values are as follows.</p>

<pre><code>(100, 'train sequences')
(20, 'test sequences')
('X_train shape:', (100, 99, 13))
('X_test shape:', (20, 99, 13))
('y_train shape:', (100, 99, 11))
('y_test shape:', (20, 99, 11))
</code></pre>",35180313,4,0,,2016/2/3 5:54,13,2019/3/26 13:56,,,,,3609599,,1,19,deep-learning|keras|lstm,36437,68.4462,,3,implement deep bidirectional lstm kera try implement lstm base speech recognizer far could set bidirectional lstm think work bidirectional lstm follow example merge layer want try another bidirectional lstm layer make deep bidirectional lstm unable figure connect output previously merge two layer second set lstm layer know whether possible kera hope someone help code single layer bidirectional lstm follow dimension x value follow
815,815,53623472,How do I display a single image in PyTorch?,"<p>I want to display a single image loaded using <code>ImageLoader</code> and stored in a PyTorch <code>Tensor</code>. When I try to display it via <code>plt.imshow(image)</code> I get:</p>
<pre><code>TypeError: Invalid dimensions for image data
</code></pre>
<p>The <code>.shape</code> of the tensor is:</p>
<pre><code>torch.Size([3, 244, 244])
</code></pre>
<p>How do I display a PyTorch tensor as an image?</p>",55196345,6,0,,2018/12/5 0:33,7,2021/5/19 4:33,2021/3/16 13:34,,9067615,,5353461,,1,37,python|image-processing|pytorch,65582,164.667,,3,display single image pytorch want display single image load use store pytorch try display via get tensor display pytorch tensor image
619,619,50746096,How to match cv2.imread to the keras image.img_load output,"<p>I'm studying deep learning. Trained an image classification algorithm. The problem is, however, that to train images I used:</p>

<pre><code>test_image = image.load_img('some.png', target_size = (64, 64))
test_image = image.img_to_array(test_image)
</code></pre>

<p>While for actual application I use:</p>

<pre><code>test_image = cv2.imread('trick.png')
test_image = cv2.resize(test_image, (64, 64))
</code></pre>

<p>But I found that those give a different ndarray (different data):</p>

<p>Last entries from load_image:</p>

<pre><code>  [ 64.  71.  66.]
  [ 64.  71.  66.]
  [ 62.  69.  67.]]]
</code></pre>

<p>Last entries from cv2.imread:</p>

<pre><code>  [ 15  23  27]
  [ 16  24  28]
  [ 14  24  28]]]
</code></pre>

<p>, so the system is not working. Is there a way to match results of one to another?</p>",50748868,3,3,,2018/6/7 16:26,5,2019/9/27 4:28,2018/6/7 17:00,,4363045,,4363045,,1,20,python-3.x|numpy|opencv|image-processing|keras,9952,59.9916,,2,match cv imread kera image img load output study deep learning train image classification algorithm problem however train image use actual application use find give different ndarray different data last entry load image last entry cv imread system work way match result one another
304,304,57858433,How to clear GPU memory after PyTorch model training without restarting kernel,"<p>I am training PyTorch deep learning models on a Jupyter-Lab notebook, using CUDA on a Tesla K80 GPU to train. While doing training iterations, the 12 GB of GPU memory are used. I finish training by saving the model checkpoint, but want to continue using the notebook for further analysis (analyze intermediate results, etc.).</p>

<p>However, these 12 GB continue being occupied (as seen from <code>nvtop</code>) after finishing training. I would like to free up this memory so that I can use it for other notebooks.</p>

<p>My solution so far is to restart this notebook's kernel, but that is not solving my issue because I can't continue using the same notebook and its respective output computed so far.</p>",,4,0,,2019/9/9 17:12,5,2020/5/10 5:36,2019/9/9 19:12,,681865,,9178303,,1,18,python|pytorch|jupyter,18853,50.5015,,4,clear gpu memory pytorch model train without restart kernel train pytorch deep learning model jupyter lab notebook use cuda tesla k gpu train train iteration gb gpu memory use finish training save model checkpoint want continue use notebook analysis analyze intermediate result etc however gb continue occupy see finish training would like free memory use notebook solution far restart notebook kernel solve issue continue use notebook respective output compute far
488,488,29788075,Setting GLOG_minloglevel=1 to prevent output in shell from Caffe,"<p>I'm using Caffe, which is printing a lot of output to the shell when loading the neural net.<br>
I'd like to suppress that output, which supposedly can be done by setting <code>GLOG_minloglevel=1</code> when running the Python script. I've tried doing that using the following code, but I still get all the output from loading the net. How do I suppress the output correctly?</p>

<pre><code>os.environ[""GLOG_minloglevel""] = ""1""
net = caffe.Net(model_file, pretrained, caffe.TEST)
os.environ[""GLOG_minloglevel""] = ""0""
</code></pre>",29788785,2,0,,2015/4/22 4:56,5,2017/6/28 5:00,2017/6/5 9:17,,1714410,,1452257,,1,24,python|neural-network|deep-learning|caffe|glog,15351,102.345,,3,set glog minloglevel prevent output shell caffe use caffe print lot output shell load neural net like suppress output supposedly set run python script try use following code still get output load net suppress output correctly
464,464,25729969,Installing theano on Windows 8 with GPU enabled,"<p>I understand that the Theano support for Windows 8.1 is at experimental stage only but I wonder if anyone had any luck with resolving my issues. Depending on my config, I get three distinct types of errors. I assume that the resolution of any of my errors would solve my problem.</p>

<p>I have installed Python using WinPython 32-bit system, using MinGW as described <a href=""http://deeplearning.net/software/theano/install.html"">here</a>. The contents of my <code>.theanorc</code> file are as follows:</p>

<pre><code>[global]
openmp=False
device = gpu

[nvcc]
flags=-LC:\TheanoPython\python-2.7.6\libs
compiler_bindir=C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\bin\

[blas]
ldflags = 
</code></pre>

<p>When I run <code>import theano</code> the error is as follows:</p>

<pre><code>nvcc fatal   : nvcc cannot find a supported version of Microsoft Visual Studio.
Only the versions 2010, 2012, and 2013 are supported

['nvcc', '-shared', '-g', '-O3', '--compiler-bindir', 'C:\\Program Files (x86)\\
Microsoft Visual Studio 10.0\\VC\\bin# flags=-m32 # we have this hard coded for
now', '-Xlinker', '/DEBUG', '-m32', '-Xcompiler', '-DCUDA_NDARRAY_CUH=d67f7c8a21
306c67152a70a88a837011,/Zi,/MD', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-pa
ckages\\theano\\sandbox\\cuda', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-pac
kages\\numpy\\core\\include', '-IC:\\TheanoPython\\python-2.7.6\\include', '-o',
 'C:\\Users\\Matej\\AppData\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel6
4_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray
.pyd', 'mod.cu', '-LC:\\TheanoPython\\python-2.7.6\\libs', '-LNone\\lib', '-LNon
e\\lib64', '-LC:\\TheanoPython\\python-2.7.6', '-lpython27', '-lcublas', '-lcuda
rt']
ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return st
atus', 1, 'for cmd', 'nvcc -shared -g -O3 --compiler-bindir C:\\Program Files (x
86)\\Microsoft Visual Studio 10.0\\VC\\bin# flags=-m32 # we have this hard coded
 for now -Xlinker /DEBUG -m32 -Xcompiler -DCUDA_NDARRAY_CUH=d67f7c8a21306c67152a
70a88a837011,/Zi,/MD -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\thean
o\\sandbox\\cuda -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\numpy\\co
re\\include -IC:\\TheanoPython\\python-2.7.6\\include -o C:\\Users\\Matej\\AppDa
ta\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Family_6_Model_60_Stepp
ing_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd mod.cu -LC:\\TheanoP
ython\\python-2.7.6\\libs -LNone\\lib -LNone\\lib64 -LC:\\TheanoPython\\python-2
.7.6 -lpython27 -lcublas -lcudart')
WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not availabl
e
</code></pre>

<p>I have also tested it using <code>Visual Studio 12.0</code> which is installed on my system with the following error:</p>

<pre><code>mod.cu
nvlink fatal   : Could not open input file 'C:/Users/Matej/AppData/Local/Temp/tm
pxft_00001b70_00000000-28_mod.obj'

['nvcc', '-shared', '-g', '-O3', '--compiler-bindir', 'C:\\Program Files (x86)\\
Microsoft Visual Studio 12.0\\VC\\bin\\', '-Xlinker', '/DEBUG', '-m32', '-Xcompi
ler', '-LC:\\TheanoPython\\python-2.7.6\\libs,-DCUDA_NDARRAY_CUH=d67f7c8a21306c6
7152a70a88a837011,/Zi,/MD', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-package
s\\theano\\sandbox\\cuda', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages
\\numpy\\core\\include', '-IC:\\TheanoPython\\python-2.7.6\\include', '-o', 'C:\
\Users\\Matej\\AppData\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Fam
ily_6_Model_60_Stepping_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd'
, 'mod.cu', '-LC:\\TheanoPython\\python-2.7.6\\libs', '-LNone\\lib', '-LNone\\li
b64', '-LC:\\TheanoPython\\python-2.7.6', '-lpython27', '-lcublas', '-lcudart']
ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return st
atus', 1, 'for cmd', 'nvcc -shared -g -O3 --compiler-bindir C:\\Program Files (x
86)\\Microsoft Visual Studio 12.0\\VC\\bin\\ -Xlinker /DEBUG -m32 -Xcompiler -LC
:\\TheanoPython\\python-2.7.6\\libs,-DCUDA_NDARRAY_CUH=d67f7c8a21306c67152a70a88
a837011,/Zi,/MD -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\theano\\sa
ndbox\\cuda -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\numpy\\core\\i
nclude -IC:\\TheanoPython\\python-2.7.6\\include -o C:\\Users\\Matej\\AppData\\L
ocal\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Family_6_Model_60_Stepping_3
_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd mod.cu -LC:\\TheanoPython
\\python-2.7.6\\libs -LNone\\lib -LNone\\lib64 -LC:\\TheanoPython\\python-2.7.6
-lpython27 -lcublas -lcudart')
WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not availabl
e
</code></pre>

<p>In the latter error, several pop-up windows ask me how would I like to open (.res) file before error is thrown.</p>

<p><code>cl.exe</code> is present in both folders (i.e. VS 2010 and VS 2013). </p>

<p>Finally, if I set VS 2013 in the environment path and set <code>.theanorc</code> contents as follows:</p>

<pre><code>[global]
base_compiledir=C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\bin
openmp=False
floatX = float32
device = gpu

[nvcc]
flags=-LC:\TheanoPython\python-2.7.6\libs
compiler_bindir=C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC\bin\

[blas]
ldflags = 
</code></pre>

<p>I get the following error:</p>

<pre><code>c:\theanopython\python-2.7.6\include\pymath.h(22): warning: dllexport/dllimport conflict with ""round""
c:\program files\nvidia gpu computing toolkit\cuda\v6.5\include\math_functions.h(2455): here; dllimport/dllexport dropped

mod.cu(954): warning: statement is unreachable

mod.cu(1114): error: namespace ""std"" has no member ""min""

mod.cu(1145): error: namespace ""std"" has no member ""min""

mod.cu(1173): error: namespace ""std"" has no member ""min""

mod.cu(1174): error: namespace ""std"" has no member ""min""

mod.cu(1317): error: namespace ""std"" has no member ""min""

mod.cu(1318): error: namespace ""std"" has no member ""min""

mod.cu(1442): error: namespace ""std"" has no member ""min""

mod.cu(1443): error: namespace ""std"" has no member ""min""

mod.cu(1742): error: namespace ""std"" has no member ""min""

mod.cu(1777): error: namespace ""std"" has no member ""min""

mod.cu(1781): error: namespace ""std"" has no member ""min""

mod.cu(1814): error: namespace ""std"" has no member ""min""

mod.cu(1821): error: namespace ""std"" has no member ""min""

mod.cu(1853): error: namespace ""std"" has no member ""min""

mod.cu(1861): error: namespace ""std"" has no member ""min""

mod.cu(1898): error: namespace ""std"" has no member ""min""

mod.cu(1905): error: namespace ""std"" has no member ""min""

mod.cu(1946): error: namespace ""std"" has no member ""min""

mod.cu(1960): error: namespace ""std"" has no member ""min""

mod.cu(3750): error: namespace ""std"" has no member ""min""

mod.cu(3752): error: namespace ""std"" has no member ""min""

mod.cu(3784): error: namespace ""std"" has no member ""min""

mod.cu(3786): error: namespace ""std"" has no member ""min""

mod.cu(3789): error: namespace ""std"" has no member ""min""

mod.cu(3791): error: namespace ""std"" has no member ""min""

mod.cu(3794): error: namespace ""std"" has no member ""min""

mod.cu(3795): error: namespace ""std"" has no member ""min""

mod.cu(3836): error: namespace ""std"" has no member ""min""

mod.cu(3838): error: namespace ""std"" has no member ""min""

mod.cu(4602): error: namespace ""std"" has no member ""min""

mod.cu(4604): error: namespace ""std"" has no member ""min""

31 errors detected in the compilation of ""C:/Users/Matej/AppData/Local/Temp/tmpxft_00001d84_00000000-10_mod.cpp1.ii"".
ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 2, 'for cmd', 'nvcc -shared -g -O3 -Xlinker /DEBUG -m32 -Xcompiler -DCUDA_NDARRAY_CUH=d67f7c8a21306c67152a70a88a837011,/Zi,/MD -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\theano\\sandbox\\cuda -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\numpy\\core\\include -IC:\\TheanoPython\\python-2.7.6\\include -o C:\\Users\\Matej\\AppData\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd mod.cu -LC:\\TheanoPython\\python-2.7.6\\libs -LNone\\lib -LNone\\lib64 -LC:\\TheanoPython\\python-2.7.6 -lpython27 -lcublas -lcudart')
ERROR:theano.sandbox.cuda:Failed to compile cuda_ndarray.cu: ('nvcc return status', 2, 'for cmd', 'nvcc -shared -g -O3 -Xlinker /DEBUG -m32 -Xcompiler -DCUDA_NDARRAY_CUH=d67f7c8a21306c67152a70a88a837011,/Zi,/MD -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\theano\\sandbox\\cuda -IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\numpy\\core\\include -IC:\\TheanoPython\\python-2.7.6\\include -o C:\\Users\\Matej\\AppData\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd mod.cu -LC:\\TheanoPython\\python-2.7.6\\libs -LNone\\lib -LNone\\lib64 -LC:\\TheanoPython\\python-2.7.6 -lpython27 -lcublas -lcudart')
mod.cu

['nvcc', '-shared', '-g', '-O3', '-Xlinker', '/DEBUG', '-m32', '-Xcompiler', '-DCUDA_NDARRAY_CUH=d67f7c8a21306c67152a70a88a837011,/Zi,/MD', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\theano\\sandbox\\cuda', '-IC:\\TheanoPython\\python-2.7.6\\lib\\site-packages\\numpy\\core\\include', '-IC:\\TheanoPython\\python-2.7.6\\include', '-o', 'C:\\Users\\Matej\\AppData\\Local\\Theano\\compiledir_Windows-8-6.2.9200-Intel64_Family_6_Model_60_Stepping_3_GenuineIntel-2.7.6-32\\cuda_ndarray\\cuda_ndarray.pyd', 'mod.cu', '-LC:\\TheanoPython\\python-2.7.6\\libs', '-LNone\\lib', '-LNone\\lib64', '-LC:\\TheanoPython\\python-2.7.6', '-lpython27', '-lcublas', '-lcudart']
</code></pre>

<p>If I run <code>import theano</code> without the GPU option on, it runs without a problem. Also CUDA samples run without a problem. </p>",26073714,7,1,,2014/9/8 17:37,14,2016/4/7 1:59,2014/9/8 19:10,,1663093,,1663093,,1,15,python|windows|cuda|mingw|theano,21468,75.3272,,1,instal theano window gpu enable understand theano support window experimental stage wonder anyone luck resolve issue depend config get three distinct type error assume resolution error would solve problem instal python use winpython bit system use mingw describe content file follow run error follow also test use instal system follow error latter error several pop window ask would like open file error throw present folder e v v finally set v environment path set content follow get following error run without gpu option run without problem also cuda sample run without problem
267,267,44747343,"Keras input explanation: input_shape, units, batch_size, dim, etc","<p>For any Keras layer (<code>Layer</code> class), can someone explain how to understand the difference between <code>input_shape</code>, <code>units</code>, <code>dim</code>, etc.?  </p>

<p>For example the doc says <code>units</code> specify the output shape of a layer. </p>

<p>In the image of the neural net below <code>hidden layer1</code> has 4 units. Does this directly translate to the <code>units</code> attribute of the <code>Layer</code> object? Or does <code>units</code> in Keras equal the shape of every weight in the hidden layer times the number of units? </p>

<p>In short how does one understand/visualize the attributes of the model -  in particular the layers - with the image below? 
<a href=""https://i.stack.imgur.com/iHW2o.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/iHW2o.jpg"" alt=""enter image description here""></a></p>",44748370,2,0,,2017/6/25 14:29,274,2021/4/20 6:35,2018/9/12 15:50,,4590385,,1849998,,1,338,neural-network|deep-learning|keras|keras-layer|tensor,212201,727.507,,3,kera input explanation input shape unit batch size dim etc kera layer class someone explain understand difference etc example doc say specify output shape layer image neural net unit directly translate attribute object keras equal shape every weight hidden layer time number unit short one understand visualize attribute model particular layer image
787,787,52261597,Keras Model.fit Verbose Formatting,"<p>I'm running Keras model.fit() in Jupyter notebook, and the output is very messy if verbose is set to 1:</p>

<pre><code>    Train on 6400 samples, validate on 800 samples
    Epoch 1/200
    2080/6400 [========&gt;.....................] - ETA: 39s - loss: 0.4383 - acc: 0.79 
    - ETA: 34s - loss: 0.3585 - acc: 0.84 - ETA: 33s - loss: 0.3712 - acc: 0.84 
    - ETA: 34s - loss: 0.3716 - acc: 0.84 - ETA: 33s - loss: 0.3675 - acc: 0.84 
    - ETA: 33s - loss: 0.3650 - acc: 0.84 - ETA: 34s - loss: 0.3759 - acc: 0.83 
    - ETA: 34s - loss: 0.3933 - acc: 0.82 - ETA: 34s - loss: 0.3985 - acc: 0.82 
    - ETA: 34s - loss: 0.4057 - acc: 0.82 - ETA: 33s - loss: 0.4071 - acc: 0.81 
    ....
</code></pre>

<p>As you can see, the ETA, loss, acc outputs kept appending to the log, instead of replacing the original ETA/loss/acc values within the first line, just like how the progress bar works.</p>

<p>How do I fix it it so that only 1 line of progress bar, ETA, loss &amp; acc are shown per epoch? Right now, my cell output has tons of these lines as the training continues.</p>

<p>I'm running Python 3.6.1 on Windows 10, with the following module versions:</p>

<pre><code>jupyter                            1.0.0
jupyter-client                     5.0.1
jupyter-console                    5.1.0
jupyter-core                       4.3.0
jupyterthemes                      0.19.0
Keras                              2.2.0
Keras-Applications                 1.0.2
Keras-Preprocessing                1.0.1
tensorflow-gpu                     1.7.0
</code></pre>

<p>Thank you.</p>",52263390,2,0,,2018/9/10 16:01,6,2021/6/23 18:23,2018/9/10 16:07,,1052199,,1052199,,1,13,python|python-3.x|keras|jupyter-notebook|jupyter,8793,57.9765,,5,kera model fit verbose formatting run kera model fit jupyter notebook output messy verbose set see eta loss acc output keep append log instead replace original eta loss acc value within first line like progress bar work fix line progress bar eta loss acc show per epoch right cell output ton line training continue run python window following module version thank
331,331,63582590,Why do we call .detach() before calling .numpy() on a Pytorch Tensor?,"<p><a href=""https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num"">It has been firmly established that <code>my_tensor.detach().numpy()</code> is the correct way to get a numpy array from a <code>torch</code> tensor.</a></p>
<p>I'm trying to get a better understanding of why.</p>
<p>In the <a href=""https://stackoverflow.com/a/57014852/1048186"">accepted answer</a> to the question just linked, Blupon states that:</p>
<blockquote>
<p>You need to convert your tensor to another tensor that isn't requiring a gradient in addition to its actual value definition.</p>
</blockquote>
<p>In the first discussion he links to, albanD states:</p>
<blockquote>
<p>This is expected behavior because moving to numpy will break the graph and so no gradient will be computed.</p>
<p>If you don闂備胶鍋ㄩ崕鏌ユ偘?actually need gradients, then you can explicitly .detach() the Tensor that requires grad to get a tensor with the same content that does not require grad. This other Tensor can then be converted to a numpy array.</p>
</blockquote>
<p>In the second discussion he links to, apaszke writes:</p>
<blockquote>
<p>Variable's can闂備胶鍋ㄩ崕鏌ユ偘?be transformed to numpy, because they闂備胶鍋ㄩ崕鏌ユ偘椤?wrappers around tensors that save the operation history, and numpy doesn闂備胶鍋ㄩ崕鏌ユ偘?have such objects. You can retrieve a tensor held by the Variable, using the .data attribute. Then, this should work: var.data.numpy().</p>
</blockquote>
<p>I have studied the internal workings of PyTorch's autodifferentiation library, and I'm still confused by these answers.  Why does it break the graph to to move to numpy? Is it because any operations on the numpy array will not be tracked in the autodiff graph?</p>
<p>What is a Variable? How does it relate to a tensor?</p>
<p>I feel that a thorough high-quality Stack-Overflow answer that explains the reason for this to new users of PyTorch who don't yet understand autodifferentiation is called for here.</p>
<p>In particular, I think it would be helpful to illustrate the graph through a figure and show how the disconnection occurs in this example:</p>
<blockquote>
<pre><code>import torch

tensor1 = torch.tensor([1.0,2.0],requires_grad=True)

print(tensor1)
print(type(tensor1))

tensor1 = tensor1.numpy()

print(tensor1)
print(type(tensor1))
</code></pre>
</blockquote>",63869655,3,8,,2020/8/25 15:48,12,2020/9/16 14:06,2020/9/12 22:06,,1048186,,1048186,,1,30,numpy|pytorch|autodiff,18829,93.0993,,3,call detach call numpy pytorch tensor firmly establish correct way get numpy array tensor try get good understanding accepted answer question link blupon state need convert tensor another tensor require gradient addition actual value definition first discussion link alband state expect behavior move numpy break graph gradient compute actually need gradient explicitly detach tensor require grad get tensor content require grad tensor convert numpy array second discussion link apaszke writes variable transform numpy wrapper around tensor save operation history numpy object retrieve tensor hold variable use data attribute work var data numpy study internal working pytorch autodifferentiation library still confuse answer break graph move numpy operation numpy array track autodiff graph variable relate tensor feel thorough high quality stack overflow answer explain reason new user pytorch yet understand autodifferentiation call particular think would helpful illustrate graph figure show disconnection occur example
568,568,49035200,"Keras early stopping callback error, val_loss metric not available","<p>I am training a Keras (Tensorflow backend, Python, on MacBook) and am getting an error in the early stopping callback in fit_generator function.  The error is as follows:</p>

<pre><code>RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))),
RuntimeWarning: Can save best model only with val_acc available, skipping.

'skipping.' % (self.monitor), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:497: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are:
  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning
[local-dir]/lib/python3.6/site-packages/keras/callbacks.py:406: RuntimeWarning: Can save best model only with val_acc available, skipping.
  'skipping.' % (self.monitor), RuntimeWarning)
Traceback (most recent call last):
  :
  [my-code]
  :
  File ""[local-dir]/lib/python3.6/site-packages/keras/legacy/interfaces.py"", line 91, in wrapper
return func(*args, **kwargs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/engine/training.py"", line 2213, in fit_generator
callbacks.on_epoch_end(epoch, epoch_logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 76, in on_epoch_end
callback.on_epoch_end(epoch, logs)
  File ""[local-dir]/lib/python3.6/site-packages/keras/callbacks.py"", line 310, in on_epoch_end
self.progbar.update(self.seen, self.log_values, force=True)
AttributeError: 'ProgbarLogger' object has no attribute 'log_values'
</code></pre>

<p>My code is as follows (which looks OK):</p>

<pre><code>:
ES = EarlyStopping(monitor=""val_loss"", min_delta=0.001, patience=3, mode=""min"", verbose=1)
:
self.model.fit_generator(
        generator        = train_batch,
        validation_data  = valid_batch,
        validation_steps = validation_steps,
        steps_per_epoch  = steps_per_epoch,
        epochs           = epochs,
        callbacks        = [ES],
        verbose          = 1,
        workers          = 3,
        max_queue_size   = 8)
</code></pre>

<p>The error message appears to relate to the early stopping callback but the callback looks OK.  Also the error states that the val_loss is not appropriate, but I am not sure why... one more unusual thing about this is that the error only occurs when I use smaller data sets.</p>

<p>Any help is appreciated.</p>",49055577,9,0,,2018/2/28 17:19,2,2021/7/5 8:41,,,,,3011570,,1,20,python|tensorflow|keras,28775,97.8361,,4,kera early stop callback error val loss metric available train kera tensorflow backend python macbook get error early stopping callback fit generator function error follow code follow look ok error message appear relate early stopping callback callback look ok also error state val loss appropriate sure one unusual thing error occur use small data set help appreciate
323,323,58638701,ImportError: cannot import name 'set_random_seed' from 'tensorflow' (C:\Users\polon\Anaconda3\lib\site-packages\tensorflow\__init__.py),"<p>Good day,</p>

<p>Here is the error. Can somebody help how can i solve it?</p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-18-c29f17706012&gt; in &lt;module&gt;
      7 import numpy as np
      8 import numpy.random as nr
----&gt; 9 from tensorflow import set_random_seed
     10 import matplotlib.pyplot as plt
     11 get_ipython().run_line_magic('matplotlib', 'inline')

ImportError: cannot import name 'set_random_seed' from 'tensorflow' (C:\Users\polon\Anaconda3\lib\site-packages\tensorflow\__init__.py)
</code></pre>

<p>Looked for similar problems on Stack, but nothing worked for me.</p>",58639060,6,4,,2019/10/31 7:35,2,2021/6/11 18:07,,,,,12301726,,1,20,python|tensorflow|keras,27804,87.7764,,1,importerror import name set random seed tensorflow c user polon anaconda lib site package tensorflow init py good day error somebody help solve look similar problem stack nothing work
333,333,45046525,How can I get the number of trainable parameters of a model in Keras?,"<p>I am setting <code>trainable=False</code> in all my layers, implemented through the <code>Model</code> API, but I want to verify whether that is working. <code>model.count_params()</code> returns the total number of parameters, but is there any way in which I can get the total number of trainable parameters, other than looking at the last few lines of <code>model.summary()</code>?</p>",45048910,4,1,,2017/7/12 0:40,1,2020/10/20 15:35,2019/10/23 20:53,,3924118,,8021672,,1,24,python|keras,13659,83.7417,,5,get number trainable parameter model kera set layer implement api want verify whether work return total number parameter way get total number trainable parameter look last line
726,726,40396042,How to save Scikit-Learn-Keras Model into a Persistence File (pickle/hd5/json/yaml),"<p>I have the following code, using <a href=""https://github.com/fchollet/keras/blob/master/keras/wrappers/scikit_learn.py"">Keras Scikit-Learn Wrapper</a>:</p>

<pre><code>from keras.models import Sequential
from sklearn import datasets
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn import preprocessing
import pickle
import numpy as np
import json

def classifier(X, y):
    """"""
    Description of classifier
    """"""
    NOF_ROW, NOF_COL =  X.shape

    def create_model():
        # create model
        model = Sequential()
        model.add(Dense(12, input_dim=NOF_COL, init='uniform', activation='relu'))
        model.add(Dense(6, init='uniform', activation='relu'))
        model.add(Dense(1, init='uniform', activation='sigmoid'))
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model

    # evaluate using 10-fold cross validation
    seed = 7
    np.random.seed(seed)
    model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10, verbose=0)
    return model


def main():
    """"""
    Description of main
    """"""

    iris = datasets.load_iris()
    X, y = iris.data, iris.target
    X = preprocessing.scale(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)
    model_tt = classifier(X_train, y_train)
    model_tt.fit(X_train,y_train)

    #--------------------------------------------------
    # This fail
    #-------------------------------------------------- 
    filename = 'finalized_model.sav'
    pickle.dump(model_tt, open(filename, 'wb'))
    # load the model from disk
    loaded_model = pickle.load(open(filename, 'rb'))
    result = loaded_model.score(X_test, Y_test)
    print(result)

    #--------------------------------------------------
    # This also fail
    #--------------------------------------------------
    # from keras.models import load_model       
    # model_tt.save('test_model.h5')


    #--------------------------------------------------
    # This works OK 
    #-------------------------------------------------- 
    # print model_tt.score(X_test, y_test)
    # print model_tt.predict_proba(X_test)
    # print model_tt.predict(X_test)


# Output of predict_proba
# 2nd column is the probability that the prediction is 1
# this value is used as final score, which can be used
# with other method as comparison
# [   [ 0.25311464  0.74688536]
#     [ 0.84401423  0.15598579]
#     [ 0.96047372  0.03952631]
#     ...,
#     [ 0.25518912  0.74481088]
#     [ 0.91467732  0.08532269]
#     [ 0.25473493  0.74526507]]

# Output of predict
# [[1]
# [0]
# [0]
# ...,
# [1]
# [0]
# [1]]


if __name__ == '__main__':
    main()
</code></pre>

<p>As stated in the code there it fails at this line:</p>

<pre><code>pickle.dump(model_tt, open(filename, 'wb'))
</code></pre>

<p>With this error:</p>

<pre><code>pickle.PicklingError: Can't pickle &lt;function create_model at 0x101c09320&gt;: it's not found as __main__.create_model
</code></pre>

<p>How can I get around it?</p>",40397312,5,2,,2016/11/3 7:34,11,2021/1/21 18:47,2016/11/4 1:04,,67405,,67405,,1,26,python|scikit-learn|persistence|pickle|keras,15589,79.7713,,3,save scikit learn kera model persistence file pickle hd json yaml following code use kera scikit learn wrapper state code fail line error get around
233,233,44340848,How to convert Pytorch autograd.Variable to Numpy?,"<p>The title says it all. I want to convert a <code>PyTorch autograd.Variable</code> to its equivalent <code>numpy</code> array. In their <a href=""http://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html"" rel=""noreferrer"">official documentation</a> they advocated using <code>a.numpy()</code> to get the equivalent <code>numpy</code> array (for <code>PyTorch tensor</code>). But this gives me the following error:</p>

<blockquote>
  <p>Traceback (most recent call last): File ""stdin"", line 1, in module
  File
  ""/home/bishwajit/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py"",
  line 63, in <strong>getattr</strong>    raise AttributeError(name) AttributeError:
  numpy</p>
</blockquote>

<p>Is there any way I can circumvent this?</p>",44351506,2,0,,2017/6/3 5:57,7,2017/6/4 6:49,,,,,4933403,,1,17,numpy|pytorch|tensor,17319,58.7541,,3,convert pytorch autograd variable numpy title say want convert equivalent array official documentation advocate use get equivalent array give follow error traceback recent call last file stdin line module file home bishwajit anaconda lib python site package torch autograd variable py line getattr raise attributeerror name attributeerror numpy way circumvent
186,186,43430056,What is the purpose of the ROI layer in a Fast R-CNN?,"<p>In <a href=""https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html"" rel=""noreferrer"">this</a> tutorial about object detection, the fast R-CNN is mentioned. The ROI (region of interest) layer is also mentioned.</p>

<p>What is happening, mathematically, when region proposals get resized according to final convolution layer activation functions (in each cell)?</p>",43432719,2,0,,2017/4/15 18:58,11,2018/9/28 10:04,2018/9/28 9:57,,3924118,,5915270,,1,26,deep-learning|computer-vision|conv-neural-network|object-detection,20884,70.6793,,0,purpose roi layer fast r cnn tutorial object detection fast r cnn mention roi region interest layer also mention happen mathematically region proposal get resize accord final convolution layer activation function cell
527,527,35520587,How to determine the number of layers and nodes of a neural network,"<p>I am currently building a nn for a dataset with 387 features and 3000 samples. The outputs are 3 classes. I configured the network structure as following:</p>

<p><strong>input->200->{300->100}->50->output</strong> </p>

<p>Did I choose the correct number of nodes and layers? How to determine the number of nodes of each layers(input,hidden and output)? Is there any rule?</p>",35522597,4,2,,2016/2/20 7:51,9,2021/7/16 18:49,2016/2/20 11:37,,2412846,,5954650,,1,21,machine-learning|neural-network|deep-learning,38190,88.1278,,3,determine number layer node neural network currently build nn dataset feature sample output class configure network structure follow input output choose correct number node layer determine number node layer input hidden output rule
821,821,52660985,PyTorch - How to get learning rate during training?,"<p>While training, I'd like to know the value of learning_rate.
What should I do?</p>

<p>It's my code, like this:</p>

<pre><code>my_optimizer = torch.optim.SGD(my_model.parameters(), 
                               lr=0.001, 
                               momentum=0.99, 
                               weight_decay=2e-3)
</code></pre>

<p>Thank you.</p>",52671057,2,0,,2018/10/5 8:01,5,2021/5/26 20:56,2018/10/6 14:29,,7483494,,10020793,,1,18,python|machine-learning|deep-learning|pytorch,19737,59.3811,,3,pytorch get learning rate training train like know value learn rate code like thank
570,570,49079115,"ValueError: Negative dimension size caused by subtracting 2 from 1 for 'max_pooling2d_6/MaxPool' (op: 'MaxPool') with input shapes: [?,1,1,64]","<p>I am getting an error of Negative dimension size when I am keeping height and width of the input image anything below 362X362. I am surprised because this error is generally caused because of wrong input dimensions. I did not find any reason why number or rows and columns can cause an error. Below is my code-</p>



<pre class=""lang-py prettyprint-override""><code>batch_size = 32
num_classes = 7
epochs=50
height = 362
width = 362

train_datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'train',
        target_size=(height, width),
        batch_size=batch_size,
        class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
     'validation',
        target_size=(height, width),
        batch_size=batch_size,
        class_mode='categorical')

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=
(height,width,3))

x = base_model.output
x = Conv2D(32, (3, 3), use_bias=True, activation='relu') (x) #line2
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Conv2D(64, (3, 3), activation='relu') (x) #line3
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Flatten()(x)
x = Dense(batch_size, activation='relu')(x) #line1
x = (Dropout(0.5))(x)
predictions = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=
['accuracy'])

model.fit_generator(
        train_generator,
        samples_per_epoch=128,
        nb_epoch=epochs,
        validation_data=validation_generator,
        verbose=2)

for i, layer in enumerate(base_model.layers):
    print(i, layer.name)

for layer in model.layers[:309]:
    layer.trainable = False
for layer in model.layers[309:]:
    layer.trainable = True

from keras.optimizers import SGD
model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), 
loss='categorical_crossentropy', metrics=['accuracy'])

model.save('my_model.h5')
model.fit_generator(
        train_generator,
        samples_per_epoch=512,
        nb_epoch=epochs,
        validation_data=validation_generator,
        verbose=2)
</code></pre>",,5,0,,2018/3/2 23:51,4,2021/7/3 5:23,2018/3/3 21:06,,712995,,1578191,,1,10,python|neural-network|deep-learning|keras|conv-neural-network,20935,61.2835,,4,valueerror negative dimension size cause subtract max pool maxpool op maxpool input shape get error negative dimension size keep height width input image anything x surprise error generally cause wrong input dimension find reason number row column cause error code
342,342,45278286,How to choose LSTM Keras parameters?,"<p>I have multiple time series in input and I want to properly build an LSTM model.  </p>

<p>I'm really confused about how to choose the parameters. My code:</p>

<pre><code>model.add(keras.layers.LSTM(hidden_nodes, input_shape=(window, num_features), consume_less=""mem""))
model.add(Dropout(0.2))
model.add(keras.layers.Dense(num_features, activation='sigmoid'))

optimizer = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)
</code></pre>

<p>I want to understand, for each line, the meaning of the input parameters and how those have to be choosed.  </p>

<p>Actually I don't have any problems with the code but I need to understand clearly the parameters in order to obtain better results.  </p>

<p>Thanks a lot!</p>",45281837,1,2,,2017/7/24 10:33,7,2017/7/24 16:15,2017/7/24 11:17,,7117003,,3789716,,1,10,python|time-series|deep-learning|keras|lstm,13782,51.5572,,3,choose lstm kera parameter multiple time series input want properly build lstm model really confuse choose parameter code want understand line meaning input parameter choose actually problem code need understand clearly parameter order obtain good result thank lot
274,274,44843581,What is the difference between model.fit() an model.evaluate() in Keras?,"<p>I am using Keras with TensorFlow backend to train CNN models. </p>

<p>What is the between <code>model.fit()</code> and <code>model.evaluate()</code>? Which one should I ideally use? (I am using <code>model.fit()</code> as of now). </p>

<p>I know the utility of <code>model.fit()</code> and <code>model.predict()</code>. But I am unable to understand the utility of <code>model.evaluate()</code>. Keras documentation just says:</p>

<blockquote>
  <p>It is used to evaluate the model.  </p>
</blockquote>

<p>I feel this is a very vague definition. </p>",44843804,4,1,,2017/6/30 9:51,15,2020/2/2 18:54,2019/3/31 9:17,,3924118,,8199433,,1,34,tensorflow|model|keras|evaluate,28557,98.0229,,3,difference model fit model evaluate kera use kera tensorflow backend train cnn model one ideally use use know utility unable understand utility keras documentation say use evaluate model feel vague definition
375,375,46202519,Keras Tokenizer num_words doesn't seem to work,"<pre><code>&gt;&gt;&gt; t = Tokenizer(num_words=3)
&gt;&gt;&gt; l = [""Hello, World! This is so&amp;#$ fantastic!"", ""There is no other world like this one""]
&gt;&gt;&gt; t.fit_on_texts(l)
&gt;&gt;&gt; t.word_index
{'fantastic': 6, 'like': 10, 'no': 8, 'this': 2, 'is': 3, 'there': 7, 'one': 11, 'other': 9, 'so': 5, 'world': 1, 'hello': 4}
</code></pre>

<p>I'd have expected <code>t.word_index</code> to have just the top 3 words. What am I doing wrong?</p>",46205217,4,1,,2017/9/13 16:24,7,2021/5/12 3:30,2018/5/11 16:16,,5974433,,3508752,,1,24,machine-learning|neural-network|keras|deep-learning|tokenize,8744,69.9668,,3,kera tokenizer num word seem work expect top word wrong
474,474,59380430,How to use Model.fit which supports generators (after fit_generator deprecation),"<p>I have got this deprecation warning while using <code>Model.fit_generator</code> in tensorflow:</p>

<pre><code>WARNING:tensorflow: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
</code></pre>

<p>How can I use <code>Model.fit</code> instead of <code>Model.fit_generator</code>?</p>",59381897,2,0,,2019/12/17 18:54,3,2021/5/9 20:04,,,,,3134275,,1,23,python|tensorflow|keras,28290,66.0065,,4,use model fit support generator fit generator deprecation get deprecation warn use tensorflow use instead
711,711,39779710,Setting up a LearningRateScheduler in Keras,"<p>I'm setting up a Learning Rate Scheduler in Keras, using history loss as an updater to self.model.optimizer.lr, but the value on self.model.optimizer.lr does not get inserted in the SGD optimizer and the optimizer is using the dafault learning rate. The code is:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.preprocessing import StandardScaler

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.model.optimizer.lr=3
    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.model.optimizer.lr=lr-10000*self.losses[-1]

def base_model():
    model=Sequential()
    model.add(Dense(4, input_dim=2, init='uniform'))
    model.add(Dense(1, init='uniform'))
    sgd = SGD(decay=2e-5, momentum=0.9, nesterov=True)


model.compile(loss='mean_squared_error',optimizer=sgd,metrics['mean_absolute_error'])
    return model

history=LossHistory()

estimator = KerasRegressor(build_fn=base_model,nb_epoch=10,batch_size=16,verbose=2,callbacks=[history])

estimator.fit(X_train,y_train,callbacks=[history])

res = estimator.predict(X_test)
</code></pre>

<p>Everything works fine using Keras as a regressor for continuous variables, But I want to reach a smaller derivative by updating the optimizer learning rate.</p>",39807000,3,0,,2016/9/29 20:46,5,2018/11/1 9:05,,,,,6901690,,1,14,python|optimization|keras,24588,58.9629,,4,set learningratescheduler kera set learning rate scheduler kera use history loss updater self model optimizer lr value self model optimizer lr get insert sgd optimizer optimizer use dafault learn rate code everything work fine use kera regressor continuous variable want reach small derivative update optimizer learn rate
87,87,16798888,2-D convolution as a matrix-matrix multiplication,"<p>I know that, in the 1D case, the convolution between two vectors, <code>a</code> and <code>b</code>, can be computed as <code>conv(a, b)</code>, but also as the product between the <code>T_a</code> and <code>b</code>, where <code>T_a</code> is the corresponding Toeplitz matrix for <code>a</code>.</p>

<p>Is it possible to extend this idea to 2D?</p>

<p>Given <code>a = [5 1 3; 1 1 2; 2 1 3]</code> and <code>b=[4 3; 1 2]</code>, is it possible to convert <code>a</code> in a Toeplitz matrix and compute the matrix-matrix product between <code>T_a</code> and <code>b</code> as in the 1-D case?</p>",,4,2,,2013/5/28 18:24,38,2020/9/3 16:00,2019/3/12 15:29,,3924118,,1391371,,1,49,neural-network|deep-learning|conv-neural-network|matrix-multiplication|convolution,48142,141.93,2021/6/28 16:12,0,convolution matrix matrix multiplication know case convolution two vector compute also product corresponding toeplitz matrix possible extend idea give possible convert toeplitz matrix compute matrix matrix product case
558,558,48714407,RNN Regularization: Which Component to Regularize?,"<p>I am building an RNN for classification (there is a softmax layer after the RNN). There are so many options for what to regularize and I am not sure if to just try all of them, would the effect be the same? which components do I regularize for what situation?</p>

<p>The components being:</p>

<ul>
<li>Kernel weights (layer input)</li>
<li>Recurrent weights</li>
<li>Bias</li>
<li>Activation function (layer output)</li>
</ul>",,1,3,,2018/2/9 21:34,19,2020/4/30 19:55,2019/11/15 0:04,,10133797,,1581390,,1,25,python|keras|deep-learning|recurrent-neural-network|regularized,11998,84.3164,,0,rnn regularization component regularize build rnn classification softmax layer rnn many option regularize sure try would effect component regularize situation component kernel weight layer input recurrent weight bias activation function layer output
332,332,63886762,Tensorflow: None of the MLIR optimization passes are enabled (registered 1),"<p>I am using a very small model for testing purposes using tensorflow 2.3 and keras.
Looking at my terminal, I get the following warning:</p>
<pre><code>I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)
</code></pre>
<p>However, the code works as expected. But what does this message mean?</p>
<p>Thanks.</p>",64376619,1,2,,2020/9/14 14:52,1,2021/4/25 12:09,2020/9/14 15:25,,13804443,,13804443,,1,50,python|tensorflow|keras|deep-learning,38962,87.3626,,4,tensorflow none mlir optimization pass enable registered use small model test purpose use tensorflow keras look terminal get following warning however code work expect message mean thanks
628,628,50935345,Understanding torch.nn.Parameter,"<p>I am new to pytorch and I have difficulty in understanding how <code>torch.nn.Parameter()</code> works.</p>

<p>I have gone through the documentation in <a href=""https://pytorch.org/docs/stable/nn.html"" rel=""noreferrer"">https://pytorch.org/docs/stable/nn.html</a> but could make a very little sense out of it.</p>

<p>Can someone please help?</p>

<p>The code snippet that I am working on:</p>

<pre><code>def __init__(self, weight):
    super(Net, self).__init__()
    # initializes the weights of the convolutional layer to be the weights of the 4 defined filters
    k_height, k_width = weight.shape[2:]
    # assumes there are 4 grayscale filters
    self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
    self.conv.weight = torch.nn.Parameter(weight)
</code></pre>",51027227,2,2,,2018/6/19 19:03,30,2021/7/22 20:37,2018/6/19 19:36,,9761439,,9761439,,1,69,python|pytorch,70944,194.004,,3,understand torch nn parameter new pytorch difficulty understand work go documentation could make little sense someone please help code snippet work
597,597,49837638,What is volatile variable in Pytorch,"<p>What is volatile attribute of a Variable in Pytorch? Here's a sample code for defining a variable in PyTorch.</p>

<pre><code>datatensor = Variable(data, volatile=True)
</code></pre>",49837895,2,0,,2018/4/15 1:15,3,2019/1/17 19:38,,,,,2751532,,1,24,pytorch,19027,66.7175,,3,volatile variable pytorch volatile attribute variable pytorch sample code define variable pytorch
824,824,43385565,ImportError:'save_weights' requires h5py,"<p>When I save weights during training my CNN model using keras, it says <code>ImportError:'save_weights' requires h5py</code>, but I have already installed h5py. </p>

<p>I would greatly appreciate if someone could explain how to fix this issue.</p>",,9,1,,2017/4/13 7:01,6,2021/4/14 9:31,2017/4/13 7:40,,2455224,,7835541,,1,13,keras,20583,53.654,,4,importerror save weight require h py save weight train cnn model use kera say already instal h py would greatly appreciate someone could explain fix issue
350,350,45544928,Tensorflow serving No versions of servable <MODEL> found under base path,"<p>I was following <a href=""https://medium.com/towards-data-science/how-to-deploy-machine-learning-models-with-tensorflow-part-2-containerize-it-db0ad7ca35a7"" rel=""noreferrer"">this</a> tutorial to use <code>tensorflow serving</code> using my object detection model. I am using <a href=""https://github.com/tensorflow/models/tree/master/object_detection"" rel=""noreferrer"">tensorflow object detection</a> for generating the model. I have created a frozen model using <a href=""https://github.com/tensorflow/models/blob/master/object_detection/g3doc/exporting_models.md"" rel=""noreferrer"">this</a> exporter (the generated frozen model <strong>works</strong> using python script).</p>
<p>The frozen graph directory has following contents ( nothing on <code>variables</code> directory)</p>
<blockquote>
<p>variables/</p>
<p>saved_model.pb</p>
</blockquote>
<p>Now when I try to serve the model using the following command,</p>
<pre><code>tensorflow_model_server --port=9000 --model_name=ssd --model_base_path=/serving/ssd_frozen/
</code></pre>
<p>It always shows me</p>
<blockquote>
<p>...</p>
<p>tensorflow_serving/model_servers/server_core.cc:421]  (Re-)adding
model: ssd 2017-08-07 10:22:43.892834: W
tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:262]
No versions of servable ssd found under base path /serving/ssd_frozen/
2017-08-07 10:22:44.892901: W
tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:262]
No versions of servable ssd found under base path /serving/ssd_frozen/</p>
<p>...</p>
</blockquote>",45552938,5,0,,2017/8/7 10:37,10,2021/1/25 17:15,2020/6/20 9:12,,-1,,5330223,,1,40,tensorflow|deep-learning|object-detection|tensorflow-serving,13352,151.502,,3,tensorflow serve version servable find base path follow tutorial use use object detection model use tensorflow object detection generate model create frozen model use exporter generate frozen model work use python script frozen graph directory follow content nothing directory variable save model pb try serve model use following command always show tensorflow serve model server server core cc add model ssd w tensorflow serve source storage path file system storage path source cc version servable ssd find base path serve ssd frozen w tensorflow serve source storage path file system storage path source cc version servable ssd find base path serve ssd frozen
781,781,52041931,Is there an optimizer in keras based on precision or recall instead of loss?,"<p>I am developping a segmentation neural network with only two classes, 0 and 1 (0 is the background and 1 the object that I want to find on the image). On each image, there are about 80% of 1 and 20% of 0. As you can see, the dataset is unbalanced and it makes the results wrong. My accuracy is 85% and my loss is low, but that is only because my model is good at finding the background !</p>

<p>I would like to base the optimizer on another metric, like precision or recall which is more usefull in this case.</p>

<p>Does anyone know how to implement this ?</p>",52042520,6,13,,2018/8/27 14:53,3,2020/12/17 20:04,2018/8/27 15:01,,4685471,,9998692,,1,14,machine-learning|keras|metrics,6993,52.1787,,4,optimizer kera base precision recall instead loss developping segmentation neural network two class background object want find image image see dataset unbalanced make result wrong accuracy loss low model good find background would like base optimizer another metric like precision recall usefull case anyone know implement
366,366,46080634,Keras with TensorFlow backend not using GPU,"<p>I built the gpu version of the docker image <a href=""https://github.com/floydhub/dl-docker"" rel=""noreferrer"">https://github.com/floydhub/dl-docker</a> with keras version 2.0.0 and tensorflow version 0.12.1. I then ran the mnist tutorial <a href=""https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py"" rel=""noreferrer"">https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py</a> but realized that keras is not using GPU. Below is the output that I have</p>

<pre><code>root@b79b8a57fb1f:~/sharedfolder# python test.py
Using TensorFlow backend.
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/12
2017-09-06 16:26:54.866833: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-06 16:26:54.866855: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-06 16:26:54.866863: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-06 16:26:54.866870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-06 16:26:54.866876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</code></pre>

<p>Can anyone let me know if there are some settings that need to be made before keras uses GPU ? I am very new to all these so do let me know if I need to provide more information. </p>

<p><strong>I have installed the pre-requisites as mentioned on the <a href=""https://github.com/floydhub/dl-docker#prerequisites"" rel=""noreferrer"">page</a></strong></p>

<ul>
<li>Install Docker following the installation guide for your platform: <a href=""https://docs.docker.com/engine/installation/"" rel=""noreferrer"">https://docs.docker.com/engine/installation/</a></li>
</ul>

<p>I am able to launch the docker image </p>

<pre><code>docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:cpu bash
</code></pre>

<ul>
<li>GPU Version Only: Install Nvidia drivers on your machine either from Nvidia directly or follow the instructions <a href=""https://github.com/floydhub/dl-setup#nvidia-drivers"" rel=""noreferrer"">here</a>. Note that you don't have to install CUDA or cuDNN. These are included in the Docker container. </li>
</ul>

<p>I am able to run the last step </p>

<pre><code>cv@cv-P15SM:~$ cat /proc/driver/nvidia/version
NVRM version: NVIDIA UNIX x86_64 Kernel Module  375.66  Mon May  1 15:29:16 PDT 2017
GCC version:  gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)
</code></pre>

<ul>
<li>GPU Version Only: Install nvidia-docker: <a href=""https://github.com/NVIDIA/nvidia-docker"" rel=""noreferrer"">https://github.com/NVIDIA/nvidia-docker</a>, following the instructions here. This will install a replacement for the docker CLI. It takes care of setting up the Nvidia host driver environment inside the Docker containers and a few other things.</li>
</ul>

<p>I am able to run the step <a href=""https://github.com/NVIDIA/nvidia-docker"" rel=""noreferrer"">here</a></p>

<pre><code># Test nvidia-smi
cv@cv-P15SM:~$ nvidia-docker run --rm nvidia/cuda nvidia-smi

Thu Sep  7 00:33:06 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 780M    Off  | 0000:01:00.0     N/A |                  N/A |
| N/A   55C    P0    N/A /  N/A |    310MiB /  4036MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0                  Not Supported                                         |
+-----------------------------------------------------------------------------+
</code></pre>

<p>I am also able to run the nvidia-docker command to launch a gpu supported image.</p>

<p><strong>What I have tried</strong></p>

<p>I have tried the following suggestions below </p>

<ol>
<li>Check if you have completed step 9 of this tutorial ( <a href=""https://github.com/ignaciorlando/skinner/wiki/Keras-and-TensorFlow-installation"" rel=""noreferrer"">https://github.com/ignaciorlando/skinner/wiki/Keras-and-TensorFlow-installation</a> ). Note: Your file paths may be completely different inside that docker image, you'll have to locate them somehow. </li>
</ol>

<p>I appended the suggested lines to my bashrc and have verified that the bashrc file is updated.</p>

<pre><code>echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64' &gt;&gt; ~/.bashrc
echo 'export CUDA_HOME=/usr/local/cuda-8.0' &gt;&gt; ~/.bashrc
</code></pre>

<ol>
<li><p>To import the following commands in my python file</p>

<p><code>import os
os.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   # see issue #152
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""</code></p></li>
</ol>

<p>Both steps, done separately or together unfortunately did not solve the issue. Keras is still running with the CPU version of tensorflow as its backend. However, I might have found the possible issue. I checked the version of my tensorflow via the following commands and found two of them.</p>

<p>This is the CPU version</p>

<pre><code>root@08b5fff06800:~# pip show tensorflow
Name: tensorflow
Version: 1.3.0
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: tensorflow-tensorboard, six, protobuf, mock, numpy, backports.weakref, wheel
</code></pre>

<p>And this is the GPU version</p>

<pre><code>root@08b5fff06800:~# pip show tensorflow-gpu
Name: tensorflow-gpu
Version: 0.12.1
Summary: TensorFlow helps the tensors flow
Home-page: http://tensorflow.org/
Author: Google Inc.
Author-email: opensource@google.com
License: Apache 2.0
Location: /usr/local/lib/python2.7/dist-packages
Requires: mock, numpy, protobuf, wheel, six
</code></pre>

<p>Interestingly, the output shows that keras is using tensorflow version 1.3.0 which is the CPU version and not 0.12.1, the GPU version</p>

<pre><code>import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K

import tensorflow as tf
print('Tensorflow: ', tf.__version__)
</code></pre>

<p>Output</p>

<pre><code>root@08b5fff06800:~/sharedfolder# python test.py
Using TensorFlow backend.
Tensorflow:  1.3.0
</code></pre>

<p>I guess now I need to figure out how to have keras use the gpu version of tensorflow.</p>",46095279,4,9,,2017/9/6 16:54,5,2021/1/17 6:26,2017/9/7 3:48,,6467567,,6467567,,1,18,docker|tensorflow|keras|tensorflow-gpu,30650,70.3457,,1,kera tensorflow backend use gpu build gpu version docker image keras version tensorflow version run mnist tutorial realize kera use gpu output anyone let know setting need make keras us gpu new let know need provide information instal pre requisites mention page install docker follow installation guide platform able launch docker image gpu version install nvidia driver machine either nvidia directly follow instruction note install cuda cudnn include docker container able run last step gpu version install nvidia docker follow instruction install replacement docker cli take care set nvidia host driver environment inside docker container thing able run step also able run nvidia docker command launch gpu support image try try following suggestion check complete step tutorial note file path may completely different inside docker image locate somehow append suggested line bashrc verify bashrc file update import following command python file step separately together unfortunately solve issue kera still run cpu version tensorflow backend however might find possible issue check version tensorflow via following command find two cpu version gpu version interestingly output show kera use tensorflow version cpu version gpu version output guess need figure keras use gpu version tensorflow
285,285,44993324,Unique values in PyTorch tensor,"<p>I'm tying to find distinct values in a PyTorch tensor. Is there an efficient analogue of Tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/unique"" rel=""nofollow noreferrer"">unique op</a>?</p>",,3,2,,2017/7/9 5:34,1,2021/3/24 15:56,2021/3/24 15:56,,9067615,,6723594,,1,19,python|pytorch|unique,18222,56.4424,,3,unique value pytorch tensor tie find distinct value pytorch tensor efficient analogue tensorflow unique op
690,690,38553927,Batch Normalization in Convolutional Neural Network,"<p>I am newbie in convolutional neural networks and just have idea about feature maps and how convolution is done on images to extract features. I would be glad to know some details on applying batch normalisation in CNN.</p>

<p>I read this paper <a href=""https://arxiv.org/pdf/1502.03167v3.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1502.03167v3.pdf</a> and could understand the BN algorithm applied on a data but in the end they mentioned that a slight modification is required when applied to CNN:</p>

<blockquote>
  <p>For convolutional layers, we additionally want the normalization to obey the convolutional property 闂?so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a mini- batch, over all locations. In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations 闂?so for a mini-batch of size m and feature maps of size p 闂?q, we use the effec- tive mini-batch of size m闂?= |B| = m 闁?pq. We learn a pair of parameters 缂?k) and 闁?k) per feature map, rather than per activation. Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.</p>
</blockquote>

<p>I am total confused when they say
<strong>""so that different elements of the same feature map, at different locations, are normalized in the same way""</strong></p>

<p>I know what feature maps mean and different elements are the weights in every feature map. But I could not understand what location or spatial location means.</p>

<p>I could not understand the below sentence at all
<strong>""In Alg. 1, we let B be the set of all values in a feature map across both the elements of a mini-batch and spatial locations""</strong></p>

<p>I would be glad if someone cold elaborate and explain me in much simpler terms </p>",,4,1,,2016/7/24 15:54,45,2020/3/17 6:54,2017/11/21 16:57,,712995,,6631923,,1,78,machine-learning|computer-vision|deep-learning|conv-neural-network|batch-normalization,42430,189.911,,0,batch normalization convolutional neural network newbie convolutional neural network idea feature map convolution image extract feature would glad know detail apply batch normalisation cnn read paper could understand bn algorithm apply data end mention slight modification require apply cnn convolutional layer additionally want normalization obey convolutional property different element feature map different location normalize way achieve jointly normalize activation mini batch location alg let b set value feature map across element mini batch spatial location mini batch size feature map size p q use effec tive mini batch size b pq learn pair parameter k k per feature map rather per activation alg modify similarly inference bn transform apply linear transformation activation give feature map total confuse say different element feature map different location normalize way know feature map mean different element weight every feature map could understand location spatial location mean could understand sentence alg let b set value feature map across element mini batch spatial location would glad someone cold elaborate explain much simpler term
555,555,48668369,How to add Dropout in Keras functional model?,"<p>Let's say I have an LSTM layer in Keras like this:</p>

<pre><code>x = Input(shape=(input_shape), dtype='int32')

x = LSTM(128,return_sequences=True)(x)
</code></pre>

<p>Now I am trying to add Dropout to this layer using:</p>

<pre><code>X = Dropout(0.5)
</code></pre>

<p>but this gives error, which I am assuming the above line is redefining X instead of adding Dropout to it. 
How to fix this?</p>",48668450,1,0,,2018/2/7 16:11,1,2018/2/9 15:51,2018/2/9 15:51,,4685471,,7242276,,1,11,python|machine-learning|neural-network|keras|rnn,9116,53.0392,,3,add dropout keras functional model let say lstm layer kera like try add dropout layer use give error assume line redefine x instead add dropout fix
150,150,42666046,Loading a trained Keras model and continue training,"<p>I was wondering if it was possible to save a partly trained Keras model and continue the training after loading the model again.</p>
<p>The reason for this is that I will have more training data in the future and I do not want to retrain the whole model again.</p>
<p>The functions which I am using are:</p>
<pre><code>#Partly train model
model.fit(first_training, first_classes, batch_size=32, nb_epoch=20)

#Save partly trained model
model.save('partly_trained.h5')

#Load partly trained model
from keras.models import load_model
model = load_model('partly_trained.h5')

#Continue training
model.fit(second_training, second_classes, batch_size=32, nb_epoch=20)
</code></pre>
<hr />
<p><strong>Edit 1: added fully working example</strong></p>
<p>With the first dataset after 10 epochs the loss of the last epoch will be 0.0748 and the accuracy 0.9863.</p>
<p>After saving, deleting and reloading the model the loss and accuracy of the model trained on the second dataset will be 0.1711 and 0.9504 respectively.</p>
<p>Is this caused by the new training data or by a completely re-trained model?</p>
<pre><code>&quot;&quot;&quot;
Model by: http://machinelearningmastery.com/
&quot;&quot;&quot;
# load (downloaded if needed) the MNIST dataset
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from keras.models import load_model
numpy.random.seed(7)

def baseline_model():
    model = Sequential()
    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))
    model.add(Dense(num_classes, init='normal', activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

if __name__ == '__main__':
    # load data
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    # flatten 28*28 images to a 784 vector for each image
    num_pixels = X_train.shape[1] * X_train.shape[2]
    X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
    X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
    # normalize inputs from 0-255 to 0-1
    X_train = X_train / 255
    X_test = X_test / 255
    # one hot encode outputs
    y_train = np_utils.to_categorical(y_train)
    y_test = np_utils.to_categorical(y_test)
    num_classes = y_test.shape[1]

    # build the model
    model = baseline_model()

    #Partly train model
    dataset1_x = X_train[:3000]
    dataset1_y = y_train[:3000]
    model.fit(dataset1_x, dataset1_y, nb_epoch=10, batch_size=200, verbose=2)

    # Final evaluation of the model
    scores = model.evaluate(X_test, y_test, verbose=0)
    print(&quot;Baseline Error: %.2f%%&quot; % (100-scores[1]*100))

    #Save partly trained model
    model.save('partly_trained.h5')
    del model

    #Reload model
    model = load_model('partly_trained.h5')

    #Continue training
    dataset2_x = X_train[3000:]
    dataset2_y = y_train[3000:]
    model.fit(dataset2_x, dataset2_y, nb_epoch=10, batch_size=200, verbose=2)
    scores = model.evaluate(X_test, y_test, verbose=0)
    print(&quot;Baseline Error: %.2f%%&quot; % (100-scores[1]*100))
</code></pre>
<hr />
<p><strong>Edit 2: tensorflow.keras remarks</strong></p>
<p>For tensorflow.keras change the parameter nb_epochs to epochs in the model fit. The imports and basemodel function are:</p>
<pre><code>import numpy
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import load_model


numpy.random.seed(7)

def baseline_model():
    model = Sequential()
    model.add(Dense(num_pixels, input_dim=num_pixels, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
</code></pre>",42670479,7,8,,2017/3/8 8:07,59,2021/1/30 19:49,2021/1/11 14:13,,5359882,,5359882,,1,116,python|tensorflow|neural-network|keras|resuming-training,95361,262.317,,3,load trained kera model continue training wonder possible save partly train kera model continue training load model reason training data future want retrain whole model function use edit add fully work example first dataset epochs loss last epoch accuracy save deleting reload model loss accuracy model train second dataset respectively cause new training data completely train model edit tensorflow kera remark tensorflow kera change parameter nb epoch epochs model fit import basemodel function
696,696,38988582,Q-learning vs dynamic programming,"<p>Is the classic Q-learning algorithm, using lookup table (instead of function approximation), equivalent to dynamic programming?</p>",47333847,3,0,,2016/8/17 5:16,9,2018/11/22 16:01,2018/11/22 16:01,,3924118,,4702833,,1,11,machine-learning|dynamic-programming|reinforcement-learning|q-learning,11291,59.8109,,0,q learn vs dynamic programming classic q learn algorithm use lookup table instead function approximation equivalent dynamic programming
